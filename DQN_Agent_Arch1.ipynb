{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cab-Driver Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "from collections import deque\n",
    "import collections\n",
    "import pickle\n",
    "\n",
    "# for building DQN model\n",
    "from keras import layers\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# for plotting graphs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import the environment\n",
    "from Env import CabDriver\n",
    "\n",
    "import warnings\n",
    "warnings. filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining Time Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the time matrix provided\n",
    "Time_matrix = np.load(\"TM.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tracking the state-action pairs for checking convergence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "States_track = collections.defaultdict(dict)\n",
    "\n",
    "# Initialising states to be tracked\n",
    "def initialise_tracking_states():\n",
    "    # selecting any 5 Q-values\n",
    "    sample_q_values = [((1,11,5),(1,4)),((1,6,4),(1,4))]\n",
    "    for q_values in sample_q_values:\n",
    "        state = q_values[0]\n",
    "        action = q_values[1]\n",
    "        States_track[state][action] = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "initialise_tracking_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining a function to save the Q-dictionary as a pickle file\n",
    "def save_obj(obj, name ):\n",
    "    with open(name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent Class\n",
    "\n",
    "If you are using this framework, you need to fill the following to complete the following code block:\n",
    "1. State and Action Size\n",
    "2. Hyperparameters\n",
    "3. Create a neural-network model in function 'build_model()'\n",
    "4. Define epsilon-greedy strategy in function 'get_action()'\n",
    "5. Complete the function 'append_sample()'. This function appends the recent experience tuple <state, action, reward, new-state> to the memory\n",
    "6. Complete the 'train_model()' function with following logic:\n",
    "   - If the memory size is greater than mini-batch size, you randomly sample experiences from memory as per the mini-batch size and do the following:\n",
    "      - Initialise your input and output batch for training the model\n",
    "      - Calculate the target Q value for each sample: reward + gamma*max(Q(s'a,))\n",
    "      - Get Q(s', a) values from the last trained model\n",
    "      - Update the input batch as your encoded state and output batch as your Q-values\n",
    "      - Then fit your DQN model using the updated input and output batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        # Define size of state and action\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        # Write here: Specify you hyper parameters for the DQN\n",
    "        self.discount_factor = 0.95\n",
    "        self.learning_rate =  0.01      \n",
    "        self.epsilon_max = 1\n",
    "        self.epsilon=1\n",
    "        self.epsilon_decay = 0.999\n",
    "        self.epsilon_min = 0.01\n",
    "        \n",
    "        self.batch_size = 32        \n",
    "        # create replay memory using deque\n",
    "        self.memory = deque(maxlen=2000)\n",
    "\n",
    "        # create main model and target model\n",
    "        self.model = self.build_model()\n",
    "\n",
    "    # approximate Q function using Neural Network\n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        # Write your code here: Add layers to your neural nets       \n",
    "        # hidden layers\n",
    "        model.add(Dense(32, input_dim=self.state_size, activation='relu', kernel_initializer='he_uniform'))\n",
    "        model.add(Dense(32, activation='relu', kernel_initializer='he_uniform'))\n",
    "\n",
    "        # the output layer: output is of size num_actions\n",
    "        model.add(Dense(self.action_size, activation='relu', kernel_initializer='he_uniform'))\n",
    "        \n",
    "        model.compile(loss='mse',optimizer=Adam(lr=self.learning_rate))\n",
    "        model.summary\n",
    "        return model\n",
    "\n",
    "\n",
    "\n",
    "    def get_action(self, state,env):\n",
    "    # Write your code here:\n",
    "    # get action from model using epsilon-greedy policy\n",
    "    # Decay in Îµ after we generate each sample from the environment       \n",
    "        possible_actions_index,actions = env.requests(state) # Find possible action indexes and append 0\n",
    "        possible_actions_index.append(0)\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            # explore: choose a random action from all possible actions\n",
    "            # Give a random action only amongst possible action\n",
    "            return random.sample(possible_actions_index,1)[0]\n",
    "        else:\n",
    "            # choose the action with the highest q(s, a)\n",
    "            # the first index corresponds to the batch size, so\n",
    "            # reshape state to (1, state_size) so that the first index corresponds to the batch size\n",
    "            state = state.reshape(1, self.state_size)\n",
    "            q_value = self.model.predict(state)\n",
    "            # Give action with max q_value only amongst possible action\n",
    "            return np.where(q_value[0] == np.max(np.array([q_value[0][i] for i in possible_actions_index])))[0][0]    \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "    def append_sample(self, state, action, reward, next_state,done):\n",
    "        # Write your code here:\n",
    "        # save sample <s,a,r,s'> to the replay memory\n",
    "        self.memory.append((state, action, reward, next_state,done))\n",
    "    \n",
    "    \n",
    "    \n",
    "    # pick samples randomly from replay memory (with batch_size) and train the network\n",
    "    def train_model(self,env):\n",
    "        \n",
    "        if len(self.memory) > self.batch_size:\n",
    "            # Sample batch from the memory\n",
    "            mini_batch = random.sample(self.memory, self.batch_size)\n",
    "            update_output = np.zeros((self.batch_size, self.state_size))# write here\n",
    "            update_input = np.zeros((self.batch_size, self.state_size))# write here\n",
    "            \n",
    "            actions, rewards, done = [], [], []\n",
    "            \n",
    "            for i in range(self.batch_size):\n",
    "                state, action, reward, next_state,done_boolean = mini_batch[i]\n",
    "                update_input[i] = state\n",
    "                actions.append(action)\n",
    "                rewards.append(reward)\n",
    "                done.append(done_boolean)\n",
    "                update_output[i] = next_state\n",
    "                \n",
    "            # Write your code from here\n",
    "            # 1. Predict the target from earlier model\n",
    "            target = self.model.predict(update_input)\n",
    "                \n",
    "            # 2. Get the target for the Q-network\n",
    "            q_val= self.model.predict(update_output)\n",
    "                \n",
    "            #3. Update your 'update_output' and 'update_input' batch\n",
    "            for i in range(self.batch_size):\n",
    "                # Find possible actions from next state\n",
    "                next_possible_actions_index,_ = env.requests(update_output[i])\n",
    "                next_possible_actions_index.append(0)\n",
    "                if not done[i]:\n",
    "                    # Only take the max q_value from valid actions from next state\n",
    "                    target[i][actions[i]] = rewards[i] + self.discount_factor * np.max(np.array([q_val[i][j] for j in next_possible_actions_index]))\n",
    "                else:\n",
    "                    target[i][actions[i]] = rewards[i]\n",
    "                \n",
    "                \n",
    "        # 4. Fit your model and track the loss values\n",
    "            self.model.fit(update_input, target, batch_size=self.batch_size, epochs=1, verbose=0)\n",
    "            \n",
    "    def save_tracking_states(self):\n",
    "        \n",
    "        for state in States_track.keys():\n",
    "            qvals = self.model.predict(env.state_encod_arch1(state).reshape(1, self.state_size))\n",
    "            for action in States_track[state].keys():\n",
    "                i = action_space.index(list(action))\n",
    "                States_track[state][action].append(qvals[0][i])\n",
    "                \n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "Episodes = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial State is  [4, 16, 2]\n",
      "episode 0, reward -260.0, memory_length 127, epsilon 0.999, time 727.0, rides 126\n",
      "Initial State is  [1, 13, 3]\n",
      "episode 1, reward -14.0, memory_length 249, epsilon 0.998001, time 725.0, rides 121\n",
      "Initial State is  [3, 6, 1]\n",
      "episode 2, reward -80.0, memory_length 379, epsilon 0.997002999, time 725.0, rides 129\n",
      "Initial State is  [3, 12, 3]\n",
      "episode 3, reward -225.0, memory_length 492, epsilon 0.996005996001, time 723.0, rides 112\n",
      "Initial State is  [4, 4, 3]\n",
      "episode 4, reward -339.0, memory_length 620, epsilon 0.995009990004999, time 730.0, rides 127\n",
      "Initial State is  [2, 0, 6]\n",
      "episode 5, reward -249.0, memory_length 744, epsilon 0.994014980014994, time 733.0, rides 123\n",
      "Initial State is  [1, 20, 6]\n",
      "episode 6, reward -499.0, memory_length 877, epsilon 0.993020965034979, time 726.0, rides 132\n",
      "Initial State is  [2, 10, 2]\n",
      "episode 7, reward -200.0, memory_length 1009, epsilon 0.9920279440699441, time 731.0, rides 131\n",
      "Initial State is  [1, 3, 4]\n",
      "episode 8, reward -191.0, memory_length 1128, epsilon 0.9910359161258742, time 724.0, rides 118\n",
      "Initial State is  [0, 2, 5]\n",
      "episode 9, reward -291.0, memory_length 1251, epsilon 0.9900448802097482, time 732.0, rides 122\n",
      "Initial State is  [2, 14, 0]\n",
      "episode 10, reward -301.0, memory_length 1395, epsilon 0.9890548353295385, time 728.0, rides 143\n",
      "Initial State is  [0, 3, 3]\n",
      "episode 11, reward -124.0, memory_length 1510, epsilon 0.988065780494209, time 732.0, rides 114\n",
      "Initial State is  [4, 16, 2]\n",
      "episode 12, reward -37.0, memory_length 1624, epsilon 0.9870777147137147, time 731.0, rides 113\n",
      "Initial State is  [0, 16, 5]\n",
      "episode 13, reward -560.0, memory_length 1750, epsilon 0.986090636999001, time 735.0, rides 125\n",
      "Initial State is  [1, 11, 3]\n",
      "episode 14, reward -255.0, memory_length 1869, epsilon 0.9851045463620021, time 728.0, rides 118\n",
      "Initial State is  [4, 19, 4]\n",
      "episode 15, reward -386.0, memory_length 1989, epsilon 0.98411944181564, time 729.0, rides 119\n",
      "Initial State is  [2, 16, 0]\n",
      "episode 16, reward 46.0, memory_length 2000, epsilon 0.9831353223738244, time 735.0, rides 132\n",
      "Initial State is  [2, 0, 6]\n",
      "episode 17, reward -342.0, memory_length 2000, epsilon 0.9821521870514506, time 732.0, rides 129\n",
      "Initial State is  [3, 19, 1]\n",
      "episode 18, reward -232.0, memory_length 2000, epsilon 0.9811700348643991, time 729.0, rides 109\n",
      "Initial State is  [0, 10, 3]\n",
      "episode 19, reward -350.0, memory_length 2000, epsilon 0.9801888648295347, time 725.0, rides 116\n",
      "Initial State is  [0, 23, 0]\n",
      "episode 20, reward -293.0, memory_length 2000, epsilon 0.9792086759647052, time 724.0, rides 134\n",
      "Initial State is  [1, 7, 0]\n",
      "episode 21, reward -349.0, memory_length 2000, epsilon 0.9782294672887405, time 730.0, rides 120\n",
      "Initial State is  [0, 13, 4]\n",
      "episode 22, reward -186.0, memory_length 2000, epsilon 0.9772512378214517, time 733.0, rides 130\n",
      "Initial State is  [2, 0, 6]\n",
      "episode 23, reward -189.0, memory_length 2000, epsilon 0.9762739865836303, time 724.0, rides 144\n",
      "Initial State is  [4, 1, 5]\n",
      "episode 24, reward -128.0, memory_length 2000, epsilon 0.9752977125970467, time 724.0, rides 125\n",
      "Initial State is  [1, 12, 4]\n",
      "episode 25, reward -262.0, memory_length 2000, epsilon 0.9743224148844496, time 731.0, rides 138\n",
      "Initial State is  [2, 13, 1]\n",
      "episode 26, reward -298.0, memory_length 2000, epsilon 0.9733480924695651, time 724.0, rides 113\n",
      "Initial State is  [3, 3, 4]\n",
      "episode 27, reward -558.0, memory_length 2000, epsilon 0.9723747443770956, time 726.0, rides 120\n",
      "Initial State is  [3, 6, 4]\n",
      "episode 28, reward -470.0, memory_length 2000, epsilon 0.9714023696327185, time 735.0, rides 125\n",
      "Initial State is  [0, 21, 3]\n",
      "episode 29, reward -583.0, memory_length 2000, epsilon 0.9704309672630859, time 733.0, rides 129\n",
      "Initial State is  [1, 16, 2]\n",
      "episode 30, reward -341.0, memory_length 2000, epsilon 0.9694605362958227, time 730.0, rides 124\n",
      "Initial State is  [3, 22, 5]\n",
      "episode 31, reward -454.0, memory_length 2000, epsilon 0.9684910757595269, time 735.0, rides 125\n",
      "Initial State is  [3, 11, 1]\n",
      "episode 32, reward 112.0, memory_length 2000, epsilon 0.9675225846837673, time 730.0, rides 127\n",
      "Initial State is  [2, 23, 1]\n",
      "episode 33, reward -144.0, memory_length 2000, epsilon 0.9665550620990835, time 730.0, rides 132\n",
      "Initial State is  [1, 11, 5]\n",
      "episode 34, reward -30.0, memory_length 2000, epsilon 0.9655885070369844, time 728.0, rides 112\n",
      "Initial State is  [2, 17, 3]\n",
      "episode 35, reward -43.0, memory_length 2000, epsilon 0.9646229185299474, time 731.0, rides 124\n",
      "Initial State is  [4, 18, 2]\n",
      "episode 36, reward -182.0, memory_length 2000, epsilon 0.9636582956114175, time 721.0, rides 125\n",
      "Initial State is  [3, 20, 6]\n",
      "episode 37, reward -232.0, memory_length 2000, epsilon 0.9626946373158061, time 724.0, rides 131\n",
      "Initial State is  [1, 8, 4]\n",
      "episode 38, reward -378.0, memory_length 2000, epsilon 0.9617319426784903, time 735.0, rides 122\n",
      "Initial State is  [0, 4, 6]\n",
      "episode 39, reward -216.0, memory_length 2000, epsilon 0.9607702107358118, time 732.0, rides 127\n",
      "Initial State is  [3, 0, 4]\n",
      "episode 40, reward -290.0, memory_length 2000, epsilon 0.959809440525076, time 727.0, rides 130\n",
      "Initial State is  [0, 20, 5]\n",
      "episode 41, reward -153.0, memory_length 2000, epsilon 0.9588496310845509, time 733.0, rides 117\n",
      "Initial State is  [1, 18, 0]\n",
      "episode 42, reward -177.0, memory_length 2000, epsilon 0.9578907814534664, time 729.0, rides 121\n",
      "Initial State is  [4, 22, 0]\n",
      "episode 43, reward -160.0, memory_length 2000, epsilon 0.9569328906720129, time 727.0, rides 120\n",
      "Initial State is  [3, 12, 0]\n",
      "episode 44, reward -68.0, memory_length 2000, epsilon 0.9559759577813409, time 724.0, rides 125\n",
      "Initial State is  [4, 16, 0]\n",
      "episode 45, reward -456.0, memory_length 2000, epsilon 0.9550199818235596, time 730.0, rides 131\n",
      "Initial State is  [1, 7, 3]\n",
      "episode 46, reward -220.0, memory_length 2000, epsilon 0.9540649618417361, time 731.0, rides 135\n",
      "Initial State is  [3, 21, 0]\n",
      "episode 47, reward 267.0, memory_length 2000, epsilon 0.9531108968798944, time 726.0, rides 125\n",
      "Initial State is  [4, 14, 4]\n",
      "episode 48, reward -98.0, memory_length 2000, epsilon 0.9521577859830145, time 728.0, rides 112\n",
      "Initial State is  [3, 14, 3]\n",
      "episode 49, reward -76.0, memory_length 2000, epsilon 0.9512056281970315, time 731.0, rides 128\n",
      "Initial State is  [3, 9, 6]\n",
      "episode 50, reward 41.0, memory_length 2000, epsilon 0.9502544225688344, time 728.0, rides 129\n",
      "Initial State is  [4, 14, 3]\n",
      "episode 51, reward -323.0, memory_length 2000, epsilon 0.9493041681462656, time 733.0, rides 130\n",
      "Initial State is  [1, 11, 3]\n",
      "episode 52, reward -270.0, memory_length 2000, epsilon 0.9483548639781193, time 732.0, rides 128\n",
      "Initial State is  [3, 3, 2]\n",
      "episode 53, reward -437.0, memory_length 2000, epsilon 0.9474065091141411, time 724.0, rides 127\n",
      "Initial State is  [4, 18, 6]\n",
      "episode 54, reward -434.0, memory_length 2000, epsilon 0.946459102605027, time 728.0, rides 129\n",
      "Initial State is  [0, 6, 5]\n",
      "episode 55, reward -320.0, memory_length 2000, epsilon 0.9455126435024219, time 725.0, rides 134\n",
      "Initial State is  [2, 9, 2]\n",
      "episode 56, reward -307.0, memory_length 2000, epsilon 0.9445671308589195, time 724.0, rides 126\n",
      "Initial State is  [0, 15, 5]\n",
      "episode 57, reward -195.0, memory_length 2000, epsilon 0.9436225637280606, time 726.0, rides 130\n",
      "Initial State is  [4, 7, 3]\n",
      "episode 58, reward -467.0, memory_length 2000, epsilon 0.9426789411643326, time 727.0, rides 109\n",
      "Initial State is  [0, 5, 4]\n",
      "episode 59, reward -139.0, memory_length 2000, epsilon 0.9417362622231683, time 729.0, rides 136\n",
      "Initial State is  [4, 1, 6]\n",
      "episode 60, reward -341.0, memory_length 2000, epsilon 0.9407945259609451, time 727.0, rides 143\n",
      "Initial State is  [3, 2, 1]\n",
      "episode 61, reward -177.0, memory_length 2000, epsilon 0.9398537314349842, time 728.0, rides 116\n",
      "Initial State is  [0, 16, 6]\n",
      "episode 62, reward -218.0, memory_length 2000, epsilon 0.9389138777035492, time 739.0, rides 135\n",
      "Initial State is  [3, 22, 5]\n",
      "episode 63, reward 37.0, memory_length 2000, epsilon 0.9379749638258457, time 727.0, rides 126\n",
      "Initial State is  [4, 21, 2]\n",
      "episode 64, reward 72.0, memory_length 2000, epsilon 0.9370369888620198, time 742.0, rides 119\n",
      "Initial State is  [4, 3, 2]\n",
      "episode 65, reward -155.0, memory_length 2000, epsilon 0.9360999518731578, time 729.0, rides 123\n",
      "Initial State is  [0, 21, 4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 66, reward -212.0, memory_length 2000, epsilon 0.9351638519212846, time 737.0, rides 129\n",
      "Initial State is  [1, 10, 1]\n",
      "episode 67, reward -310.0, memory_length 2000, epsilon 0.9342286880693633, time 725.0, rides 141\n",
      "Initial State is  [2, 4, 4]\n",
      "episode 68, reward -454.0, memory_length 2000, epsilon 0.933294459381294, time 738.0, rides 129\n",
      "Initial State is  [4, 17, 3]\n",
      "episode 69, reward -232.0, memory_length 2000, epsilon 0.9323611649219127, time 732.0, rides 131\n",
      "Initial State is  [0, 22, 6]\n",
      "episode 70, reward -109.0, memory_length 2000, epsilon 0.9314288037569908, time 728.0, rides 128\n",
      "Initial State is  [2, 5, 3]\n",
      "episode 71, reward -405.0, memory_length 2000, epsilon 0.9304973749532338, time 731.0, rides 123\n",
      "Initial State is  [4, 11, 3]\n",
      "episode 72, reward -53.0, memory_length 2000, epsilon 0.9295668775782806, time 732.0, rides 121\n",
      "Initial State is  [4, 18, 1]\n",
      "episode 73, reward -184.0, memory_length 2000, epsilon 0.9286373107007023, time 726.0, rides 114\n",
      "Initial State is  [1, 16, 5]\n",
      "episode 74, reward -305.0, memory_length 2000, epsilon 0.9277086733900016, time 723.0, rides 112\n",
      "Initial State is  [2, 10, 5]\n",
      "episode 75, reward -58.0, memory_length 2000, epsilon 0.9267809647166116, time 736.0, rides 143\n",
      "Initial State is  [2, 20, 3]\n",
      "episode 76, reward -42.0, memory_length 2000, epsilon 0.925854183751895, time 733.0, rides 120\n",
      "Initial State is  [3, 12, 6]\n",
      "episode 77, reward -147.0, memory_length 2000, epsilon 0.9249283295681431, time 734.0, rides 112\n",
      "Initial State is  [4, 5, 5]\n",
      "episode 78, reward -55.0, memory_length 2000, epsilon 0.9240034012385749, time 737.0, rides 131\n",
      "Initial State is  [2, 22, 1]\n",
      "episode 79, reward -189.0, memory_length 2000, epsilon 0.9230793978373364, time 732.0, rides 134\n",
      "Initial State is  [4, 12, 4]\n",
      "episode 80, reward -63.0, memory_length 2000, epsilon 0.9221563184394991, time 722.0, rides 137\n",
      "Initial State is  [4, 9, 1]\n",
      "episode 81, reward -109.0, memory_length 2000, epsilon 0.9212341621210596, time 727.0, rides 113\n",
      "Initial State is  [3, 13, 1]\n",
      "episode 82, reward -221.0, memory_length 2000, epsilon 0.9203129279589385, time 725.0, rides 113\n",
      "Initial State is  [2, 3, 0]\n",
      "episode 83, reward -80.0, memory_length 2000, epsilon 0.9193926150309796, time 723.0, rides 126\n",
      "Initial State is  [0, 3, 6]\n",
      "episode 84, reward -229.0, memory_length 2000, epsilon 0.9184732224159486, time 727.0, rides 116\n",
      "Initial State is  [4, 9, 3]\n",
      "episode 85, reward -80.0, memory_length 2000, epsilon 0.9175547491935327, time 727.0, rides 117\n",
      "Initial State is  [4, 15, 0]\n",
      "episode 86, reward -205.0, memory_length 2000, epsilon 0.9166371944443392, time 724.0, rides 134\n",
      "Initial State is  [4, 17, 5]\n",
      "episode 87, reward -131.0, memory_length 2000, epsilon 0.9157205572498949, time 732.0, rides 117\n",
      "Initial State is  [1, 22, 1]\n",
      "episode 88, reward -157.0, memory_length 2000, epsilon 0.914804836692645, time 729.0, rides 118\n",
      "Initial State is  [1, 14, 6]\n",
      "episode 89, reward -86.0, memory_length 2000, epsilon 0.9138900318559524, time 732.0, rides 132\n",
      "Initial State is  [4, 10, 2]\n",
      "episode 90, reward -63.0, memory_length 2000, epsilon 0.9129761418240965, time 727.0, rides 121\n",
      "Initial State is  [4, 16, 6]\n",
      "episode 91, reward -291.0, memory_length 2000, epsilon 0.9120631656822724, time 725.0, rides 130\n",
      "Initial State is  [3, 2, 5]\n",
      "episode 92, reward -405.0, memory_length 2000, epsilon 0.9111511025165902, time 732.0, rides 146\n",
      "Initial State is  [2, 12, 4]\n",
      "episode 93, reward -229.0, memory_length 2000, epsilon 0.9102399514140735, time 730.0, rides 133\n",
      "Initial State is  [4, 13, 0]\n",
      "episode 94, reward -304.0, memory_length 2000, epsilon 0.9093297114626595, time 730.0, rides 126\n",
      "Initial State is  [2, 17, 3]\n",
      "episode 95, reward -123.0, memory_length 2000, epsilon 0.9084203817511969, time 728.0, rides 110\n",
      "Initial State is  [4, 21, 6]\n",
      "episode 96, reward -259.0, memory_length 2000, epsilon 0.9075119613694457, time 724.0, rides 122\n",
      "Initial State is  [1, 3, 2]\n",
      "episode 97, reward -281.0, memory_length 2000, epsilon 0.9066044494080763, time 729.0, rides 141\n",
      "Initial State is  [2, 15, 4]\n",
      "episode 98, reward -206.0, memory_length 2000, epsilon 0.9056978449586682, time 732.0, rides 114\n",
      "Initial State is  [0, 19, 5]\n",
      "episode 99, reward -520.0, memory_length 2000, epsilon 0.9047921471137096, time 738.0, rides 134\n",
      "Initial State is  [2, 14, 2]\n",
      "episode 100, reward -220.0, memory_length 2000, epsilon 0.9038873549665959, time 728.0, rides 118\n",
      "Initial State is  [3, 0, 1]\n",
      "episode 101, reward -309.0, memory_length 2000, epsilon 0.9029834676116293, time 725.0, rides 124\n",
      "Initial State is  [2, 16, 1]\n",
      "episode 102, reward -176.0, memory_length 2000, epsilon 0.9020804841440176, time 725.0, rides 117\n",
      "Initial State is  [0, 2, 2]\n",
      "episode 103, reward -141.0, memory_length 2000, epsilon 0.9011784036598737, time 726.0, rides 119\n",
      "Initial State is  [1, 8, 0]\n",
      "episode 104, reward 21.0, memory_length 2000, epsilon 0.9002772252562138, time 727.0, rides 133\n",
      "Initial State is  [1, 23, 2]\n",
      "episode 105, reward -566.0, memory_length 2000, epsilon 0.8993769480309576, time 735.0, rides 136\n",
      "Initial State is  [3, 13, 2]\n",
      "episode 106, reward -80.0, memory_length 2000, epsilon 0.8984775710829266, time 734.0, rides 132\n",
      "Initial State is  [1, 14, 5]\n",
      "episode 107, reward -81.0, memory_length 2000, epsilon 0.8975790935118436, time 727.0, rides 129\n",
      "Initial State is  [0, 22, 0]\n",
      "episode 108, reward -10.0, memory_length 2000, epsilon 0.8966815144183318, time 736.0, rides 117\n",
      "Initial State is  [2, 7, 0]\n",
      "episode 109, reward -120.0, memory_length 2000, epsilon 0.8957848329039134, time 728.0, rides 113\n",
      "Initial State is  [1, 15, 2]\n",
      "episode 110, reward 104.0, memory_length 2000, epsilon 0.8948890480710096, time 731.0, rides 136\n",
      "Initial State is  [0, 14, 4]\n",
      "episode 111, reward -111.0, memory_length 2000, epsilon 0.8939941590229386, time 729.0, rides 143\n",
      "Initial State is  [1, 10, 4]\n",
      "episode 112, reward -362.0, memory_length 2000, epsilon 0.8931001648639156, time 734.0, rides 131\n",
      "Initial State is  [1, 10, 1]\n",
      "episode 113, reward -114.0, memory_length 2000, epsilon 0.8922070646990518, time 731.0, rides 134\n",
      "Initial State is  [0, 20, 4]\n",
      "episode 114, reward 158.0, memory_length 2000, epsilon 0.8913148576343527, time 734.0, rides 133\n",
      "Initial State is  [4, 11, 3]\n",
      "episode 115, reward 190.0, memory_length 2000, epsilon 0.8904235427767183, time 725.0, rides 117\n",
      "Initial State is  [2, 1, 5]\n",
      "episode 116, reward -81.0, memory_length 2000, epsilon 0.8895331192339416, time 729.0, rides 130\n",
      "Initial State is  [4, 0, 5]\n",
      "episode 117, reward -313.0, memory_length 2000, epsilon 0.8886435861147077, time 727.0, rides 122\n",
      "Initial State is  [2, 20, 2]\n",
      "episode 118, reward -187.0, memory_length 2000, epsilon 0.887754942528593, time 727.0, rides 120\n",
      "Initial State is  [3, 9, 4]\n",
      "episode 119, reward -64.0, memory_length 2000, epsilon 0.8868671875860644, time 724.0, rides 120\n",
      "Initial State is  [1, 2, 1]\n",
      "episode 120, reward -49.0, memory_length 2000, epsilon 0.8859803203984784, time 729.0, rides 130\n",
      "Initial State is  [4, 8, 2]\n",
      "episode 121, reward -275.0, memory_length 2000, epsilon 0.88509434007808, time 737.0, rides 140\n",
      "Initial State is  [4, 9, 3]\n",
      "episode 122, reward -423.0, memory_length 2000, epsilon 0.8842092457380019, time 745.0, rides 123\n",
      "Initial State is  [3, 17, 6]\n",
      "episode 123, reward -266.0, memory_length 2000, epsilon 0.8833250364922639, time 733.0, rides 127\n",
      "Initial State is  [2, 2, 4]\n",
      "episode 124, reward -348.0, memory_length 2000, epsilon 0.8824417114557717, time 727.0, rides 126\n",
      "Initial State is  [0, 14, 2]\n",
      "episode 125, reward 25.0, memory_length 2000, epsilon 0.8815592697443159, time 729.0, rides 118\n",
      "Initial State is  [0, 7, 1]\n",
      "episode 126, reward 69.0, memory_length 2000, epsilon 0.8806777104745716, time 726.0, rides 123\n",
      "Initial State is  [0, 14, 4]\n",
      "episode 127, reward -56.0, memory_length 2000, epsilon 0.8797970327640969, time 733.0, rides 124\n",
      "Initial State is  [1, 6, 4]\n",
      "episode 128, reward -69.0, memory_length 2000, epsilon 0.8789172357313328, time 730.0, rides 130\n",
      "Initial State is  [3, 21, 1]\n",
      "episode 129, reward -92.0, memory_length 2000, epsilon 0.8780383184956015, time 730.0, rides 120\n",
      "Initial State is  [4, 18, 6]\n",
      "episode 130, reward -164.0, memory_length 2000, epsilon 0.8771602801771059, time 730.0, rides 121\n",
      "Initial State is  [3, 19, 2]\n",
      "episode 131, reward -114.0, memory_length 2000, epsilon 0.8762831198969288, time 723.0, rides 132\n",
      "Initial State is  [4, 13, 2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 132, reward -339.0, memory_length 2000, epsilon 0.8754068367770318, time 737.0, rides 147\n",
      "Initial State is  [3, 6, 1]\n",
      "episode 133, reward -302.0, memory_length 2000, epsilon 0.8745314299402548, time 727.0, rides 130\n",
      "Initial State is  [4, 6, 6]\n",
      "episode 134, reward -93.0, memory_length 2000, epsilon 0.8736568985103146, time 731.0, rides 127\n",
      "Initial State is  [1, 5, 4]\n",
      "episode 135, reward -114.0, memory_length 2000, epsilon 0.8727832416118043, time 733.0, rides 128\n",
      "Initial State is  [0, 12, 5]\n",
      "episode 136, reward -262.0, memory_length 2000, epsilon 0.8719104583701925, time 731.0, rides 115\n",
      "Initial State is  [0, 15, 5]\n",
      "episode 137, reward 35.0, memory_length 2000, epsilon 0.8710385479118223, time 724.0, rides 131\n",
      "Initial State is  [3, 18, 6]\n",
      "episode 138, reward -126.0, memory_length 2000, epsilon 0.8701675093639105, time 734.0, rides 118\n",
      "Initial State is  [4, 17, 0]\n",
      "episode 139, reward -387.0, memory_length 2000, epsilon 0.8692973418545467, time 725.0, rides 121\n",
      "Initial State is  [2, 5, 0]\n",
      "episode 140, reward -335.0, memory_length 2000, epsilon 0.8684280445126921, time 732.0, rides 125\n",
      "Initial State is  [0, 1, 2]\n",
      "episode 141, reward -24.0, memory_length 2000, epsilon 0.8675596164681794, time 729.0, rides 117\n",
      "Initial State is  [1, 22, 5]\n",
      "episode 142, reward -219.0, memory_length 2000, epsilon 0.8666920568517111, time 726.0, rides 139\n",
      "Initial State is  [2, 21, 0]\n",
      "episode 143, reward -216.0, memory_length 2000, epsilon 0.8658253647948594, time 727.0, rides 127\n",
      "Initial State is  [1, 3, 1]\n",
      "episode 144, reward -34.0, memory_length 2000, epsilon 0.8649595394300645, time 723.0, rides 127\n",
      "Initial State is  [0, 11, 4]\n",
      "episode 145, reward -61.0, memory_length 2000, epsilon 0.8640945798906344, time 737.0, rides 131\n",
      "Initial State is  [3, 21, 4]\n",
      "episode 146, reward -158.0, memory_length 2000, epsilon 0.8632304853107438, time 728.0, rides 121\n",
      "Initial State is  [2, 11, 6]\n",
      "episode 147, reward 34.0, memory_length 2000, epsilon 0.862367254825433, time 733.0, rides 115\n",
      "Initial State is  [2, 0, 4]\n",
      "episode 148, reward -250.0, memory_length 2000, epsilon 0.8615048875706075, time 725.0, rides 128\n",
      "Initial State is  [2, 1, 0]\n",
      "episode 149, reward -119.0, memory_length 2000, epsilon 0.8606433826830369, time 727.0, rides 127\n",
      "Initial State is  [0, 14, 5]\n",
      "episode 150, reward 40.0, memory_length 2000, epsilon 0.8597827393003539, time 728.0, rides 119\n",
      "Initial State is  [3, 5, 5]\n",
      "episode 151, reward -278.0, memory_length 2000, epsilon 0.8589229565610536, time 728.0, rides 131\n",
      "Initial State is  [0, 17, 3]\n",
      "episode 152, reward -73.0, memory_length 2000, epsilon 0.8580640336044925, time 726.0, rides 136\n",
      "Initial State is  [1, 18, 5]\n",
      "episode 153, reward -252.0, memory_length 2000, epsilon 0.857205969570888, time 730.0, rides 137\n",
      "Initial State is  [1, 6, 4]\n",
      "episode 154, reward -75.0, memory_length 2000, epsilon 0.8563487636013172, time 727.0, rides 120\n",
      "Initial State is  [0, 11, 5]\n",
      "episode 155, reward 66.0, memory_length 2000, epsilon 0.8554924148377159, time 728.0, rides 133\n",
      "Initial State is  [4, 4, 0]\n",
      "episode 156, reward -14.0, memory_length 2000, epsilon 0.8546369224228781, time 731.0, rides 124\n",
      "Initial State is  [1, 13, 5]\n",
      "episode 157, reward -156.0, memory_length 2000, epsilon 0.8537822855004553, time 729.0, rides 120\n",
      "Initial State is  [4, 20, 0]\n",
      "episode 158, reward -64.0, memory_length 2000, epsilon 0.8529285032149548, time 732.0, rides 124\n",
      "Initial State is  [1, 8, 6]\n",
      "episode 159, reward -438.0, memory_length 2000, epsilon 0.8520755747117399, time 731.0, rides 130\n",
      "Initial State is  [2, 18, 0]\n",
      "episode 160, reward -208.0, memory_length 2000, epsilon 0.8512234991370281, time 721.0, rides 128\n",
      "Initial State is  [1, 19, 4]\n",
      "episode 161, reward 7.0, memory_length 2000, epsilon 0.8503722756378911, time 729.0, rides 130\n",
      "Initial State is  [2, 12, 3]\n",
      "episode 162, reward -97.0, memory_length 2000, epsilon 0.8495219033622532, time 730.0, rides 137\n",
      "Initial State is  [1, 11, 1]\n",
      "episode 163, reward -58.0, memory_length 2000, epsilon 0.8486723814588909, time 731.0, rides 128\n",
      "Initial State is  [3, 22, 2]\n",
      "episode 164, reward -104.0, memory_length 2000, epsilon 0.847823709077432, time 731.0, rides 130\n",
      "Initial State is  [3, 4, 3]\n",
      "episode 165, reward -76.0, memory_length 2000, epsilon 0.8469758853683546, time 735.0, rides 127\n",
      "Initial State is  [2, 2, 3]\n",
      "episode 166, reward -333.0, memory_length 2000, epsilon 0.8461289094829862, time 730.0, rides 129\n",
      "Initial State is  [0, 8, 1]\n",
      "episode 167, reward -1.0, memory_length 2000, epsilon 0.8452827805735033, time 726.0, rides 130\n",
      "Initial State is  [2, 23, 5]\n",
      "episode 168, reward -39.0, memory_length 2000, epsilon 0.8444374977929298, time 731.0, rides 124\n",
      "Initial State is  [2, 8, 5]\n",
      "episode 169, reward -158.0, memory_length 2000, epsilon 0.8435930602951368, time 733.0, rides 122\n",
      "Initial State is  [4, 23, 2]\n",
      "episode 170, reward -260.0, memory_length 2000, epsilon 0.8427494672348417, time 738.0, rides 113\n",
      "Initial State is  [1, 2, 6]\n",
      "episode 171, reward -317.0, memory_length 2000, epsilon 0.8419067177676068, time 734.0, rides 137\n",
      "Initial State is  [0, 4, 0]\n",
      "episode 172, reward 325.0, memory_length 2000, epsilon 0.8410648110498392, time 732.0, rides 125\n",
      "Initial State is  [0, 6, 0]\n",
      "episode 173, reward -179.0, memory_length 2000, epsilon 0.8402237462387894, time 737.0, rides 130\n",
      "Initial State is  [1, 21, 2]\n",
      "episode 174, reward 24.0, memory_length 2000, epsilon 0.8393835224925505, time 726.0, rides 126\n",
      "Initial State is  [0, 10, 2]\n",
      "episode 175, reward -51.0, memory_length 2000, epsilon 0.838544138970058, time 731.0, rides 117\n",
      "Initial State is  [2, 19, 0]\n",
      "episode 176, reward 29.0, memory_length 2000, epsilon 0.8377055948310879, time 723.0, rides 121\n",
      "Initial State is  [0, 12, 2]\n",
      "episode 177, reward 17.0, memory_length 2000, epsilon 0.8368678892362568, time 729.0, rides 111\n",
      "Initial State is  [4, 23, 5]\n",
      "episode 178, reward -75.0, memory_length 2000, epsilon 0.8360310213470206, time 730.0, rides 127\n",
      "Initial State is  [3, 8, 5]\n",
      "episode 179, reward 261.0, memory_length 2000, epsilon 0.8351949903256736, time 730.0, rides 126\n",
      "Initial State is  [4, 21, 1]\n",
      "episode 180, reward -260.0, memory_length 2000, epsilon 0.8343597953353479, time 727.0, rides 121\n",
      "Initial State is  [1, 4, 6]\n",
      "episode 181, reward -232.0, memory_length 2000, epsilon 0.8335254355400126, time 737.0, rides 130\n",
      "Initial State is  [2, 17, 0]\n",
      "episode 182, reward -202.0, memory_length 2000, epsilon 0.8326919101044725, time 730.0, rides 115\n",
      "Initial State is  [2, 13, 5]\n",
      "episode 183, reward 17.0, memory_length 2000, epsilon 0.831859218194368, time 735.0, rides 129\n",
      "Initial State is  [3, 14, 4]\n",
      "episode 184, reward -393.0, memory_length 2000, epsilon 0.8310273589761736, time 738.0, rides 125\n",
      "Initial State is  [4, 0, 3]\n",
      "episode 185, reward -116.0, memory_length 2000, epsilon 0.8301963316171974, time 737.0, rides 135\n",
      "Initial State is  [3, 0, 2]\n",
      "episode 186, reward -221.0, memory_length 2000, epsilon 0.8293661352855802, time 725.0, rides 119\n",
      "Initial State is  [1, 21, 3]\n",
      "episode 187, reward -478.0, memory_length 2000, epsilon 0.8285367691502946, time 726.0, rides 124\n",
      "Initial State is  [2, 12, 1]\n",
      "episode 188, reward -54.0, memory_length 2000, epsilon 0.8277082323811443, time 727.0, rides 136\n",
      "Initial State is  [4, 16, 5]\n",
      "episode 189, reward -379.0, memory_length 2000, epsilon 0.8268805241487632, time 735.0, rides 132\n",
      "Initial State is  [3, 14, 1]\n",
      "episode 190, reward -67.0, memory_length 2000, epsilon 0.8260536436246144, time 730.0, rides 128\n",
      "Initial State is  [2, 9, 4]\n",
      "episode 191, reward -101.0, memory_length 2000, epsilon 0.8252275899809898, time 736.0, rides 125\n",
      "Initial State is  [3, 14, 2]\n",
      "episode 192, reward -118.0, memory_length 2000, epsilon 0.8244023623910088, time 727.0, rides 122\n",
      "Initial State is  [2, 14, 1]\n",
      "episode 193, reward -33.0, memory_length 2000, epsilon 0.8235779600286178, time 724.0, rides 117\n",
      "Initial State is  [4, 5, 6]\n",
      "episode 194, reward 10.0, memory_length 2000, epsilon 0.8227543820685892, time 734.0, rides 138\n",
      "Initial State is  [0, 15, 1]\n",
      "episode 195, reward -264.0, memory_length 2000, epsilon 0.8219316276865206, time 735.0, rides 131\n",
      "Initial State is  [3, 9, 3]\n",
      "episode 196, reward -484.0, memory_length 2000, epsilon 0.8211096960588341, time 730.0, rides 128\n",
      "Initial State is  [2, 22, 0]\n",
      "episode 197, reward -44.0, memory_length 2000, epsilon 0.8202885863627752, time 724.0, rides 128\n",
      "Initial State is  [4, 8, 6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 198, reward -102.0, memory_length 2000, epsilon 0.8194682977764125, time 727.0, rides 150\n",
      "Initial State is  [3, 22, 3]\n",
      "episode 199, reward 74.0, memory_length 2000, epsilon 0.818648829478636, time 725.0, rides 132\n",
      "Initial State is  [1, 6, 2]\n",
      "episode 200, reward 55.0, memory_length 2000, epsilon 0.8178301806491574, time 735.0, rides 117\n",
      "Initial State is  [1, 23, 3]\n",
      "episode 201, reward -280.0, memory_length 2000, epsilon 0.8170123504685082, time 745.0, rides 125\n",
      "Initial State is  [2, 18, 2]\n",
      "episode 202, reward 178.0, memory_length 2000, epsilon 0.8161953381180397, time 731.0, rides 131\n",
      "Initial State is  [4, 0, 1]\n",
      "episode 203, reward -91.0, memory_length 2000, epsilon 0.8153791427799216, time 731.0, rides 127\n",
      "Initial State is  [0, 7, 4]\n",
      "episode 204, reward -164.0, memory_length 2000, epsilon 0.8145637636371417, time 731.0, rides 134\n",
      "Initial State is  [1, 12, 5]\n",
      "episode 205, reward 130.0, memory_length 2000, epsilon 0.8137491998735046, time 727.0, rides 118\n",
      "Initial State is  [1, 19, 1]\n",
      "episode 206, reward 284.0, memory_length 2000, epsilon 0.812935450673631, time 738.0, rides 121\n",
      "Initial State is  [0, 13, 2]\n",
      "episode 207, reward -141.0, memory_length 2000, epsilon 0.8121225152229574, time 724.0, rides 130\n",
      "Initial State is  [0, 23, 5]\n",
      "episode 208, reward -170.0, memory_length 2000, epsilon 0.8113103927077344, time 728.0, rides 120\n",
      "Initial State is  [3, 5, 5]\n",
      "episode 209, reward -225.0, memory_length 2000, epsilon 0.8104990823150267, time 732.0, rides 134\n",
      "Initial State is  [1, 17, 6]\n",
      "episode 210, reward 40.0, memory_length 2000, epsilon 0.8096885832327116, time 736.0, rides 122\n",
      "Initial State is  [1, 21, 0]\n",
      "episode 211, reward -136.0, memory_length 2000, epsilon 0.8088788946494789, time 724.0, rides 123\n",
      "Initial State is  [1, 22, 4]\n",
      "episode 212, reward -283.0, memory_length 2000, epsilon 0.8080700157548294, time 737.0, rides 119\n",
      "Initial State is  [2, 22, 2]\n",
      "episode 213, reward -81.0, memory_length 2000, epsilon 0.8072619457390746, time 730.0, rides 128\n",
      "Initial State is  [4, 13, 0]\n",
      "episode 214, reward 61.0, memory_length 2000, epsilon 0.8064546837933355, time 733.0, rides 122\n",
      "Initial State is  [0, 0, 2]\n",
      "episode 215, reward -152.0, memory_length 2000, epsilon 0.8056482291095421, time 722.0, rides 121\n",
      "Initial State is  [4, 22, 2]\n",
      "episode 216, reward -163.0, memory_length 2000, epsilon 0.8048425808804326, time 730.0, rides 115\n",
      "Initial State is  [1, 14, 1]\n",
      "episode 217, reward -120.0, memory_length 2000, epsilon 0.8040377382995522, time 727.0, rides 129\n",
      "Initial State is  [1, 16, 3]\n",
      "episode 218, reward -187.0, memory_length 2000, epsilon 0.8032337005612527, time 723.0, rides 116\n",
      "Initial State is  [3, 15, 6]\n",
      "episode 219, reward -269.0, memory_length 2000, epsilon 0.8024304668606914, time 732.0, rides 126\n",
      "Initial State is  [4, 18, 0]\n",
      "episode 220, reward 13.0, memory_length 2000, epsilon 0.8016280363938307, time 725.0, rides 124\n",
      "Initial State is  [2, 23, 3]\n",
      "episode 221, reward -48.0, memory_length 2000, epsilon 0.8008264083574369, time 738.0, rides 125\n",
      "Initial State is  [2, 11, 3]\n",
      "episode 222, reward -63.0, memory_length 2000, epsilon 0.8000255819490795, time 730.0, rides 134\n",
      "Initial State is  [0, 19, 1]\n",
      "episode 223, reward -170.0, memory_length 2000, epsilon 0.7992255563671304, time 725.0, rides 132\n",
      "Initial State is  [2, 1, 0]\n",
      "episode 224, reward -187.0, memory_length 2000, epsilon 0.7984263308107633, time 732.0, rides 138\n",
      "Initial State is  [4, 7, 3]\n",
      "episode 225, reward -32.0, memory_length 2000, epsilon 0.7976279044799526, time 735.0, rides 124\n",
      "Initial State is  [2, 16, 0]\n",
      "episode 226, reward 59.0, memory_length 2000, epsilon 0.7968302765754727, time 733.0, rides 128\n",
      "Initial State is  [1, 7, 1]\n",
      "episode 227, reward -192.0, memory_length 2000, epsilon 0.7960334462988972, time 724.0, rides 126\n",
      "Initial State is  [4, 19, 6]\n",
      "episode 228, reward -67.0, memory_length 2000, epsilon 0.7952374128525983, time 720.0, rides 130\n",
      "Initial State is  [0, 6, 0]\n",
      "episode 229, reward -86.0, memory_length 2000, epsilon 0.7944421754397457, time 723.0, rides 121\n",
      "Initial State is  [2, 16, 1]\n",
      "episode 230, reward -149.0, memory_length 2000, epsilon 0.7936477332643059, time 729.0, rides 119\n",
      "Initial State is  [1, 11, 3]\n",
      "episode 231, reward -133.0, memory_length 2000, epsilon 0.7928540855310416, time 745.0, rides 130\n",
      "Initial State is  [1, 0, 5]\n",
      "episode 232, reward 261.0, memory_length 2000, epsilon 0.7920612314455105, time 725.0, rides 114\n",
      "Initial State is  [0, 17, 0]\n",
      "episode 233, reward -100.0, memory_length 2000, epsilon 0.7912691702140651, time 728.0, rides 126\n",
      "Initial State is  [3, 5, 2]\n",
      "episode 234, reward -332.0, memory_length 2000, epsilon 0.790477901043851, time 725.0, rides 114\n",
      "Initial State is  [0, 13, 6]\n",
      "episode 235, reward -131.0, memory_length 2000, epsilon 0.7896874231428072, time 738.0, rides 112\n",
      "Initial State is  [3, 14, 2]\n",
      "episode 236, reward -266.0, memory_length 2000, epsilon 0.7888977357196644, time 727.0, rides 135\n",
      "Initial State is  [2, 0, 6]\n",
      "episode 237, reward -19.0, memory_length 2000, epsilon 0.7881088379839447, time 730.0, rides 126\n",
      "Initial State is  [4, 7, 4]\n",
      "episode 238, reward -303.0, memory_length 2000, epsilon 0.7873207291459607, time 722.0, rides 126\n",
      "Initial State is  [2, 23, 0]\n",
      "episode 239, reward 38.0, memory_length 2000, epsilon 0.7865334084168147, time 730.0, rides 125\n",
      "Initial State is  [3, 13, 0]\n",
      "episode 240, reward -184.0, memory_length 2000, epsilon 0.7857468750083979, time 734.0, rides 135\n",
      "Initial State is  [3, 17, 0]\n",
      "episode 241, reward 84.0, memory_length 2000, epsilon 0.7849611281333895, time 733.0, rides 124\n",
      "Initial State is  [4, 11, 2]\n",
      "episode 242, reward -12.0, memory_length 2000, epsilon 0.784176167005256, time 734.0, rides 132\n",
      "Initial State is  [1, 1, 6]\n",
      "episode 243, reward -2.0, memory_length 2000, epsilon 0.7833919908382508, time 731.0, rides 119\n",
      "Initial State is  [3, 9, 4]\n",
      "episode 244, reward 115.0, memory_length 2000, epsilon 0.7826085988474126, time 724.0, rides 125\n",
      "Initial State is  [0, 8, 1]\n",
      "episode 245, reward 74.0, memory_length 2000, epsilon 0.7818259902485653, time 730.0, rides 144\n",
      "Initial State is  [3, 3, 6]\n",
      "episode 246, reward 310.0, memory_length 2000, epsilon 0.7810441642583167, time 734.0, rides 122\n",
      "Initial State is  [4, 12, 2]\n",
      "episode 247, reward -62.0, memory_length 2000, epsilon 0.7802631200940584, time 737.0, rides 127\n",
      "Initial State is  [0, 2, 2]\n",
      "episode 248, reward -74.0, memory_length 2000, epsilon 0.7794828569739644, time 725.0, rides 117\n",
      "Initial State is  [1, 6, 6]\n",
      "episode 249, reward 53.0, memory_length 2000, epsilon 0.7787033741169904, time 735.0, rides 127\n",
      "Initial State is  [0, 19, 2]\n",
      "episode 250, reward 202.0, memory_length 2000, epsilon 0.7779246707428734, time 734.0, rides 128\n",
      "Initial State is  [0, 13, 5]\n",
      "episode 251, reward -99.0, memory_length 2000, epsilon 0.7771467460721305, time 725.0, rides 128\n",
      "Initial State is  [3, 10, 0]\n",
      "episode 252, reward -67.0, memory_length 2000, epsilon 0.7763695993260584, time 729.0, rides 131\n",
      "Initial State is  [0, 7, 0]\n",
      "episode 253, reward -170.0, memory_length 2000, epsilon 0.7755932297267324, time 728.0, rides 118\n",
      "Initial State is  [0, 11, 4]\n",
      "episode 254, reward -120.0, memory_length 2000, epsilon 0.7748176364970056, time 734.0, rides 123\n",
      "Initial State is  [4, 17, 2]\n",
      "episode 255, reward -44.0, memory_length 2000, epsilon 0.7740428188605086, time 736.0, rides 128\n",
      "Initial State is  [0, 19, 0]\n",
      "episode 256, reward -352.0, memory_length 2000, epsilon 0.7732687760416481, time 731.0, rides 124\n",
      "Initial State is  [3, 4, 4]\n",
      "episode 257, reward 41.0, memory_length 2000, epsilon 0.7724955072656065, time 731.0, rides 128\n",
      "Initial State is  [2, 4, 1]\n",
      "episode 258, reward 52.0, memory_length 2000, epsilon 0.7717230117583408, time 735.0, rides 122\n",
      "Initial State is  [2, 21, 5]\n",
      "episode 259, reward 63.0, memory_length 2000, epsilon 0.7709512887465825, time 733.0, rides 125\n",
      "Initial State is  [4, 9, 1]\n",
      "episode 260, reward 43.0, memory_length 2000, epsilon 0.7701803374578359, time 722.0, rides 132\n",
      "Initial State is  [0, 7, 3]\n",
      "episode 261, reward 57.0, memory_length 2000, epsilon 0.7694101571203781, time 728.0, rides 126\n",
      "Initial State is  [1, 14, 4]\n",
      "episode 262, reward -46.0, memory_length 2000, epsilon 0.7686407469632577, time 725.0, rides 127\n",
      "Initial State is  [2, 7, 4]\n",
      "episode 263, reward 115.0, memory_length 2000, epsilon 0.7678721062162944, time 733.0, rides 120\n",
      "Initial State is  [0, 10, 6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 264, reward -191.0, memory_length 2000, epsilon 0.7671042341100781, time 726.0, rides 119\n",
      "Initial State is  [2, 8, 3]\n",
      "episode 265, reward 120.0, memory_length 2000, epsilon 0.766337129875968, time 736.0, rides 126\n",
      "Initial State is  [4, 20, 0]\n",
      "episode 266, reward -359.0, memory_length 2000, epsilon 0.7655707927460921, time 722.0, rides 105\n",
      "Initial State is  [0, 8, 4]\n",
      "episode 267, reward -28.0, memory_length 2000, epsilon 0.764805221953346, time 734.0, rides 130\n",
      "Initial State is  [4, 7, 1]\n",
      "episode 268, reward 12.0, memory_length 2000, epsilon 0.7640404167313927, time 726.0, rides 126\n",
      "Initial State is  [1, 14, 5]\n",
      "episode 269, reward -107.0, memory_length 2000, epsilon 0.7632763763146613, time 726.0, rides 128\n",
      "Initial State is  [1, 20, 3]\n",
      "episode 270, reward 24.0, memory_length 2000, epsilon 0.7625130999383466, time 724.0, rides 124\n",
      "Initial State is  [1, 14, 1]\n",
      "episode 271, reward -337.0, memory_length 2000, epsilon 0.7617505868384082, time 731.0, rides 118\n",
      "Initial State is  [2, 16, 5]\n",
      "episode 272, reward -345.0, memory_length 2000, epsilon 0.7609888362515699, time 734.0, rides 125\n",
      "Initial State is  [3, 13, 3]\n",
      "episode 273, reward 46.0, memory_length 2000, epsilon 0.7602278474153183, time 733.0, rides 127\n",
      "Initial State is  [0, 14, 0]\n",
      "episode 274, reward -73.0, memory_length 2000, epsilon 0.759467619567903, time 730.0, rides 131\n",
      "Initial State is  [3, 4, 2]\n",
      "episode 275, reward -50.0, memory_length 2000, epsilon 0.7587081519483351, time 725.0, rides 137\n",
      "Initial State is  [4, 1, 1]\n",
      "episode 276, reward -157.0, memory_length 2000, epsilon 0.7579494437963867, time 734.0, rides 130\n",
      "Initial State is  [3, 3, 2]\n",
      "episode 277, reward -118.0, memory_length 2000, epsilon 0.7571914943525904, time 729.0, rides 132\n",
      "Initial State is  [1, 12, 0]\n",
      "episode 278, reward 87.0, memory_length 2000, epsilon 0.7564343028582378, time 727.0, rides 124\n",
      "Initial State is  [0, 16, 0]\n",
      "episode 279, reward -171.0, memory_length 2000, epsilon 0.7556778685553796, time 724.0, rides 144\n",
      "Initial State is  [2, 7, 1]\n",
      "episode 280, reward -347.0, memory_length 2000, epsilon 0.7549221906868242, time 725.0, rides 126\n",
      "Initial State is  [2, 15, 6]\n",
      "episode 281, reward -125.0, memory_length 2000, epsilon 0.7541672684961374, time 741.0, rides 127\n",
      "Initial State is  [3, 18, 0]\n",
      "episode 282, reward 66.0, memory_length 2000, epsilon 0.7534131012276413, time 737.0, rides 127\n",
      "Initial State is  [1, 15, 1]\n",
      "episode 283, reward -278.0, memory_length 2000, epsilon 0.7526596881264136, time 733.0, rides 124\n",
      "Initial State is  [2, 21, 2]\n",
      "episode 284, reward -193.0, memory_length 2000, epsilon 0.7519070284382872, time 726.0, rides 125\n",
      "Initial State is  [4, 3, 6]\n",
      "episode 285, reward 201.0, memory_length 2000, epsilon 0.7511551214098489, time 725.0, rides 125\n",
      "Initial State is  [3, 2, 3]\n",
      "episode 286, reward 307.0, memory_length 2000, epsilon 0.750403966288439, time 725.0, rides 118\n",
      "Initial State is  [1, 18, 3]\n",
      "episode 287, reward -232.0, memory_length 2000, epsilon 0.7496535623221505, time 738.0, rides 135\n",
      "Initial State is  [1, 7, 4]\n",
      "episode 288, reward -12.0, memory_length 2000, epsilon 0.7489039087598284, time 724.0, rides 121\n",
      "Initial State is  [0, 11, 1]\n",
      "episode 289, reward -272.0, memory_length 2000, epsilon 0.7481550048510686, time 728.0, rides 148\n",
      "Initial State is  [3, 17, 1]\n",
      "episode 290, reward 54.0, memory_length 2000, epsilon 0.7474068498462175, time 729.0, rides 126\n",
      "Initial State is  [0, 17, 2]\n",
      "episode 291, reward -140.0, memory_length 2000, epsilon 0.7466594429963713, time 726.0, rides 115\n",
      "Initial State is  [0, 9, 3]\n",
      "episode 292, reward 196.0, memory_length 2000, epsilon 0.745912783553375, time 722.0, rides 133\n",
      "Initial State is  [2, 2, 2]\n",
      "episode 293, reward 127.0, memory_length 2000, epsilon 0.7451668707698216, time 728.0, rides 127\n",
      "Initial State is  [2, 6, 6]\n",
      "episode 294, reward 244.0, memory_length 2000, epsilon 0.7444217038990517, time 728.0, rides 125\n",
      "Initial State is  [3, 12, 1]\n",
      "episode 295, reward -188.0, memory_length 2000, epsilon 0.7436772821951526, time 732.0, rides 126\n",
      "Initial State is  [1, 22, 1]\n",
      "episode 296, reward -83.0, memory_length 2000, epsilon 0.7429336049129575, time 725.0, rides 120\n",
      "Initial State is  [3, 5, 6]\n",
      "episode 297, reward 191.0, memory_length 2000, epsilon 0.7421906713080445, time 733.0, rides 129\n",
      "Initial State is  [0, 10, 3]\n",
      "episode 298, reward -23.0, memory_length 2000, epsilon 0.7414484806367364, time 731.0, rides 130\n",
      "Initial State is  [0, 1, 4]\n",
      "episode 299, reward -69.0, memory_length 2000, epsilon 0.7407070321560997, time 731.0, rides 123\n",
      "Initial State is  [4, 15, 6]\n",
      "episode 300, reward 33.0, memory_length 2000, epsilon 0.7399663251239436, time 729.0, rides 111\n",
      "Initial State is  [2, 3, 1]\n",
      "episode 301, reward 113.0, memory_length 2000, epsilon 0.7392263587988196, time 731.0, rides 136\n",
      "Initial State is  [2, 8, 3]\n",
      "episode 302, reward -399.0, memory_length 2000, epsilon 0.7384871324400207, time 726.0, rides 133\n",
      "Initial State is  [2, 1, 5]\n",
      "episode 303, reward 16.0, memory_length 2000, epsilon 0.7377486453075807, time 729.0, rides 127\n",
      "Initial State is  [1, 8, 4]\n",
      "episode 304, reward 77.0, memory_length 2000, epsilon 0.737010896662273, time 730.0, rides 118\n",
      "Initial State is  [2, 1, 3]\n",
      "episode 305, reward -154.0, memory_length 2000, epsilon 0.7362738857656108, time 735.0, rides 123\n",
      "Initial State is  [2, 11, 0]\n",
      "episode 306, reward -140.0, memory_length 2000, epsilon 0.7355376118798452, time 730.0, rides 125\n",
      "Initial State is  [4, 18, 4]\n",
      "episode 307, reward -230.0, memory_length 2000, epsilon 0.7348020742679654, time 747.0, rides 141\n",
      "Initial State is  [1, 12, 2]\n",
      "episode 308, reward -221.0, memory_length 2000, epsilon 0.7340672721936974, time 741.0, rides 131\n",
      "Initial State is  [3, 9, 2]\n",
      "episode 309, reward 37.0, memory_length 2000, epsilon 0.7333332049215037, time 724.0, rides 134\n",
      "Initial State is  [3, 14, 3]\n",
      "episode 310, reward 274.0, memory_length 2000, epsilon 0.7325998717165821, time 729.0, rides 128\n",
      "Initial State is  [3, 22, 5]\n",
      "episode 311, reward -231.0, memory_length 2000, epsilon 0.7318672718448656, time 731.0, rides 120\n",
      "Initial State is  [3, 10, 2]\n",
      "episode 312, reward 246.0, memory_length 2000, epsilon 0.7311354045730207, time 731.0, rides 121\n",
      "Initial State is  [2, 12, 3]\n",
      "episode 313, reward 56.0, memory_length 2000, epsilon 0.7304042691684477, time 731.0, rides 134\n",
      "Initial State is  [4, 5, 4]\n",
      "episode 314, reward 40.0, memory_length 2000, epsilon 0.7296738648992792, time 735.0, rides 124\n",
      "Initial State is  [2, 11, 3]\n",
      "episode 315, reward -173.0, memory_length 2000, epsilon 0.7289441910343799, time 727.0, rides 134\n",
      "Initial State is  [2, 16, 4]\n",
      "episode 316, reward 232.0, memory_length 2000, epsilon 0.7282152468433455, time 721.0, rides 141\n",
      "Initial State is  [0, 5, 1]\n",
      "episode 317, reward -111.0, memory_length 2000, epsilon 0.7274870315965022, time 727.0, rides 119\n",
      "Initial State is  [2, 11, 4]\n",
      "episode 318, reward 36.0, memory_length 2000, epsilon 0.7267595445649057, time 733.0, rides 119\n",
      "Initial State is  [0, 14, 3]\n",
      "episode 319, reward -13.0, memory_length 2000, epsilon 0.7260327850203407, time 724.0, rides 119\n",
      "Initial State is  [3, 3, 5]\n",
      "episode 320, reward 33.0, memory_length 2000, epsilon 0.7253067522353204, time 731.0, rides 128\n",
      "Initial State is  [1, 2, 4]\n",
      "episode 321, reward 80.0, memory_length 2000, epsilon 0.724581445483085, time 728.0, rides 129\n",
      "Initial State is  [1, 6, 3]\n",
      "episode 322, reward -198.0, memory_length 2000, epsilon 0.723856864037602, time 740.0, rides 125\n",
      "Initial State is  [2, 13, 1]\n",
      "episode 323, reward 124.0, memory_length 2000, epsilon 0.7231330071735643, time 731.0, rides 125\n",
      "Initial State is  [0, 20, 4]\n",
      "episode 324, reward 149.0, memory_length 2000, epsilon 0.7224098741663908, time 731.0, rides 121\n",
      "Initial State is  [0, 12, 4]\n",
      "episode 325, reward -12.0, memory_length 2000, epsilon 0.7216874642922244, time 735.0, rides 111\n",
      "Initial State is  [2, 19, 0]\n",
      "episode 326, reward -363.0, memory_length 2000, epsilon 0.7209657768279322, time 730.0, rides 108\n",
      "Initial State is  [0, 14, 6]\n",
      "episode 327, reward -313.0, memory_length 2000, epsilon 0.7202448110511043, time 725.0, rides 125\n",
      "Initial State is  [2, 5, 5]\n",
      "episode 328, reward 180.0, memory_length 2000, epsilon 0.7195245662400531, time 728.0, rides 131\n",
      "Initial State is  [2, 10, 3]\n",
      "episode 329, reward 2.0, memory_length 2000, epsilon 0.7188050416738131, time 724.0, rides 125\n",
      "Initial State is  [3, 22, 5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 330, reward -197.0, memory_length 2000, epsilon 0.7180862366321393, time 735.0, rides 118\n",
      "Initial State is  [1, 17, 6]\n",
      "episode 331, reward 246.0, memory_length 2000, epsilon 0.7173681503955072, time 725.0, rides 129\n",
      "Initial State is  [1, 3, 0]\n",
      "episode 332, reward -39.0, memory_length 2000, epsilon 0.7166507822451117, time 735.0, rides 132\n",
      "Initial State is  [4, 11, 1]\n",
      "episode 333, reward 19.0, memory_length 2000, epsilon 0.7159341314628666, time 729.0, rides 121\n",
      "Initial State is  [1, 20, 1]\n",
      "episode 334, reward 58.0, memory_length 2000, epsilon 0.7152181973314037, time 732.0, rides 128\n",
      "Initial State is  [4, 8, 5]\n",
      "episode 335, reward -46.0, memory_length 2000, epsilon 0.7145029791340722, time 736.0, rides 127\n",
      "Initial State is  [4, 13, 5]\n",
      "episode 336, reward 314.0, memory_length 2000, epsilon 0.7137884761549381, time 726.0, rides 129\n",
      "Initial State is  [3, 4, 1]\n",
      "episode 337, reward -51.0, memory_length 2000, epsilon 0.7130746876787832, time 725.0, rides 122\n",
      "Initial State is  [4, 9, 4]\n",
      "episode 338, reward 52.0, memory_length 2000, epsilon 0.7123616129911045, time 724.0, rides 119\n",
      "Initial State is  [4, 5, 5]\n",
      "episode 339, reward 228.0, memory_length 2000, epsilon 0.7116492513781133, time 732.0, rides 121\n",
      "Initial State is  [1, 13, 6]\n",
      "episode 340, reward -181.0, memory_length 2000, epsilon 0.7109376021267352, time 727.0, rides 123\n",
      "Initial State is  [3, 21, 4]\n",
      "episode 341, reward 287.0, memory_length 2000, epsilon 0.7102266645246085, time 735.0, rides 126\n",
      "Initial State is  [1, 8, 5]\n",
      "episode 342, reward 46.0, memory_length 2000, epsilon 0.7095164378600839, time 723.0, rides 130\n",
      "Initial State is  [4, 17, 2]\n",
      "episode 343, reward -116.0, memory_length 2000, epsilon 0.7088069214222238, time 725.0, rides 132\n",
      "Initial State is  [0, 7, 1]\n",
      "episode 344, reward -147.0, memory_length 2000, epsilon 0.7080981145008015, time 730.0, rides 114\n",
      "Initial State is  [3, 18, 0]\n",
      "episode 345, reward -45.0, memory_length 2000, epsilon 0.7073900163863007, time 728.0, rides 123\n",
      "Initial State is  [4, 16, 5]\n",
      "episode 346, reward 279.0, memory_length 2000, epsilon 0.7066826263699144, time 740.0, rides 135\n",
      "Initial State is  [0, 9, 2]\n",
      "episode 347, reward 388.0, memory_length 2000, epsilon 0.7059759437435444, time 735.0, rides 129\n",
      "Initial State is  [2, 15, 2]\n",
      "episode 348, reward -119.0, memory_length 2000, epsilon 0.7052699677998009, time 726.0, rides 125\n",
      "Initial State is  [1, 23, 2]\n",
      "episode 349, reward -47.0, memory_length 2000, epsilon 0.704564697832001, time 729.0, rides 142\n",
      "Initial State is  [3, 16, 0]\n",
      "episode 350, reward 46.0, memory_length 2000, epsilon 0.7038601331341691, time 727.0, rides 125\n",
      "Initial State is  [2, 21, 6]\n",
      "episode 351, reward -15.0, memory_length 2000, epsilon 0.7031562730010349, time 727.0, rides 125\n",
      "Initial State is  [4, 4, 2]\n",
      "episode 352, reward -95.0, memory_length 2000, epsilon 0.7024531167280339, time 736.0, rides 137\n",
      "Initial State is  [2, 21, 2]\n",
      "episode 353, reward 145.0, memory_length 2000, epsilon 0.7017506636113059, time 727.0, rides 109\n",
      "Initial State is  [2, 10, 1]\n",
      "episode 354, reward 90.0, memory_length 2000, epsilon 0.7010489129476946, time 725.0, rides 120\n",
      "Initial State is  [0, 15, 3]\n",
      "episode 355, reward 28.0, memory_length 2000, epsilon 0.7003478640347469, time 730.0, rides 124\n",
      "Initial State is  [2, 11, 2]\n",
      "episode 356, reward 215.0, memory_length 2000, epsilon 0.6996475161707122, time 733.0, rides 119\n",
      "Initial State is  [3, 17, 0]\n",
      "episode 357, reward 231.0, memory_length 2000, epsilon 0.6989478686545415, time 720.0, rides 127\n",
      "Initial State is  [2, 9, 3]\n",
      "episode 358, reward -53.0, memory_length 2000, epsilon 0.698248920785887, time 724.0, rides 113\n",
      "Initial State is  [2, 13, 4]\n",
      "episode 359, reward 145.0, memory_length 2000, epsilon 0.6975506718651011, time 731.0, rides 133\n",
      "Initial State is  [0, 1, 3]\n",
      "episode 360, reward 9.0, memory_length 2000, epsilon 0.6968531211932361, time 727.0, rides 124\n",
      "Initial State is  [0, 5, 5]\n",
      "episode 361, reward -185.0, memory_length 2000, epsilon 0.6961562680720428, time 738.0, rides 117\n",
      "Initial State is  [0, 19, 5]\n",
      "episode 362, reward -208.0, memory_length 2000, epsilon 0.6954601118039707, time 733.0, rides 121\n",
      "Initial State is  [0, 10, 1]\n",
      "episode 363, reward -31.0, memory_length 2000, epsilon 0.6947646516921667, time 729.0, rides 126\n",
      "Initial State is  [3, 21, 3]\n",
      "episode 364, reward 169.0, memory_length 2000, epsilon 0.6940698870404746, time 730.0, rides 127\n",
      "Initial State is  [0, 12, 4]\n",
      "episode 365, reward 18.0, memory_length 2000, epsilon 0.6933758171534341, time 724.0, rides 138\n",
      "Initial State is  [2, 14, 4]\n",
      "episode 366, reward 69.0, memory_length 2000, epsilon 0.6926824413362807, time 726.0, rides 128\n",
      "Initial State is  [4, 5, 1]\n",
      "episode 367, reward 201.0, memory_length 2000, epsilon 0.6919897588949444, time 725.0, rides 126\n",
      "Initial State is  [4, 6, 5]\n",
      "episode 368, reward 309.0, memory_length 2000, epsilon 0.6912977691360495, time 732.0, rides 116\n",
      "Initial State is  [3, 21, 0]\n",
      "episode 369, reward 306.0, memory_length 2000, epsilon 0.6906064713669134, time 733.0, rides 112\n",
      "Initial State is  [0, 23, 5]\n",
      "episode 370, reward 104.0, memory_length 2000, epsilon 0.6899158648955466, time 728.0, rides 115\n",
      "Initial State is  [4, 1, 4]\n",
      "episode 371, reward 179.0, memory_length 2000, epsilon 0.689225949030651, time 723.0, rides 141\n",
      "Initial State is  [1, 7, 3]\n",
      "episode 372, reward 156.0, memory_length 2000, epsilon 0.6885367230816204, time 734.0, rides 119\n",
      "Initial State is  [1, 14, 3]\n",
      "episode 373, reward -105.0, memory_length 2000, epsilon 0.6878481863585387, time 724.0, rides 120\n",
      "Initial State is  [3, 0, 0]\n",
      "episode 374, reward -48.0, memory_length 2000, epsilon 0.6871603381721801, time 734.0, rides 118\n",
      "Initial State is  [3, 15, 1]\n",
      "episode 375, reward 229.0, memory_length 2000, epsilon 0.686473177834008, time 728.0, rides 121\n",
      "Initial State is  [4, 8, 3]\n",
      "episode 376, reward -222.0, memory_length 2000, epsilon 0.685786704656174, time 738.0, rides 123\n",
      "Initial State is  [3, 21, 5]\n",
      "episode 377, reward -222.0, memory_length 2000, epsilon 0.6851009179515178, time 727.0, rides 131\n",
      "Initial State is  [4, 23, 1]\n",
      "episode 378, reward -18.0, memory_length 2000, epsilon 0.6844158170335664, time 736.0, rides 119\n",
      "Initial State is  [3, 20, 3]\n",
      "episode 379, reward -331.0, memory_length 2000, epsilon 0.6837314012165328, time 726.0, rides 120\n",
      "Initial State is  [4, 17, 3]\n",
      "episode 380, reward 16.0, memory_length 2000, epsilon 0.6830476698153162, time 731.0, rides 134\n",
      "Initial State is  [0, 6, 5]\n",
      "episode 381, reward -64.0, memory_length 2000, epsilon 0.6823646221455009, time 730.0, rides 135\n",
      "Initial State is  [1, 13, 1]\n",
      "episode 382, reward 95.0, memory_length 2000, epsilon 0.6816822575233553, time 729.0, rides 117\n",
      "Initial State is  [3, 7, 6]\n",
      "episode 383, reward 44.0, memory_length 2000, epsilon 0.6810005752658319, time 736.0, rides 106\n",
      "Initial State is  [4, 4, 1]\n",
      "episode 384, reward -91.0, memory_length 2000, epsilon 0.680319574690566, time 732.0, rides 125\n",
      "Initial State is  [0, 23, 3]\n",
      "episode 385, reward 117.0, memory_length 2000, epsilon 0.6796392551158754, time 730.0, rides 131\n",
      "Initial State is  [3, 6, 1]\n",
      "episode 386, reward -223.0, memory_length 2000, epsilon 0.6789596158607596, time 724.0, rides 120\n",
      "Initial State is  [0, 18, 6]\n",
      "episode 387, reward -2.0, memory_length 2000, epsilon 0.6782806562448989, time 724.0, rides 126\n",
      "Initial State is  [4, 9, 0]\n",
      "episode 388, reward -294.0, memory_length 2000, epsilon 0.677602375588654, time 729.0, rides 128\n",
      "Initial State is  [1, 23, 6]\n",
      "episode 389, reward -63.0, memory_length 2000, epsilon 0.6769247732130653, time 729.0, rides 120\n",
      "Initial State is  [3, 10, 6]\n",
      "episode 390, reward -124.0, memory_length 2000, epsilon 0.6762478484398523, time 736.0, rides 122\n",
      "Initial State is  [2, 0, 1]\n",
      "episode 391, reward -27.0, memory_length 2000, epsilon 0.6755716005914124, time 723.0, rides 131\n",
      "Initial State is  [3, 15, 2]\n",
      "episode 392, reward 509.0, memory_length 2000, epsilon 0.674896028990821, time 728.0, rides 122\n",
      "Initial State is  [3, 7, 6]\n",
      "episode 393, reward 121.0, memory_length 2000, epsilon 0.6742211329618302, time 724.0, rides 133\n",
      "Initial State is  [0, 14, 2]\n",
      "episode 394, reward 31.0, memory_length 2000, epsilon 0.6735469118288684, time 726.0, rides 129\n",
      "Initial State is  [2, 16, 2]\n",
      "episode 395, reward -346.0, memory_length 2000, epsilon 0.6728733649170395, time 725.0, rides 130\n",
      "Initial State is  [0, 13, 3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 396, reward 69.0, memory_length 2000, epsilon 0.6722004915521225, time 725.0, rides 122\n",
      "Initial State is  [0, 12, 6]\n",
      "episode 397, reward -98.0, memory_length 2000, epsilon 0.6715282910605703, time 723.0, rides 133\n",
      "Initial State is  [3, 14, 5]\n",
      "episode 398, reward -100.0, memory_length 2000, epsilon 0.6708567627695098, time 728.0, rides 128\n",
      "Initial State is  [0, 21, 0]\n",
      "episode 399, reward -116.0, memory_length 2000, epsilon 0.6701859060067403, time 728.0, rides 124\n",
      "Initial State is  [3, 22, 0]\n",
      "episode 400, reward -52.0, memory_length 2000, epsilon 0.6695157201007336, time 738.0, rides 122\n",
      "Initial State is  [0, 3, 0]\n",
      "episode 401, reward -117.0, memory_length 2000, epsilon 0.6688462043806328, time 730.0, rides 130\n",
      "Initial State is  [1, 18, 2]\n",
      "episode 402, reward -78.0, memory_length 2000, epsilon 0.6681773581762521, time 727.0, rides 129\n",
      "Initial State is  [4, 14, 4]\n",
      "episode 403, reward 11.0, memory_length 2000, epsilon 0.6675091808180759, time 735.0, rides 130\n",
      "Initial State is  [3, 7, 4]\n",
      "episode 404, reward -47.0, memory_length 2000, epsilon 0.6668416716372578, time 731.0, rides 123\n",
      "Initial State is  [4, 19, 0]\n",
      "episode 405, reward -192.0, memory_length 2000, epsilon 0.6661748299656206, time 729.0, rides 111\n",
      "Initial State is  [1, 8, 0]\n",
      "episode 406, reward 290.0, memory_length 2000, epsilon 0.6655086551356549, time 727.0, rides 113\n",
      "Initial State is  [3, 3, 1]\n",
      "episode 407, reward 341.0, memory_length 2000, epsilon 0.6648431464805192, time 732.0, rides 122\n",
      "Initial State is  [4, 22, 1]\n",
      "episode 408, reward -285.0, memory_length 2000, epsilon 0.6641783033340387, time 730.0, rides 119\n",
      "Initial State is  [3, 4, 2]\n",
      "episode 409, reward 112.0, memory_length 2000, epsilon 0.6635141250307047, time 728.0, rides 122\n",
      "Initial State is  [4, 13, 5]\n",
      "episode 410, reward 211.0, memory_length 2000, epsilon 0.662850610905674, time 723.0, rides 130\n",
      "Initial State is  [0, 14, 1]\n",
      "episode 411, reward 258.0, memory_length 2000, epsilon 0.6621877602947683, time 723.0, rides 122\n",
      "Initial State is  [0, 18, 3]\n",
      "episode 412, reward 277.0, memory_length 2000, epsilon 0.6615255725344735, time 730.0, rides 119\n",
      "Initial State is  [3, 1, 1]\n",
      "episode 413, reward 218.0, memory_length 2000, epsilon 0.660864046961939, time 728.0, rides 118\n",
      "Initial State is  [2, 20, 2]\n",
      "episode 414, reward 149.0, memory_length 2000, epsilon 0.660203182914977, time 730.0, rides 115\n",
      "Initial State is  [2, 0, 6]\n",
      "episode 415, reward 267.0, memory_length 2000, epsilon 0.6595429797320621, time 731.0, rides 119\n",
      "Initial State is  [1, 17, 6]\n",
      "episode 416, reward -51.0, memory_length 2000, epsilon 0.6588834367523301, time 727.0, rides 120\n",
      "Initial State is  [0, 15, 4]\n",
      "episode 417, reward 7.0, memory_length 2000, epsilon 0.6582245533155777, time 730.0, rides 126\n",
      "Initial State is  [4, 3, 0]\n",
      "episode 418, reward 232.0, memory_length 2000, epsilon 0.6575663287622622, time 731.0, rides 140\n",
      "Initial State is  [3, 8, 5]\n",
      "episode 419, reward 222.0, memory_length 2000, epsilon 0.6569087624334999, time 728.0, rides 131\n",
      "Initial State is  [4, 20, 3]\n",
      "episode 420, reward 72.0, memory_length 2000, epsilon 0.6562518536710664, time 727.0, rides 111\n",
      "Initial State is  [1, 13, 1]\n",
      "episode 421, reward 73.0, memory_length 2000, epsilon 0.6555956018173954, time 725.0, rides 151\n",
      "Initial State is  [1, 7, 2]\n",
      "episode 422, reward 426.0, memory_length 2000, epsilon 0.654940006215578, time 725.0, rides 127\n",
      "Initial State is  [0, 11, 0]\n",
      "episode 423, reward 279.0, memory_length 2000, epsilon 0.6542850662093624, time 726.0, rides 135\n",
      "Initial State is  [1, 0, 5]\n",
      "episode 424, reward -53.0, memory_length 2000, epsilon 0.6536307811431531, time 741.0, rides 115\n",
      "Initial State is  [4, 8, 1]\n",
      "episode 425, reward -131.0, memory_length 2000, epsilon 0.65297715036201, time 730.0, rides 116\n",
      "Initial State is  [1, 17, 6]\n",
      "episode 426, reward 238.0, memory_length 2000, epsilon 0.6523241732116479, time 731.0, rides 134\n",
      "Initial State is  [1, 1, 2]\n",
      "episode 427, reward 80.0, memory_length 2000, epsilon 0.6516718490384363, time 737.0, rides 134\n",
      "Initial State is  [4, 12, 0]\n",
      "episode 428, reward -86.0, memory_length 2000, epsilon 0.6510201771893979, time 728.0, rides 110\n",
      "Initial State is  [3, 8, 2]\n",
      "episode 429, reward -76.0, memory_length 2000, epsilon 0.6503691570122084, time 740.0, rides 120\n",
      "Initial State is  [4, 21, 2]\n",
      "episode 430, reward 433.0, memory_length 2000, epsilon 0.6497187878551962, time 731.0, rides 119\n",
      "Initial State is  [1, 16, 5]\n",
      "episode 431, reward -126.0, memory_length 2000, epsilon 0.649069069067341, time 730.0, rides 134\n",
      "Initial State is  [1, 8, 1]\n",
      "episode 432, reward -121.0, memory_length 2000, epsilon 0.6484199999982736, time 724.0, rides 127\n",
      "Initial State is  [1, 4, 2]\n",
      "episode 433, reward 324.0, memory_length 2000, epsilon 0.6477715799982753, time 724.0, rides 113\n",
      "Initial State is  [3, 11, 5]\n",
      "episode 434, reward 216.0, memory_length 2000, epsilon 0.647123808418277, time 725.0, rides 130\n",
      "Initial State is  [2, 3, 5]\n",
      "episode 435, reward -20.0, memory_length 2000, epsilon 0.6464766846098587, time 731.0, rides 131\n",
      "Initial State is  [4, 1, 1]\n",
      "episode 436, reward 178.0, memory_length 2000, epsilon 0.6458302079252489, time 736.0, rides 117\n",
      "Initial State is  [2, 1, 0]\n",
      "episode 437, reward -92.0, memory_length 2000, epsilon 0.6451843777173236, time 726.0, rides 113\n",
      "Initial State is  [3, 3, 1]\n",
      "episode 438, reward -342.0, memory_length 2000, epsilon 0.6445391933396063, time 732.0, rides 129\n",
      "Initial State is  [4, 12, 0]\n",
      "episode 439, reward 272.0, memory_length 2000, epsilon 0.6438946541462667, time 728.0, rides 126\n",
      "Initial State is  [0, 8, 0]\n",
      "episode 440, reward 121.0, memory_length 2000, epsilon 0.6432507594921204, time 731.0, rides 116\n",
      "Initial State is  [3, 22, 4]\n",
      "episode 441, reward 83.0, memory_length 2000, epsilon 0.6426075087326283, time 741.0, rides 120\n",
      "Initial State is  [2, 9, 0]\n",
      "episode 442, reward 22.0, memory_length 2000, epsilon 0.6419649012238956, time 726.0, rides 119\n",
      "Initial State is  [2, 22, 5]\n",
      "episode 443, reward 311.0, memory_length 2000, epsilon 0.6413229363226717, time 732.0, rides 132\n",
      "Initial State is  [2, 3, 5]\n",
      "episode 444, reward -21.0, memory_length 2000, epsilon 0.640681613386349, time 729.0, rides 115\n",
      "Initial State is  [4, 18, 4]\n",
      "episode 445, reward 102.0, memory_length 2000, epsilon 0.6400409317729626, time 722.0, rides 121\n",
      "Initial State is  [3, 3, 2]\n",
      "episode 446, reward -312.0, memory_length 2000, epsilon 0.6394008908411897, time 735.0, rides 133\n",
      "Initial State is  [1, 0, 0]\n",
      "episode 447, reward 259.0, memory_length 2000, epsilon 0.6387614899503485, time 725.0, rides 122\n",
      "Initial State is  [3, 4, 6]\n",
      "episode 448, reward 220.0, memory_length 2000, epsilon 0.6381227284603982, time 730.0, rides 125\n",
      "Initial State is  [2, 3, 6]\n",
      "episode 449, reward -252.0, memory_length 2000, epsilon 0.6374846057319378, time 728.0, rides 126\n",
      "Initial State is  [2, 21, 1]\n",
      "episode 450, reward 82.0, memory_length 2000, epsilon 0.6368471211262058, time 734.0, rides 124\n",
      "Initial State is  [3, 18, 4]\n",
      "episode 451, reward 305.0, memory_length 2000, epsilon 0.6362102740050796, time 733.0, rides 120\n",
      "Initial State is  [0, 13, 2]\n",
      "episode 452, reward 143.0, memory_length 2000, epsilon 0.6355740637310745, time 727.0, rides 118\n",
      "Initial State is  [2, 0, 5]\n",
      "episode 453, reward 488.0, memory_length 2000, epsilon 0.6349384896673435, time 724.0, rides 128\n",
      "Initial State is  [0, 1, 2]\n",
      "episode 454, reward 146.0, memory_length 2000, epsilon 0.6343035511776761, time 725.0, rides 114\n",
      "Initial State is  [3, 7, 6]\n",
      "episode 455, reward 389.0, memory_length 2000, epsilon 0.6336692476264985, time 730.0, rides 123\n",
      "Initial State is  [0, 21, 6]\n",
      "episode 456, reward 135.0, memory_length 2000, epsilon 0.6330355783788719, time 729.0, rides 132\n",
      "Initial State is  [3, 10, 0]\n",
      "episode 457, reward 210.0, memory_length 2000, epsilon 0.632402542800493, time 722.0, rides 126\n",
      "Initial State is  [3, 19, 5]\n",
      "episode 458, reward 4.0, memory_length 2000, epsilon 0.6317701402576925, time 730.0, rides 129\n",
      "Initial State is  [4, 11, 0]\n",
      "episode 459, reward 19.0, memory_length 2000, epsilon 0.6311383701174348, time 728.0, rides 120\n",
      "Initial State is  [2, 22, 4]\n",
      "episode 460, reward 95.0, memory_length 2000, epsilon 0.6305072317473174, time 726.0, rides 131\n",
      "Initial State is  [3, 6, 6]\n",
      "episode 461, reward 323.0, memory_length 2000, epsilon 0.6298767245155701, time 732.0, rides 121\n",
      "Initial State is  [4, 11, 6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 462, reward -144.0, memory_length 2000, epsilon 0.6292468477910546, time 731.0, rides 122\n",
      "Initial State is  [4, 3, 5]\n",
      "episode 463, reward 68.0, memory_length 2000, epsilon 0.6286176009432635, time 739.0, rides 114\n",
      "Initial State is  [4, 5, 5]\n",
      "episode 464, reward -59.0, memory_length 2000, epsilon 0.6279889833423202, time 733.0, rides 119\n",
      "Initial State is  [1, 16, 1]\n",
      "episode 465, reward 409.0, memory_length 2000, epsilon 0.6273609943589779, time 723.0, rides 122\n",
      "Initial State is  [2, 9, 6]\n",
      "episode 466, reward -72.0, memory_length 2000, epsilon 0.6267336333646188, time 727.0, rides 118\n",
      "Initial State is  [4, 14, 2]\n",
      "episode 467, reward -74.0, memory_length 2000, epsilon 0.6261068997312542, time 730.0, rides 128\n",
      "Initial State is  [3, 15, 2]\n",
      "episode 468, reward 33.0, memory_length 2000, epsilon 0.6254807928315229, time 730.0, rides 118\n",
      "Initial State is  [0, 1, 2]\n",
      "episode 469, reward -206.0, memory_length 2000, epsilon 0.6248553120386914, time 735.0, rides 121\n",
      "Initial State is  [2, 13, 0]\n",
      "episode 470, reward 241.0, memory_length 2000, epsilon 0.6242304567266527, time 727.0, rides 130\n",
      "Initial State is  [4, 22, 0]\n",
      "episode 471, reward -231.0, memory_length 2000, epsilon 0.623606226269926, time 726.0, rides 120\n",
      "Initial State is  [0, 5, 0]\n",
      "episode 472, reward 17.0, memory_length 2000, epsilon 0.6229826200436561, time 736.0, rides 117\n",
      "Initial State is  [4, 17, 1]\n",
      "episode 473, reward -69.0, memory_length 2000, epsilon 0.6223596374236124, time 726.0, rides 117\n",
      "Initial State is  [3, 22, 0]\n",
      "episode 474, reward 189.0, memory_length 2000, epsilon 0.6217372777861888, time 738.0, rides 121\n",
      "Initial State is  [0, 11, 4]\n",
      "episode 475, reward -126.0, memory_length 2000, epsilon 0.6211155405084027, time 722.0, rides 124\n",
      "Initial State is  [2, 6, 2]\n",
      "episode 476, reward 225.0, memory_length 2000, epsilon 0.6204944249678943, time 736.0, rides 127\n",
      "Initial State is  [3, 3, 4]\n",
      "episode 477, reward 292.0, memory_length 2000, epsilon 0.6198739305429264, time 731.0, rides 128\n",
      "Initial State is  [2, 4, 4]\n",
      "episode 478, reward -85.0, memory_length 2000, epsilon 0.6192540566123834, time 728.0, rides 111\n",
      "Initial State is  [1, 23, 3]\n",
      "episode 479, reward 20.0, memory_length 2000, epsilon 0.6186348025557711, time 734.0, rides 123\n",
      "Initial State is  [2, 2, 6]\n",
      "episode 480, reward 229.0, memory_length 2000, epsilon 0.6180161677532153, time 722.0, rides 119\n",
      "Initial State is  [4, 18, 0]\n",
      "episode 481, reward 141.0, memory_length 2000, epsilon 0.6173981515854621, time 734.0, rides 121\n",
      "Initial State is  [2, 21, 0]\n",
      "episode 482, reward 232.0, memory_length 2000, epsilon 0.6167807534338766, time 733.0, rides 123\n",
      "Initial State is  [1, 1, 3]\n",
      "episode 483, reward 87.0, memory_length 2000, epsilon 0.6161639726804428, time 726.0, rides 126\n",
      "Initial State is  [3, 1, 5]\n",
      "episode 484, reward 206.0, memory_length 2000, epsilon 0.6155478087077623, time 735.0, rides 135\n",
      "Initial State is  [3, 22, 5]\n",
      "episode 485, reward 420.0, memory_length 2000, epsilon 0.6149322608990545, time 728.0, rides 123\n",
      "Initial State is  [3, 22, 6]\n",
      "episode 486, reward 228.0, memory_length 2000, epsilon 0.6143173286381555, time 728.0, rides 128\n",
      "Initial State is  [2, 23, 2]\n",
      "episode 487, reward 155.0, memory_length 2000, epsilon 0.6137030113095173, time 727.0, rides 110\n",
      "Initial State is  [2, 4, 0]\n",
      "episode 488, reward -156.0, memory_length 2000, epsilon 0.6130893082982078, time 731.0, rides 118\n",
      "Initial State is  [2, 22, 6]\n",
      "episode 489, reward 124.0, memory_length 2000, epsilon 0.6124762189899097, time 728.0, rides 125\n",
      "Initial State is  [4, 9, 4]\n",
      "episode 490, reward 261.0, memory_length 2000, epsilon 0.6118637427709198, time 733.0, rides 139\n",
      "Initial State is  [2, 4, 4]\n",
      "episode 491, reward 258.0, memory_length 2000, epsilon 0.6112518790281489, time 740.0, rides 128\n",
      "Initial State is  [3, 12, 4]\n",
      "episode 492, reward 112.0, memory_length 2000, epsilon 0.6106406271491208, time 721.0, rides 117\n",
      "Initial State is  [0, 10, 2]\n",
      "episode 493, reward 91.0, memory_length 2000, epsilon 0.6100299865219717, time 725.0, rides 128\n",
      "Initial State is  [0, 16, 0]\n",
      "episode 494, reward -55.0, memory_length 2000, epsilon 0.6094199565354498, time 723.0, rides 125\n",
      "Initial State is  [2, 9, 2]\n",
      "episode 495, reward 57.0, memory_length 2000, epsilon 0.6088105365789144, time 730.0, rides 124\n",
      "Initial State is  [0, 15, 2]\n",
      "episode 496, reward 164.0, memory_length 2000, epsilon 0.6082017260423355, time 730.0, rides 112\n",
      "Initial State is  [3, 13, 4]\n",
      "episode 497, reward 293.0, memory_length 2000, epsilon 0.6075935243162931, time 737.0, rides 124\n",
      "Initial State is  [2, 7, 4]\n",
      "episode 498, reward 2.0, memory_length 2000, epsilon 0.6069859307919768, time 735.0, rides 125\n",
      "Initial State is  [4, 13, 2]\n",
      "episode 499, reward 99.0, memory_length 2000, epsilon 0.6063789448611848, time 727.0, rides 117\n",
      "Initial State is  [1, 20, 6]\n",
      "episode 500, reward 237.0, memory_length 2000, epsilon 0.6057725659163237, time 731.0, rides 132\n",
      "Initial State is  [0, 14, 4]\n",
      "episode 501, reward 312.0, memory_length 2000, epsilon 0.6051667933504074, time 730.0, rides 128\n",
      "Initial State is  [2, 4, 2]\n",
      "episode 502, reward 409.0, memory_length 2000, epsilon 0.6045616265570569, time 742.0, rides 117\n",
      "Initial State is  [3, 8, 3]\n",
      "episode 503, reward 195.0, memory_length 2000, epsilon 0.6039570649304998, time 727.0, rides 123\n",
      "Initial State is  [1, 9, 1]\n",
      "episode 504, reward 181.0, memory_length 2000, epsilon 0.6033531078655693, time 734.0, rides 125\n",
      "Initial State is  [4, 9, 4]\n",
      "episode 505, reward 273.0, memory_length 2000, epsilon 0.6027497547577038, time 733.0, rides 136\n",
      "Initial State is  [4, 0, 0]\n",
      "episode 506, reward 324.0, memory_length 2000, epsilon 0.602147005002946, time 732.0, rides 128\n",
      "Initial State is  [4, 3, 2]\n",
      "episode 507, reward 97.0, memory_length 2000, epsilon 0.6015448579979431, time 734.0, rides 138\n",
      "Initial State is  [4, 2, 0]\n",
      "episode 508, reward 470.0, memory_length 2000, epsilon 0.6009433131399452, time 729.0, rides 134\n",
      "Initial State is  [4, 14, 1]\n",
      "episode 509, reward 147.0, memory_length 2000, epsilon 0.6003423698268052, time 738.0, rides 120\n",
      "Initial State is  [4, 8, 0]\n",
      "episode 510, reward 386.0, memory_length 2000, epsilon 0.5997420274569785, time 727.0, rides 128\n",
      "Initial State is  [3, 17, 2]\n",
      "episode 511, reward 18.0, memory_length 2000, epsilon 0.5991422854295215, time 723.0, rides 134\n",
      "Initial State is  [4, 5, 3]\n",
      "episode 512, reward -231.0, memory_length 2000, epsilon 0.598543143144092, time 735.0, rides 119\n",
      "Initial State is  [1, 13, 0]\n",
      "episode 513, reward 4.0, memory_length 2000, epsilon 0.5979446000009478, time 729.0, rides 119\n",
      "Initial State is  [1, 14, 4]\n",
      "episode 514, reward 267.0, memory_length 2000, epsilon 0.5973466554009469, time 732.0, rides 135\n",
      "Initial State is  [0, 15, 2]\n",
      "episode 515, reward -112.0, memory_length 2000, epsilon 0.596749308745546, time 733.0, rides 121\n",
      "Initial State is  [1, 7, 5]\n",
      "episode 516, reward 43.0, memory_length 2000, epsilon 0.5961525594368005, time 724.0, rides 120\n",
      "Initial State is  [4, 15, 5]\n",
      "episode 517, reward 177.0, memory_length 2000, epsilon 0.5955564068773637, time 731.0, rides 135\n",
      "Initial State is  [1, 1, 6]\n",
      "episode 518, reward 96.0, memory_length 2000, epsilon 0.5949608504704863, time 722.0, rides 121\n",
      "Initial State is  [1, 5, 4]\n",
      "episode 519, reward -99.0, memory_length 2000, epsilon 0.5943658896200158, time 733.0, rides 140\n",
      "Initial State is  [1, 18, 0]\n",
      "episode 520, reward 58.0, memory_length 2000, epsilon 0.5937715237303958, time 727.0, rides 127\n",
      "Initial State is  [0, 6, 0]\n",
      "episode 521, reward -46.0, memory_length 2000, epsilon 0.5931777522066654, time 728.0, rides 121\n",
      "Initial State is  [0, 15, 1]\n",
      "episode 522, reward 239.0, memory_length 2000, epsilon 0.5925845744544587, time 737.0, rides 117\n",
      "Initial State is  [0, 20, 1]\n",
      "episode 523, reward 150.0, memory_length 2000, epsilon 0.5919919898800042, time 722.0, rides 129\n",
      "Initial State is  [3, 15, 4]\n",
      "episode 524, reward 184.0, memory_length 2000, epsilon 0.5913999978901242, time 726.0, rides 122\n",
      "Initial State is  [1, 19, 6]\n",
      "episode 525, reward 193.0, memory_length 2000, epsilon 0.590808597892234, time 731.0, rides 127\n",
      "Initial State is  [0, 7, 2]\n",
      "episode 526, reward 95.0, memory_length 2000, epsilon 0.5902177892943418, time 730.0, rides 131\n",
      "Initial State is  [1, 23, 3]\n",
      "episode 527, reward 81.0, memory_length 2000, epsilon 0.5896275715050474, time 732.0, rides 127\n",
      "Initial State is  [1, 9, 6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 528, reward 21.0, memory_length 2000, epsilon 0.5890379439335424, time 733.0, rides 130\n",
      "Initial State is  [0, 20, 6]\n",
      "episode 529, reward 497.0, memory_length 2000, epsilon 0.5884489059896089, time 726.0, rides 123\n",
      "Initial State is  [1, 6, 6]\n",
      "episode 530, reward 303.0, memory_length 2000, epsilon 0.5878604570836192, time 734.0, rides 121\n",
      "Initial State is  [1, 16, 6]\n",
      "episode 531, reward -220.0, memory_length 2000, epsilon 0.5872725966265356, time 728.0, rides 109\n",
      "Initial State is  [2, 3, 2]\n",
      "episode 532, reward 97.0, memory_length 2000, epsilon 0.5866853240299091, time 736.0, rides 118\n",
      "Initial State is  [1, 18, 4]\n",
      "episode 533, reward -228.0, memory_length 2000, epsilon 0.5860986387058792, time 725.0, rides 121\n",
      "Initial State is  [0, 13, 2]\n",
      "episode 534, reward 409.0, memory_length 2000, epsilon 0.5855125400671733, time 730.0, rides 130\n",
      "Initial State is  [2, 10, 1]\n",
      "episode 535, reward 152.0, memory_length 2000, epsilon 0.5849270275271061, time 725.0, rides 122\n",
      "Initial State is  [0, 18, 1]\n",
      "episode 536, reward 20.0, memory_length 2000, epsilon 0.584342100499579, time 727.0, rides 118\n",
      "Initial State is  [2, 2, 0]\n",
      "episode 537, reward 371.0, memory_length 2000, epsilon 0.5837577583990794, time 731.0, rides 130\n",
      "Initial State is  [0, 2, 5]\n",
      "episode 538, reward 141.0, memory_length 2000, epsilon 0.5831740006406804, time 729.0, rides 121\n",
      "Initial State is  [3, 9, 1]\n",
      "episode 539, reward 34.0, memory_length 2000, epsilon 0.5825908266400397, time 728.0, rides 130\n",
      "Initial State is  [2, 11, 4]\n",
      "episode 540, reward 179.0, memory_length 2000, epsilon 0.5820082358133997, time 731.0, rides 126\n",
      "Initial State is  [0, 22, 1]\n",
      "episode 541, reward -8.0, memory_length 2000, epsilon 0.5814262275775862, time 736.0, rides 129\n",
      "Initial State is  [3, 8, 2]\n",
      "episode 542, reward -39.0, memory_length 2000, epsilon 0.5808448013500086, time 734.0, rides 122\n",
      "Initial State is  [1, 0, 6]\n",
      "episode 543, reward 85.0, memory_length 2000, epsilon 0.5802639565486586, time 724.0, rides 143\n",
      "Initial State is  [0, 8, 0]\n",
      "episode 544, reward 294.0, memory_length 2000, epsilon 0.57968369259211, time 731.0, rides 118\n",
      "Initial State is  [0, 0, 6]\n",
      "episode 545, reward -110.0, memory_length 2000, epsilon 0.5791040088995179, time 725.0, rides 123\n",
      "Initial State is  [1, 16, 6]\n",
      "episode 546, reward -95.0, memory_length 2000, epsilon 0.5785249048906184, time 734.0, rides 134\n",
      "Initial State is  [1, 18, 5]\n",
      "episode 547, reward 216.0, memory_length 2000, epsilon 0.5779463799857277, time 722.0, rides 123\n",
      "Initial State is  [1, 23, 5]\n",
      "episode 548, reward 129.0, memory_length 2000, epsilon 0.577368433605742, time 737.0, rides 125\n",
      "Initial State is  [0, 17, 0]\n",
      "episode 549, reward 151.0, memory_length 2000, epsilon 0.5767910651721362, time 722.0, rides 125\n",
      "Initial State is  [4, 7, 6]\n",
      "episode 550, reward -100.0, memory_length 2000, epsilon 0.576214274106964, time 734.0, rides 124\n",
      "Initial State is  [1, 8, 6]\n",
      "episode 551, reward 555.0, memory_length 2000, epsilon 0.5756380598328571, time 728.0, rides 127\n",
      "Initial State is  [3, 18, 6]\n",
      "episode 552, reward 113.0, memory_length 2000, epsilon 0.5750624217730242, time 731.0, rides 138\n",
      "Initial State is  [1, 1, 0]\n",
      "episode 553, reward 179.0, memory_length 2000, epsilon 0.5744873593512512, time 735.0, rides 132\n",
      "Initial State is  [3, 21, 0]\n",
      "episode 554, reward 582.0, memory_length 2000, epsilon 0.5739128719918999, time 730.0, rides 132\n",
      "Initial State is  [3, 2, 2]\n",
      "episode 555, reward 157.0, memory_length 2000, epsilon 0.573338959119908, time 725.0, rides 118\n",
      "Initial State is  [3, 13, 1]\n",
      "episode 556, reward 606.0, memory_length 2000, epsilon 0.572765620160788, time 729.0, rides 129\n",
      "Initial State is  [0, 1, 2]\n",
      "episode 557, reward 55.0, memory_length 2000, epsilon 0.5721928545406272, time 733.0, rides 132\n",
      "Initial State is  [0, 19, 6]\n",
      "episode 558, reward 216.0, memory_length 2000, epsilon 0.5716206616860866, time 726.0, rides 121\n",
      "Initial State is  [1, 23, 2]\n",
      "episode 559, reward 421.0, memory_length 2000, epsilon 0.5710490410244006, time 724.0, rides 123\n",
      "Initial State is  [4, 19, 5]\n",
      "episode 560, reward 196.0, memory_length 2000, epsilon 0.5704779919833761, time 739.0, rides 123\n",
      "Initial State is  [1, 11, 0]\n",
      "episode 561, reward 118.0, memory_length 2000, epsilon 0.5699075139913927, time 733.0, rides 116\n",
      "Initial State is  [1, 18, 2]\n",
      "episode 562, reward -108.0, memory_length 2000, epsilon 0.5693376064774013, time 726.0, rides 138\n",
      "Initial State is  [1, 6, 0]\n",
      "episode 563, reward 322.0, memory_length 2000, epsilon 0.5687682688709239, time 730.0, rides 140\n",
      "Initial State is  [0, 18, 1]\n",
      "episode 564, reward 191.0, memory_length 2000, epsilon 0.5681995006020529, time 724.0, rides 130\n",
      "Initial State is  [1, 14, 5]\n",
      "episode 565, reward 157.0, memory_length 2000, epsilon 0.5676313011014509, time 730.0, rides 126\n",
      "Initial State is  [2, 13, 6]\n",
      "episode 566, reward 199.0, memory_length 2000, epsilon 0.5670636698003494, time 731.0, rides 132\n",
      "Initial State is  [3, 1, 4]\n",
      "episode 567, reward 314.0, memory_length 2000, epsilon 0.5664966061305491, time 726.0, rides 123\n",
      "Initial State is  [1, 5, 2]\n",
      "episode 568, reward -144.0, memory_length 2000, epsilon 0.5659301095244186, time 723.0, rides 126\n",
      "Initial State is  [2, 5, 1]\n",
      "episode 569, reward 239.0, memory_length 2000, epsilon 0.5653641794148941, time 723.0, rides 120\n",
      "Initial State is  [4, 9, 3]\n",
      "episode 570, reward 339.0, memory_length 2000, epsilon 0.5647988152354793, time 725.0, rides 127\n",
      "Initial State is  [0, 7, 3]\n",
      "episode 571, reward -188.0, memory_length 2000, epsilon 0.5642340164202437, time 725.0, rides 118\n",
      "Initial State is  [2, 16, 1]\n",
      "episode 572, reward 322.0, memory_length 2000, epsilon 0.5636697824038235, time 727.0, rides 109\n",
      "Initial State is  [2, 16, 1]\n",
      "episode 573, reward 173.0, memory_length 2000, epsilon 0.5631061126214196, time 729.0, rides 127\n",
      "Initial State is  [3, 12, 4]\n",
      "episode 574, reward 310.0, memory_length 2000, epsilon 0.5625430065087982, time 726.0, rides 128\n",
      "Initial State is  [0, 18, 6]\n",
      "episode 575, reward 180.0, memory_length 2000, epsilon 0.5619804635022894, time 730.0, rides 127\n",
      "Initial State is  [2, 18, 5]\n",
      "episode 576, reward 189.0, memory_length 2000, epsilon 0.561418483038787, time 729.0, rides 119\n",
      "Initial State is  [1, 11, 6]\n",
      "episode 577, reward -21.0, memory_length 2000, epsilon 0.5608570645557482, time 737.0, rides 134\n",
      "Initial State is  [4, 18, 4]\n",
      "episode 578, reward 245.0, memory_length 2000, epsilon 0.5602962074911925, time 731.0, rides 127\n",
      "Initial State is  [0, 19, 1]\n",
      "episode 579, reward 365.0, memory_length 2000, epsilon 0.5597359112837013, time 729.0, rides 129\n",
      "Initial State is  [2, 1, 5]\n",
      "episode 580, reward 7.0, memory_length 2000, epsilon 0.5591761753724176, time 732.0, rides 123\n",
      "Initial State is  [3, 17, 4]\n",
      "episode 581, reward 40.0, memory_length 2000, epsilon 0.5586169991970452, time 724.0, rides 119\n",
      "Initial State is  [1, 9, 5]\n",
      "episode 582, reward 119.0, memory_length 2000, epsilon 0.5580583821978482, time 730.0, rides 132\n",
      "Initial State is  [0, 16, 3]\n",
      "episode 583, reward 135.0, memory_length 2000, epsilon 0.5575003238156504, time 727.0, rides 113\n",
      "Initial State is  [4, 20, 3]\n",
      "episode 584, reward 129.0, memory_length 2000, epsilon 0.5569428234918348, time 737.0, rides 129\n",
      "Initial State is  [3, 5, 3]\n",
      "episode 585, reward -113.0, memory_length 2000, epsilon 0.5563858806683429, time 740.0, rides 123\n",
      "Initial State is  [4, 20, 5]\n",
      "episode 586, reward 135.0, memory_length 2000, epsilon 0.5558294947876746, time 731.0, rides 134\n",
      "Initial State is  [0, 2, 3]\n",
      "episode 587, reward -33.0, memory_length 2000, epsilon 0.5552736652928869, time 733.0, rides 120\n",
      "Initial State is  [1, 20, 3]\n",
      "episode 588, reward 374.0, memory_length 2000, epsilon 0.5547183916275941, time 727.0, rides 140\n",
      "Initial State is  [1, 5, 4]\n",
      "episode 589, reward -52.0, memory_length 2000, epsilon 0.5541636732359665, time 723.0, rides 141\n",
      "Initial State is  [1, 11, 0]\n",
      "episode 590, reward 225.0, memory_length 2000, epsilon 0.5536095095627305, time 731.0, rides 137\n",
      "Initial State is  [0, 20, 3]\n",
      "episode 591, reward 106.0, memory_length 2000, epsilon 0.5530559000531677, time 724.0, rides 134\n",
      "Initial State is  [4, 13, 5]\n",
      "episode 592, reward 182.0, memory_length 2000, epsilon 0.5525028441531146, time 726.0, rides 131\n",
      "Initial State is  [4, 23, 2]\n",
      "episode 593, reward 60.0, memory_length 2000, epsilon 0.5519503413089615, time 724.0, rides 125\n",
      "Initial State is  [3, 11, 1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 594, reward 171.0, memory_length 2000, epsilon 0.5513983909676525, time 737.0, rides 124\n",
      "Initial State is  [4, 20, 6]\n",
      "episode 595, reward -21.0, memory_length 2000, epsilon 0.5508469925766849, time 730.0, rides 131\n",
      "Initial State is  [3, 19, 4]\n",
      "episode 596, reward 265.0, memory_length 2000, epsilon 0.5502961455841082, time 724.0, rides 122\n",
      "Initial State is  [1, 1, 5]\n",
      "episode 597, reward 15.0, memory_length 2000, epsilon 0.5497458494385241, time 729.0, rides 125\n",
      "Initial State is  [3, 8, 4]\n",
      "episode 598, reward 278.0, memory_length 2000, epsilon 0.5491961035890855, time 737.0, rides 121\n",
      "Initial State is  [3, 0, 3]\n",
      "episode 599, reward 35.0, memory_length 2000, epsilon 0.5486469074854965, time 729.0, rides 135\n",
      "Initial State is  [4, 20, 3]\n",
      "episode 600, reward 382.0, memory_length 2000, epsilon 0.548098260578011, time 732.0, rides 131\n",
      "Initial State is  [0, 20, 3]\n",
      "episode 601, reward 58.0, memory_length 2000, epsilon 0.547550162317433, time 736.0, rides 123\n",
      "Initial State is  [3, 1, 0]\n",
      "episode 602, reward 291.0, memory_length 2000, epsilon 0.5470026121551156, time 727.0, rides 132\n",
      "Initial State is  [0, 9, 1]\n",
      "episode 603, reward -107.0, memory_length 2000, epsilon 0.5464556095429605, time 726.0, rides 113\n",
      "Initial State is  [3, 11, 1]\n",
      "episode 604, reward 540.0, memory_length 2000, epsilon 0.5459091539334175, time 733.0, rides 122\n",
      "Initial State is  [1, 13, 1]\n",
      "episode 605, reward 78.0, memory_length 2000, epsilon 0.5453632447794842, time 731.0, rides 129\n",
      "Initial State is  [0, 11, 5]\n",
      "episode 606, reward 304.0, memory_length 2000, epsilon 0.5448178815347047, time 725.0, rides 121\n",
      "Initial State is  [0, 11, 5]\n",
      "episode 607, reward 236.0, memory_length 2000, epsilon 0.54427306365317, time 723.0, rides 117\n",
      "Initial State is  [3, 8, 4]\n",
      "episode 608, reward 164.0, memory_length 2000, epsilon 0.5437287905895168, time 725.0, rides 125\n",
      "Initial State is  [1, 16, 2]\n",
      "episode 609, reward 165.0, memory_length 2000, epsilon 0.5431850617989273, time 732.0, rides 135\n",
      "Initial State is  [3, 20, 0]\n",
      "episode 610, reward 269.0, memory_length 2000, epsilon 0.5426418767371284, time 722.0, rides 136\n",
      "Initial State is  [0, 11, 5]\n",
      "episode 611, reward 339.0, memory_length 2000, epsilon 0.5420992348603912, time 722.0, rides 132\n",
      "Initial State is  [4, 5, 5]\n",
      "episode 612, reward 36.0, memory_length 2000, epsilon 0.5415571356255309, time 731.0, rides 143\n",
      "Initial State is  [2, 21, 2]\n",
      "episode 613, reward 307.0, memory_length 2000, epsilon 0.5410155784899053, time 737.0, rides 135\n",
      "Initial State is  [2, 22, 0]\n",
      "episode 614, reward 400.0, memory_length 2000, epsilon 0.5404745629114154, time 735.0, rides 139\n",
      "Initial State is  [2, 7, 3]\n",
      "episode 615, reward 377.0, memory_length 2000, epsilon 0.539934088348504, time 734.0, rides 136\n",
      "Initial State is  [2, 9, 5]\n",
      "episode 616, reward 40.0, memory_length 2000, epsilon 0.5393941542601555, time 728.0, rides 118\n",
      "Initial State is  [4, 1, 2]\n",
      "episode 617, reward 141.0, memory_length 2000, epsilon 0.5388547601058953, time 729.0, rides 125\n",
      "Initial State is  [4, 22, 0]\n",
      "episode 618, reward 387.0, memory_length 2000, epsilon 0.5383159053457894, time 728.0, rides 130\n",
      "Initial State is  [3, 5, 4]\n",
      "episode 619, reward 301.0, memory_length 2000, epsilon 0.5377775894404436, time 733.0, rides 129\n",
      "Initial State is  [3, 14, 1]\n",
      "episode 620, reward 59.0, memory_length 2000, epsilon 0.5372398118510032, time 734.0, rides 117\n",
      "Initial State is  [1, 13, 4]\n",
      "episode 621, reward 153.0, memory_length 2000, epsilon 0.5367025720391522, time 726.0, rides 101\n",
      "Initial State is  [2, 17, 6]\n",
      "episode 622, reward 529.0, memory_length 2000, epsilon 0.536165869467113, time 722.0, rides 118\n",
      "Initial State is  [2, 20, 0]\n",
      "episode 623, reward 21.0, memory_length 2000, epsilon 0.5356297035976458, time 725.0, rides 127\n",
      "Initial State is  [1, 15, 1]\n",
      "episode 624, reward 224.0, memory_length 2000, epsilon 0.5350940738940482, time 725.0, rides 133\n",
      "Initial State is  [3, 5, 6]\n",
      "episode 625, reward 181.0, memory_length 2000, epsilon 0.5345589798201541, time 731.0, rides 133\n",
      "Initial State is  [3, 1, 1]\n",
      "episode 626, reward 113.0, memory_length 2000, epsilon 0.534024420840334, time 728.0, rides 113\n",
      "Initial State is  [1, 9, 5]\n",
      "episode 627, reward 345.0, memory_length 2000, epsilon 0.5334903964194936, time 735.0, rides 120\n",
      "Initial State is  [1, 5, 0]\n",
      "episode 628, reward 296.0, memory_length 2000, epsilon 0.5329569060230741, time 728.0, rides 113\n",
      "Initial State is  [1, 3, 5]\n",
      "episode 629, reward -12.0, memory_length 2000, epsilon 0.532423949117051, time 729.0, rides 124\n",
      "Initial State is  [3, 7, 0]\n",
      "episode 630, reward 95.0, memory_length 2000, epsilon 0.531891525167934, time 733.0, rides 125\n",
      "Initial State is  [2, 17, 1]\n",
      "episode 631, reward 102.0, memory_length 2000, epsilon 0.5313596336427661, time 731.0, rides 123\n",
      "Initial State is  [1, 17, 1]\n",
      "episode 632, reward 340.0, memory_length 2000, epsilon 0.5308282740091232, time 724.0, rides 130\n",
      "Initial State is  [0, 11, 3]\n",
      "episode 633, reward 198.0, memory_length 2000, epsilon 0.5302974457351142, time 741.0, rides 140\n",
      "Initial State is  [0, 4, 4]\n",
      "episode 634, reward 13.0, memory_length 2000, epsilon 0.5297671482893791, time 736.0, rides 122\n",
      "Initial State is  [1, 20, 4]\n",
      "episode 635, reward 146.0, memory_length 2000, epsilon 0.5292373811410898, time 733.0, rides 109\n",
      "Initial State is  [3, 4, 6]\n",
      "episode 636, reward 161.0, memory_length 2000, epsilon 0.5287081437599487, time 737.0, rides 121\n",
      "Initial State is  [3, 1, 4]\n",
      "episode 637, reward -39.0, memory_length 2000, epsilon 0.5281794356161887, time 736.0, rides 136\n",
      "Initial State is  [0, 16, 2]\n",
      "episode 638, reward 74.0, memory_length 2000, epsilon 0.5276512561805725, time 728.0, rides 112\n",
      "Initial State is  [4, 6, 2]\n",
      "episode 639, reward 258.0, memory_length 2000, epsilon 0.5271236049243919, time 733.0, rides 123\n",
      "Initial State is  [3, 5, 0]\n",
      "episode 640, reward -23.0, memory_length 2000, epsilon 0.5265964813194676, time 740.0, rides 129\n",
      "Initial State is  [0, 14, 3]\n",
      "episode 641, reward 65.0, memory_length 2000, epsilon 0.5260698848381481, time 724.0, rides 125\n",
      "Initial State is  [0, 7, 6]\n",
      "episode 642, reward 259.0, memory_length 2000, epsilon 0.5255438149533099, time 729.0, rides 132\n",
      "Initial State is  [3, 2, 3]\n",
      "episode 643, reward 328.0, memory_length 2000, epsilon 0.5250182711383566, time 726.0, rides 133\n",
      "Initial State is  [2, 12, 6]\n",
      "episode 644, reward 293.0, memory_length 2000, epsilon 0.5244932528672183, time 737.0, rides 128\n",
      "Initial State is  [4, 2, 5]\n",
      "episode 645, reward -14.0, memory_length 2000, epsilon 0.5239687596143511, time 727.0, rides 131\n",
      "Initial State is  [0, 8, 0]\n",
      "episode 646, reward 6.0, memory_length 2000, epsilon 0.5234447908547367, time 725.0, rides 129\n",
      "Initial State is  [1, 0, 2]\n",
      "episode 647, reward 287.0, memory_length 2000, epsilon 0.522921346063882, time 728.0, rides 120\n",
      "Initial State is  [2, 3, 5]\n",
      "episode 648, reward 371.0, memory_length 2000, epsilon 0.522398424717818, time 730.0, rides 121\n",
      "Initial State is  [0, 21, 5]\n",
      "episode 649, reward 163.0, memory_length 2000, epsilon 0.5218760262931003, time 735.0, rides 131\n",
      "Initial State is  [4, 9, 5]\n",
      "episode 650, reward 117.0, memory_length 2000, epsilon 0.5213541502668072, time 739.0, rides 129\n",
      "Initial State is  [3, 5, 6]\n",
      "episode 651, reward 222.0, memory_length 2000, epsilon 0.5208327961165404, time 725.0, rides 123\n",
      "Initial State is  [0, 11, 6]\n",
      "episode 652, reward 243.0, memory_length 2000, epsilon 0.5203119633204238, time 722.0, rides 118\n",
      "Initial State is  [1, 1, 5]\n",
      "episode 653, reward 53.0, memory_length 2000, epsilon 0.5197916513571034, time 727.0, rides 122\n",
      "Initial State is  [4, 3, 6]\n",
      "episode 654, reward 512.0, memory_length 2000, epsilon 0.5192718597057463, time 724.0, rides 133\n",
      "Initial State is  [2, 11, 1]\n",
      "episode 655, reward 255.0, memory_length 2000, epsilon 0.5187525878460405, time 731.0, rides 126\n",
      "Initial State is  [3, 11, 2]\n",
      "episode 656, reward 115.0, memory_length 2000, epsilon 0.5182338352581944, time 726.0, rides 135\n",
      "Initial State is  [2, 21, 6]\n",
      "episode 657, reward 96.0, memory_length 2000, epsilon 0.5177156014229363, time 725.0, rides 122\n",
      "Initial State is  [1, 3, 5]\n",
      "episode 658, reward 127.0, memory_length 2000, epsilon 0.5171978858215134, time 723.0, rides 124\n",
      "Initial State is  [1, 14, 4]\n",
      "episode 659, reward 474.0, memory_length 2000, epsilon 0.5166806879356919, time 727.0, rides 135\n",
      "Initial State is  [2, 21, 5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 660, reward 27.0, memory_length 2000, epsilon 0.5161640072477562, time 734.0, rides 131\n",
      "Initial State is  [2, 17, 3]\n",
      "episode 661, reward -115.0, memory_length 2000, epsilon 0.5156478432405085, time 732.0, rides 125\n",
      "Initial State is  [4, 5, 1]\n",
      "episode 662, reward 256.0, memory_length 2000, epsilon 0.515132195397268, time 727.0, rides 124\n",
      "Initial State is  [3, 9, 1]\n",
      "episode 663, reward 220.0, memory_length 2000, epsilon 0.5146170632018707, time 723.0, rides 118\n",
      "Initial State is  [1, 13, 1]\n",
      "episode 664, reward -25.0, memory_length 2000, epsilon 0.5141024461386688, time 735.0, rides 134\n",
      "Initial State is  [2, 0, 6]\n",
      "episode 665, reward 394.0, memory_length 2000, epsilon 0.5135883436925301, time 729.0, rides 127\n",
      "Initial State is  [2, 9, 3]\n",
      "episode 666, reward -161.0, memory_length 2000, epsilon 0.5130747553488376, time 728.0, rides 114\n",
      "Initial State is  [1, 6, 3]\n",
      "episode 667, reward 526.0, memory_length 2000, epsilon 0.5125616805934888, time 735.0, rides 131\n",
      "Initial State is  [0, 12, 0]\n",
      "episode 668, reward 451.0, memory_length 2000, epsilon 0.5120491189128954, time 725.0, rides 134\n",
      "Initial State is  [0, 13, 1]\n",
      "episode 669, reward 317.0, memory_length 2000, epsilon 0.5115370697939825, time 728.0, rides 124\n",
      "Initial State is  [0, 4, 6]\n",
      "episode 670, reward 376.0, memory_length 2000, epsilon 0.5110255327241885, time 735.0, rides 122\n",
      "Initial State is  [4, 18, 2]\n",
      "episode 671, reward 321.0, memory_length 2000, epsilon 0.5105145071914643, time 736.0, rides 133\n",
      "Initial State is  [3, 1, 1]\n",
      "episode 672, reward 362.0, memory_length 2000, epsilon 0.5100039926842729, time 729.0, rides 127\n",
      "Initial State is  [2, 10, 3]\n",
      "episode 673, reward 142.0, memory_length 2000, epsilon 0.5094939886915886, time 732.0, rides 129\n",
      "Initial State is  [2, 9, 2]\n",
      "episode 674, reward 124.0, memory_length 2000, epsilon 0.508984494702897, time 742.0, rides 133\n",
      "Initial State is  [3, 8, 4]\n",
      "episode 675, reward 261.0, memory_length 2000, epsilon 0.508475510208194, time 724.0, rides 136\n",
      "Initial State is  [1, 6, 1]\n",
      "episode 676, reward -119.0, memory_length 2000, epsilon 0.5079670346979859, time 732.0, rides 123\n",
      "Initial State is  [4, 21, 5]\n",
      "episode 677, reward 373.0, memory_length 2000, epsilon 0.5074590676632879, time 727.0, rides 114\n",
      "Initial State is  [0, 7, 5]\n",
      "episode 678, reward 14.0, memory_length 2000, epsilon 0.5069516085956246, time 727.0, rides 127\n",
      "Initial State is  [4, 5, 5]\n",
      "episode 679, reward 164.0, memory_length 2000, epsilon 0.506444656987029, time 722.0, rides 133\n",
      "Initial State is  [3, 15, 0]\n",
      "episode 680, reward 333.0, memory_length 2000, epsilon 0.505938212330042, time 729.0, rides 136\n",
      "Initial State is  [2, 22, 2]\n",
      "episode 681, reward 209.0, memory_length 2000, epsilon 0.505432274117712, time 725.0, rides 130\n",
      "Initial State is  [2, 14, 3]\n",
      "episode 682, reward 176.0, memory_length 2000, epsilon 0.5049268418435943, time 727.0, rides 129\n",
      "Initial State is  [0, 13, 5]\n",
      "episode 683, reward -25.0, memory_length 2000, epsilon 0.5044219150017507, time 728.0, rides 137\n",
      "Initial State is  [3, 3, 6]\n",
      "episode 684, reward 258.0, memory_length 2000, epsilon 0.5039174930867489, time 730.0, rides 123\n",
      "Initial State is  [0, 2, 4]\n",
      "episode 685, reward 123.0, memory_length 2000, epsilon 0.5034135755936622, time 733.0, rides 127\n",
      "Initial State is  [3, 5, 0]\n",
      "episode 686, reward 441.0, memory_length 2000, epsilon 0.5029101620180685, time 732.0, rides 124\n",
      "Initial State is  [3, 8, 0]\n",
      "episode 687, reward 7.0, memory_length 2000, epsilon 0.5024072518560504, time 729.0, rides 124\n",
      "Initial State is  [0, 1, 1]\n",
      "episode 688, reward 188.0, memory_length 2000, epsilon 0.5019048446041944, time 730.0, rides 129\n",
      "Initial State is  [1, 18, 2]\n",
      "episode 689, reward 162.0, memory_length 2000, epsilon 0.5014029397595902, time 729.0, rides 134\n",
      "Initial State is  [0, 0, 4]\n",
      "episode 690, reward -18.0, memory_length 2000, epsilon 0.5009015368198305, time 730.0, rides 128\n",
      "Initial State is  [3, 20, 3]\n",
      "episode 691, reward 455.0, memory_length 2000, epsilon 0.5004006352830107, time 729.0, rides 124\n",
      "Initial State is  [1, 13, 3]\n",
      "episode 692, reward 391.0, memory_length 2000, epsilon 0.4999002346477277, time 733.0, rides 134\n",
      "Initial State is  [4, 14, 1]\n",
      "episode 693, reward 411.0, memory_length 2000, epsilon 0.49940033441307996, time 733.0, rides 125\n",
      "Initial State is  [4, 10, 5]\n",
      "episode 694, reward 163.0, memory_length 2000, epsilon 0.49890093407866687, time 723.0, rides 110\n",
      "Initial State is  [3, 9, 5]\n",
      "episode 695, reward 88.0, memory_length 2000, epsilon 0.4984020331445882, time 726.0, rides 122\n",
      "Initial State is  [3, 3, 4]\n",
      "episode 696, reward 188.0, memory_length 2000, epsilon 0.4979036311114436, time 728.0, rides 135\n",
      "Initial State is  [4, 4, 6]\n",
      "episode 697, reward 451.0, memory_length 2000, epsilon 0.4974057274803321, time 732.0, rides 125\n",
      "Initial State is  [0, 8, 2]\n",
      "episode 698, reward 436.0, memory_length 2000, epsilon 0.49690832175285177, time 734.0, rides 128\n",
      "Initial State is  [2, 4, 6]\n",
      "episode 699, reward -94.0, memory_length 2000, epsilon 0.4964114134310989, time 722.0, rides 135\n",
      "Initial State is  [4, 16, 5]\n",
      "episode 700, reward 258.0, memory_length 2000, epsilon 0.4959150020176678, time 732.0, rides 119\n",
      "Initial State is  [2, 19, 2]\n",
      "episode 701, reward 400.0, memory_length 2000, epsilon 0.49541908701565013, time 732.0, rides 135\n",
      "Initial State is  [1, 21, 3]\n",
      "episode 702, reward 30.0, memory_length 2000, epsilon 0.49492366792863446, time 730.0, rides 129\n",
      "Initial State is  [3, 18, 0]\n",
      "episode 703, reward 21.0, memory_length 2000, epsilon 0.4944287442607058, time 728.0, rides 145\n",
      "Initial State is  [2, 19, 0]\n",
      "episode 704, reward 328.0, memory_length 2000, epsilon 0.4939343155164451, time 729.0, rides 122\n",
      "Initial State is  [4, 14, 6]\n",
      "episode 705, reward 224.0, memory_length 2000, epsilon 0.4934403812009287, time 725.0, rides 127\n",
      "Initial State is  [1, 13, 1]\n",
      "episode 706, reward 280.0, memory_length 2000, epsilon 0.4929469408197278, time 735.0, rides 117\n",
      "Initial State is  [4, 16, 5]\n",
      "episode 707, reward 125.0, memory_length 2000, epsilon 0.49245399387890804, time 725.0, rides 157\n",
      "Initial State is  [2, 12, 3]\n",
      "episode 708, reward 167.0, memory_length 2000, epsilon 0.4919615398850291, time 726.0, rides 129\n",
      "Initial State is  [0, 7, 0]\n",
      "episode 709, reward 289.0, memory_length 2000, epsilon 0.4914695783451441, time 729.0, rides 140\n",
      "Initial State is  [0, 1, 2]\n",
      "episode 710, reward 553.0, memory_length 2000, epsilon 0.4909781087667989, time 735.0, rides 119\n",
      "Initial State is  [2, 23, 1]\n",
      "episode 711, reward 437.0, memory_length 2000, epsilon 0.4904871306580321, time 721.0, rides 139\n",
      "Initial State is  [1, 10, 0]\n",
      "episode 712, reward 218.0, memory_length 2000, epsilon 0.4899966435273741, time 729.0, rides 131\n",
      "Initial State is  [1, 16, 4]\n",
      "episode 713, reward 454.0, memory_length 2000, epsilon 0.4895066468838467, time 732.0, rides 135\n",
      "Initial State is  [0, 21, 3]\n",
      "episode 714, reward 229.0, memory_length 2000, epsilon 0.48901714023696286, time 735.0, rides 122\n",
      "Initial State is  [3, 12, 2]\n",
      "episode 715, reward 90.0, memory_length 2000, epsilon 0.4885281230967259, time 735.0, rides 127\n",
      "Initial State is  [3, 20, 6]\n",
      "episode 716, reward 523.0, memory_length 2000, epsilon 0.48803959497362914, time 728.0, rides 141\n",
      "Initial State is  [2, 23, 2]\n",
      "episode 717, reward 409.0, memory_length 2000, epsilon 0.4875515553786555, time 725.0, rides 121\n",
      "Initial State is  [1, 3, 1]\n",
      "episode 718, reward 230.0, memory_length 2000, epsilon 0.48706400382327686, time 727.0, rides 121\n",
      "Initial State is  [4, 21, 2]\n",
      "episode 719, reward 321.0, memory_length 2000, epsilon 0.4865769398194536, time 735.0, rides 118\n",
      "Initial State is  [4, 3, 5]\n",
      "episode 720, reward 80.0, memory_length 2000, epsilon 0.48609036287963414, time 727.0, rides 122\n",
      "Initial State is  [0, 19, 3]\n",
      "episode 721, reward 249.0, memory_length 2000, epsilon 0.48560427251675453, time 730.0, rides 134\n",
      "Initial State is  [1, 7, 1]\n",
      "episode 722, reward 158.0, memory_length 2000, epsilon 0.48511866824423777, time 724.0, rides 124\n",
      "Initial State is  [1, 3, 2]\n",
      "episode 723, reward 235.0, memory_length 2000, epsilon 0.4846335495759935, time 724.0, rides 125\n",
      "Initial State is  [3, 10, 1]\n",
      "episode 724, reward 238.0, memory_length 2000, epsilon 0.4841489160264175, time 724.0, rides 119\n",
      "Initial State is  [1, 13, 5]\n",
      "episode 725, reward 330.0, memory_length 2000, epsilon 0.4836647671103911, time 724.0, rides 116\n",
      "Initial State is  [3, 11, 1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 726, reward 189.0, memory_length 2000, epsilon 0.4831811023432807, time 728.0, rides 125\n",
      "Initial State is  [1, 4, 6]\n",
      "episode 727, reward 181.0, memory_length 2000, epsilon 0.4826979212409374, time 727.0, rides 128\n",
      "Initial State is  [1, 17, 4]\n",
      "episode 728, reward 157.0, memory_length 2000, epsilon 0.48221522331969646, time 725.0, rides 115\n",
      "Initial State is  [0, 22, 0]\n",
      "episode 729, reward 109.0, memory_length 2000, epsilon 0.48173300809637676, time 725.0, rides 125\n",
      "Initial State is  [2, 22, 5]\n",
      "episode 730, reward 515.0, memory_length 2000, epsilon 0.48125127508828036, time 728.0, rides 120\n",
      "Initial State is  [1, 3, 6]\n",
      "episode 731, reward 204.0, memory_length 2000, epsilon 0.48077002381319206, time 737.0, rides 126\n",
      "Initial State is  [4, 18, 0]\n",
      "episode 732, reward 245.0, memory_length 2000, epsilon 0.48028925378937887, time 731.0, rides 124\n",
      "Initial State is  [3, 4, 0]\n",
      "episode 733, reward 461.0, memory_length 2000, epsilon 0.47980896453558947, time 729.0, rides 134\n",
      "Initial State is  [0, 11, 3]\n",
      "episode 734, reward 407.0, memory_length 2000, epsilon 0.4793291555710539, time 726.0, rides 114\n",
      "Initial State is  [2, 0, 4]\n",
      "episode 735, reward 142.0, memory_length 2000, epsilon 0.47884982641548285, time 723.0, rides 127\n",
      "Initial State is  [3, 10, 5]\n",
      "episode 736, reward 19.0, memory_length 2000, epsilon 0.47837097658906735, time 726.0, rides 117\n",
      "Initial State is  [3, 12, 5]\n",
      "episode 737, reward 294.0, memory_length 2000, epsilon 0.47789260561247826, time 730.0, rides 129\n",
      "Initial State is  [3, 3, 6]\n",
      "episode 738, reward 173.0, memory_length 2000, epsilon 0.4774147130068658, time 728.0, rides 132\n",
      "Initial State is  [4, 3, 0]\n",
      "episode 739, reward 168.0, memory_length 2000, epsilon 0.4769372982938589, time 727.0, rides 129\n",
      "Initial State is  [1, 9, 2]\n",
      "episode 740, reward 97.0, memory_length 2000, epsilon 0.47646036099556505, time 734.0, rides 115\n",
      "Initial State is  [2, 18, 2]\n",
      "episode 741, reward 133.0, memory_length 2000, epsilon 0.47598390063456947, time 724.0, rides 113\n",
      "Initial State is  [2, 1, 3]\n",
      "episode 742, reward 406.0, memory_length 2000, epsilon 0.4755079167339349, time 723.0, rides 125\n",
      "Initial State is  [0, 17, 3]\n",
      "episode 743, reward 461.0, memory_length 2000, epsilon 0.47503240881720094, time 729.0, rides 124\n",
      "Initial State is  [4, 4, 2]\n",
      "episode 744, reward 154.0, memory_length 2000, epsilon 0.47455737640838375, time 726.0, rides 119\n",
      "Initial State is  [1, 21, 0]\n",
      "episode 745, reward 277.0, memory_length 2000, epsilon 0.47408281903197536, time 727.0, rides 141\n",
      "Initial State is  [1, 10, 5]\n",
      "episode 746, reward 166.0, memory_length 2000, epsilon 0.47360873621294336, time 731.0, rides 131\n",
      "Initial State is  [3, 3, 1]\n",
      "episode 747, reward 439.0, memory_length 2000, epsilon 0.4731351274767304, time 736.0, rides 117\n",
      "Initial State is  [2, 10, 4]\n",
      "episode 748, reward 2.0, memory_length 2000, epsilon 0.4726619923492537, time 732.0, rides 124\n",
      "Initial State is  [4, 12, 6]\n",
      "episode 749, reward -145.0, memory_length 2000, epsilon 0.47218933035690447, time 725.0, rides 135\n",
      "Initial State is  [2, 2, 2]\n",
      "episode 750, reward 237.0, memory_length 2000, epsilon 0.47171714102654755, time 724.0, rides 127\n",
      "Initial State is  [1, 7, 3]\n",
      "episode 751, reward 192.0, memory_length 2000, epsilon 0.471245423885521, time 722.0, rides 119\n",
      "Initial State is  [2, 13, 6]\n",
      "episode 752, reward 389.0, memory_length 2000, epsilon 0.47077417846163544, time 737.0, rides 129\n",
      "Initial State is  [4, 15, 1]\n",
      "episode 753, reward 43.0, memory_length 2000, epsilon 0.4703034042831738, time 724.0, rides 128\n",
      "Initial State is  [2, 11, 1]\n",
      "episode 754, reward 269.0, memory_length 2000, epsilon 0.46983310087889063, time 727.0, rides 136\n",
      "Initial State is  [2, 4, 0]\n",
      "episode 755, reward 294.0, memory_length 2000, epsilon 0.46936326777801174, time 734.0, rides 126\n",
      "Initial State is  [1, 1, 6]\n",
      "episode 756, reward -1.0, memory_length 2000, epsilon 0.4688939045102337, time 737.0, rides 128\n",
      "Initial State is  [2, 23, 3]\n",
      "episode 757, reward 104.0, memory_length 2000, epsilon 0.4684250106057235, time 727.0, rides 124\n",
      "Initial State is  [1, 1, 3]\n",
      "episode 758, reward 205.0, memory_length 2000, epsilon 0.46795658559511777, time 727.0, rides 143\n",
      "Initial State is  [4, 8, 4]\n",
      "episode 759, reward 407.0, memory_length 2000, epsilon 0.46748862900952265, time 730.0, rides 134\n",
      "Initial State is  [3, 4, 6]\n",
      "episode 760, reward 235.0, memory_length 2000, epsilon 0.4670211403805131, time 724.0, rides 132\n",
      "Initial State is  [2, 15, 1]\n",
      "episode 761, reward 98.0, memory_length 2000, epsilon 0.4665541192401326, time 731.0, rides 131\n",
      "Initial State is  [2, 11, 3]\n",
      "episode 762, reward 375.0, memory_length 2000, epsilon 0.4660875651208925, time 739.0, rides 121\n",
      "Initial State is  [3, 12, 3]\n",
      "episode 763, reward 167.0, memory_length 2000, epsilon 0.4656214775557716, time 728.0, rides 138\n",
      "Initial State is  [4, 18, 1]\n",
      "episode 764, reward -192.0, memory_length 2000, epsilon 0.46515585607821586, time 721.0, rides 126\n",
      "Initial State is  [4, 11, 4]\n",
      "episode 765, reward 283.0, memory_length 2000, epsilon 0.46469070022213765, time 724.0, rides 126\n",
      "Initial State is  [4, 14, 4]\n",
      "episode 766, reward 297.0, memory_length 2000, epsilon 0.4642260095219155, time 728.0, rides 133\n",
      "Initial State is  [4, 1, 5]\n",
      "episode 767, reward 522.0, memory_length 2000, epsilon 0.4637617835123936, time 729.0, rides 122\n",
      "Initial State is  [2, 1, 6]\n",
      "episode 768, reward 148.0, memory_length 2000, epsilon 0.46329802172888124, time 730.0, rides 134\n",
      "Initial State is  [1, 13, 2]\n",
      "episode 769, reward 386.0, memory_length 2000, epsilon 0.46283472370715234, time 730.0, rides 116\n",
      "Initial State is  [3, 4, 6]\n",
      "episode 770, reward 259.0, memory_length 2000, epsilon 0.46237188898344517, time 731.0, rides 123\n",
      "Initial State is  [1, 9, 6]\n",
      "episode 771, reward 354.0, memory_length 2000, epsilon 0.4619095170944617, time 736.0, rides 122\n",
      "Initial State is  [2, 7, 5]\n",
      "episode 772, reward 631.0, memory_length 2000, epsilon 0.46144760757736725, time 733.0, rides 122\n",
      "Initial State is  [1, 2, 4]\n",
      "episode 773, reward 398.0, memory_length 2000, epsilon 0.46098615996978987, time 733.0, rides 121\n",
      "Initial State is  [2, 2, 1]\n",
      "episode 774, reward 234.0, memory_length 2000, epsilon 0.46052517380982005, time 741.0, rides 129\n",
      "Initial State is  [2, 18, 2]\n",
      "episode 775, reward 50.0, memory_length 2000, epsilon 0.4600646486360102, time 728.0, rides 126\n",
      "Initial State is  [3, 6, 5]\n",
      "episode 776, reward 73.0, memory_length 2000, epsilon 0.4596045839873742, time 726.0, rides 132\n",
      "Initial State is  [0, 4, 0]\n",
      "episode 777, reward 365.0, memory_length 2000, epsilon 0.45914497940338683, time 724.0, rides 129\n",
      "Initial State is  [2, 4, 5]\n",
      "episode 778, reward 21.0, memory_length 2000, epsilon 0.4586858344239834, time 735.0, rides 130\n",
      "Initial State is  [1, 23, 5]\n",
      "episode 779, reward 117.0, memory_length 2000, epsilon 0.45822714858955943, time 734.0, rides 120\n",
      "Initial State is  [4, 2, 4]\n",
      "episode 780, reward 281.0, memory_length 2000, epsilon 0.45776892144096987, time 731.0, rides 114\n",
      "Initial State is  [1, 11, 1]\n",
      "episode 781, reward 348.0, memory_length 2000, epsilon 0.4573111525195289, time 730.0, rides 125\n",
      "Initial State is  [4, 2, 5]\n",
      "episode 782, reward 84.0, memory_length 2000, epsilon 0.4568538413670094, time 726.0, rides 129\n",
      "Initial State is  [4, 17, 5]\n",
      "episode 783, reward 559.0, memory_length 2000, epsilon 0.4563969875256424, time 728.0, rides 132\n",
      "Initial State is  [2, 10, 5]\n",
      "episode 784, reward 103.0, memory_length 2000, epsilon 0.45594059053811675, time 722.0, rides 131\n",
      "Initial State is  [3, 21, 5]\n",
      "episode 785, reward 252.0, memory_length 2000, epsilon 0.45548464994757865, time 735.0, rides 128\n",
      "Initial State is  [4, 19, 3]\n",
      "episode 786, reward 456.0, memory_length 2000, epsilon 0.45502916529763104, time 726.0, rides 128\n",
      "Initial State is  [0, 18, 6]\n",
      "episode 787, reward 322.0, memory_length 2000, epsilon 0.45457413613233344, time 736.0, rides 115\n",
      "Initial State is  [4, 9, 6]\n",
      "episode 788, reward 201.0, memory_length 2000, epsilon 0.4541195619962011, time 725.0, rides 122\n",
      "Initial State is  [4, 4, 4]\n",
      "episode 789, reward 273.0, memory_length 2000, epsilon 0.4536654424342049, time 729.0, rides 124\n",
      "Initial State is  [2, 6, 6]\n",
      "episode 790, reward 308.0, memory_length 2000, epsilon 0.45321177699177073, time 723.0, rides 120\n",
      "Initial State is  [2, 16, 6]\n",
      "episode 791, reward 284.0, memory_length 2000, epsilon 0.452758565214779, time 724.0, rides 141\n",
      "Initial State is  [2, 18, 6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 792, reward 303.0, memory_length 2000, epsilon 0.4523058066495642, time 727.0, rides 136\n",
      "Initial State is  [3, 21, 1]\n",
      "episode 793, reward 232.0, memory_length 2000, epsilon 0.45185350084291465, time 732.0, rides 141\n",
      "Initial State is  [2, 10, 5]\n",
      "episode 794, reward 372.0, memory_length 2000, epsilon 0.4514016473420717, time 732.0, rides 105\n",
      "Initial State is  [2, 14, 5]\n",
      "episode 795, reward 328.0, memory_length 2000, epsilon 0.45095024569472963, time 730.0, rides 121\n",
      "Initial State is  [2, 17, 0]\n",
      "episode 796, reward 303.0, memory_length 2000, epsilon 0.4504992954490349, time 727.0, rides 119\n",
      "Initial State is  [3, 12, 2]\n",
      "episode 797, reward 259.0, memory_length 2000, epsilon 0.45004879615358584, time 723.0, rides 116\n",
      "Initial State is  [0, 5, 4]\n",
      "episode 798, reward 189.0, memory_length 2000, epsilon 0.44959874735743227, time 732.0, rides 129\n",
      "Initial State is  [3, 12, 0]\n",
      "episode 799, reward 380.0, memory_length 2000, epsilon 0.4491491486100748, time 729.0, rides 125\n",
      "Initial State is  [4, 6, 1]\n",
      "episode 800, reward 170.0, memory_length 2000, epsilon 0.44869999946146477, time 724.0, rides 138\n",
      "Initial State is  [3, 9, 3]\n",
      "episode 801, reward 459.0, memory_length 2000, epsilon 0.4482512994620033, time 729.0, rides 131\n",
      "Initial State is  [4, 1, 6]\n",
      "episode 802, reward -20.0, memory_length 2000, epsilon 0.4478030481625413, time 728.0, rides 130\n",
      "Initial State is  [1, 3, 1]\n",
      "episode 803, reward 18.0, memory_length 2000, epsilon 0.44735524511437874, time 726.0, rides 124\n",
      "Initial State is  [4, 16, 2]\n",
      "episode 804, reward 261.0, memory_length 2000, epsilon 0.4469078898692644, time 730.0, rides 121\n",
      "Initial State is  [1, 6, 1]\n",
      "episode 805, reward 161.0, memory_length 2000, epsilon 0.4464609819793951, time 732.0, rides 119\n",
      "Initial State is  [1, 4, 2]\n",
      "episode 806, reward 409.0, memory_length 2000, epsilon 0.44601452099741573, time 738.0, rides 139\n",
      "Initial State is  [2, 7, 4]\n",
      "episode 807, reward 298.0, memory_length 2000, epsilon 0.4455685064764183, time 731.0, rides 138\n",
      "Initial State is  [0, 9, 6]\n",
      "episode 808, reward 355.0, memory_length 2000, epsilon 0.4451229379699419, time 728.0, rides 127\n",
      "Initial State is  [1, 12, 1]\n",
      "episode 809, reward 116.0, memory_length 2000, epsilon 0.44467781503197196, time 729.0, rides 124\n",
      "Initial State is  [2, 18, 1]\n",
      "episode 810, reward 433.0, memory_length 2000, epsilon 0.44423313721693997, time 731.0, rides 133\n",
      "Initial State is  [0, 21, 6]\n",
      "episode 811, reward 263.0, memory_length 2000, epsilon 0.44378890407972305, time 722.0, rides 124\n",
      "Initial State is  [0, 8, 2]\n",
      "episode 812, reward 301.0, memory_length 2000, epsilon 0.44334511517564335, time 728.0, rides 113\n",
      "Initial State is  [0, 9, 0]\n",
      "episode 813, reward 427.0, memory_length 2000, epsilon 0.4429017700604677, time 728.0, rides 108\n",
      "Initial State is  [1, 0, 2]\n",
      "episode 814, reward 342.0, memory_length 2000, epsilon 0.44245886829040726, time 726.0, rides 129\n",
      "Initial State is  [1, 9, 0]\n",
      "episode 815, reward 544.0, memory_length 2000, epsilon 0.44201640942211684, time 724.0, rides 116\n",
      "Initial State is  [0, 3, 4]\n",
      "episode 816, reward 179.0, memory_length 2000, epsilon 0.44157439301269474, time 730.0, rides 124\n",
      "Initial State is  [4, 4, 0]\n",
      "episode 817, reward 396.0, memory_length 2000, epsilon 0.44113281861968207, time 722.0, rides 124\n",
      "Initial State is  [1, 7, 0]\n",
      "episode 818, reward 410.0, memory_length 2000, epsilon 0.4406916858010624, time 722.0, rides 123\n",
      "Initial State is  [3, 5, 5]\n",
      "episode 819, reward 334.0, memory_length 2000, epsilon 0.4402509941152613, time 729.0, rides 120\n",
      "Initial State is  [1, 21, 5]\n",
      "episode 820, reward 519.0, memory_length 2000, epsilon 0.43981074312114604, time 733.0, rides 144\n",
      "Initial State is  [4, 5, 6]\n",
      "episode 821, reward -6.0, memory_length 2000, epsilon 0.4393709323780249, time 736.0, rides 120\n",
      "Initial State is  [2, 9, 0]\n",
      "episode 822, reward 249.0, memory_length 2000, epsilon 0.4389315614456469, time 727.0, rides 132\n",
      "Initial State is  [0, 23, 6]\n",
      "episode 823, reward 387.0, memory_length 2000, epsilon 0.43849262988420123, time 728.0, rides 119\n",
      "Initial State is  [3, 17, 2]\n",
      "episode 824, reward 252.0, memory_length 2000, epsilon 0.438054137254317, time 721.0, rides 123\n",
      "Initial State is  [3, 10, 4]\n",
      "episode 825, reward 390.0, memory_length 2000, epsilon 0.4376160831170627, time 725.0, rides 121\n",
      "Initial State is  [2, 0, 0]\n",
      "episode 826, reward 81.0, memory_length 2000, epsilon 0.43717846703394564, time 725.0, rides 133\n",
      "Initial State is  [3, 12, 4]\n",
      "episode 827, reward -11.0, memory_length 2000, epsilon 0.4367412885669117, time 724.0, rides 126\n",
      "Initial State is  [4, 21, 5]\n",
      "episode 828, reward 176.0, memory_length 2000, epsilon 0.4363045472783448, time 727.0, rides 130\n",
      "Initial State is  [1, 22, 4]\n",
      "episode 829, reward 321.0, memory_length 2000, epsilon 0.43586824273106645, time 729.0, rides 121\n",
      "Initial State is  [1, 11, 2]\n",
      "episode 830, reward 323.0, memory_length 2000, epsilon 0.4354323744883354, time 729.0, rides 133\n",
      "Initial State is  [0, 3, 5]\n",
      "episode 831, reward 174.0, memory_length 2000, epsilon 0.43499694211384704, time 732.0, rides 128\n",
      "Initial State is  [2, 10, 5]\n",
      "episode 832, reward 167.0, memory_length 2000, epsilon 0.4345619451717332, time 731.0, rides 126\n",
      "Initial State is  [2, 14, 2]\n",
      "episode 833, reward 149.0, memory_length 2000, epsilon 0.43412738322656147, time 726.0, rides 133\n",
      "Initial State is  [4, 22, 5]\n",
      "episode 834, reward 389.0, memory_length 2000, epsilon 0.4336932558433349, time 727.0, rides 129\n",
      "Initial State is  [1, 12, 2]\n",
      "episode 835, reward 355.0, memory_length 2000, epsilon 0.43325956258749154, time 730.0, rides 120\n",
      "Initial State is  [2, 20, 6]\n",
      "episode 836, reward 136.0, memory_length 2000, epsilon 0.43282630302490405, time 734.0, rides 116\n",
      "Initial State is  [2, 18, 3]\n",
      "episode 837, reward 141.0, memory_length 2000, epsilon 0.43239347672187917, time 728.0, rides 110\n",
      "Initial State is  [0, 14, 1]\n",
      "episode 838, reward 114.0, memory_length 2000, epsilon 0.4319610832451573, time 730.0, rides 121\n",
      "Initial State is  [3, 9, 1]\n",
      "episode 839, reward 485.0, memory_length 2000, epsilon 0.43152912216191214, time 731.0, rides 127\n",
      "Initial State is  [1, 22, 3]\n",
      "episode 840, reward 210.0, memory_length 2000, epsilon 0.4310975930397502, time 727.0, rides 114\n",
      "Initial State is  [4, 6, 1]\n",
      "episode 841, reward 439.0, memory_length 2000, epsilon 0.43066649544671043, time 721.0, rides 121\n",
      "Initial State is  [3, 2, 3]\n",
      "episode 842, reward -4.0, memory_length 2000, epsilon 0.4302358289512637, time 729.0, rides 121\n",
      "Initial State is  [2, 9, 1]\n",
      "episode 843, reward 284.0, memory_length 2000, epsilon 0.4298055931223125, time 730.0, rides 136\n",
      "Initial State is  [3, 11, 0]\n",
      "episode 844, reward 177.0, memory_length 2000, epsilon 0.42937578752919014, time 728.0, rides 114\n",
      "Initial State is  [3, 1, 2]\n",
      "episode 845, reward 720.0, memory_length 2000, epsilon 0.4289464117416609, time 725.0, rides 122\n",
      "Initial State is  [3, 2, 2]\n",
      "episode 846, reward 321.0, memory_length 2000, epsilon 0.42851746532991924, time 724.0, rides 121\n",
      "Initial State is  [0, 13, 0]\n",
      "episode 847, reward 215.0, memory_length 2000, epsilon 0.42808894786458934, time 733.0, rides 121\n",
      "Initial State is  [2, 20, 2]\n",
      "episode 848, reward 179.0, memory_length 2000, epsilon 0.42766085891672473, time 734.0, rides 125\n",
      "Initial State is  [3, 6, 0]\n",
      "episode 849, reward 371.0, memory_length 2000, epsilon 0.427233198057808, time 728.0, rides 127\n",
      "Initial State is  [4, 23, 3]\n",
      "episode 850, reward 211.0, memory_length 2000, epsilon 0.4268059648597502, time 733.0, rides 128\n",
      "Initial State is  [3, 1, 6]\n",
      "episode 851, reward 551.0, memory_length 2000, epsilon 0.4263791588948904, time 727.0, rides 125\n",
      "Initial State is  [3, 17, 1]\n",
      "episode 852, reward 227.0, memory_length 2000, epsilon 0.42595277973599555, time 727.0, rides 123\n",
      "Initial State is  [0, 11, 0]\n",
      "episode 853, reward 409.0, memory_length 2000, epsilon 0.42552682695625954, time 732.0, rides 132\n",
      "Initial State is  [3, 22, 1]\n",
      "episode 854, reward 629.0, memory_length 2000, epsilon 0.4251013001293033, time 730.0, rides 128\n",
      "Initial State is  [3, 18, 3]\n",
      "episode 855, reward 234.0, memory_length 2000, epsilon 0.424676198829174, time 730.0, rides 116\n",
      "Initial State is  [3, 10, 0]\n",
      "episode 856, reward 369.0, memory_length 2000, epsilon 0.4242515226303448, time 729.0, rides 116\n",
      "Initial State is  [4, 6, 4]\n",
      "episode 857, reward 305.0, memory_length 2000, epsilon 0.42382727110771445, time 726.0, rides 128\n",
      "Initial State is  [1, 7, 4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 858, reward 281.0, memory_length 2000, epsilon 0.4234034438366067, time 733.0, rides 121\n",
      "Initial State is  [1, 18, 4]\n",
      "episode 859, reward 282.0, memory_length 2000, epsilon 0.4229800403927701, time 721.0, rides 129\n",
      "Initial State is  [1, 10, 1]\n",
      "episode 860, reward 275.0, memory_length 2000, epsilon 0.42255706035237733, time 730.0, rides 124\n",
      "Initial State is  [1, 14, 5]\n",
      "episode 861, reward 220.0, memory_length 2000, epsilon 0.42213450329202495, time 731.0, rides 124\n",
      "Initial State is  [1, 16, 3]\n",
      "episode 862, reward 181.0, memory_length 2000, epsilon 0.4217123687887329, time 737.0, rides 133\n",
      "Initial State is  [0, 18, 1]\n",
      "episode 863, reward 406.0, memory_length 2000, epsilon 0.4212906564199442, time 733.0, rides 118\n",
      "Initial State is  [4, 16, 5]\n",
      "episode 864, reward 609.0, memory_length 2000, epsilon 0.42086936576352424, time 734.0, rides 134\n",
      "Initial State is  [2, 8, 0]\n",
      "episode 865, reward 430.0, memory_length 2000, epsilon 0.4204484963977607, time 727.0, rides 128\n",
      "Initial State is  [0, 12, 4]\n",
      "episode 866, reward 280.0, memory_length 2000, epsilon 0.42002804790136294, time 725.0, rides 121\n",
      "Initial State is  [2, 12, 4]\n",
      "episode 867, reward 365.0, memory_length 2000, epsilon 0.4196080198534616, time 730.0, rides 122\n",
      "Initial State is  [0, 5, 6]\n",
      "episode 868, reward 444.0, memory_length 2000, epsilon 0.4191884118336081, time 722.0, rides 132\n",
      "Initial State is  [3, 6, 6]\n",
      "episode 869, reward 332.0, memory_length 2000, epsilon 0.41876922342177453, time 722.0, rides 122\n",
      "Initial State is  [4, 17, 2]\n",
      "episode 870, reward 280.0, memory_length 2000, epsilon 0.41835045419835276, time 725.0, rides 121\n",
      "Initial State is  [1, 23, 0]\n",
      "episode 871, reward 145.0, memory_length 2000, epsilon 0.4179321037441544, time 726.0, rides 134\n",
      "Initial State is  [1, 11, 1]\n",
      "episode 872, reward 377.0, memory_length 2000, epsilon 0.41751417164041027, time 728.0, rides 120\n",
      "Initial State is  [3, 13, 1]\n",
      "episode 873, reward 233.0, memory_length 2000, epsilon 0.41709665746876984, time 734.0, rides 123\n",
      "Initial State is  [3, 23, 4]\n",
      "episode 874, reward 23.0, memory_length 2000, epsilon 0.4166795608113011, time 734.0, rides 110\n",
      "Initial State is  [0, 8, 0]\n",
      "episode 875, reward 61.0, memory_length 2000, epsilon 0.41626288125048977, time 727.0, rides 136\n",
      "Initial State is  [0, 10, 4]\n",
      "episode 876, reward 594.0, memory_length 2000, epsilon 0.41584661836923925, time 723.0, rides 126\n",
      "Initial State is  [4, 9, 4]\n",
      "episode 877, reward 407.0, memory_length 2000, epsilon 0.41543077175087, time 728.0, rides 131\n",
      "Initial State is  [2, 20, 1]\n",
      "episode 878, reward 357.0, memory_length 2000, epsilon 0.41501534097911913, time 736.0, rides 128\n",
      "Initial State is  [0, 2, 2]\n",
      "episode 879, reward 252.0, memory_length 2000, epsilon 0.41460032563814003, time 729.0, rides 122\n",
      "Initial State is  [2, 1, 5]\n",
      "episode 880, reward 674.0, memory_length 2000, epsilon 0.4141857253125019, time 738.0, rides 134\n",
      "Initial State is  [1, 15, 0]\n",
      "episode 881, reward 297.0, memory_length 2000, epsilon 0.41377153958718943, time 728.0, rides 130\n",
      "Initial State is  [4, 7, 1]\n",
      "episode 882, reward 183.0, memory_length 2000, epsilon 0.4133577680476022, time 731.0, rides 121\n",
      "Initial State is  [2, 7, 4]\n",
      "episode 883, reward 497.0, memory_length 2000, epsilon 0.4129444102795546, time 743.0, rides 139\n",
      "Initial State is  [4, 11, 3]\n",
      "episode 884, reward 261.0, memory_length 2000, epsilon 0.412531465869275, time 730.0, rides 125\n",
      "Initial State is  [4, 0, 5]\n",
      "episode 885, reward 522.0, memory_length 2000, epsilon 0.41211893440340575, time 722.0, rides 131\n",
      "Initial State is  [4, 18, 0]\n",
      "episode 886, reward 166.0, memory_length 2000, epsilon 0.41170681546900234, time 726.0, rides 135\n",
      "Initial State is  [1, 11, 5]\n",
      "episode 887, reward 179.0, memory_length 2000, epsilon 0.41129510865353336, time 726.0, rides 130\n",
      "Initial State is  [0, 0, 5]\n",
      "episode 888, reward 405.0, memory_length 2000, epsilon 0.41088381354487985, time 739.0, rides 132\n",
      "Initial State is  [0, 8, 3]\n",
      "episode 889, reward 204.0, memory_length 2000, epsilon 0.410472929731335, time 731.0, rides 146\n",
      "Initial State is  [4, 0, 0]\n",
      "episode 890, reward 427.0, memory_length 2000, epsilon 0.41006245680160364, time 728.0, rides 130\n",
      "Initial State is  [1, 1, 4]\n",
      "episode 891, reward 174.0, memory_length 2000, epsilon 0.40965239434480205, time 740.0, rides 120\n",
      "Initial State is  [0, 3, 5]\n",
      "episode 892, reward -64.0, memory_length 2000, epsilon 0.40924274195045723, time 721.0, rides 133\n",
      "Initial State is  [3, 21, 3]\n",
      "episode 893, reward 167.0, memory_length 2000, epsilon 0.40883349920850676, time 724.0, rides 129\n",
      "Initial State is  [3, 13, 2]\n",
      "episode 894, reward 306.0, memory_length 2000, epsilon 0.40842466570929825, time 727.0, rides 125\n",
      "Initial State is  [1, 21, 2]\n",
      "episode 895, reward 396.0, memory_length 2000, epsilon 0.40801624104358897, time 729.0, rides 128\n",
      "Initial State is  [4, 18, 1]\n",
      "episode 896, reward 274.0, memory_length 2000, epsilon 0.4076082248025454, time 729.0, rides 141\n",
      "Initial State is  [1, 19, 4]\n",
      "episode 897, reward 336.0, memory_length 2000, epsilon 0.4072006165777428, time 729.0, rides 131\n",
      "Initial State is  [0, 8, 5]\n",
      "episode 898, reward 544.0, memory_length 2000, epsilon 0.4067934159611651, time 727.0, rides 128\n",
      "Initial State is  [1, 16, 5]\n",
      "episode 899, reward 108.0, memory_length 2000, epsilon 0.4063866225452039, time 728.0, rides 116\n",
      "Initial State is  [4, 5, 4]\n",
      "episode 900, reward 402.0, memory_length 2000, epsilon 0.4059802359226587, time 727.0, rides 135\n",
      "Initial State is  [0, 17, 0]\n",
      "episode 901, reward 384.0, memory_length 2000, epsilon 0.40557425568673605, time 736.0, rides 127\n",
      "Initial State is  [1, 19, 1]\n",
      "episode 902, reward 258.0, memory_length 2000, epsilon 0.40516868143104934, time 727.0, rides 111\n",
      "Initial State is  [3, 7, 1]\n",
      "episode 903, reward 364.0, memory_length 2000, epsilon 0.4047635127496183, time 729.0, rides 117\n",
      "Initial State is  [0, 20, 6]\n",
      "episode 904, reward 270.0, memory_length 2000, epsilon 0.4043587492368687, time 731.0, rides 140\n",
      "Initial State is  [2, 22, 1]\n",
      "episode 905, reward 479.0, memory_length 2000, epsilon 0.4039543904876318, time 728.0, rides 133\n",
      "Initial State is  [3, 15, 6]\n",
      "episode 906, reward 160.0, memory_length 2000, epsilon 0.4035504360971442, time 731.0, rides 124\n",
      "Initial State is  [2, 22, 4]\n",
      "episode 907, reward 289.0, memory_length 2000, epsilon 0.40314688566104706, time 733.0, rides 130\n",
      "Initial State is  [0, 6, 5]\n",
      "episode 908, reward 444.0, memory_length 2000, epsilon 0.402743738775386, time 734.0, rides 127\n",
      "Initial State is  [1, 22, 3]\n",
      "episode 909, reward 343.0, memory_length 2000, epsilon 0.4023409950366106, time 731.0, rides 123\n",
      "Initial State is  [4, 12, 1]\n",
      "episode 910, reward 266.0, memory_length 2000, epsilon 0.401938654041574, time 726.0, rides 138\n",
      "Initial State is  [3, 8, 4]\n",
      "episode 911, reward 197.0, memory_length 2000, epsilon 0.4015367153875324, time 730.0, rides 118\n",
      "Initial State is  [3, 1, 6]\n",
      "episode 912, reward 100.0, memory_length 2000, epsilon 0.4011351786721449, time 736.0, rides 121\n",
      "Initial State is  [2, 13, 1]\n",
      "episode 913, reward 388.0, memory_length 2000, epsilon 0.40073404349347275, time 742.0, rides 114\n",
      "Initial State is  [2, 21, 5]\n",
      "episode 914, reward 322.0, memory_length 2000, epsilon 0.40033330944997925, time 725.0, rides 134\n",
      "Initial State is  [1, 6, 1]\n",
      "episode 915, reward 364.0, memory_length 2000, epsilon 0.39993297614052925, time 725.0, rides 115\n",
      "Initial State is  [0, 2, 4]\n",
      "episode 916, reward 245.0, memory_length 2000, epsilon 0.3995330431643887, time 725.0, rides 140\n",
      "Initial State is  [4, 13, 2]\n",
      "episode 917, reward 538.0, memory_length 2000, epsilon 0.39913351012122433, time 737.0, rides 131\n",
      "Initial State is  [2, 11, 2]\n",
      "episode 918, reward 567.0, memory_length 2000, epsilon 0.3987343766111031, time 725.0, rides 123\n",
      "Initial State is  [4, 13, 1]\n",
      "episode 919, reward 409.0, memory_length 2000, epsilon 0.398335642234492, time 724.0, rides 118\n",
      "Initial State is  [1, 19, 6]\n",
      "episode 920, reward 306.0, memory_length 2000, epsilon 0.3979373065922575, time 736.0, rides 128\n",
      "Initial State is  [3, 18, 5]\n",
      "episode 921, reward 296.0, memory_length 2000, epsilon 0.39753936928566525, time 729.0, rides 131\n",
      "Initial State is  [4, 10, 3]\n",
      "episode 922, reward 230.0, memory_length 2000, epsilon 0.3971418299163796, time 732.0, rides 120\n",
      "Initial State is  [1, 8, 0]\n",
      "episode 923, reward 374.0, memory_length 2000, epsilon 0.3967446880864632, time 723.0, rides 119\n",
      "Initial State is  [2, 18, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 924, reward 650.0, memory_length 2000, epsilon 0.3963479433983767, time 734.0, rides 136\n",
      "Initial State is  [0, 16, 2]\n",
      "episode 925, reward 408.0, memory_length 2000, epsilon 0.3959515954549783, time 724.0, rides 121\n",
      "Initial State is  [1, 12, 6]\n",
      "episode 926, reward 429.0, memory_length 2000, epsilon 0.39555564385952335, time 728.0, rides 131\n",
      "Initial State is  [2, 11, 6]\n",
      "episode 927, reward 276.0, memory_length 2000, epsilon 0.39516008821566384, time 731.0, rides 137\n",
      "Initial State is  [2, 9, 0]\n",
      "episode 928, reward 334.0, memory_length 2000, epsilon 0.3947649281274482, time 728.0, rides 118\n",
      "Initial State is  [4, 23, 0]\n",
      "episode 929, reward 397.0, memory_length 2000, epsilon 0.3943701631993207, time 725.0, rides 120\n",
      "Initial State is  [3, 3, 0]\n",
      "episode 930, reward 261.0, memory_length 2000, epsilon 0.3939757930361214, time 732.0, rides 132\n",
      "Initial State is  [1, 6, 5]\n",
      "episode 931, reward 377.0, memory_length 2000, epsilon 0.3935818172430853, time 731.0, rides 126\n",
      "Initial State is  [2, 7, 5]\n",
      "episode 932, reward 436.0, memory_length 2000, epsilon 0.3931882354258422, time 734.0, rides 139\n",
      "Initial State is  [0, 18, 3]\n",
      "episode 933, reward 386.0, memory_length 2000, epsilon 0.39279504719041636, time 733.0, rides 130\n",
      "Initial State is  [0, 18, 2]\n",
      "episode 934, reward 729.0, memory_length 2000, epsilon 0.39240225214322594, time 725.0, rides 123\n",
      "Initial State is  [4, 22, 1]\n",
      "episode 935, reward 277.0, memory_length 2000, epsilon 0.39200984989108273, time 727.0, rides 136\n",
      "Initial State is  [2, 18, 4]\n",
      "episode 936, reward 428.0, memory_length 2000, epsilon 0.39161784004119166, time 728.0, rides 136\n",
      "Initial State is  [3, 18, 3]\n",
      "episode 937, reward -2.0, memory_length 2000, epsilon 0.39122622220115044, time 733.0, rides 134\n",
      "Initial State is  [3, 20, 0]\n",
      "episode 938, reward 186.0, memory_length 2000, epsilon 0.3908349959789493, time 740.0, rides 121\n",
      "Initial State is  [0, 23, 3]\n",
      "episode 939, reward 240.0, memory_length 2000, epsilon 0.3904441609829703, time 725.0, rides 133\n",
      "Initial State is  [0, 18, 0]\n",
      "episode 940, reward 294.0, memory_length 2000, epsilon 0.3900537168219873, time 733.0, rides 138\n",
      "Initial State is  [4, 13, 3]\n",
      "episode 941, reward 204.0, memory_length 2000, epsilon 0.3896636631051653, time 736.0, rides 131\n",
      "Initial State is  [3, 16, 5]\n",
      "episode 942, reward 102.0, memory_length 2000, epsilon 0.38927399944206015, time 731.0, rides 121\n",
      "Initial State is  [3, 13, 5]\n",
      "episode 943, reward 446.0, memory_length 2000, epsilon 0.3888847254426181, time 734.0, rides 132\n",
      "Initial State is  [1, 2, 4]\n",
      "episode 944, reward 420.0, memory_length 2000, epsilon 0.38849584071717547, time 731.0, rides 128\n",
      "Initial State is  [0, 14, 2]\n",
      "episode 945, reward 339.0, memory_length 2000, epsilon 0.3881073448764583, time 730.0, rides 118\n",
      "Initial State is  [1, 11, 2]\n",
      "episode 946, reward 41.0, memory_length 2000, epsilon 0.3877192375315819, time 731.0, rides 140\n",
      "Initial State is  [3, 8, 3]\n",
      "episode 947, reward 401.0, memory_length 2000, epsilon 0.3873315182940503, time 728.0, rides 150\n",
      "Initial State is  [3, 22, 5]\n",
      "episode 948, reward 409.0, memory_length 2000, epsilon 0.38694418677575626, time 725.0, rides 130\n",
      "Initial State is  [4, 16, 3]\n",
      "episode 949, reward 338.0, memory_length 2000, epsilon 0.3865572425889805, time 727.0, rides 129\n",
      "Initial State is  [4, 0, 4]\n",
      "episode 950, reward 498.0, memory_length 2000, epsilon 0.38617068534639154, time 726.0, rides 115\n",
      "Initial State is  [3, 6, 6]\n",
      "episode 951, reward 222.0, memory_length 2000, epsilon 0.38578451466104513, time 728.0, rides 115\n",
      "Initial State is  [4, 19, 6]\n",
      "episode 952, reward 411.0, memory_length 2000, epsilon 0.3853987301463841, time 732.0, rides 129\n",
      "Initial State is  [0, 20, 1]\n",
      "episode 953, reward 246.0, memory_length 2000, epsilon 0.3850133314162377, time 729.0, rides 138\n",
      "Initial State is  [4, 21, 5]\n",
      "episode 954, reward 606.0, memory_length 2000, epsilon 0.3846283180848215, time 729.0, rides 127\n",
      "Initial State is  [3, 22, 0]\n",
      "episode 955, reward 530.0, memory_length 2000, epsilon 0.3842436897667367, time 735.0, rides 116\n",
      "Initial State is  [3, 10, 2]\n",
      "episode 956, reward 456.0, memory_length 2000, epsilon 0.3838594460769699, time 723.0, rides 136\n",
      "Initial State is  [2, 0, 0]\n",
      "episode 957, reward -52.0, memory_length 2000, epsilon 0.38347558663089293, time 732.0, rides 129\n",
      "Initial State is  [4, 10, 2]\n",
      "episode 958, reward 335.0, memory_length 2000, epsilon 0.38309211104426205, time 737.0, rides 136\n",
      "Initial State is  [2, 9, 4]\n",
      "episode 959, reward 466.0, memory_length 2000, epsilon 0.3827090189332178, time 734.0, rides 123\n",
      "Initial State is  [4, 2, 5]\n",
      "episode 960, reward 397.0, memory_length 2000, epsilon 0.38232630991428457, time 723.0, rides 128\n",
      "Initial State is  [4, 20, 4]\n",
      "episode 961, reward 543.0, memory_length 2000, epsilon 0.38194398360437026, time 728.0, rides 126\n",
      "Initial State is  [1, 0, 4]\n",
      "episode 962, reward 405.0, memory_length 2000, epsilon 0.3815620396207659, time 727.0, rides 130\n",
      "Initial State is  [1, 8, 3]\n",
      "episode 963, reward 545.0, memory_length 2000, epsilon 0.38118047758114515, time 733.0, rides 129\n",
      "Initial State is  [0, 5, 1]\n",
      "episode 964, reward 333.0, memory_length 2000, epsilon 0.380799297103564, time 726.0, rides 135\n",
      "Initial State is  [2, 16, 2]\n",
      "episode 965, reward 296.0, memory_length 2000, epsilon 0.3804184978064605, time 735.0, rides 136\n",
      "Initial State is  [1, 12, 2]\n",
      "episode 966, reward 302.0, memory_length 2000, epsilon 0.380038079308654, time 725.0, rides 131\n",
      "Initial State is  [0, 11, 3]\n",
      "episode 967, reward 261.0, memory_length 2000, epsilon 0.3796580412293454, time 729.0, rides 125\n",
      "Initial State is  [3, 18, 6]\n",
      "episode 968, reward 444.0, memory_length 2000, epsilon 0.379278383188116, time 728.0, rides 131\n",
      "Initial State is  [1, 9, 2]\n",
      "episode 969, reward 173.0, memory_length 2000, epsilon 0.3788991048049279, time 733.0, rides 119\n",
      "Initial State is  [4, 23, 5]\n",
      "episode 970, reward 251.0, memory_length 2000, epsilon 0.37852020570012296, time 729.0, rides 124\n",
      "Initial State is  [1, 12, 0]\n",
      "episode 971, reward 489.0, memory_length 2000, epsilon 0.37814168549442284, time 731.0, rides 110\n",
      "Initial State is  [3, 19, 4]\n",
      "episode 972, reward 506.0, memory_length 2000, epsilon 0.3777635438089284, time 728.0, rides 134\n",
      "Initial State is  [2, 7, 1]\n",
      "episode 973, reward 382.0, memory_length 2000, epsilon 0.3773857802651195, time 720.0, rides 126\n",
      "Initial State is  [0, 16, 0]\n",
      "episode 974, reward 286.0, memory_length 2000, epsilon 0.37700839448485435, time 729.0, rides 136\n",
      "Initial State is  [3, 4, 2]\n",
      "episode 975, reward 390.0, memory_length 2000, epsilon 0.3766313860903695, time 731.0, rides 130\n",
      "Initial State is  [2, 15, 3]\n",
      "episode 976, reward 256.0, memory_length 2000, epsilon 0.37625475470427916, time 722.0, rides 146\n",
      "Initial State is  [1, 21, 3]\n",
      "episode 977, reward 253.0, memory_length 2000, epsilon 0.3758784999495749, time 732.0, rides 138\n",
      "Initial State is  [4, 18, 6]\n",
      "episode 978, reward 444.0, memory_length 2000, epsilon 0.3755026214496253, time 730.0, rides 128\n",
      "Initial State is  [1, 5, 1]\n",
      "episode 979, reward 353.0, memory_length 2000, epsilon 0.3751271188281757, time 727.0, rides 123\n",
      "Initial State is  [2, 12, 3]\n",
      "episode 980, reward 166.0, memory_length 2000, epsilon 0.3747519917093475, time 734.0, rides 126\n",
      "Initial State is  [2, 5, 6]\n",
      "episode 981, reward 378.0, memory_length 2000, epsilon 0.37437723971763814, time 733.0, rides 130\n",
      "Initial State is  [1, 16, 2]\n",
      "episode 982, reward 645.0, memory_length 2000, epsilon 0.37400286247792053, time 722.0, rides 139\n",
      "Initial State is  [4, 4, 6]\n",
      "episode 983, reward 610.0, memory_length 2000, epsilon 0.3736288596154426, time 734.0, rides 124\n",
      "Initial State is  [1, 8, 5]\n",
      "episode 984, reward 421.0, memory_length 2000, epsilon 0.37325523075582717, time 732.0, rides 128\n",
      "Initial State is  [1, 15, 6]\n",
      "episode 985, reward 155.0, memory_length 2000, epsilon 0.37288197552507135, time 735.0, rides 119\n",
      "Initial State is  [4, 10, 2]\n",
      "episode 986, reward 465.0, memory_length 2000, epsilon 0.37250909354954626, time 727.0, rides 138\n",
      "Initial State is  [2, 15, 5]\n",
      "episode 987, reward 295.0, memory_length 2000, epsilon 0.37213658445599673, time 733.0, rides 128\n",
      "Initial State is  [0, 22, 1]\n",
      "episode 988, reward 423.0, memory_length 2000, epsilon 0.3717644478715407, time 728.0, rides 133\n",
      "Initial State is  [3, 7, 6]\n",
      "episode 989, reward 458.0, memory_length 2000, epsilon 0.3713926834236692, time 727.0, rides 111\n",
      "Initial State is  [2, 21, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 990, reward 115.0, memory_length 2000, epsilon 0.37102129074024554, time 731.0, rides 127\n",
      "Initial State is  [1, 6, 0]\n",
      "episode 991, reward 312.0, memory_length 2000, epsilon 0.3706502694495053, time 730.0, rides 139\n",
      "Initial State is  [1, 23, 4]\n",
      "episode 992, reward 534.0, memory_length 2000, epsilon 0.3702796191800558, time 726.0, rides 129\n",
      "Initial State is  [0, 13, 0]\n",
      "episode 993, reward 325.0, memory_length 2000, epsilon 0.36990933956087574, time 734.0, rides 134\n",
      "Initial State is  [2, 21, 6]\n",
      "episode 994, reward 374.0, memory_length 2000, epsilon 0.36953943022131486, time 724.0, rides 126\n",
      "Initial State is  [1, 16, 1]\n",
      "episode 995, reward 423.0, memory_length 2000, epsilon 0.3691698907910935, time 741.0, rides 141\n",
      "Initial State is  [4, 1, 0]\n",
      "episode 996, reward 528.0, memory_length 2000, epsilon 0.3688007209003024, time 729.0, rides 120\n",
      "Initial State is  [1, 22, 4]\n",
      "episode 997, reward 468.0, memory_length 2000, epsilon 0.36843192017940213, time 728.0, rides 136\n",
      "Initial State is  [2, 3, 6]\n",
      "episode 998, reward 584.0, memory_length 2000, epsilon 0.36806348825922275, time 730.0, rides 129\n",
      "Initial State is  [1, 9, 2]\n",
      "episode 999, reward 513.0, memory_length 2000, epsilon 0.3676954247709635, time 727.0, rides 122\n",
      "Initial State is  [3, 7, 6]\n",
      "episode 1000, reward 444.0, memory_length 2000, epsilon 0.36732772934619257, time 726.0, rides 128\n",
      "Initial State is  [1, 21, 3]\n",
      "episode 1001, reward 639.0, memory_length 2000, epsilon 0.3669604016168464, time 729.0, rides 139\n",
      "Initial State is  [3, 17, 3]\n",
      "episode 1002, reward 412.0, memory_length 2000, epsilon 0.3665934412152296, time 729.0, rides 126\n",
      "Initial State is  [1, 20, 2]\n",
      "episode 1003, reward 233.0, memory_length 2000, epsilon 0.3662268477740143, time 733.0, rides 126\n",
      "Initial State is  [0, 20, 6]\n",
      "episode 1004, reward 555.0, memory_length 2000, epsilon 0.36586062092624033, time 726.0, rides 141\n",
      "Initial State is  [2, 11, 2]\n",
      "episode 1005, reward 389.0, memory_length 2000, epsilon 0.3654947603053141, time 733.0, rides 124\n",
      "Initial State is  [2, 4, 2]\n",
      "episode 1006, reward 601.0, memory_length 2000, epsilon 0.36512926554500874, time 720.0, rides 138\n",
      "Initial State is  [2, 14, 1]\n",
      "episode 1007, reward 511.0, memory_length 2000, epsilon 0.36476413627946375, time 726.0, rides 117\n",
      "Initial State is  [2, 22, 0]\n",
      "episode 1008, reward 370.0, memory_length 2000, epsilon 0.3643993721431843, time 733.0, rides 116\n",
      "Initial State is  [0, 7, 3]\n",
      "episode 1009, reward 746.0, memory_length 2000, epsilon 0.36403497277104113, time 727.0, rides 129\n",
      "Initial State is  [4, 1, 2]\n",
      "episode 1010, reward 230.0, memory_length 2000, epsilon 0.3636709377982701, time 729.0, rides 130\n",
      "Initial State is  [4, 2, 5]\n",
      "episode 1011, reward 397.0, memory_length 2000, epsilon 0.3633072668604718, time 728.0, rides 118\n",
      "Initial State is  [3, 15, 2]\n",
      "episode 1012, reward 697.0, memory_length 2000, epsilon 0.36294395959361136, time 726.0, rides 143\n",
      "Initial State is  [4, 7, 2]\n",
      "episode 1013, reward 129.0, memory_length 2000, epsilon 0.36258101563401773, time 724.0, rides 137\n",
      "Initial State is  [1, 3, 3]\n",
      "episode 1014, reward 35.0, memory_length 2000, epsilon 0.3622184346183837, time 732.0, rides 141\n",
      "Initial State is  [1, 12, 4]\n",
      "episode 1015, reward 368.0, memory_length 2000, epsilon 0.36185621618376534, time 725.0, rides 117\n",
      "Initial State is  [3, 13, 2]\n",
      "episode 1016, reward 456.0, memory_length 2000, epsilon 0.36149435996758156, time 728.0, rides 117\n",
      "Initial State is  [1, 16, 1]\n",
      "episode 1017, reward 395.0, memory_length 2000, epsilon 0.361132865607614, time 726.0, rides 128\n",
      "Initial State is  [3, 17, 0]\n",
      "episode 1018, reward 579.0, memory_length 2000, epsilon 0.36077173274200636, time 737.0, rides 116\n",
      "Initial State is  [2, 3, 1]\n",
      "episode 1019, reward 429.0, memory_length 2000, epsilon 0.36041096100926434, time 726.0, rides 123\n",
      "Initial State is  [3, 6, 6]\n",
      "episode 1020, reward 271.0, memory_length 2000, epsilon 0.3600505500482551, time 727.0, rides 114\n",
      "Initial State is  [2, 21, 4]\n",
      "episode 1021, reward 230.0, memory_length 2000, epsilon 0.35969049949820686, time 722.0, rides 114\n",
      "Initial State is  [0, 0, 3]\n",
      "episode 1022, reward 200.0, memory_length 2000, epsilon 0.3593308089987087, time 727.0, rides 137\n",
      "Initial State is  [3, 2, 2]\n",
      "episode 1023, reward 113.0, memory_length 2000, epsilon 0.35897147818971, time 729.0, rides 121\n",
      "Initial State is  [1, 11, 2]\n",
      "episode 1024, reward 277.0, memory_length 2000, epsilon 0.3586125067115203, time 727.0, rides 123\n",
      "Initial State is  [0, 2, 3]\n",
      "episode 1025, reward 590.0, memory_length 2000, epsilon 0.35825389420480874, time 731.0, rides 138\n",
      "Initial State is  [1, 14, 4]\n",
      "episode 1026, reward 594.0, memory_length 2000, epsilon 0.35789564031060395, time 725.0, rides 126\n",
      "Initial State is  [4, 16, 1]\n",
      "episode 1027, reward 373.0, memory_length 2000, epsilon 0.35753774467029337, time 733.0, rides 135\n",
      "Initial State is  [3, 19, 1]\n",
      "episode 1028, reward 435.0, memory_length 2000, epsilon 0.3571802069256231, time 733.0, rides 127\n",
      "Initial State is  [1, 17, 6]\n",
      "episode 1029, reward 565.0, memory_length 2000, epsilon 0.3568230267186975, time 731.0, rides 122\n",
      "Initial State is  [2, 7, 4]\n",
      "episode 1030, reward -138.0, memory_length 2000, epsilon 0.3564662036919788, time 736.0, rides 114\n",
      "Initial State is  [0, 10, 4]\n",
      "episode 1031, reward 274.0, memory_length 2000, epsilon 0.35610973748828684, time 726.0, rides 134\n",
      "Initial State is  [1, 19, 6]\n",
      "episode 1032, reward 283.0, memory_length 2000, epsilon 0.35575362775079855, time 728.0, rides 111\n",
      "Initial State is  [2, 9, 5]\n",
      "episode 1033, reward 254.0, memory_length 2000, epsilon 0.35539787412304774, time 730.0, rides 129\n",
      "Initial State is  [0, 21, 1]\n",
      "episode 1034, reward 368.0, memory_length 2000, epsilon 0.3550424762489247, time 726.0, rides 123\n",
      "Initial State is  [1, 22, 4]\n",
      "episode 1035, reward 271.0, memory_length 2000, epsilon 0.3546874337726758, time 731.0, rides 122\n",
      "Initial State is  [4, 6, 1]\n",
      "episode 1036, reward 370.0, memory_length 2000, epsilon 0.35433274633890316, time 739.0, rides 116\n",
      "Initial State is  [3, 22, 6]\n",
      "episode 1037, reward 236.0, memory_length 2000, epsilon 0.35397841359256427, time 724.0, rides 130\n",
      "Initial State is  [0, 18, 1]\n",
      "episode 1038, reward 542.0, memory_length 2000, epsilon 0.3536244351789717, time 723.0, rides 142\n",
      "Initial State is  [2, 1, 6]\n",
      "episode 1039, reward 525.0, memory_length 2000, epsilon 0.35327081074379274, time 727.0, rides 122\n",
      "Initial State is  [4, 7, 1]\n",
      "episode 1040, reward 167.0, memory_length 2000, epsilon 0.352917539933049, time 723.0, rides 136\n",
      "Initial State is  [3, 7, 6]\n",
      "episode 1041, reward 353.0, memory_length 2000, epsilon 0.3525646223931159, time 727.0, rides 119\n",
      "Initial State is  [2, 13, 6]\n",
      "episode 1042, reward 195.0, memory_length 2000, epsilon 0.3522120577707228, time 739.0, rides 124\n",
      "Initial State is  [3, 9, 2]\n",
      "episode 1043, reward 272.0, memory_length 2000, epsilon 0.3518598457129521, time 735.0, rides 123\n",
      "Initial State is  [2, 8, 3]\n",
      "episode 1044, reward 312.0, memory_length 2000, epsilon 0.35150798586723914, time 732.0, rides 115\n",
      "Initial State is  [4, 18, 4]\n",
      "episode 1045, reward 277.0, memory_length 2000, epsilon 0.3511564778813719, time 739.0, rides 127\n",
      "Initial State is  [4, 10, 0]\n",
      "episode 1046, reward 349.0, memory_length 2000, epsilon 0.3508053214034905, time 722.0, rides 126\n",
      "Initial State is  [0, 16, 4]\n",
      "episode 1047, reward 475.0, memory_length 2000, epsilon 0.35045451608208705, time 731.0, rides 124\n",
      "Initial State is  [1, 14, 2]\n",
      "episode 1048, reward 550.0, memory_length 2000, epsilon 0.350104061566005, time 724.0, rides 126\n",
      "Initial State is  [3, 13, 1]\n",
      "episode 1049, reward 532.0, memory_length 2000, epsilon 0.349753957504439, time 735.0, rides 127\n",
      "Initial State is  [0, 7, 4]\n",
      "episode 1050, reward 66.0, memory_length 2000, epsilon 0.3494042035469346, time 732.0, rides 131\n",
      "Initial State is  [1, 19, 5]\n",
      "episode 1051, reward 567.0, memory_length 2000, epsilon 0.3490547993433876, time 730.0, rides 135\n",
      "Initial State is  [1, 13, 3]\n",
      "episode 1052, reward 376.0, memory_length 2000, epsilon 0.34870574454404424, time 723.0, rides 126\n",
      "Initial State is  [4, 10, 1]\n",
      "episode 1053, reward 531.0, memory_length 2000, epsilon 0.3483570387995002, time 733.0, rides 121\n",
      "Initial State is  [4, 8, 1]\n",
      "episode 1054, reward 256.0, memory_length 2000, epsilon 0.3480086817607007, time 728.0, rides 111\n",
      "Initial State is  [2, 3, 4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 1055, reward 303.0, memory_length 2000, epsilon 0.34766067307894, time 726.0, rides 121\n",
      "Initial State is  [1, 9, 5]\n",
      "episode 1056, reward 452.0, memory_length 2000, epsilon 0.34731301240586104, time 735.0, rides 130\n",
      "Initial State is  [1, 2, 1]\n",
      "episode 1057, reward 259.0, memory_length 2000, epsilon 0.3469656993934552, time 729.0, rides 112\n",
      "Initial State is  [4, 14, 0]\n",
      "episode 1058, reward 475.0, memory_length 2000, epsilon 0.3466187336940617, time 722.0, rides 130\n",
      "Initial State is  [2, 20, 6]\n",
      "episode 1059, reward 498.0, memory_length 2000, epsilon 0.34627211496036764, time 725.0, rides 130\n",
      "Initial State is  [1, 6, 6]\n",
      "episode 1060, reward 214.0, memory_length 2000, epsilon 0.3459258428454073, time 729.0, rides 126\n",
      "Initial State is  [3, 5, 0]\n",
      "episode 1061, reward 534.0, memory_length 2000, epsilon 0.34557991700256185, time 741.0, rides 126\n",
      "Initial State is  [2, 19, 2]\n",
      "episode 1062, reward 4.0, memory_length 2000, epsilon 0.3452343370855593, time 731.0, rides 122\n",
      "Initial State is  [0, 0, 5]\n",
      "episode 1063, reward 392.0, memory_length 2000, epsilon 0.34488910274847373, time 725.0, rides 117\n",
      "Initial State is  [3, 18, 0]\n",
      "episode 1064, reward 654.0, memory_length 2000, epsilon 0.34454421364572524, time 735.0, rides 115\n",
      "Initial State is  [2, 13, 6]\n",
      "episode 1065, reward 523.0, memory_length 2000, epsilon 0.3441996694320795, time 723.0, rides 130\n",
      "Initial State is  [0, 8, 6]\n",
      "episode 1066, reward 312.0, memory_length 2000, epsilon 0.34385546976264747, time 728.0, rides 132\n",
      "Initial State is  [4, 14, 1]\n",
      "episode 1067, reward 428.0, memory_length 2000, epsilon 0.34351161429288485, time 737.0, rides 117\n",
      "Initial State is  [4, 19, 6]\n",
      "episode 1068, reward 332.0, memory_length 2000, epsilon 0.34316810267859194, time 736.0, rides 120\n",
      "Initial State is  [2, 10, 2]\n",
      "episode 1069, reward 376.0, memory_length 2000, epsilon 0.34282493457591334, time 725.0, rides 121\n",
      "Initial State is  [3, 21, 1]\n",
      "episode 1070, reward 698.0, memory_length 2000, epsilon 0.3424821096413374, time 727.0, rides 131\n",
      "Initial State is  [2, 17, 1]\n",
      "episode 1071, reward 508.0, memory_length 2000, epsilon 0.3421396275316961, time 732.0, rides 128\n",
      "Initial State is  [3, 23, 0]\n",
      "episode 1072, reward 342.0, memory_length 2000, epsilon 0.34179748790416437, time 722.0, rides 130\n",
      "Initial State is  [3, 18, 3]\n",
      "episode 1073, reward 260.0, memory_length 2000, epsilon 0.3414556904162602, time 723.0, rides 131\n",
      "Initial State is  [2, 5, 0]\n",
      "episode 1074, reward 442.0, memory_length 2000, epsilon 0.34111423472584396, time 729.0, rides 124\n",
      "Initial State is  [4, 18, 1]\n",
      "episode 1075, reward 718.0, memory_length 2000, epsilon 0.34077312049111813, time 733.0, rides 129\n",
      "Initial State is  [1, 14, 1]\n",
      "episode 1076, reward 516.0, memory_length 2000, epsilon 0.340432347370627, time 729.0, rides 127\n",
      "Initial State is  [3, 1, 5]\n",
      "episode 1077, reward 222.0, memory_length 2000, epsilon 0.34009191502325636, time 727.0, rides 125\n",
      "Initial State is  [3, 14, 6]\n",
      "episode 1078, reward 378.0, memory_length 2000, epsilon 0.3397518231082331, time 732.0, rides 125\n",
      "Initial State is  [2, 20, 4]\n",
      "episode 1079, reward 139.0, memory_length 2000, epsilon 0.33941207128512485, time 728.0, rides 113\n",
      "Initial State is  [4, 20, 0]\n",
      "episode 1080, reward 214.0, memory_length 2000, epsilon 0.3390726592138397, time 725.0, rides 115\n",
      "Initial State is  [1, 23, 4]\n",
      "episode 1081, reward 318.0, memory_length 2000, epsilon 0.3387335865546259, time 728.0, rides 127\n",
      "Initial State is  [1, 3, 2]\n",
      "episode 1082, reward 203.0, memory_length 2000, epsilon 0.33839485296807126, time 731.0, rides 121\n",
      "Initial State is  [0, 12, 4]\n",
      "episode 1083, reward 607.0, memory_length 2000, epsilon 0.3380564581151032, time 728.0, rides 117\n",
      "Initial State is  [4, 11, 5]\n",
      "episode 1084, reward 31.0, memory_length 2000, epsilon 0.33771840165698813, time 731.0, rides 120\n",
      "Initial State is  [3, 21, 4]\n",
      "episode 1085, reward 347.0, memory_length 2000, epsilon 0.33738068325533116, time 729.0, rides 133\n",
      "Initial State is  [4, 21, 5]\n",
      "episode 1086, reward 257.0, memory_length 2000, epsilon 0.3370433025720758, time 732.0, rides 118\n",
      "Initial State is  [0, 17, 6]\n",
      "episode 1087, reward 520.0, memory_length 2000, epsilon 0.33670625926950376, time 733.0, rides 127\n",
      "Initial State is  [4, 16, 3]\n",
      "episode 1088, reward 307.0, memory_length 2000, epsilon 0.33636955301023425, time 722.0, rides 131\n",
      "Initial State is  [3, 18, 2]\n",
      "episode 1089, reward 537.0, memory_length 2000, epsilon 0.336033183457224, time 728.0, rides 131\n",
      "Initial State is  [2, 12, 6]\n",
      "episode 1090, reward 482.0, memory_length 2000, epsilon 0.3356971502737668, time 730.0, rides 124\n",
      "Initial State is  [1, 10, 6]\n",
      "episode 1091, reward 768.0, memory_length 2000, epsilon 0.335361453123493, time 724.0, rides 124\n",
      "Initial State is  [2, 10, 2]\n",
      "episode 1092, reward 216.0, memory_length 2000, epsilon 0.3350260916703695, time 734.0, rides 112\n",
      "Initial State is  [0, 23, 0]\n",
      "episode 1093, reward 464.0, memory_length 2000, epsilon 0.33469106557869915, time 727.0, rides 121\n",
      "Initial State is  [2, 0, 3]\n",
      "episode 1094, reward 700.0, memory_length 2000, epsilon 0.33435637451312045, time 737.0, rides 129\n",
      "Initial State is  [4, 20, 1]\n",
      "episode 1095, reward 208.0, memory_length 2000, epsilon 0.3340220181386073, time 731.0, rides 129\n",
      "Initial State is  [2, 23, 6]\n",
      "episode 1096, reward 458.0, memory_length 2000, epsilon 0.33368799612046873, time 735.0, rides 119\n",
      "Initial State is  [1, 7, 3]\n",
      "episode 1097, reward 623.0, memory_length 2000, epsilon 0.33335430812434824, time 738.0, rides 126\n",
      "Initial State is  [0, 17, 3]\n",
      "episode 1098, reward 366.0, memory_length 2000, epsilon 0.3330209538162239, time 720.0, rides 130\n",
      "Initial State is  [0, 10, 2]\n",
      "episode 1099, reward 683.0, memory_length 2000, epsilon 0.33268793286240766, time 732.0, rides 134\n",
      "Initial State is  [4, 12, 6]\n",
      "episode 1100, reward 207.0, memory_length 2000, epsilon 0.33235524492954527, time 726.0, rides 122\n",
      "Initial State is  [3, 2, 2]\n",
      "episode 1101, reward 385.0, memory_length 2000, epsilon 0.33202288968461574, time 730.0, rides 128\n",
      "Initial State is  [2, 17, 6]\n",
      "episode 1102, reward 266.0, memory_length 2000, epsilon 0.33169086679493115, time 724.0, rides 121\n",
      "Initial State is  [4, 6, 2]\n",
      "episode 1103, reward 277.0, memory_length 2000, epsilon 0.33135917592813624, time 727.0, rides 123\n",
      "Initial State is  [0, 4, 5]\n",
      "episode 1104, reward 270.0, memory_length 2000, epsilon 0.3310278167522081, time 731.0, rides 113\n",
      "Initial State is  [3, 22, 3]\n",
      "episode 1105, reward 205.0, memory_length 2000, epsilon 0.3306967889354559, time 724.0, rides 113\n",
      "Initial State is  [3, 8, 3]\n",
      "episode 1106, reward 486.0, memory_length 2000, epsilon 0.33036609214652046, time 722.0, rides 118\n",
      "Initial State is  [3, 21, 6]\n",
      "episode 1107, reward 257.0, memory_length 2000, epsilon 0.3300357260543739, time 734.0, rides 139\n",
      "Initial State is  [2, 13, 5]\n",
      "episode 1108, reward 298.0, memory_length 2000, epsilon 0.32970569032831953, time 725.0, rides 112\n",
      "Initial State is  [4, 5, 2]\n",
      "episode 1109, reward 552.0, memory_length 2000, epsilon 0.3293759846379912, time 722.0, rides 123\n",
      "Initial State is  [1, 10, 5]\n",
      "episode 1110, reward 392.0, memory_length 2000, epsilon 0.3290466086533532, time 725.0, rides 119\n",
      "Initial State is  [0, 15, 5]\n",
      "episode 1111, reward 268.0, memory_length 2000, epsilon 0.3287175620446999, time 731.0, rides 125\n",
      "Initial State is  [2, 20, 1]\n",
      "episode 1112, reward 552.0, memory_length 2000, epsilon 0.32838884448265515, time 726.0, rides 147\n",
      "Initial State is  [1, 23, 4]\n",
      "episode 1113, reward 424.0, memory_length 2000, epsilon 0.3280604556381725, time 728.0, rides 126\n",
      "Initial State is  [3, 10, 5]\n",
      "episode 1114, reward 537.0, memory_length 2000, epsilon 0.32773239518253433, time 727.0, rides 137\n",
      "Initial State is  [0, 8, 5]\n",
      "episode 1115, reward 338.0, memory_length 2000, epsilon 0.3274046627873518, time 730.0, rides 123\n",
      "Initial State is  [3, 11, 0]\n",
      "episode 1116, reward 151.0, memory_length 2000, epsilon 0.32707725812456445, time 723.0, rides 123\n",
      "Initial State is  [4, 7, 3]\n",
      "episode 1117, reward 525.0, memory_length 2000, epsilon 0.32675018086643987, time 728.0, rides 127\n",
      "Initial State is  [2, 12, 4]\n",
      "episode 1118, reward 154.0, memory_length 2000, epsilon 0.32642343068557345, time 726.0, rides 132\n",
      "Initial State is  [4, 11, 5]\n",
      "episode 1119, reward 378.0, memory_length 2000, epsilon 0.3260970072548879, time 727.0, rides 119\n",
      "Initial State is  [2, 0, 2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 1120, reward 522.0, memory_length 2000, epsilon 0.325770910247633, time 732.0, rides 127\n",
      "Initial State is  [0, 6, 3]\n",
      "episode 1121, reward 300.0, memory_length 2000, epsilon 0.32544513933738534, time 728.0, rides 126\n",
      "Initial State is  [1, 14, 0]\n",
      "episode 1122, reward 313.0, memory_length 2000, epsilon 0.3251196941980479, time 725.0, rides 131\n",
      "Initial State is  [1, 11, 5]\n",
      "episode 1123, reward 268.0, memory_length 2000, epsilon 0.32479457450384985, time 732.0, rides 117\n",
      "Initial State is  [4, 19, 2]\n",
      "episode 1124, reward 583.0, memory_length 2000, epsilon 0.324469779929346, time 734.0, rides 115\n",
      "Initial State is  [3, 0, 2]\n",
      "episode 1125, reward 422.0, memory_length 2000, epsilon 0.32414531014941667, time 731.0, rides 117\n",
      "Initial State is  [2, 4, 3]\n",
      "episode 1126, reward 470.0, memory_length 2000, epsilon 0.32382116483926726, time 732.0, rides 124\n",
      "Initial State is  [0, 10, 5]\n",
      "episode 1127, reward 391.0, memory_length 2000, epsilon 0.323497343674428, time 731.0, rides 138\n",
      "Initial State is  [4, 9, 2]\n",
      "episode 1128, reward 442.0, memory_length 2000, epsilon 0.3231738463307536, time 723.0, rides 135\n",
      "Initial State is  [2, 15, 3]\n",
      "episode 1129, reward 342.0, memory_length 2000, epsilon 0.32285067248442284, time 738.0, rides 134\n",
      "Initial State is  [3, 1, 4]\n",
      "episode 1130, reward 657.0, memory_length 2000, epsilon 0.3225278218119384, time 729.0, rides 125\n",
      "Initial State is  [3, 23, 3]\n",
      "episode 1131, reward 428.0, memory_length 2000, epsilon 0.3222052939901265, time 731.0, rides 117\n",
      "Initial State is  [3, 21, 2]\n",
      "episode 1132, reward 224.0, memory_length 2000, epsilon 0.32188308869613635, time 721.0, rides 107\n",
      "Initial State is  [3, 20, 6]\n",
      "episode 1133, reward 200.0, memory_length 2000, epsilon 0.3215612056074402, time 732.0, rides 119\n",
      "Initial State is  [4, 19, 2]\n",
      "episode 1134, reward 358.0, memory_length 2000, epsilon 0.3212396444018328, time 734.0, rides 127\n",
      "Initial State is  [4, 23, 5]\n",
      "episode 1135, reward 702.0, memory_length 2000, epsilon 0.32091840475743094, time 729.0, rides 139\n",
      "Initial State is  [0, 17, 1]\n",
      "episode 1136, reward 118.0, memory_length 2000, epsilon 0.3205974863526735, time 732.0, rides 126\n",
      "Initial State is  [1, 9, 5]\n",
      "episode 1137, reward 136.0, memory_length 2000, epsilon 0.32027688886632083, time 725.0, rides 126\n",
      "Initial State is  [4, 17, 5]\n",
      "episode 1138, reward 643.0, memory_length 2000, epsilon 0.3199566119774545, time 730.0, rides 119\n",
      "Initial State is  [4, 2, 2]\n",
      "episode 1139, reward 137.0, memory_length 2000, epsilon 0.31963665536547703, time 739.0, rides 130\n",
      "Initial State is  [1, 16, 0]\n",
      "episode 1140, reward 562.0, memory_length 2000, epsilon 0.31931701871011153, time 731.0, rides 132\n",
      "Initial State is  [1, 7, 0]\n",
      "episode 1141, reward 260.0, memory_length 2000, epsilon 0.3189977016914014, time 727.0, rides 129\n",
      "Initial State is  [4, 19, 0]\n",
      "episode 1142, reward 354.0, memory_length 2000, epsilon 0.31867870398971, time 731.0, rides 133\n",
      "Initial State is  [0, 7, 4]\n",
      "episode 1143, reward 406.0, memory_length 2000, epsilon 0.31836002528572027, time 731.0, rides 128\n",
      "Initial State is  [1, 12, 3]\n",
      "episode 1144, reward 455.0, memory_length 2000, epsilon 0.31804166526043454, time 723.0, rides 116\n",
      "Initial State is  [3, 11, 0]\n",
      "episode 1145, reward 787.0, memory_length 2000, epsilon 0.3177236235951741, time 727.0, rides 132\n",
      "Initial State is  [0, 14, 2]\n",
      "episode 1146, reward 216.0, memory_length 2000, epsilon 0.3174058999715789, time 725.0, rides 130\n",
      "Initial State is  [3, 20, 4]\n",
      "episode 1147, reward 141.0, memory_length 2000, epsilon 0.31708849407160733, time 735.0, rides 122\n",
      "Initial State is  [3, 9, 5]\n",
      "episode 1148, reward 535.0, memory_length 2000, epsilon 0.31677140557753575, time 730.0, rides 134\n",
      "Initial State is  [3, 14, 5]\n",
      "episode 1149, reward 350.0, memory_length 2000, epsilon 0.31645463417195824, time 730.0, rides 122\n",
      "Initial State is  [0, 18, 4]\n",
      "episode 1150, reward 488.0, memory_length 2000, epsilon 0.3161381795377863, time 722.0, rides 136\n",
      "Initial State is  [0, 13, 0]\n",
      "episode 1151, reward 630.0, memory_length 2000, epsilon 0.3158220413582485, time 737.0, rides 118\n",
      "Initial State is  [1, 10, 2]\n",
      "episode 1152, reward 589.0, memory_length 2000, epsilon 0.3155062193168902, time 739.0, rides 135\n",
      "Initial State is  [2, 23, 4]\n",
      "episode 1153, reward 481.0, memory_length 2000, epsilon 0.31519071309757335, time 730.0, rides 128\n",
      "Initial State is  [2, 15, 5]\n",
      "episode 1154, reward 547.0, memory_length 2000, epsilon 0.3148755223844758, time 738.0, rides 135\n",
      "Initial State is  [1, 9, 0]\n",
      "episode 1155, reward 295.0, memory_length 2000, epsilon 0.31456064686209134, time 721.0, rides 129\n",
      "Initial State is  [0, 4, 4]\n",
      "episode 1156, reward 413.0, memory_length 2000, epsilon 0.31424608621522926, time 728.0, rides 120\n",
      "Initial State is  [3, 3, 6]\n",
      "episode 1157, reward 259.0, memory_length 2000, epsilon 0.31393184012901404, time 724.0, rides 130\n",
      "Initial State is  [1, 13, 1]\n",
      "episode 1158, reward 452.0, memory_length 2000, epsilon 0.31361790828888503, time 721.0, rides 118\n",
      "Initial State is  [3, 21, 3]\n",
      "episode 1159, reward 90.0, memory_length 2000, epsilon 0.31330429038059615, time 735.0, rides 125\n",
      "Initial State is  [1, 5, 3]\n",
      "episode 1160, reward 303.0, memory_length 2000, epsilon 0.31299098609021553, time 732.0, rides 115\n",
      "Initial State is  [4, 2, 1]\n",
      "episode 1161, reward 620.0, memory_length 2000, epsilon 0.3126779951041253, time 725.0, rides 125\n",
      "Initial State is  [0, 1, 5]\n",
      "episode 1162, reward 250.0, memory_length 2000, epsilon 0.31236531710902116, time 731.0, rides 129\n",
      "Initial State is  [4, 0, 2]\n",
      "episode 1163, reward 53.0, memory_length 2000, epsilon 0.3120529517919121, time 726.0, rides 141\n",
      "Initial State is  [4, 15, 5]\n",
      "episode 1164, reward 502.0, memory_length 2000, epsilon 0.3117408988401202, time 732.0, rides 130\n",
      "Initial State is  [2, 8, 3]\n",
      "episode 1165, reward 212.0, memory_length 2000, epsilon 0.31142915794128007, time 726.0, rides 122\n",
      "Initial State is  [2, 18, 2]\n",
      "episode 1166, reward 485.0, memory_length 2000, epsilon 0.31111772878333877, time 744.0, rides 126\n",
      "Initial State is  [1, 4, 2]\n",
      "episode 1167, reward 551.0, memory_length 2000, epsilon 0.3108066110545554, time 736.0, rides 141\n",
      "Initial State is  [4, 6, 0]\n",
      "episode 1168, reward 544.0, memory_length 2000, epsilon 0.3104958044435009, time 728.0, rides 142\n",
      "Initial State is  [3, 14, 4]\n",
      "episode 1169, reward 352.0, memory_length 2000, epsilon 0.3101853086390574, time 731.0, rides 116\n",
      "Initial State is  [4, 11, 2]\n",
      "episode 1170, reward 756.0, memory_length 2000, epsilon 0.30987512333041833, time 735.0, rides 126\n",
      "Initial State is  [1, 16, 3]\n",
      "episode 1171, reward 372.0, memory_length 2000, epsilon 0.3095652482070879, time 725.0, rides 142\n",
      "Initial State is  [2, 9, 0]\n",
      "episode 1172, reward 249.0, memory_length 2000, epsilon 0.30925568295888084, time 731.0, rides 121\n",
      "Initial State is  [4, 10, 0]\n",
      "episode 1173, reward 543.0, memory_length 2000, epsilon 0.308946427275922, time 726.0, rides 133\n",
      "Initial State is  [1, 3, 1]\n",
      "episode 1174, reward 284.0, memory_length 2000, epsilon 0.30863748084864606, time 729.0, rides 117\n",
      "Initial State is  [3, 18, 2]\n",
      "episode 1175, reward 847.0, memory_length 2000, epsilon 0.3083288433677974, time 730.0, rides 129\n",
      "Initial State is  [0, 16, 0]\n",
      "episode 1176, reward 323.0, memory_length 2000, epsilon 0.30802051452442963, time 731.0, rides 133\n",
      "Initial State is  [0, 8, 1]\n",
      "episode 1177, reward 648.0, memory_length 2000, epsilon 0.3077124940099052, time 733.0, rides 131\n",
      "Initial State is  [4, 17, 0]\n",
      "episode 1178, reward 705.0, memory_length 2000, epsilon 0.3074047815158953, time 732.0, rides 130\n",
      "Initial State is  [3, 5, 5]\n",
      "episode 1179, reward 219.0, memory_length 2000, epsilon 0.30709737673437937, time 730.0, rides 113\n",
      "Initial State is  [1, 12, 0]\n",
      "episode 1180, reward 384.0, memory_length 2000, epsilon 0.30679027935764497, time 731.0, rides 119\n",
      "Initial State is  [0, 4, 0]\n",
      "episode 1181, reward 232.0, memory_length 2000, epsilon 0.3064834890782873, time 732.0, rides 116\n",
      "Initial State is  [0, 16, 6]\n",
      "episode 1182, reward 431.0, memory_length 2000, epsilon 0.306177005589209, time 728.0, rides 139\n",
      "Initial State is  [2, 0, 5]\n",
      "episode 1183, reward 624.0, memory_length 2000, epsilon 0.3058708285836198, time 727.0, rides 121\n",
      "Initial State is  [2, 21, 0]\n",
      "episode 1184, reward 361.0, memory_length 2000, epsilon 0.30556495775503617, time 731.0, rides 118\n",
      "Initial State is  [0, 20, 2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 1185, reward 306.0, memory_length 2000, epsilon 0.30525939279728115, time 729.0, rides 130\n",
      "Initial State is  [2, 3, 4]\n",
      "episode 1186, reward 407.0, memory_length 2000, epsilon 0.30495413340448385, time 725.0, rides 131\n",
      "Initial State is  [0, 18, 2]\n",
      "episode 1187, reward 307.0, memory_length 2000, epsilon 0.3046491792710794, time 727.0, rides 128\n",
      "Initial State is  [2, 22, 4]\n",
      "episode 1188, reward 676.0, memory_length 2000, epsilon 0.3043445300918083, time 732.0, rides 138\n",
      "Initial State is  [4, 4, 4]\n",
      "episode 1189, reward 321.0, memory_length 2000, epsilon 0.30404018556171647, time 723.0, rides 130\n",
      "Initial State is  [3, 7, 5]\n",
      "episode 1190, reward 236.0, memory_length 2000, epsilon 0.30373614537615473, time 726.0, rides 116\n",
      "Initial State is  [3, 16, 6]\n",
      "episode 1191, reward 554.0, memory_length 2000, epsilon 0.3034324092307786, time 730.0, rides 131\n",
      "Initial State is  [1, 15, 0]\n",
      "episode 1192, reward 600.0, memory_length 2000, epsilon 0.3031289768215478, time 735.0, rides 140\n",
      "Initial State is  [3, 16, 0]\n",
      "episode 1193, reward 372.0, memory_length 2000, epsilon 0.30282584784472627, time 729.0, rides 134\n",
      "Initial State is  [3, 23, 2]\n",
      "episode 1194, reward 283.0, memory_length 2000, epsilon 0.3025230219968815, time 731.0, rides 116\n",
      "Initial State is  [0, 14, 2]\n",
      "episode 1195, reward 427.0, memory_length 2000, epsilon 0.3022204989748846, time 728.0, rides 115\n",
      "Initial State is  [3, 10, 0]\n",
      "episode 1196, reward 430.0, memory_length 2000, epsilon 0.30191827847590974, time 733.0, rides 132\n",
      "Initial State is  [1, 23, 0]\n",
      "episode 1197, reward 409.0, memory_length 2000, epsilon 0.3016163601974338, time 728.0, rides 136\n",
      "Initial State is  [4, 17, 1]\n",
      "episode 1198, reward 472.0, memory_length 2000, epsilon 0.3013147438372364, time 738.0, rides 130\n",
      "Initial State is  [4, 21, 5]\n",
      "episode 1199, reward 601.0, memory_length 2000, epsilon 0.3010134290933992, time 727.0, rides 131\n",
      "Initial State is  [4, 4, 3]\n",
      "episode 1200, reward 538.0, memory_length 2000, epsilon 0.3007124156643058, time 736.0, rides 130\n",
      "Initial State is  [4, 15, 4]\n",
      "episode 1201, reward 339.0, memory_length 2000, epsilon 0.30041170324864147, time 727.0, rides 118\n",
      "Initial State is  [4, 12, 0]\n",
      "episode 1202, reward 329.0, memory_length 2000, epsilon 0.30011129154539284, time 723.0, rides 111\n",
      "Initial State is  [3, 13, 3]\n",
      "episode 1203, reward 295.0, memory_length 2000, epsilon 0.29981118025384745, time 729.0, rides 128\n",
      "Initial State is  [1, 9, 3]\n",
      "episode 1204, reward 300.0, memory_length 2000, epsilon 0.2995113690735936, time 726.0, rides 129\n",
      "Initial State is  [2, 5, 0]\n",
      "episode 1205, reward 300.0, memory_length 2000, epsilon 0.29921185770452, time 729.0, rides 114\n",
      "Initial State is  [1, 4, 1]\n",
      "episode 1206, reward 105.0, memory_length 2000, epsilon 0.2989126458468155, time 734.0, rides 123\n",
      "Initial State is  [3, 15, 3]\n",
      "episode 1207, reward 843.0, memory_length 2000, epsilon 0.2986137332009687, time 735.0, rides 121\n",
      "Initial State is  [1, 5, 4]\n",
      "episode 1208, reward 401.0, memory_length 2000, epsilon 0.29831511946776773, time 732.0, rides 130\n",
      "Initial State is  [4, 10, 5]\n",
      "episode 1209, reward 170.0, memory_length 2000, epsilon 0.29801680434829997, time 724.0, rides 121\n",
      "Initial State is  [1, 14, 1]\n",
      "episode 1210, reward 230.0, memory_length 2000, epsilon 0.29771878754395165, time 727.0, rides 119\n",
      "Initial State is  [1, 8, 4]\n",
      "episode 1211, reward 714.0, memory_length 2000, epsilon 0.29742106875640767, time 737.0, rides 122\n",
      "Initial State is  [4, 0, 0]\n",
      "episode 1212, reward 575.0, memory_length 2000, epsilon 0.29712364768765126, time 736.0, rides 119\n",
      "Initial State is  [2, 15, 4]\n",
      "episode 1213, reward 829.0, memory_length 2000, epsilon 0.2968265240399636, time 737.0, rides 126\n",
      "Initial State is  [3, 16, 1]\n",
      "episode 1214, reward 142.0, memory_length 2000, epsilon 0.2965296975159237, time 732.0, rides 128\n",
      "Initial State is  [4, 19, 1]\n",
      "episode 1215, reward 249.0, memory_length 2000, epsilon 0.29623316781840775, time 732.0, rides 122\n",
      "Initial State is  [2, 8, 6]\n",
      "episode 1216, reward 452.0, memory_length 2000, epsilon 0.29593693465058934, time 723.0, rides 124\n",
      "Initial State is  [0, 3, 4]\n",
      "episode 1217, reward 398.0, memory_length 2000, epsilon 0.29564099771593877, time 731.0, rides 133\n",
      "Initial State is  [3, 8, 1]\n",
      "episode 1218, reward 631.0, memory_length 2000, epsilon 0.2953453567182228, time 733.0, rides 130\n",
      "Initial State is  [0, 7, 5]\n",
      "episode 1219, reward 400.0, memory_length 2000, epsilon 0.2950500113615046, time 726.0, rides 125\n",
      "Initial State is  [1, 3, 5]\n",
      "episode 1220, reward 227.0, memory_length 2000, epsilon 0.2947549613501431, time 724.0, rides 127\n",
      "Initial State is  [0, 19, 6]\n",
      "episode 1221, reward 146.0, memory_length 2000, epsilon 0.294460206388793, time 722.0, rides 126\n",
      "Initial State is  [2, 5, 3]\n",
      "episode 1222, reward 391.0, memory_length 2000, epsilon 0.2941657461824042, time 729.0, rides 118\n",
      "Initial State is  [4, 17, 1]\n",
      "episode 1223, reward 621.0, memory_length 2000, epsilon 0.2938715804362218, time 730.0, rides 131\n",
      "Initial State is  [1, 5, 1]\n",
      "episode 1224, reward 340.0, memory_length 2000, epsilon 0.2935777088557856, time 727.0, rides 114\n",
      "Initial State is  [2, 1, 4]\n",
      "episode 1225, reward 405.0, memory_length 2000, epsilon 0.2932841311469298, time 730.0, rides 122\n",
      "Initial State is  [0, 18, 0]\n",
      "episode 1226, reward 408.0, memory_length 2000, epsilon 0.2929908470157829, time 729.0, rides 146\n",
      "Initial State is  [3, 14, 6]\n",
      "episode 1227, reward 610.0, memory_length 2000, epsilon 0.2926978561687671, time 724.0, rides 131\n",
      "Initial State is  [1, 7, 1]\n",
      "episode 1228, reward 582.0, memory_length 2000, epsilon 0.29240515831259833, time 727.0, rides 132\n",
      "Initial State is  [1, 21, 1]\n",
      "episode 1229, reward 727.0, memory_length 2000, epsilon 0.29211275315428575, time 730.0, rides 140\n",
      "Initial State is  [0, 15, 5]\n",
      "episode 1230, reward 543.0, memory_length 2000, epsilon 0.2918206404011315, time 736.0, rides 133\n",
      "Initial State is  [3, 19, 5]\n",
      "episode 1231, reward 639.0, memory_length 2000, epsilon 0.29152881976073036, time 725.0, rides 124\n",
      "Initial State is  [0, 19, 2]\n",
      "episode 1232, reward 346.0, memory_length 2000, epsilon 0.2912372909409696, time 737.0, rides 123\n",
      "Initial State is  [2, 8, 5]\n",
      "episode 1233, reward 440.0, memory_length 2000, epsilon 0.2909460536500286, time 730.0, rides 121\n",
      "Initial State is  [4, 8, 5]\n",
      "episode 1234, reward 604.0, memory_length 2000, epsilon 0.2906551075963786, time 728.0, rides 113\n",
      "Initial State is  [0, 9, 6]\n",
      "episode 1235, reward 559.0, memory_length 2000, epsilon 0.2903644524887822, time 724.0, rides 124\n",
      "Initial State is  [0, 13, 6]\n",
      "episode 1236, reward 562.0, memory_length 2000, epsilon 0.2900740880362934, time 733.0, rides 115\n",
      "Initial State is  [0, 19, 2]\n",
      "episode 1237, reward 429.0, memory_length 2000, epsilon 0.28978401394825715, time 734.0, rides 128\n",
      "Initial State is  [4, 22, 3]\n",
      "episode 1238, reward 318.0, memory_length 2000, epsilon 0.2894942299343089, time 727.0, rides 139\n",
      "Initial State is  [4, 2, 0]\n",
      "episode 1239, reward 735.0, memory_length 2000, epsilon 0.2892047357043746, time 729.0, rides 115\n",
      "Initial State is  [1, 15, 4]\n",
      "episode 1240, reward 468.0, memory_length 2000, epsilon 0.28891553096867023, time 730.0, rides 130\n",
      "Initial State is  [1, 6, 2]\n",
      "episode 1241, reward 510.0, memory_length 2000, epsilon 0.2886266154377016, time 725.0, rides 128\n",
      "Initial State is  [2, 4, 6]\n",
      "episode 1242, reward 351.0, memory_length 2000, epsilon 0.28833798882226386, time 727.0, rides 126\n",
      "Initial State is  [0, 4, 1]\n",
      "episode 1243, reward 787.0, memory_length 2000, epsilon 0.2880496508334416, time 724.0, rides 134\n",
      "Initial State is  [1, 6, 6]\n",
      "episode 1244, reward 496.0, memory_length 2000, epsilon 0.28776160118260813, time 727.0, rides 129\n",
      "Initial State is  [2, 16, 6]\n",
      "episode 1245, reward 583.0, memory_length 2000, epsilon 0.2874738395814255, time 727.0, rides 131\n",
      "Initial State is  [3, 18, 1]\n",
      "episode 1246, reward 830.0, memory_length 2000, epsilon 0.2871863657418441, time 736.0, rides 129\n",
      "Initial State is  [2, 23, 1]\n",
      "episode 1247, reward 696.0, memory_length 2000, epsilon 0.28689917937610226, time 732.0, rides 135\n",
      "Initial State is  [0, 20, 6]\n",
      "episode 1248, reward 924.0, memory_length 2000, epsilon 0.2866122801967262, time 723.0, rides 137\n",
      "Initial State is  [2, 15, 2]\n",
      "episode 1249, reward 473.0, memory_length 2000, epsilon 0.28632566791652947, time 728.0, rides 120\n",
      "Initial State is  [1, 5, 6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 1250, reward 143.0, memory_length 2000, epsilon 0.28603934224861294, time 722.0, rides 126\n",
      "Initial State is  [2, 9, 2]\n",
      "episode 1251, reward 252.0, memory_length 2000, epsilon 0.2857533029063643, time 727.0, rides 144\n",
      "Initial State is  [3, 1, 6]\n",
      "episode 1252, reward 392.0, memory_length 2000, epsilon 0.28546754960345794, time 733.0, rides 118\n",
      "Initial State is  [2, 16, 5]\n",
      "episode 1253, reward 535.0, memory_length 2000, epsilon 0.2851820820538545, time 724.0, rides 120\n",
      "Initial State is  [1, 2, 4]\n",
      "episode 1254, reward 566.0, memory_length 2000, epsilon 0.2848968999718006, time 722.0, rides 124\n",
      "Initial State is  [2, 23, 4]\n",
      "episode 1255, reward 374.0, memory_length 2000, epsilon 0.2846120030718288, time 725.0, rides 120\n",
      "Initial State is  [2, 10, 2]\n",
      "episode 1256, reward 280.0, memory_length 2000, epsilon 0.284327391068757, time 723.0, rides 121\n",
      "Initial State is  [2, 10, 6]\n",
      "episode 1257, reward 297.0, memory_length 2000, epsilon 0.2840430636776882, time 730.0, rides 117\n",
      "Initial State is  [3, 13, 1]\n",
      "episode 1258, reward 177.0, memory_length 2000, epsilon 0.28375902061401054, time 721.0, rides 115\n",
      "Initial State is  [3, 10, 6]\n",
      "episode 1259, reward 472.0, memory_length 2000, epsilon 0.28347526159339653, time 726.0, rides 118\n",
      "Initial State is  [4, 13, 4]\n",
      "episode 1260, reward 392.0, memory_length 2000, epsilon 0.28319178633180314, time 732.0, rides 123\n",
      "Initial State is  [2, 14, 4]\n",
      "episode 1261, reward 468.0, memory_length 2000, epsilon 0.28290859454547135, time 729.0, rides 129\n",
      "Initial State is  [4, 20, 4]\n",
      "episode 1262, reward 352.0, memory_length 2000, epsilon 0.28262568595092585, time 727.0, rides 108\n",
      "Initial State is  [0, 8, 4]\n",
      "episode 1263, reward 415.0, memory_length 2000, epsilon 0.2823430602649749, time 730.0, rides 132\n",
      "Initial State is  [0, 5, 3]\n",
      "episode 1264, reward 654.0, memory_length 2000, epsilon 0.28206071720470993, time 723.0, rides 134\n",
      "Initial State is  [4, 16, 0]\n",
      "episode 1265, reward 403.0, memory_length 2000, epsilon 0.2817786564875052, time 728.0, rides 120\n",
      "Initial State is  [1, 3, 4]\n",
      "episode 1266, reward 274.0, memory_length 2000, epsilon 0.28149687783101773, time 729.0, rides 126\n",
      "Initial State is  [1, 22, 0]\n",
      "episode 1267, reward 469.0, memory_length 2000, epsilon 0.2812153809531867, time 731.0, rides 123\n",
      "Initial State is  [0, 7, 6]\n",
      "episode 1268, reward 360.0, memory_length 2000, epsilon 0.28093416557223355, time 723.0, rides 113\n",
      "Initial State is  [1, 7, 5]\n",
      "episode 1269, reward 566.0, memory_length 2000, epsilon 0.2806532314066613, time 734.0, rides 111\n",
      "Initial State is  [4, 22, 6]\n",
      "episode 1270, reward 318.0, memory_length 2000, epsilon 0.2803725781752547, time 734.0, rides 107\n",
      "Initial State is  [4, 16, 6]\n",
      "episode 1271, reward 410.0, memory_length 2000, epsilon 0.2800922055970794, time 724.0, rides 124\n",
      "Initial State is  [4, 2, 5]\n",
      "episode 1272, reward 320.0, memory_length 2000, epsilon 0.2798121133914823, time 734.0, rides 130\n",
      "Initial State is  [4, 0, 2]\n",
      "episode 1273, reward 264.0, memory_length 2000, epsilon 0.2795323012780908, time 724.0, rides 115\n",
      "Initial State is  [1, 20, 0]\n",
      "episode 1274, reward 541.0, memory_length 2000, epsilon 0.2792527689768127, time 737.0, rides 127\n",
      "Initial State is  [2, 5, 3]\n",
      "episode 1275, reward 654.0, memory_length 2000, epsilon 0.2789735162078359, time 730.0, rides 130\n",
      "Initial State is  [3, 23, 1]\n",
      "episode 1276, reward 183.0, memory_length 2000, epsilon 0.2786945426916281, time 722.0, rides 129\n",
      "Initial State is  [3, 2, 4]\n",
      "episode 1277, reward 429.0, memory_length 2000, epsilon 0.27841584814893644, time 732.0, rides 116\n",
      "Initial State is  [2, 3, 4]\n",
      "episode 1278, reward 362.0, memory_length 2000, epsilon 0.2781374323007875, time 730.0, rides 130\n",
      "Initial State is  [1, 16, 1]\n",
      "episode 1279, reward 638.0, memory_length 2000, epsilon 0.27785929486848676, time 723.0, rides 127\n",
      "Initial State is  [4, 4, 3]\n",
      "episode 1280, reward 473.0, memory_length 2000, epsilon 0.27758143557361825, time 727.0, rides 115\n",
      "Initial State is  [3, 11, 5]\n",
      "episode 1281, reward 345.0, memory_length 2000, epsilon 0.27730385413804465, time 727.0, rides 125\n",
      "Initial State is  [4, 6, 1]\n",
      "episode 1282, reward 523.0, memory_length 2000, epsilon 0.2770265502839066, time 740.0, rides 123\n",
      "Initial State is  [1, 3, 4]\n",
      "episode 1283, reward 582.0, memory_length 2000, epsilon 0.2767495237336227, time 725.0, rides 121\n",
      "Initial State is  [1, 23, 5]\n",
      "episode 1284, reward 279.0, memory_length 2000, epsilon 0.2764727742098891, time 727.0, rides 126\n",
      "Initial State is  [0, 10, 1]\n",
      "episode 1285, reward 291.0, memory_length 2000, epsilon 0.2761963014356792, time 736.0, rides 126\n",
      "Initial State is  [1, 16, 0]\n",
      "episode 1286, reward 363.0, memory_length 2000, epsilon 0.27592010513424353, time 735.0, rides 118\n",
      "Initial State is  [3, 12, 6]\n",
      "episode 1287, reward 471.0, memory_length 2000, epsilon 0.2756441850291093, time 729.0, rides 128\n",
      "Initial State is  [3, 3, 5]\n",
      "episode 1288, reward 363.0, memory_length 2000, epsilon 0.2753685408440802, time 723.0, rides 114\n",
      "Initial State is  [1, 7, 0]\n",
      "episode 1289, reward 493.0, memory_length 2000, epsilon 0.2750931723032361, time 720.0, rides 112\n",
      "Initial State is  [2, 18, 6]\n",
      "episode 1290, reward 459.0, memory_length 2000, epsilon 0.27481807913093287, time 732.0, rides 112\n",
      "Initial State is  [4, 18, 5]\n",
      "episode 1291, reward 194.0, memory_length 2000, epsilon 0.27454326105180193, time 731.0, rides 122\n",
      "Initial State is  [3, 21, 1]\n",
      "episode 1292, reward 493.0, memory_length 2000, epsilon 0.2742687177907501, time 723.0, rides 122\n",
      "Initial State is  [3, 9, 6]\n",
      "episode 1293, reward 345.0, memory_length 2000, epsilon 0.2739944490729594, time 730.0, rides 133\n",
      "Initial State is  [4, 13, 2]\n",
      "episode 1294, reward 312.0, memory_length 2000, epsilon 0.27372045462388644, time 722.0, rides 125\n",
      "Initial State is  [2, 17, 5]\n",
      "episode 1295, reward 531.0, memory_length 2000, epsilon 0.2734467341692626, time 731.0, rides 125\n",
      "Initial State is  [3, 0, 4]\n",
      "episode 1296, reward 421.0, memory_length 2000, epsilon 0.27317328743509334, time 734.0, rides 127\n",
      "Initial State is  [4, 7, 3]\n",
      "episode 1297, reward 626.0, memory_length 2000, epsilon 0.27290011414765825, time 728.0, rides 131\n",
      "Initial State is  [1, 13, 3]\n",
      "episode 1298, reward 774.0, memory_length 2000, epsilon 0.2726272140335106, time 730.0, rides 125\n",
      "Initial State is  [3, 3, 2]\n",
      "episode 1299, reward 438.0, memory_length 2000, epsilon 0.27235458681947705, time 731.0, rides 126\n",
      "Initial State is  [2, 6, 2]\n",
      "episode 1300, reward 420.0, memory_length 2000, epsilon 0.2720822322326576, time 724.0, rides 123\n",
      "Initial State is  [0, 8, 3]\n",
      "episode 1301, reward 770.0, memory_length 2000, epsilon 0.2718101500004249, time 733.0, rides 123\n",
      "Initial State is  [1, 0, 2]\n",
      "episode 1302, reward 550.0, memory_length 2000, epsilon 0.27153833985042447, time 727.0, rides 116\n",
      "Initial State is  [0, 5, 4]\n",
      "episode 1303, reward 422.0, memory_length 2000, epsilon 0.27126680151057403, time 725.0, rides 119\n",
      "Initial State is  [4, 12, 4]\n",
      "episode 1304, reward 330.0, memory_length 2000, epsilon 0.27099553470906346, time 728.0, rides 140\n",
      "Initial State is  [0, 2, 1]\n",
      "episode 1305, reward 356.0, memory_length 2000, epsilon 0.2707245391743544, time 729.0, rides 134\n",
      "Initial State is  [0, 23, 4]\n",
      "episode 1306, reward 365.0, memory_length 2000, epsilon 0.27045381463518003, time 730.0, rides 131\n",
      "Initial State is  [2, 10, 5]\n",
      "episode 1307, reward 894.0, memory_length 2000, epsilon 0.27018336082054484, time 731.0, rides 129\n",
      "Initial State is  [3, 13, 5]\n",
      "episode 1308, reward 620.0, memory_length 2000, epsilon 0.2699131774597243, time 726.0, rides 127\n",
      "Initial State is  [3, 17, 5]\n",
      "episode 1309, reward 399.0, memory_length 2000, epsilon 0.2696432642822646, time 728.0, rides 129\n",
      "Initial State is  [0, 19, 6]\n",
      "episode 1310, reward 327.0, memory_length 2000, epsilon 0.26937362101798235, time 727.0, rides 120\n",
      "Initial State is  [2, 23, 0]\n",
      "episode 1311, reward 392.0, memory_length 2000, epsilon 0.26910424739696437, time 726.0, rides 119\n",
      "Initial State is  [3, 7, 2]\n",
      "episode 1312, reward 732.0, memory_length 2000, epsilon 0.2688351431495674, time 735.0, rides 121\n",
      "Initial State is  [3, 4, 5]\n",
      "episode 1313, reward 673.0, memory_length 2000, epsilon 0.26856630800641784, time 729.0, rides 128\n",
      "Initial State is  [1, 18, 5]\n",
      "episode 1314, reward 383.0, memory_length 2000, epsilon 0.2682977416984114, time 723.0, rides 129\n",
      "Initial State is  [0, 3, 5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 1315, reward 589.0, memory_length 2000, epsilon 0.268029443956713, time 729.0, rides 123\n",
      "Initial State is  [3, 18, 1]\n",
      "episode 1316, reward 422.0, memory_length 2000, epsilon 0.2677614145127563, time 727.0, rides 131\n",
      "Initial State is  [3, 16, 3]\n",
      "episode 1317, reward 451.0, memory_length 2000, epsilon 0.2674936530982436, time 723.0, rides 109\n",
      "Initial State is  [1, 11, 1]\n",
      "episode 1318, reward 352.0, memory_length 2000, epsilon 0.26722615944514533, time 730.0, rides 135\n",
      "Initial State is  [0, 8, 0]\n",
      "episode 1319, reward 536.0, memory_length 2000, epsilon 0.2669589332857002, time 728.0, rides 128\n",
      "Initial State is  [3, 13, 4]\n",
      "episode 1320, reward 310.0, memory_length 2000, epsilon 0.2666919743524145, time 733.0, rides 128\n",
      "Initial State is  [4, 4, 5]\n",
      "episode 1321, reward 495.0, memory_length 2000, epsilon 0.26642528237806207, time 732.0, rides 133\n",
      "Initial State is  [1, 19, 1]\n",
      "episode 1322, reward 487.0, memory_length 2000, epsilon 0.266158857095684, time 730.0, rides 121\n",
      "Initial State is  [3, 11, 0]\n",
      "episode 1323, reward 398.0, memory_length 2000, epsilon 0.2658926982385883, time 727.0, rides 123\n",
      "Initial State is  [3, 8, 0]\n",
      "episode 1324, reward 443.0, memory_length 2000, epsilon 0.26562680554034973, time 730.0, rides 118\n",
      "Initial State is  [4, 22, 4]\n",
      "episode 1325, reward 681.0, memory_length 2000, epsilon 0.26536117873480936, time 733.0, rides 130\n",
      "Initial State is  [1, 19, 2]\n",
      "episode 1326, reward 452.0, memory_length 2000, epsilon 0.26509581755607453, time 721.0, rides 126\n",
      "Initial State is  [2, 11, 0]\n",
      "episode 1327, reward 386.0, memory_length 2000, epsilon 0.26483072173851846, time 740.0, rides 128\n",
      "Initial State is  [4, 20, 5]\n",
      "episode 1328, reward 342.0, memory_length 2000, epsilon 0.26456589101677996, time 726.0, rides 136\n",
      "Initial State is  [2, 11, 3]\n",
      "episode 1329, reward 573.0, memory_length 2000, epsilon 0.2643013251257632, time 726.0, rides 136\n",
      "Initial State is  [2, 9, 2]\n",
      "episode 1330, reward 751.0, memory_length 2000, epsilon 0.2640370238006374, time 731.0, rides 117\n",
      "Initial State is  [0, 19, 6]\n",
      "episode 1331, reward 697.0, memory_length 2000, epsilon 0.2637729867768368, time 733.0, rides 110\n",
      "Initial State is  [4, 3, 3]\n",
      "episode 1332, reward 539.0, memory_length 2000, epsilon 0.26350921379006, time 726.0, rides 126\n",
      "Initial State is  [3, 10, 2]\n",
      "episode 1333, reward 374.0, memory_length 2000, epsilon 0.26324570457626995, time 729.0, rides 126\n",
      "Initial State is  [1, 21, 0]\n",
      "episode 1334, reward 378.0, memory_length 2000, epsilon 0.2629824588716937, time 737.0, rides 131\n",
      "Initial State is  [1, 12, 3]\n",
      "episode 1335, reward 521.0, memory_length 2000, epsilon 0.262719476412822, time 733.0, rides 139\n",
      "Initial State is  [0, 19, 2]\n",
      "episode 1336, reward 196.0, memory_length 2000, epsilon 0.2624567569364092, time 732.0, rides 123\n",
      "Initial State is  [0, 1, 5]\n",
      "episode 1337, reward 375.0, memory_length 2000, epsilon 0.26219430017947276, time 731.0, rides 129\n",
      "Initial State is  [4, 9, 5]\n",
      "episode 1338, reward 519.0, memory_length 2000, epsilon 0.2619321058792933, time 731.0, rides 134\n",
      "Initial State is  [3, 12, 5]\n",
      "episode 1339, reward 632.0, memory_length 2000, epsilon 0.261670173773414, time 725.0, rides 122\n",
      "Initial State is  [1, 14, 5]\n",
      "episode 1340, reward 361.0, memory_length 2000, epsilon 0.2614085035996406, time 728.0, rides 121\n",
      "Initial State is  [4, 13, 3]\n",
      "episode 1341, reward 404.0, memory_length 2000, epsilon 0.26114709509604095, time 730.0, rides 111\n",
      "Initial State is  [4, 17, 0]\n",
      "episode 1342, reward 760.0, memory_length 2000, epsilon 0.2608859480009449, time 732.0, rides 132\n",
      "Initial State is  [0, 4, 2]\n",
      "episode 1343, reward 503.0, memory_length 2000, epsilon 0.26062506205294395, time 724.0, rides 143\n",
      "Initial State is  [4, 7, 5]\n",
      "episode 1344, reward 327.0, memory_length 2000, epsilon 0.260364436990891, time 729.0, rides 132\n",
      "Initial State is  [4, 6, 6]\n",
      "episode 1345, reward 293.0, memory_length 2000, epsilon 0.2601040725539001, time 729.0, rides 133\n",
      "Initial State is  [0, 13, 1]\n",
      "episode 1346, reward 427.0, memory_length 2000, epsilon 0.2598439684813462, time 727.0, rides 120\n",
      "Initial State is  [4, 12, 4]\n",
      "episode 1347, reward 428.0, memory_length 2000, epsilon 0.2595841245128649, time 732.0, rides 125\n",
      "Initial State is  [1, 18, 6]\n",
      "episode 1348, reward 457.0, memory_length 2000, epsilon 0.259324540388352, time 727.0, rides 133\n",
      "Initial State is  [4, 4, 3]\n",
      "episode 1349, reward 496.0, memory_length 2000, epsilon 0.25906521584796366, time 736.0, rides 121\n",
      "Initial State is  [0, 6, 2]\n",
      "episode 1350, reward 626.0, memory_length 2000, epsilon 0.2588061506321157, time 731.0, rides 139\n",
      "Initial State is  [4, 14, 0]\n",
      "episode 1351, reward 393.0, memory_length 2000, epsilon 0.2585473444814836, time 732.0, rides 142\n",
      "Initial State is  [3, 15, 0]\n",
      "episode 1352, reward 431.0, memory_length 2000, epsilon 0.2582887971370021, time 726.0, rides 137\n",
      "Initial State is  [2, 23, 6]\n",
      "episode 1353, reward 418.0, memory_length 2000, epsilon 0.2580305083398651, time 731.0, rides 133\n",
      "Initial State is  [3, 17, 5]\n",
      "episode 1354, reward 511.0, memory_length 2000, epsilon 0.2577724778315252, time 733.0, rides 131\n",
      "Initial State is  [2, 5, 3]\n",
      "episode 1355, reward 326.0, memory_length 2000, epsilon 0.25751470535369364, time 725.0, rides 123\n",
      "Initial State is  [0, 11, 5]\n",
      "episode 1356, reward 508.0, memory_length 2000, epsilon 0.25725719064833996, time 724.0, rides 139\n",
      "Initial State is  [4, 5, 6]\n",
      "episode 1357, reward 482.0, memory_length 2000, epsilon 0.2569999334576916, time 733.0, rides 113\n",
      "Initial State is  [2, 19, 4]\n",
      "episode 1358, reward 462.0, memory_length 2000, epsilon 0.25674293352423394, time 729.0, rides 125\n",
      "Initial State is  [2, 18, 5]\n",
      "episode 1359, reward 451.0, memory_length 2000, epsilon 0.2564861905907097, time 732.0, rides 126\n",
      "Initial State is  [1, 12, 4]\n",
      "episode 1360, reward 833.0, memory_length 2000, epsilon 0.256229704400119, time 729.0, rides 129\n",
      "Initial State is  [3, 6, 2]\n",
      "episode 1361, reward 551.0, memory_length 2000, epsilon 0.25597347469571885, time 727.0, rides 117\n",
      "Initial State is  [1, 12, 4]\n",
      "episode 1362, reward 386.0, memory_length 2000, epsilon 0.25571750122102316, time 731.0, rides 143\n",
      "Initial State is  [2, 11, 2]\n",
      "episode 1363, reward 737.0, memory_length 2000, epsilon 0.25546178371980216, time 728.0, rides 123\n",
      "Initial State is  [2, 14, 6]\n",
      "episode 1364, reward 167.0, memory_length 2000, epsilon 0.25520632193608234, time 741.0, rides 131\n",
      "Initial State is  [4, 22, 4]\n",
      "episode 1365, reward 628.0, memory_length 2000, epsilon 0.25495111561414624, time 732.0, rides 132\n",
      "Initial State is  [2, 18, 2]\n",
      "episode 1366, reward 444.0, memory_length 2000, epsilon 0.2546961644985321, time 732.0, rides 134\n",
      "Initial State is  [0, 4, 2]\n",
      "episode 1367, reward 357.0, memory_length 2000, epsilon 0.2544414683340336, time 732.0, rides 121\n",
      "Initial State is  [3, 11, 4]\n",
      "episode 1368, reward 260.0, memory_length 2000, epsilon 0.25418702686569955, time 727.0, rides 120\n",
      "Initial State is  [2, 3, 0]\n",
      "episode 1369, reward 207.0, memory_length 2000, epsilon 0.25393283983883386, time 733.0, rides 125\n",
      "Initial State is  [0, 13, 0]\n",
      "episode 1370, reward 678.0, memory_length 2000, epsilon 0.25367890699899504, time 734.0, rides 130\n",
      "Initial State is  [1, 5, 6]\n",
      "episode 1371, reward 851.0, memory_length 2000, epsilon 0.253425228091996, time 725.0, rides 131\n",
      "Initial State is  [3, 3, 3]\n",
      "episode 1372, reward 475.0, memory_length 2000, epsilon 0.253171802863904, time 721.0, rides 136\n",
      "Initial State is  [0, 15, 1]\n",
      "episode 1373, reward 413.0, memory_length 2000, epsilon 0.2529186310610401, time 731.0, rides 136\n",
      "Initial State is  [2, 22, 3]\n",
      "episode 1374, reward 326.0, memory_length 2000, epsilon 0.2526657124299791, time 735.0, rides 133\n",
      "Initial State is  [1, 20, 2]\n",
      "episode 1375, reward 751.0, memory_length 2000, epsilon 0.2524130467175491, time 732.0, rides 141\n",
      "Initial State is  [4, 19, 6]\n",
      "episode 1376, reward 496.0, memory_length 2000, epsilon 0.2521606336708316, time 731.0, rides 122\n",
      "Initial State is  [4, 2, 0]\n",
      "episode 1377, reward 630.0, memory_length 2000, epsilon 0.25190847303716074, time 732.0, rides 137\n",
      "Initial State is  [1, 23, 1]\n",
      "episode 1378, reward 608.0, memory_length 2000, epsilon 0.25165656456412355, time 732.0, rides 127\n",
      "Initial State is  [1, 12, 6]\n",
      "episode 1379, reward 594.0, memory_length 2000, epsilon 0.25140490799955945, time 724.0, rides 142\n",
      "Initial State is  [3, 1, 2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 1380, reward 655.0, memory_length 2000, epsilon 0.25115350309155987, time 724.0, rides 116\n",
      "Initial State is  [0, 9, 5]\n",
      "episode 1381, reward 466.0, memory_length 2000, epsilon 0.2509023495884683, time 729.0, rides 132\n",
      "Initial State is  [0, 4, 1]\n",
      "episode 1382, reward 429.0, memory_length 2000, epsilon 0.25065144723887983, time 731.0, rides 123\n",
      "Initial State is  [2, 21, 5]\n",
      "episode 1383, reward 692.0, memory_length 2000, epsilon 0.25040079579164093, time 730.0, rides 130\n",
      "Initial State is  [3, 15, 5]\n",
      "episode 1384, reward 567.0, memory_length 2000, epsilon 0.2501503949958493, time 728.0, rides 137\n",
      "Initial State is  [1, 9, 5]\n",
      "episode 1385, reward 353.0, memory_length 2000, epsilon 0.24990024460085344, time 724.0, rides 124\n",
      "Initial State is  [0, 5, 4]\n",
      "episode 1386, reward 373.0, memory_length 2000, epsilon 0.24965034435625258, time 727.0, rides 130\n",
      "Initial State is  [0, 16, 2]\n",
      "episode 1387, reward 588.0, memory_length 2000, epsilon 0.24940069401189632, time 723.0, rides 133\n",
      "Initial State is  [3, 3, 3]\n",
      "episode 1388, reward 825.0, memory_length 2000, epsilon 0.24915129331788444, time 728.0, rides 122\n",
      "Initial State is  [3, 3, 0]\n",
      "episode 1389, reward 325.0, memory_length 2000, epsilon 0.24890214202456656, time 727.0, rides 121\n",
      "Initial State is  [4, 17, 0]\n",
      "episode 1390, reward 573.0, memory_length 2000, epsilon 0.248653239882542, time 732.0, rides 130\n",
      "Initial State is  [3, 2, 1]\n",
      "episode 1391, reward 688.0, memory_length 2000, epsilon 0.24840458664265946, time 727.0, rides 114\n",
      "Initial State is  [1, 0, 2]\n",
      "episode 1392, reward 416.0, memory_length 2000, epsilon 0.2481561820560168, time 739.0, rides 120\n",
      "Initial State is  [4, 18, 3]\n",
      "episode 1393, reward 482.0, memory_length 2000, epsilon 0.2479080258739608, time 723.0, rides 130\n",
      "Initial State is  [1, 7, 3]\n",
      "episode 1394, reward 676.0, memory_length 2000, epsilon 0.24766011784808684, time 722.0, rides 140\n",
      "Initial State is  [0, 4, 6]\n",
      "episode 1395, reward 437.0, memory_length 2000, epsilon 0.24741245773023876, time 722.0, rides 145\n",
      "Initial State is  [0, 9, 0]\n",
      "episode 1396, reward 702.0, memory_length 2000, epsilon 0.24716504527250852, time 733.0, rides 137\n",
      "Initial State is  [3, 0, 6]\n",
      "episode 1397, reward 463.0, memory_length 2000, epsilon 0.24691788022723601, time 737.0, rides 131\n",
      "Initial State is  [3, 20, 6]\n",
      "episode 1398, reward 693.0, memory_length 2000, epsilon 0.24667096234700878, time 726.0, rides 119\n",
      "Initial State is  [2, 4, 0]\n",
      "episode 1399, reward 479.0, memory_length 2000, epsilon 0.24642429138466176, time 727.0, rides 137\n",
      "Initial State is  [4, 6, 1]\n",
      "episode 1400, reward 554.0, memory_length 2000, epsilon 0.2461778670932771, time 742.0, rides 123\n",
      "Initial State is  [1, 6, 1]\n",
      "episode 1401, reward 661.0, memory_length 2000, epsilon 0.24593168922618383, time 737.0, rides 137\n",
      "Initial State is  [4, 19, 0]\n",
      "episode 1402, reward 600.0, memory_length 2000, epsilon 0.24568575753695765, time 730.0, rides 121\n",
      "Initial State is  [2, 9, 2]\n",
      "episode 1403, reward 687.0, memory_length 2000, epsilon 0.2454400717794207, time 729.0, rides 119\n",
      "Initial State is  [4, 7, 1]\n",
      "episode 1404, reward 587.0, memory_length 2000, epsilon 0.24519463170764128, time 725.0, rides 134\n",
      "Initial State is  [3, 5, 0]\n",
      "episode 1405, reward 428.0, memory_length 2000, epsilon 0.24494943707593364, time 727.0, rides 131\n",
      "Initial State is  [2, 17, 0]\n",
      "episode 1406, reward 641.0, memory_length 2000, epsilon 0.24470448763885772, time 736.0, rides 123\n",
      "Initial State is  [4, 17, 3]\n",
      "episode 1407, reward 946.0, memory_length 2000, epsilon 0.24445978315121886, time 736.0, rides 127\n",
      "Initial State is  [3, 0, 4]\n",
      "episode 1408, reward 434.0, memory_length 2000, epsilon 0.24421532336806764, time 735.0, rides 122\n",
      "Initial State is  [4, 3, 4]\n",
      "episode 1409, reward 835.0, memory_length 2000, epsilon 0.24397110804469957, time 727.0, rides 130\n",
      "Initial State is  [3, 13, 6]\n",
      "episode 1410, reward 503.0, memory_length 2000, epsilon 0.24372713693665488, time 734.0, rides 126\n",
      "Initial State is  [0, 10, 0]\n",
      "episode 1411, reward 356.0, memory_length 2000, epsilon 0.24348340979971822, time 728.0, rides 126\n",
      "Initial State is  [0, 17, 3]\n",
      "episode 1412, reward 440.0, memory_length 2000, epsilon 0.2432399263899185, time 726.0, rides 130\n",
      "Initial State is  [3, 15, 4]\n",
      "episode 1413, reward 538.0, memory_length 2000, epsilon 0.2429966864635286, time 731.0, rides 130\n",
      "Initial State is  [0, 17, 6]\n",
      "episode 1414, reward 576.0, memory_length 2000, epsilon 0.24275368977706507, time 730.0, rides 130\n",
      "Initial State is  [0, 2, 4]\n",
      "episode 1415, reward 893.0, memory_length 2000, epsilon 0.24251093608728802, time 731.0, rides 132\n",
      "Initial State is  [1, 6, 1]\n",
      "episode 1416, reward 570.0, memory_length 2000, epsilon 0.24226842515120073, time 736.0, rides 125\n",
      "Initial State is  [3, 23, 1]\n",
      "episode 1417, reward 619.0, memory_length 2000, epsilon 0.24202615672604952, time 728.0, rides 128\n",
      "Initial State is  [3, 13, 5]\n",
      "episode 1418, reward 696.0, memory_length 2000, epsilon 0.24178413056932346, time 728.0, rides 133\n",
      "Initial State is  [3, 6, 3]\n",
      "episode 1419, reward 791.0, memory_length 2000, epsilon 0.24154234643875414, time 730.0, rides 122\n",
      "Initial State is  [4, 23, 1]\n",
      "episode 1420, reward 748.0, memory_length 2000, epsilon 0.24130080409231539, time 728.0, rides 135\n",
      "Initial State is  [4, 13, 4]\n",
      "episode 1421, reward 897.0, memory_length 2000, epsilon 0.24105950328822306, time 723.0, rides 134\n",
      "Initial State is  [4, 3, 4]\n",
      "episode 1422, reward 509.0, memory_length 2000, epsilon 0.24081844378493483, time 729.0, rides 124\n",
      "Initial State is  [0, 23, 1]\n",
      "episode 1423, reward 757.0, memory_length 2000, epsilon 0.2405776253411499, time 729.0, rides 130\n",
      "Initial State is  [4, 12, 6]\n",
      "episode 1424, reward 530.0, memory_length 2000, epsilon 0.24033704771580874, time 727.0, rides 129\n",
      "Initial State is  [1, 19, 2]\n",
      "episode 1425, reward 397.0, memory_length 2000, epsilon 0.24009671066809293, time 728.0, rides 136\n",
      "Initial State is  [4, 1, 5]\n",
      "episode 1426, reward 481.0, memory_length 2000, epsilon 0.23985661395742483, time 723.0, rides 122\n",
      "Initial State is  [0, 23, 1]\n",
      "episode 1427, reward 532.0, memory_length 2000, epsilon 0.2396167573434674, time 726.0, rides 137\n",
      "Initial State is  [3, 16, 0]\n",
      "episode 1428, reward 662.0, memory_length 2000, epsilon 0.23937714058612394, time 732.0, rides 134\n",
      "Initial State is  [3, 19, 1]\n",
      "episode 1429, reward 517.0, memory_length 2000, epsilon 0.23913776344553783, time 730.0, rides 128\n",
      "Initial State is  [4, 3, 1]\n",
      "episode 1430, reward 564.0, memory_length 2000, epsilon 0.2388986256820923, time 731.0, rides 127\n",
      "Initial State is  [2, 13, 4]\n",
      "episode 1431, reward 689.0, memory_length 2000, epsilon 0.2386597270564102, time 724.0, rides 122\n",
      "Initial State is  [1, 3, 3]\n",
      "episode 1432, reward 593.0, memory_length 2000, epsilon 0.23842106732935378, time 733.0, rides 128\n",
      "Initial State is  [2, 3, 3]\n",
      "episode 1433, reward 493.0, memory_length 2000, epsilon 0.23818264626202443, time 728.0, rides 130\n",
      "Initial State is  [1, 19, 3]\n",
      "episode 1434, reward 549.0, memory_length 2000, epsilon 0.2379444636157624, time 727.0, rides 124\n",
      "Initial State is  [3, 18, 0]\n",
      "episode 1435, reward 700.0, memory_length 2000, epsilon 0.23770651915214663, time 731.0, rides 133\n",
      "Initial State is  [2, 12, 5]\n",
      "episode 1436, reward 277.0, memory_length 2000, epsilon 0.2374688126329945, time 746.0, rides 121\n",
      "Initial State is  [4, 12, 0]\n",
      "episode 1437, reward 571.0, memory_length 2000, epsilon 0.23723134382036148, time 724.0, rides 125\n",
      "Initial State is  [0, 11, 4]\n",
      "episode 1438, reward 584.0, memory_length 2000, epsilon 0.23699411247654112, time 735.0, rides 128\n",
      "Initial State is  [2, 4, 5]\n",
      "episode 1439, reward 584.0, memory_length 2000, epsilon 0.23675711836406457, time 722.0, rides 136\n",
      "Initial State is  [3, 19, 1]\n",
      "episode 1440, reward 608.0, memory_length 2000, epsilon 0.2365203612457005, time 732.0, rides 119\n",
      "Initial State is  [3, 21, 1]\n",
      "episode 1441, reward 503.0, memory_length 2000, epsilon 0.2362838408844548, time 731.0, rides 128\n",
      "Initial State is  [0, 19, 3]\n",
      "episode 1442, reward 402.0, memory_length 2000, epsilon 0.23604755704357036, time 729.0, rides 121\n",
      "Initial State is  [2, 17, 2]\n",
      "episode 1443, reward 642.0, memory_length 2000, epsilon 0.2358115094865268, time 741.0, rides 132\n",
      "Initial State is  [1, 14, 1]\n",
      "episode 1444, reward 622.0, memory_length 2000, epsilon 0.23557569797704025, time 724.0, rides 142\n",
      "Initial State is  [3, 13, 3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 1445, reward 779.0, memory_length 2000, epsilon 0.2353401222790632, time 734.0, rides 131\n",
      "Initial State is  [1, 11, 2]\n",
      "episode 1446, reward 782.0, memory_length 2000, epsilon 0.23510478215678413, time 730.0, rides 125\n",
      "Initial State is  [0, 6, 4]\n",
      "episode 1447, reward 771.0, memory_length 2000, epsilon 0.23486967737462733, time 728.0, rides 135\n",
      "Initial State is  [0, 8, 5]\n",
      "episode 1448, reward 568.0, memory_length 2000, epsilon 0.2346348076972527, time 730.0, rides 133\n",
      "Initial State is  [2, 3, 4]\n",
      "episode 1449, reward 626.0, memory_length 2000, epsilon 0.23440017288955545, time 730.0, rides 141\n",
      "Initial State is  [2, 2, 0]\n",
      "episode 1450, reward 553.0, memory_length 2000, epsilon 0.2341657727166659, time 729.0, rides 120\n",
      "Initial State is  [0, 11, 5]\n",
      "episode 1451, reward 683.0, memory_length 2000, epsilon 0.2339316069439492, time 732.0, rides 123\n",
      "Initial State is  [2, 18, 3]\n",
      "episode 1452, reward 695.0, memory_length 2000, epsilon 0.23369767533700525, time 729.0, rides 117\n",
      "Initial State is  [1, 14, 5]\n",
      "episode 1453, reward 700.0, memory_length 2000, epsilon 0.23346397766166824, time 730.0, rides 126\n",
      "Initial State is  [3, 15, 5]\n",
      "episode 1454, reward 517.0, memory_length 2000, epsilon 0.23323051368400657, time 729.0, rides 125\n",
      "Initial State is  [2, 23, 4]\n",
      "episode 1455, reward 534.0, memory_length 2000, epsilon 0.23299728317032256, time 722.0, rides 127\n",
      "Initial State is  [1, 15, 3]\n",
      "episode 1456, reward 445.0, memory_length 2000, epsilon 0.23276428588715223, time 741.0, rides 120\n",
      "Initial State is  [2, 11, 2]\n",
      "episode 1457, reward 560.0, memory_length 2000, epsilon 0.23253152160126508, time 734.0, rides 120\n",
      "Initial State is  [1, 19, 0]\n",
      "episode 1458, reward 637.0, memory_length 2000, epsilon 0.23229899007966381, time 734.0, rides 115\n",
      "Initial State is  [4, 11, 3]\n",
      "episode 1459, reward 485.0, memory_length 2000, epsilon 0.23206669108958414, time 734.0, rides 140\n",
      "Initial State is  [3, 5, 3]\n",
      "episode 1460, reward 673.0, memory_length 2000, epsilon 0.23183462439849456, time 735.0, rides 129\n",
      "Initial State is  [4, 12, 4]\n",
      "episode 1461, reward 270.0, memory_length 2000, epsilon 0.23160278977409607, time 723.0, rides 122\n",
      "Initial State is  [0, 6, 2]\n",
      "episode 1462, reward 652.0, memory_length 2000, epsilon 0.23137118698432196, time 736.0, rides 131\n",
      "Initial State is  [0, 20, 5]\n",
      "episode 1463, reward 376.0, memory_length 2000, epsilon 0.23113981579733764, time 732.0, rides 127\n",
      "Initial State is  [4, 5, 1]\n",
      "episode 1464, reward 656.0, memory_length 2000, epsilon 0.2309086759815403, time 731.0, rides 135\n",
      "Initial State is  [1, 1, 6]\n",
      "episode 1465, reward 379.0, memory_length 2000, epsilon 0.23067776730555875, time 723.0, rides 127\n",
      "Initial State is  [4, 23, 3]\n",
      "episode 1466, reward 470.0, memory_length 2000, epsilon 0.23044708953825319, time 724.0, rides 139\n",
      "Initial State is  [4, 10, 5]\n",
      "episode 1467, reward 454.0, memory_length 2000, epsilon 0.23021664244871493, time 731.0, rides 127\n",
      "Initial State is  [0, 17, 5]\n",
      "episode 1468, reward 578.0, memory_length 2000, epsilon 0.22998642580626621, time 729.0, rides 123\n",
      "Initial State is  [2, 13, 0]\n",
      "episode 1469, reward 312.0, memory_length 2000, epsilon 0.22975643938045995, time 734.0, rides 133\n",
      "Initial State is  [3, 9, 2]\n",
      "episode 1470, reward 530.0, memory_length 2000, epsilon 0.2295266829410795, time 724.0, rides 139\n",
      "Initial State is  [4, 4, 6]\n",
      "episode 1471, reward 495.0, memory_length 2000, epsilon 0.22929715625813843, time 726.0, rides 128\n",
      "Initial State is  [1, 18, 2]\n",
      "episode 1472, reward 644.0, memory_length 2000, epsilon 0.2290678591018803, time 722.0, rides 123\n",
      "Initial State is  [0, 9, 5]\n",
      "episode 1473, reward 699.0, memory_length 2000, epsilon 0.22883879124277842, time 728.0, rides 133\n",
      "Initial State is  [1, 18, 6]\n",
      "episode 1474, reward 693.0, memory_length 2000, epsilon 0.22860995245153565, time 724.0, rides 117\n",
      "Initial State is  [1, 14, 3]\n",
      "episode 1475, reward 600.0, memory_length 2000, epsilon 0.22838134249908412, time 730.0, rides 134\n",
      "Initial State is  [4, 11, 5]\n",
      "episode 1476, reward 595.0, memory_length 2000, epsilon 0.22815296115658504, time 735.0, rides 132\n",
      "Initial State is  [3, 3, 3]\n",
      "episode 1477, reward 618.0, memory_length 2000, epsilon 0.22792480819542846, time 730.0, rides 125\n",
      "Initial State is  [0, 19, 6]\n",
      "episode 1478, reward 510.0, memory_length 2000, epsilon 0.22769688338723304, time 728.0, rides 127\n",
      "Initial State is  [3, 18, 2]\n",
      "episode 1479, reward 403.0, memory_length 2000, epsilon 0.22746918650384582, time 736.0, rides 118\n",
      "Initial State is  [4, 16, 1]\n",
      "episode 1480, reward 839.0, memory_length 2000, epsilon 0.22724171731734197, time 725.0, rides 117\n",
      "Initial State is  [0, 23, 6]\n",
      "episode 1481, reward 578.0, memory_length 2000, epsilon 0.22701447560002463, time 729.0, rides 108\n",
      "Initial State is  [4, 3, 3]\n",
      "episode 1482, reward 266.0, memory_length 2000, epsilon 0.2267874611244246, time 727.0, rides 136\n",
      "Initial State is  [2, 19, 0]\n",
      "episode 1483, reward 550.0, memory_length 2000, epsilon 0.22656067366330018, time 734.0, rides 124\n",
      "Initial State is  [1, 2, 6]\n",
      "episode 1484, reward 652.0, memory_length 2000, epsilon 0.22633411298963688, time 731.0, rides 113\n",
      "Initial State is  [2, 17, 4]\n",
      "episode 1485, reward 480.0, memory_length 2000, epsilon 0.22610777887664724, time 732.0, rides 122\n",
      "Initial State is  [3, 9, 4]\n",
      "episode 1486, reward 643.0, memory_length 2000, epsilon 0.22588167109777058, time 729.0, rides 137\n",
      "Initial State is  [2, 10, 6]\n",
      "episode 1487, reward 699.0, memory_length 2000, epsilon 0.2256557894266728, time 730.0, rides 131\n",
      "Initial State is  [4, 21, 5]\n",
      "episode 1488, reward 490.0, memory_length 2000, epsilon 0.22543013363724612, time 721.0, rides 127\n",
      "Initial State is  [3, 15, 3]\n",
      "episode 1489, reward 714.0, memory_length 2000, epsilon 0.22520470350360888, time 722.0, rides 140\n",
      "Initial State is  [2, 19, 4]\n",
      "episode 1490, reward 604.0, memory_length 2000, epsilon 0.2249794988001053, time 736.0, rides 121\n",
      "Initial State is  [0, 17, 0]\n",
      "episode 1491, reward 677.0, memory_length 2000, epsilon 0.2247545193013052, time 732.0, rides 132\n",
      "Initial State is  [0, 12, 5]\n",
      "episode 1492, reward 727.0, memory_length 2000, epsilon 0.22452976478200387, time 736.0, rides 128\n",
      "Initial State is  [1, 6, 2]\n",
      "episode 1493, reward 793.0, memory_length 2000, epsilon 0.22430523501722185, time 730.0, rides 130\n",
      "Initial State is  [4, 6, 5]\n",
      "episode 1494, reward 773.0, memory_length 2000, epsilon 0.22408092978220462, time 725.0, rides 124\n",
      "Initial State is  [1, 12, 3]\n",
      "episode 1495, reward 563.0, memory_length 2000, epsilon 0.2238568488524224, time 728.0, rides 129\n",
      "Initial State is  [1, 7, 4]\n",
      "episode 1496, reward 687.0, memory_length 2000, epsilon 0.22363299200356998, time 723.0, rides 119\n",
      "Initial State is  [3, 7, 4]\n",
      "episode 1497, reward 462.0, memory_length 2000, epsilon 0.22340935901156642, time 722.0, rides 113\n",
      "Initial State is  [0, 3, 4]\n",
      "episode 1498, reward 663.0, memory_length 2000, epsilon 0.22318594965255484, time 732.0, rides 132\n",
      "Initial State is  [2, 20, 2]\n",
      "episode 1499, reward 661.0, memory_length 2000, epsilon 0.22296276370290227, time 728.0, rides 130\n",
      "Initial State is  [2, 17, 1]\n",
      "episode 1500, reward 653.0, memory_length 2000, epsilon 0.22273980093919937, time 723.0, rides 130\n",
      "Initial State is  [3, 0, 4]\n",
      "episode 1501, reward 691.0, memory_length 2000, epsilon 0.22251706113826017, time 723.0, rides 126\n",
      "Initial State is  [0, 17, 0]\n",
      "episode 1502, reward 372.0, memory_length 2000, epsilon 0.2222945440771219, time 726.0, rides 128\n",
      "Initial State is  [0, 15, 0]\n",
      "episode 1503, reward 224.0, memory_length 2000, epsilon 0.22207224953304477, time 731.0, rides 110\n",
      "Initial State is  [4, 22, 5]\n",
      "episode 1504, reward 504.0, memory_length 2000, epsilon 0.22185017728351172, time 723.0, rides 127\n",
      "Initial State is  [2, 2, 3]\n",
      "episode 1505, reward 784.0, memory_length 2000, epsilon 0.2216283271062282, time 736.0, rides 111\n",
      "Initial State is  [1, 9, 3]\n",
      "episode 1506, reward -5.0, memory_length 2000, epsilon 0.22140669877912197, time 736.0, rides 111\n",
      "Initial State is  [2, 14, 4]\n",
      "episode 1507, reward 472.0, memory_length 2000, epsilon 0.22118529208034285, time 728.0, rides 124\n",
      "Initial State is  [2, 13, 2]\n",
      "episode 1508, reward 820.0, memory_length 2000, epsilon 0.2209641067882625, time 728.0, rides 125\n",
      "Initial State is  [1, 16, 0]\n",
      "episode 1509, reward 567.0, memory_length 2000, epsilon 0.22074314268147424, time 729.0, rides 120\n",
      "Initial State is  [2, 19, 4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 1510, reward 612.0, memory_length 2000, epsilon 0.22052239953879277, time 723.0, rides 125\n",
      "Initial State is  [4, 12, 2]\n",
      "episode 1511, reward 625.0, memory_length 2000, epsilon 0.22030187713925398, time 725.0, rides 122\n",
      "Initial State is  [3, 3, 2]\n",
      "episode 1512, reward 581.0, memory_length 2000, epsilon 0.22008157526211472, time 727.0, rides 117\n",
      "Initial State is  [0, 6, 1]\n",
      "episode 1513, reward 355.0, memory_length 2000, epsilon 0.2198614936868526, time 724.0, rides 119\n",
      "Initial State is  [4, 20, 1]\n",
      "episode 1514, reward 588.0, memory_length 2000, epsilon 0.21964163219316574, time 726.0, rides 122\n",
      "Initial State is  [1, 16, 4]\n",
      "episode 1515, reward 509.0, memory_length 2000, epsilon 0.21942199056097256, time 730.0, rides 117\n",
      "Initial State is  [0, 16, 5]\n",
      "episode 1516, reward 272.0, memory_length 2000, epsilon 0.2192025685704116, time 731.0, rides 117\n",
      "Initial State is  [3, 3, 1]\n",
      "episode 1517, reward 535.0, memory_length 2000, epsilon 0.21898336600184118, time 725.0, rides 133\n",
      "Initial State is  [1, 5, 1]\n",
      "episode 1518, reward 1128.0, memory_length 2000, epsilon 0.21876438263583933, time 727.0, rides 118\n",
      "Initial State is  [1, 12, 6]\n",
      "episode 1519, reward 300.0, memory_length 2000, epsilon 0.21854561825320348, time 733.0, rides 125\n",
      "Initial State is  [4, 16, 5]\n",
      "episode 1520, reward 715.0, memory_length 2000, epsilon 0.21832707263495027, time 727.0, rides 120\n",
      "Initial State is  [0, 20, 0]\n",
      "episode 1521, reward 576.0, memory_length 2000, epsilon 0.21810874556231533, time 730.0, rides 126\n",
      "Initial State is  [1, 13, 3]\n",
      "episode 1522, reward 723.0, memory_length 2000, epsilon 0.217890636816753, time 734.0, rides 125\n",
      "Initial State is  [0, 1, 5]\n",
      "episode 1523, reward 442.0, memory_length 2000, epsilon 0.21767274617993623, time 722.0, rides 120\n",
      "Initial State is  [3, 7, 4]\n",
      "episode 1524, reward 792.0, memory_length 2000, epsilon 0.2174550734337563, time 730.0, rides 134\n",
      "Initial State is  [0, 6, 0]\n",
      "episode 1525, reward 518.0, memory_length 2000, epsilon 0.21723761836032254, time 729.0, rides 128\n",
      "Initial State is  [3, 22, 5]\n",
      "episode 1526, reward 468.0, memory_length 2000, epsilon 0.21702038074196223, time 736.0, rides 141\n",
      "Initial State is  [2, 6, 3]\n",
      "episode 1527, reward 716.0, memory_length 2000, epsilon 0.21680336036122028, time 727.0, rides 119\n",
      "Initial State is  [0, 12, 4]\n",
      "episode 1528, reward 698.0, memory_length 2000, epsilon 0.21658655700085905, time 733.0, rides 129\n",
      "Initial State is  [4, 13, 1]\n",
      "episode 1529, reward 736.0, memory_length 2000, epsilon 0.2163699704438582, time 727.0, rides 133\n",
      "Initial State is  [2, 3, 0]\n",
      "episode 1530, reward 429.0, memory_length 2000, epsilon 0.21615360047341434, time 726.0, rides 125\n",
      "Initial State is  [4, 7, 1]\n",
      "episode 1531, reward 664.0, memory_length 2000, epsilon 0.21593744687294092, time 730.0, rides 119\n",
      "Initial State is  [0, 10, 6]\n",
      "episode 1532, reward 544.0, memory_length 2000, epsilon 0.21572150942606796, time 726.0, rides 121\n",
      "Initial State is  [2, 14, 4]\n",
      "episode 1533, reward 629.0, memory_length 2000, epsilon 0.2155057879166419, time 724.0, rides 123\n",
      "Initial State is  [3, 20, 1]\n",
      "episode 1534, reward 982.0, memory_length 2000, epsilon 0.21529028212872525, time 729.0, rides 129\n",
      "Initial State is  [4, 5, 6]\n",
      "episode 1535, reward 362.0, memory_length 2000, epsilon 0.21507499184659654, time 734.0, rides 123\n",
      "Initial State is  [0, 22, 5]\n",
      "episode 1536, reward 692.0, memory_length 2000, epsilon 0.21485991685474995, time 733.0, rides 134\n",
      "Initial State is  [0, 10, 2]\n",
      "episode 1537, reward 838.0, memory_length 2000, epsilon 0.2146450569378952, time 729.0, rides 145\n",
      "Initial State is  [4, 23, 2]\n",
      "episode 1538, reward 724.0, memory_length 2000, epsilon 0.21443041188095732, time 726.0, rides 122\n",
      "Initial State is  [1, 17, 0]\n",
      "episode 1539, reward 176.0, memory_length 2000, epsilon 0.21421598146907636, time 730.0, rides 118\n",
      "Initial State is  [4, 9, 5]\n",
      "episode 1540, reward 546.0, memory_length 2000, epsilon 0.21400176548760727, time 735.0, rides 134\n",
      "Initial State is  [0, 22, 5]\n",
      "episode 1541, reward 720.0, memory_length 2000, epsilon 0.21378776372211966, time 726.0, rides 128\n",
      "Initial State is  [4, 2, 5]\n",
      "episode 1542, reward 671.0, memory_length 2000, epsilon 0.21357397595839755, time 729.0, rides 130\n",
      "Initial State is  [2, 13, 1]\n",
      "episode 1543, reward 767.0, memory_length 2000, epsilon 0.21336040198243916, time 737.0, rides 125\n",
      "Initial State is  [4, 12, 0]\n",
      "episode 1544, reward 657.0, memory_length 2000, epsilon 0.21314704158045672, time 728.0, rides 129\n",
      "Initial State is  [2, 9, 1]\n",
      "episode 1545, reward 346.0, memory_length 2000, epsilon 0.21293389453887626, time 724.0, rides 123\n",
      "Initial State is  [4, 15, 1]\n",
      "episode 1546, reward 488.0, memory_length 2000, epsilon 0.21272096064433738, time 727.0, rides 136\n",
      "Initial State is  [2, 10, 6]\n",
      "episode 1547, reward 674.0, memory_length 2000, epsilon 0.21250823968369303, time 733.0, rides 134\n",
      "Initial State is  [2, 17, 5]\n",
      "episode 1548, reward 720.0, memory_length 2000, epsilon 0.21229573144400934, time 729.0, rides 126\n",
      "Initial State is  [4, 19, 3]\n",
      "episode 1549, reward 743.0, memory_length 2000, epsilon 0.21208343571256533, time 725.0, rides 125\n",
      "Initial State is  [3, 2, 2]\n",
      "episode 1550, reward 539.0, memory_length 2000, epsilon 0.21187135227685275, time 728.0, rides 118\n",
      "Initial State is  [0, 13, 0]\n",
      "episode 1551, reward 280.0, memory_length 2000, epsilon 0.2116594809245759, time 725.0, rides 137\n",
      "Initial State is  [1, 21, 0]\n",
      "episode 1552, reward 689.0, memory_length 2000, epsilon 0.21144782144365132, time 728.0, rides 127\n",
      "Initial State is  [2, 22, 5]\n",
      "episode 1553, reward 303.0, memory_length 2000, epsilon 0.21123637362220768, time 729.0, rides 121\n",
      "Initial State is  [2, 22, 3]\n",
      "episode 1554, reward 693.0, memory_length 2000, epsilon 0.21102513724858546, time 732.0, rides 125\n",
      "Initial State is  [3, 3, 6]\n",
      "episode 1555, reward 724.0, memory_length 2000, epsilon 0.21081411211133688, time 731.0, rides 131\n",
      "Initial State is  [1, 19, 1]\n",
      "episode 1556, reward 763.0, memory_length 2000, epsilon 0.21060329799922556, time 725.0, rides 121\n",
      "Initial State is  [0, 22, 4]\n",
      "episode 1557, reward 700.0, memory_length 2000, epsilon 0.21039269470122635, time 732.0, rides 125\n",
      "Initial State is  [3, 14, 3]\n",
      "episode 1558, reward 548.0, memory_length 2000, epsilon 0.21018230200652513, time 730.0, rides 122\n",
      "Initial State is  [0, 15, 0]\n",
      "episode 1559, reward 914.0, memory_length 2000, epsilon 0.2099721197045186, time 735.0, rides 134\n",
      "Initial State is  [4, 23, 6]\n",
      "episode 1560, reward 788.0, memory_length 2000, epsilon 0.20976214758481407, time 735.0, rides 119\n",
      "Initial State is  [4, 0, 4]\n",
      "episode 1561, reward 508.0, memory_length 2000, epsilon 0.20955238543722926, time 733.0, rides 118\n",
      "Initial State is  [3, 10, 6]\n",
      "episode 1562, reward 645.0, memory_length 2000, epsilon 0.20934283305179202, time 730.0, rides 123\n",
      "Initial State is  [0, 8, 5]\n",
      "episode 1563, reward 390.0, memory_length 2000, epsilon 0.20913349021874023, time 724.0, rides 126\n",
      "Initial State is  [2, 10, 1]\n",
      "episode 1564, reward 573.0, memory_length 2000, epsilon 0.2089243567285215, time 733.0, rides 128\n",
      "Initial State is  [4, 6, 4]\n",
      "episode 1565, reward 430.0, memory_length 2000, epsilon 0.20871543237179296, time 722.0, rides 116\n",
      "Initial State is  [2, 17, 1]\n",
      "episode 1566, reward 773.0, memory_length 2000, epsilon 0.20850671693942116, time 734.0, rides 132\n",
      "Initial State is  [0, 22, 0]\n",
      "episode 1567, reward 535.0, memory_length 2000, epsilon 0.20829821022248174, time 731.0, rides 132\n",
      "Initial State is  [3, 2, 3]\n",
      "episode 1568, reward 581.0, memory_length 2000, epsilon 0.20808991201225926, time 724.0, rides 119\n",
      "Initial State is  [2, 20, 1]\n",
      "episode 1569, reward 800.0, memory_length 2000, epsilon 0.207881822100247, time 731.0, rides 136\n",
      "Initial State is  [0, 22, 5]\n",
      "episode 1570, reward 633.0, memory_length 2000, epsilon 0.20767394027814676, time 730.0, rides 125\n",
      "Initial State is  [4, 1, 3]\n",
      "episode 1571, reward 722.0, memory_length 2000, epsilon 0.2074662663378686, time 723.0, rides 144\n",
      "Initial State is  [0, 19, 1]\n",
      "episode 1572, reward 648.0, memory_length 2000, epsilon 0.20725880007153075, time 727.0, rides 120\n",
      "Initial State is  [4, 17, 2]\n",
      "episode 1573, reward 672.0, memory_length 2000, epsilon 0.20705154127145922, time 729.0, rides 119\n",
      "Initial State is  [4, 1, 5]\n",
      "episode 1574, reward 978.0, memory_length 2000, epsilon 0.20684448973018776, time 727.0, rides 134\n",
      "Initial State is  [3, 7, 5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 1575, reward 997.0, memory_length 2000, epsilon 0.20663764524045758, time 726.0, rides 132\n",
      "Initial State is  [2, 5, 1]\n",
      "episode 1576, reward 532.0, memory_length 2000, epsilon 0.20643100759521713, time 728.0, rides 120\n",
      "Initial State is  [0, 10, 4]\n",
      "episode 1577, reward 306.0, memory_length 2000, epsilon 0.20622457658762192, time 725.0, rides 136\n",
      "Initial State is  [0, 8, 4]\n",
      "episode 1578, reward 296.0, memory_length 2000, epsilon 0.2060183520110343, time 729.0, rides 130\n",
      "Initial State is  [0, 16, 6]\n",
      "episode 1579, reward 363.0, memory_length 2000, epsilon 0.20581233365902327, time 730.0, rides 120\n",
      "Initial State is  [2, 9, 1]\n",
      "episode 1580, reward 869.0, memory_length 2000, epsilon 0.20560652132536425, time 727.0, rides 133\n",
      "Initial State is  [2, 14, 0]\n",
      "episode 1581, reward 504.0, memory_length 2000, epsilon 0.20540091480403888, time 724.0, rides 130\n",
      "Initial State is  [0, 5, 2]\n",
      "episode 1582, reward 682.0, memory_length 2000, epsilon 0.20519551388923485, time 730.0, rides 120\n",
      "Initial State is  [4, 2, 3]\n",
      "episode 1583, reward 592.0, memory_length 2000, epsilon 0.20499031837534562, time 727.0, rides 120\n",
      "Initial State is  [0, 19, 4]\n",
      "episode 1584, reward 613.0, memory_length 2000, epsilon 0.20478532805697028, time 731.0, rides 110\n",
      "Initial State is  [3, 6, 2]\n",
      "episode 1585, reward 583.0, memory_length 2000, epsilon 0.2045805427289133, time 730.0, rides 121\n",
      "Initial State is  [1, 23, 1]\n",
      "episode 1586, reward 655.0, memory_length 2000, epsilon 0.20437596218618437, time 724.0, rides 130\n",
      "Initial State is  [3, 10, 6]\n",
      "episode 1587, reward 580.0, memory_length 2000, epsilon 0.2041715862239982, time 725.0, rides 123\n",
      "Initial State is  [4, 14, 5]\n",
      "episode 1588, reward 411.0, memory_length 2000, epsilon 0.2039674146377742, time 736.0, rides 132\n",
      "Initial State is  [4, 4, 3]\n",
      "episode 1589, reward 588.0, memory_length 2000, epsilon 0.20376344722313644, time 732.0, rides 115\n",
      "Initial State is  [1, 1, 4]\n",
      "episode 1590, reward 523.0, memory_length 2000, epsilon 0.2035596837759133, time 728.0, rides 127\n",
      "Initial State is  [4, 1, 6]\n",
      "episode 1591, reward 741.0, memory_length 2000, epsilon 0.20335612409213738, time 727.0, rides 122\n",
      "Initial State is  [0, 11, 6]\n",
      "episode 1592, reward 624.0, memory_length 2000, epsilon 0.20315276796804524, time 726.0, rides 133\n",
      "Initial State is  [1, 23, 6]\n",
      "episode 1593, reward 307.0, memory_length 2000, epsilon 0.2029496152000772, time 724.0, rides 125\n",
      "Initial State is  [1, 9, 4]\n",
      "episode 1594, reward 657.0, memory_length 2000, epsilon 0.20274666558487714, time 728.0, rides 122\n",
      "Initial State is  [3, 1, 5]\n",
      "episode 1595, reward 359.0, memory_length 2000, epsilon 0.20254391891929227, time 733.0, rides 123\n",
      "Initial State is  [4, 21, 4]\n",
      "episode 1596, reward 352.0, memory_length 2000, epsilon 0.20234137500037297, time 730.0, rides 121\n",
      "Initial State is  [3, 5, 1]\n",
      "episode 1597, reward 710.0, memory_length 2000, epsilon 0.20213903362537258, time 728.0, rides 123\n",
      "Initial State is  [4, 23, 3]\n",
      "episode 1598, reward 496.0, memory_length 2000, epsilon 0.2019368945917472, time 736.0, rides 133\n",
      "Initial State is  [1, 11, 2]\n",
      "episode 1599, reward 579.0, memory_length 2000, epsilon 0.20173495769715546, time 728.0, rides 131\n",
      "Initial State is  [2, 20, 6]\n",
      "episode 1600, reward 421.0, memory_length 2000, epsilon 0.2015332227394583, time 726.0, rides 128\n",
      "Initial State is  [2, 5, 2]\n",
      "episode 1601, reward 743.0, memory_length 2000, epsilon 0.20133168951671884, time 730.0, rides 131\n",
      "Initial State is  [1, 17, 4]\n",
      "episode 1602, reward 650.0, memory_length 2000, epsilon 0.20113035782720212, time 729.0, rides 126\n",
      "Initial State is  [2, 4, 0]\n",
      "episode 1603, reward 498.0, memory_length 2000, epsilon 0.20092922746937492, time 732.0, rides 123\n",
      "Initial State is  [2, 15, 3]\n",
      "episode 1604, reward 1063.0, memory_length 2000, epsilon 0.20072829824190555, time 727.0, rides 129\n",
      "Initial State is  [3, 21, 4]\n",
      "episode 1605, reward 899.0, memory_length 2000, epsilon 0.20052756994366364, time 722.0, rides 122\n",
      "Initial State is  [1, 23, 2]\n",
      "episode 1606, reward 414.0, memory_length 2000, epsilon 0.20032704237371998, time 731.0, rides 118\n",
      "Initial State is  [3, 15, 3]\n",
      "episode 1607, reward 525.0, memory_length 2000, epsilon 0.20012671533134627, time 728.0, rides 126\n",
      "Initial State is  [4, 11, 3]\n",
      "episode 1608, reward 137.0, memory_length 2000, epsilon 0.19992658861601492, time 730.0, rides 132\n",
      "Initial State is  [4, 18, 5]\n",
      "episode 1609, reward 370.0, memory_length 2000, epsilon 0.19972666202739892, time 734.0, rides 134\n",
      "Initial State is  [2, 20, 3]\n",
      "episode 1610, reward 724.0, memory_length 2000, epsilon 0.19952693536537153, time 726.0, rides 126\n",
      "Initial State is  [4, 9, 5]\n",
      "episode 1611, reward 641.0, memory_length 2000, epsilon 0.19932740843000615, time 730.0, rides 117\n",
      "Initial State is  [4, 3, 2]\n",
      "episode 1612, reward 500.0, memory_length 2000, epsilon 0.19912808102157614, time 723.0, rides 131\n",
      "Initial State is  [3, 14, 1]\n",
      "episode 1613, reward 377.0, memory_length 2000, epsilon 0.19892895294055457, time 741.0, rides 136\n",
      "Initial State is  [4, 7, 0]\n",
      "episode 1614, reward 534.0, memory_length 2000, epsilon 0.19873002398761402, time 724.0, rides 131\n",
      "Initial State is  [1, 17, 4]\n",
      "episode 1615, reward 635.0, memory_length 2000, epsilon 0.1985312939636264, time 729.0, rides 127\n",
      "Initial State is  [4, 22, 1]\n",
      "episode 1616, reward 463.0, memory_length 2000, epsilon 0.19833276266966277, time 730.0, rides 128\n",
      "Initial State is  [4, 12, 1]\n",
      "episode 1617, reward 901.0, memory_length 2000, epsilon 0.1981344299069931, time 725.0, rides 135\n",
      "Initial State is  [4, 18, 3]\n",
      "episode 1618, reward 432.0, memory_length 2000, epsilon 0.1979362954770861, time 744.0, rides 127\n",
      "Initial State is  [2, 15, 5]\n",
      "episode 1619, reward 614.0, memory_length 2000, epsilon 0.197738359181609, time 722.0, rides 114\n",
      "Initial State is  [1, 15, 4]\n",
      "episode 1620, reward 633.0, memory_length 2000, epsilon 0.1975406208224274, time 726.0, rides 145\n",
      "Initial State is  [0, 8, 6]\n",
      "episode 1621, reward 874.0, memory_length 2000, epsilon 0.197343080201605, time 731.0, rides 144\n",
      "Initial State is  [1, 20, 6]\n",
      "episode 1622, reward 397.0, memory_length 2000, epsilon 0.19714573712140337, time 723.0, rides 134\n",
      "Initial State is  [3, 20, 5]\n",
      "episode 1623, reward 460.0, memory_length 2000, epsilon 0.19694859138428197, time 726.0, rides 124\n",
      "Initial State is  [4, 20, 6]\n",
      "episode 1624, reward 495.0, memory_length 2000, epsilon 0.1967516427928977, time 730.0, rides 121\n",
      "Initial State is  [3, 4, 6]\n",
      "episode 1625, reward 957.0, memory_length 2000, epsilon 0.1965548911501048, time 732.0, rides 127\n",
      "Initial State is  [2, 17, 6]\n",
      "episode 1626, reward 471.0, memory_length 2000, epsilon 0.19635833625895469, time 732.0, rides 128\n",
      "Initial State is  [2, 14, 4]\n",
      "episode 1627, reward 401.0, memory_length 2000, epsilon 0.19616197792269574, time 728.0, rides 121\n",
      "Initial State is  [4, 9, 1]\n",
      "episode 1628, reward 636.0, memory_length 2000, epsilon 0.19596581594477305, time 723.0, rides 118\n",
      "Initial State is  [1, 5, 1]\n",
      "episode 1629, reward 397.0, memory_length 2000, epsilon 0.19576985012882828, time 727.0, rides 110\n",
      "Initial State is  [2, 14, 5]\n",
      "episode 1630, reward 914.0, memory_length 2000, epsilon 0.19557408027869944, time 731.0, rides 123\n",
      "Initial State is  [0, 17, 3]\n",
      "episode 1631, reward 630.0, memory_length 2000, epsilon 0.19537850619842073, time 732.0, rides 116\n",
      "Initial State is  [3, 11, 0]\n",
      "episode 1632, reward 358.0, memory_length 2000, epsilon 0.19518312769222232, time 732.0, rides 124\n",
      "Initial State is  [4, 0, 1]\n",
      "episode 1633, reward 582.0, memory_length 2000, epsilon 0.19498794456453009, time 726.0, rides 118\n",
      "Initial State is  [3, 17, 0]\n",
      "episode 1634, reward 686.0, memory_length 2000, epsilon 0.19479295661996557, time 732.0, rides 138\n",
      "Initial State is  [1, 16, 3]\n",
      "episode 1635, reward 597.0, memory_length 2000, epsilon 0.1945981636633456, time 730.0, rides 129\n",
      "Initial State is  [4, 4, 5]\n",
      "episode 1636, reward 610.0, memory_length 2000, epsilon 0.19440356549968224, time 735.0, rides 126\n",
      "Initial State is  [0, 8, 0]\n",
      "episode 1637, reward 580.0, memory_length 2000, epsilon 0.19420916193418256, time 733.0, rides 114\n",
      "Initial State is  [3, 21, 5]\n",
      "episode 1638, reward 739.0, memory_length 2000, epsilon 0.19401495277224837, time 734.0, rides 127\n",
      "Initial State is  [0, 4, 5]\n",
      "episode 1639, reward 455.0, memory_length 2000, epsilon 0.19382093781947612, time 726.0, rides 118\n",
      "Initial State is  [0, 13, 4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 1640, reward 623.0, memory_length 2000, epsilon 0.19362711688165665, time 728.0, rides 141\n",
      "Initial State is  [1, 0, 3]\n",
      "episode 1641, reward 366.0, memory_length 2000, epsilon 0.193433489764775, time 730.0, rides 131\n",
      "Initial State is  [2, 12, 0]\n",
      "episode 1642, reward 631.0, memory_length 2000, epsilon 0.19324005627501023, time 727.0, rides 125\n",
      "Initial State is  [0, 8, 0]\n",
      "episode 1643, reward 691.0, memory_length 2000, epsilon 0.1930468162187352, time 725.0, rides 120\n",
      "Initial State is  [1, 10, 5]\n",
      "episode 1644, reward 1016.0, memory_length 2000, epsilon 0.19285376940251647, time 732.0, rides 126\n",
      "Initial State is  [3, 9, 3]\n",
      "episode 1645, reward 345.0, memory_length 2000, epsilon 0.19266091563311397, time 728.0, rides 115\n",
      "Initial State is  [1, 11, 0]\n",
      "episode 1646, reward 476.0, memory_length 2000, epsilon 0.19246825471748086, time 721.0, rides 135\n",
      "Initial State is  [2, 8, 2]\n",
      "episode 1647, reward 530.0, memory_length 2000, epsilon 0.1922757864627634, time 734.0, rides 137\n",
      "Initial State is  [0, 4, 2]\n",
      "episode 1648, reward 801.0, memory_length 2000, epsilon 0.1920835106763006, time 723.0, rides 114\n",
      "Initial State is  [0, 2, 6]\n",
      "episode 1649, reward 933.0, memory_length 2000, epsilon 0.19189142716562432, time 726.0, rides 122\n",
      "Initial State is  [3, 11, 3]\n",
      "episode 1650, reward 617.0, memory_length 2000, epsilon 0.1916995357384587, time 725.0, rides 127\n",
      "Initial State is  [3, 19, 4]\n",
      "episode 1651, reward 822.0, memory_length 2000, epsilon 0.19150783620272022, time 731.0, rides 130\n",
      "Initial State is  [4, 18, 5]\n",
      "episode 1652, reward 446.0, memory_length 2000, epsilon 0.1913163283665175, time 726.0, rides 116\n",
      "Initial State is  [3, 15, 3]\n",
      "episode 1653, reward 540.0, memory_length 2000, epsilon 0.19112501203815097, time 728.0, rides 113\n",
      "Initial State is  [4, 9, 1]\n",
      "episode 1654, reward 552.0, memory_length 2000, epsilon 0.1909338870261128, time 733.0, rides 129\n",
      "Initial State is  [4, 8, 5]\n",
      "episode 1655, reward 751.0, memory_length 2000, epsilon 0.19074295313908668, time 737.0, rides 132\n",
      "Initial State is  [2, 12, 4]\n",
      "episode 1656, reward 592.0, memory_length 2000, epsilon 0.1905522101859476, time 729.0, rides 126\n",
      "Initial State is  [4, 6, 6]\n",
      "episode 1657, reward 562.0, memory_length 2000, epsilon 0.19036165797576166, time 725.0, rides 117\n",
      "Initial State is  [4, 3, 1]\n",
      "episode 1658, reward 677.0, memory_length 2000, epsilon 0.1901712963177859, time 733.0, rides 127\n",
      "Initial State is  [0, 19, 3]\n",
      "episode 1659, reward 428.0, memory_length 2000, epsilon 0.1899811250214681, time 732.0, rides 127\n",
      "Initial State is  [4, 19, 0]\n",
      "episode 1660, reward 436.0, memory_length 2000, epsilon 0.18979114389644663, time 727.0, rides 125\n",
      "Initial State is  [2, 2, 3]\n",
      "episode 1661, reward 661.0, memory_length 2000, epsilon 0.1896013527525502, time 732.0, rides 119\n",
      "Initial State is  [2, 5, 0]\n",
      "episode 1662, reward 899.0, memory_length 2000, epsilon 0.18941175139979763, time 737.0, rides 118\n",
      "Initial State is  [3, 13, 1]\n",
      "episode 1663, reward 723.0, memory_length 2000, epsilon 0.18922233964839782, time 727.0, rides 118\n",
      "Initial State is  [3, 9, 0]\n",
      "episode 1664, reward 740.0, memory_length 2000, epsilon 0.18903311730874942, time 729.0, rides 131\n",
      "Initial State is  [4, 23, 3]\n",
      "episode 1665, reward 610.0, memory_length 2000, epsilon 0.18884408419144066, time 723.0, rides 113\n",
      "Initial State is  [4, 22, 3]\n",
      "episode 1666, reward 856.0, memory_length 2000, epsilon 0.1886552401072492, time 735.0, rides 135\n",
      "Initial State is  [3, 1, 5]\n",
      "episode 1667, reward 845.0, memory_length 2000, epsilon 0.18846658486714196, time 727.0, rides 119\n",
      "Initial State is  [0, 5, 3]\n",
      "episode 1668, reward 576.0, memory_length 2000, epsilon 0.1882781182822748, time 737.0, rides 127\n",
      "Initial State is  [3, 14, 1]\n",
      "episode 1669, reward 466.0, memory_length 2000, epsilon 0.18808984016399252, time 732.0, rides 117\n",
      "Initial State is  [4, 12, 1]\n",
      "episode 1670, reward 677.0, memory_length 2000, epsilon 0.18790175032382853, time 727.0, rides 131\n",
      "Initial State is  [4, 10, 1]\n",
      "episode 1671, reward 466.0, memory_length 2000, epsilon 0.1877138485735047, time 739.0, rides 133\n",
      "Initial State is  [3, 21, 6]\n",
      "episode 1672, reward 578.0, memory_length 2000, epsilon 0.1875261347249312, time 729.0, rides 131\n",
      "Initial State is  [2, 9, 3]\n",
      "episode 1673, reward 581.0, memory_length 2000, epsilon 0.1873386085902063, time 722.0, rides 133\n",
      "Initial State is  [3, 2, 6]\n",
      "episode 1674, reward 636.0, memory_length 2000, epsilon 0.18715126998161608, time 725.0, rides 123\n",
      "Initial State is  [0, 18, 1]\n",
      "episode 1675, reward 524.0, memory_length 2000, epsilon 0.18696411871163446, time 722.0, rides 116\n",
      "Initial State is  [3, 5, 2]\n",
      "episode 1676, reward 901.0, memory_length 2000, epsilon 0.18677715459292282, time 729.0, rides 133\n",
      "Initial State is  [1, 13, 1]\n",
      "episode 1677, reward 577.0, memory_length 2000, epsilon 0.1865903774383299, time 725.0, rides 127\n",
      "Initial State is  [3, 10, 1]\n",
      "episode 1678, reward 816.0, memory_length 2000, epsilon 0.18640378706089158, time 733.0, rides 112\n",
      "Initial State is  [0, 12, 6]\n",
      "episode 1679, reward 1012.0, memory_length 2000, epsilon 0.1862173832738307, time 733.0, rides 126\n",
      "Initial State is  [3, 8, 6]\n",
      "episode 1680, reward 689.0, memory_length 2000, epsilon 0.18603116589055688, time 726.0, rides 122\n",
      "Initial State is  [0, 19, 3]\n",
      "episode 1681, reward 722.0, memory_length 2000, epsilon 0.18584513472466632, time 727.0, rides 121\n",
      "Initial State is  [2, 9, 4]\n",
      "episode 1682, reward 568.0, memory_length 2000, epsilon 0.18565928958994166, time 737.0, rides 140\n",
      "Initial State is  [0, 16, 4]\n",
      "episode 1683, reward 245.0, memory_length 2000, epsilon 0.18547363030035172, time 723.0, rides 129\n",
      "Initial State is  [0, 23, 4]\n",
      "episode 1684, reward 669.0, memory_length 2000, epsilon 0.18528815667005136, time 737.0, rides 123\n",
      "Initial State is  [3, 11, 6]\n",
      "episode 1685, reward 736.0, memory_length 2000, epsilon 0.1851028685133813, time 733.0, rides 130\n",
      "Initial State is  [0, 20, 2]\n",
      "episode 1686, reward 613.0, memory_length 2000, epsilon 0.18491776564486792, time 735.0, rides 121\n",
      "Initial State is  [1, 15, 5]\n",
      "episode 1687, reward 770.0, memory_length 2000, epsilon 0.18473284787922306, time 735.0, rides 141\n",
      "Initial State is  [0, 21, 1]\n",
      "episode 1688, reward 488.0, memory_length 2000, epsilon 0.18454811503134383, time 724.0, rides 123\n",
      "Initial State is  [1, 3, 6]\n",
      "episode 1689, reward 761.0, memory_length 2000, epsilon 0.18436356691631248, time 725.0, rides 135\n",
      "Initial State is  [1, 11, 6]\n",
      "episode 1690, reward 555.0, memory_length 2000, epsilon 0.18417920334939616, time 726.0, rides 113\n",
      "Initial State is  [1, 15, 5]\n",
      "episode 1691, reward 463.0, memory_length 2000, epsilon 0.18399502414604677, time 722.0, rides 142\n",
      "Initial State is  [1, 13, 3]\n",
      "episode 1692, reward 635.0, memory_length 2000, epsilon 0.18381102912190073, time 726.0, rides 128\n",
      "Initial State is  [0, 22, 2]\n",
      "episode 1693, reward 962.0, memory_length 2000, epsilon 0.18362721809277882, time 727.0, rides 137\n",
      "Initial State is  [2, 8, 3]\n",
      "episode 1694, reward 401.0, memory_length 2000, epsilon 0.18344359087468604, time 724.0, rides 126\n",
      "Initial State is  [2, 9, 2]\n",
      "episode 1695, reward 960.0, memory_length 2000, epsilon 0.18326014728381135, time 725.0, rides 128\n",
      "Initial State is  [2, 14, 6]\n",
      "episode 1696, reward 615.0, memory_length 2000, epsilon 0.18307688713652753, time 723.0, rides 137\n",
      "Initial State is  [4, 11, 0]\n",
      "episode 1697, reward 935.0, memory_length 2000, epsilon 0.182893810249391, time 725.0, rides 130\n",
      "Initial State is  [3, 10, 0]\n",
      "episode 1698, reward 589.0, memory_length 2000, epsilon 0.1827109164391416, time 733.0, rides 133\n",
      "Initial State is  [4, 4, 6]\n",
      "episode 1699, reward 674.0, memory_length 2000, epsilon 0.18252820552270246, time 729.0, rides 147\n",
      "Initial State is  [0, 1, 3]\n",
      "episode 1700, reward 717.0, memory_length 2000, epsilon 0.18234567731717977, time 725.0, rides 137\n",
      "Initial State is  [2, 10, 5]\n",
      "episode 1701, reward 477.0, memory_length 2000, epsilon 0.1821633316398626, time 728.0, rides 134\n",
      "Initial State is  [0, 8, 4]\n",
      "episode 1702, reward 424.0, memory_length 2000, epsilon 0.18198116830822272, time 725.0, rides 136\n",
      "Initial State is  [0, 6, 6]\n",
      "episode 1703, reward 733.0, memory_length 2000, epsilon 0.1817991871399145, time 741.0, rides 134\n",
      "Initial State is  [4, 2, 4]\n",
      "episode 1704, reward 927.0, memory_length 2000, epsilon 0.18161738795277457, time 731.0, rides 136\n",
      "Initial State is  [3, 8, 3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 1705, reward 775.0, memory_length 2000, epsilon 0.1814357705648218, time 720.0, rides 133\n",
      "Initial State is  [0, 13, 5]\n",
      "episode 1706, reward 771.0, memory_length 2000, epsilon 0.18125433479425698, time 723.0, rides 120\n",
      "Initial State is  [1, 15, 2]\n",
      "episode 1707, reward 356.0, memory_length 2000, epsilon 0.18107308045946272, time 725.0, rides 120\n",
      "Initial State is  [0, 1, 4]\n",
      "episode 1708, reward 769.0, memory_length 2000, epsilon 0.18089200737900324, time 723.0, rides 127\n",
      "Initial State is  [1, 10, 1]\n",
      "episode 1709, reward 667.0, memory_length 2000, epsilon 0.18071111537162424, time 728.0, rides 118\n",
      "Initial State is  [4, 12, 5]\n",
      "episode 1710, reward 749.0, memory_length 2000, epsilon 0.1805304042562526, time 731.0, rides 129\n",
      "Initial State is  [2, 20, 0]\n",
      "episode 1711, reward 728.0, memory_length 2000, epsilon 0.18034987385199636, time 734.0, rides 124\n",
      "Initial State is  [2, 7, 0]\n",
      "episode 1712, reward 601.0, memory_length 2000, epsilon 0.18016952397814437, time 729.0, rides 130\n",
      "Initial State is  [2, 18, 4]\n",
      "episode 1713, reward 598.0, memory_length 2000, epsilon 0.17998935445416622, time 726.0, rides 121\n",
      "Initial State is  [4, 22, 3]\n",
      "episode 1714, reward 561.0, memory_length 2000, epsilon 0.17980936509971204, time 729.0, rides 126\n",
      "Initial State is  [3, 11, 6]\n",
      "episode 1715, reward 946.0, memory_length 2000, epsilon 0.17962955573461234, time 732.0, rides 131\n",
      "Initial State is  [3, 20, 6]\n",
      "episode 1716, reward 202.0, memory_length 2000, epsilon 0.17944992617887773, time 721.0, rides 130\n",
      "Initial State is  [2, 17, 0]\n",
      "episode 1717, reward 732.0, memory_length 2000, epsilon 0.17927047625269885, time 727.0, rides 134\n",
      "Initial State is  [3, 15, 3]\n",
      "episode 1718, reward 516.0, memory_length 2000, epsilon 0.17909120577644616, time 726.0, rides 112\n",
      "Initial State is  [3, 1, 4]\n",
      "episode 1719, reward 600.0, memory_length 2000, epsilon 0.17891211457066972, time 732.0, rides 123\n",
      "Initial State is  [0, 9, 5]\n",
      "episode 1720, reward 547.0, memory_length 2000, epsilon 0.17873320245609906, time 724.0, rides 131\n",
      "Initial State is  [0, 10, 0]\n",
      "episode 1721, reward 514.0, memory_length 2000, epsilon 0.17855446925364296, time 732.0, rides 132\n",
      "Initial State is  [4, 18, 1]\n",
      "episode 1722, reward 595.0, memory_length 2000, epsilon 0.17837591478438933, time 726.0, rides 130\n",
      "Initial State is  [2, 1, 5]\n",
      "episode 1723, reward 528.0, memory_length 2000, epsilon 0.17819753886960493, time 723.0, rides 127\n",
      "Initial State is  [1, 23, 4]\n",
      "episode 1724, reward 498.0, memory_length 2000, epsilon 0.17801934133073533, time 736.0, rides 117\n",
      "Initial State is  [4, 1, 3]\n",
      "episode 1725, reward 907.0, memory_length 2000, epsilon 0.17784132198940458, time 733.0, rides 145\n",
      "Initial State is  [0, 19, 1]\n",
      "episode 1726, reward 607.0, memory_length 2000, epsilon 0.1776634806674152, time 732.0, rides 127\n",
      "Initial State is  [4, 7, 5]\n",
      "episode 1727, reward 527.0, memory_length 2000, epsilon 0.17748581718674777, time 723.0, rides 123\n",
      "Initial State is  [3, 23, 6]\n",
      "episode 1728, reward 887.0, memory_length 2000, epsilon 0.17730833136956103, time 740.0, rides 124\n",
      "Initial State is  [0, 19, 2]\n",
      "episode 1729, reward 362.0, memory_length 2000, epsilon 0.17713102303819148, time 725.0, rides 119\n",
      "Initial State is  [1, 14, 0]\n",
      "episode 1730, reward 813.0, memory_length 2000, epsilon 0.1769538920151533, time 732.0, rides 124\n",
      "Initial State is  [3, 18, 4]\n",
      "episode 1731, reward 694.0, memory_length 2000, epsilon 0.17677693812313813, time 722.0, rides 124\n",
      "Initial State is  [3, 23, 5]\n",
      "episode 1732, reward 442.0, memory_length 2000, epsilon 0.17660016118501498, time 731.0, rides 127\n",
      "Initial State is  [0, 1, 1]\n",
      "episode 1733, reward 760.0, memory_length 2000, epsilon 0.17642356102382997, time 732.0, rides 116\n",
      "Initial State is  [1, 0, 2]\n",
      "episode 1734, reward 233.0, memory_length 2000, epsilon 0.17624713746280615, time 729.0, rides 126\n",
      "Initial State is  [4, 20, 1]\n",
      "episode 1735, reward 400.0, memory_length 2000, epsilon 0.17607089032534334, time 723.0, rides 120\n",
      "Initial State is  [4, 16, 5]\n",
      "episode 1736, reward 520.0, memory_length 2000, epsilon 0.175894819435018, time 727.0, rides 125\n",
      "Initial State is  [1, 19, 4]\n",
      "episode 1737, reward 896.0, memory_length 2000, epsilon 0.17571892461558297, time 733.0, rides 126\n",
      "Initial State is  [3, 14, 5]\n",
      "episode 1738, reward 706.0, memory_length 2000, epsilon 0.1755432056909674, time 732.0, rides 138\n",
      "Initial State is  [1, 19, 0]\n",
      "episode 1739, reward 578.0, memory_length 2000, epsilon 0.17536766248527644, time 726.0, rides 131\n",
      "Initial State is  [2, 1, 5]\n",
      "episode 1740, reward 648.0, memory_length 2000, epsilon 0.17519229482279117, time 730.0, rides 122\n",
      "Initial State is  [2, 11, 2]\n",
      "episode 1741, reward 560.0, memory_length 2000, epsilon 0.17501710252796837, time 730.0, rides 135\n",
      "Initial State is  [1, 8, 2]\n",
      "episode 1742, reward 393.0, memory_length 2000, epsilon 0.1748420854254404, time 732.0, rides 139\n",
      "Initial State is  [0, 2, 3]\n",
      "episode 1743, reward 475.0, memory_length 2000, epsilon 0.17466724334001496, time 729.0, rides 126\n",
      "Initial State is  [2, 9, 2]\n",
      "episode 1744, reward 861.0, memory_length 2000, epsilon 0.17449257609667496, time 729.0, rides 140\n",
      "Initial State is  [1, 13, 6]\n",
      "episode 1745, reward 635.0, memory_length 2000, epsilon 0.1743180835205783, time 727.0, rides 130\n",
      "Initial State is  [1, 2, 0]\n",
      "episode 1746, reward 781.0, memory_length 2000, epsilon 0.1741437654370577, time 729.0, rides 127\n",
      "Initial State is  [2, 8, 1]\n",
      "episode 1747, reward 484.0, memory_length 2000, epsilon 0.17396962167162064, time 733.0, rides 129\n",
      "Initial State is  [2, 7, 0]\n",
      "episode 1748, reward 661.0, memory_length 2000, epsilon 0.173795652049949, time 726.0, rides 136\n",
      "Initial State is  [3, 15, 3]\n",
      "episode 1749, reward 888.0, memory_length 2000, epsilon 0.17362185639789907, time 735.0, rides 121\n",
      "Initial State is  [3, 1, 3]\n",
      "episode 1750, reward 709.0, memory_length 2000, epsilon 0.17344823454150116, time 730.0, rides 120\n",
      "Initial State is  [1, 9, 5]\n",
      "episode 1751, reward 710.0, memory_length 2000, epsilon 0.17327478630695967, time 728.0, rides 119\n",
      "Initial State is  [0, 21, 3]\n",
      "episode 1752, reward 683.0, memory_length 2000, epsilon 0.1731015115206527, time 728.0, rides 133\n",
      "Initial State is  [0, 8, 0]\n",
      "episode 1753, reward 544.0, memory_length 2000, epsilon 0.17292841000913206, time 730.0, rides 121\n",
      "Initial State is  [0, 15, 5]\n",
      "episode 1754, reward 1112.0, memory_length 2000, epsilon 0.17275548159912293, time 729.0, rides 125\n",
      "Initial State is  [4, 16, 2]\n",
      "episode 1755, reward 698.0, memory_length 2000, epsilon 0.17258272611752382, time 738.0, rides 128\n",
      "Initial State is  [4, 13, 1]\n",
      "episode 1756, reward 816.0, memory_length 2000, epsilon 0.17241014339140628, time 730.0, rides 118\n",
      "Initial State is  [2, 8, 1]\n",
      "episode 1757, reward 426.0, memory_length 2000, epsilon 0.1722377332480149, time 727.0, rides 139\n",
      "Initial State is  [0, 4, 4]\n",
      "episode 1758, reward 638.0, memory_length 2000, epsilon 0.17206549551476688, time 722.0, rides 146\n",
      "Initial State is  [0, 13, 5]\n",
      "episode 1759, reward 759.0, memory_length 2000, epsilon 0.1718934300192521, time 734.0, rides 134\n",
      "Initial State is  [3, 2, 0]\n",
      "episode 1760, reward 612.0, memory_length 2000, epsilon 0.17172153658923286, time 724.0, rides 139\n",
      "Initial State is  [2, 10, 6]\n",
      "episode 1761, reward 804.0, memory_length 2000, epsilon 0.17154981505264363, time 733.0, rides 126\n",
      "Initial State is  [2, 9, 4]\n",
      "episode 1762, reward 840.0, memory_length 2000, epsilon 0.17137826523759098, time 735.0, rides 133\n",
      "Initial State is  [0, 1, 4]\n",
      "episode 1763, reward 599.0, memory_length 2000, epsilon 0.1712068869723534, time 726.0, rides 137\n",
      "Initial State is  [1, 9, 3]\n",
      "episode 1764, reward 363.0, memory_length 2000, epsilon 0.17103568008538103, time 727.0, rides 131\n",
      "Initial State is  [1, 23, 1]\n",
      "episode 1765, reward 462.0, memory_length 2000, epsilon 0.17086464440529564, time 740.0, rides 124\n",
      "Initial State is  [0, 11, 5]\n",
      "episode 1766, reward 738.0, memory_length 2000, epsilon 0.17069377976089034, time 732.0, rides 114\n",
      "Initial State is  [4, 20, 5]\n",
      "episode 1767, reward 740.0, memory_length 2000, epsilon 0.17052308598112945, time 730.0, rides 139\n",
      "Initial State is  [1, 22, 5]\n",
      "episode 1768, reward 761.0, memory_length 2000, epsilon 0.17035256289514833, time 727.0, rides 128\n",
      "Initial State is  [3, 18, 6]\n",
      "episode 1769, reward 708.0, memory_length 2000, epsilon 0.17018221033225317, time 737.0, rides 134\n",
      "Initial State is  [0, 6, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 1770, reward 721.0, memory_length 2000, epsilon 0.17001202812192093, time 728.0, rides 127\n",
      "Initial State is  [2, 6, 0]\n",
      "episode 1771, reward 463.0, memory_length 2000, epsilon 0.169842016093799, time 727.0, rides 123\n",
      "Initial State is  [4, 3, 3]\n",
      "episode 1772, reward 477.0, memory_length 2000, epsilon 0.1696721740777052, time 725.0, rides 118\n",
      "Initial State is  [0, 18, 1]\n",
      "episode 1773, reward 424.0, memory_length 2000, epsilon 0.1695025019036275, time 721.0, rides 129\n",
      "Initial State is  [4, 19, 2]\n",
      "episode 1774, reward 572.0, memory_length 2000, epsilon 0.1693329994017239, time 727.0, rides 122\n",
      "Initial State is  [2, 7, 6]\n",
      "episode 1775, reward 536.0, memory_length 2000, epsilon 0.16916366640232217, time 731.0, rides 114\n",
      "Initial State is  [2, 3, 1]\n",
      "episode 1776, reward 494.0, memory_length 2000, epsilon 0.16899450273591984, time 727.0, rides 117\n",
      "Initial State is  [2, 5, 1]\n",
      "episode 1777, reward 668.0, memory_length 2000, epsilon 0.16882550823318393, time 731.0, rides 127\n",
      "Initial State is  [1, 2, 2]\n",
      "episode 1778, reward 657.0, memory_length 2000, epsilon 0.16865668272495074, time 732.0, rides 137\n",
      "Initial State is  [4, 5, 5]\n",
      "episode 1779, reward 583.0, memory_length 2000, epsilon 0.1684880260422258, time 739.0, rides 125\n",
      "Initial State is  [1, 23, 6]\n",
      "episode 1780, reward 597.0, memory_length 2000, epsilon 0.16831953801618357, time 737.0, rides 125\n",
      "Initial State is  [2, 0, 3]\n",
      "episode 1781, reward 500.0, memory_length 2000, epsilon 0.16815121847816739, time 729.0, rides 135\n",
      "Initial State is  [2, 12, 0]\n",
      "episode 1782, reward 593.0, memory_length 2000, epsilon 0.16798306725968923, time 720.0, rides 124\n",
      "Initial State is  [2, 20, 4]\n",
      "episode 1783, reward 772.0, memory_length 2000, epsilon 0.16781508419242955, time 734.0, rides 128\n",
      "Initial State is  [0, 22, 6]\n",
      "episode 1784, reward 915.0, memory_length 2000, epsilon 0.16764726910823713, time 727.0, rides 133\n",
      "Initial State is  [3, 4, 4]\n",
      "episode 1785, reward 608.0, memory_length 2000, epsilon 0.1674796218391289, time 736.0, rides 126\n",
      "Initial State is  [1, 10, 2]\n",
      "episode 1786, reward 735.0, memory_length 2000, epsilon 0.16731214221728977, time 723.0, rides 121\n",
      "Initial State is  [4, 2, 0]\n",
      "episode 1787, reward 545.0, memory_length 2000, epsilon 0.16714483007507247, time 734.0, rides 133\n",
      "Initial State is  [1, 18, 5]\n",
      "episode 1788, reward 845.0, memory_length 2000, epsilon 0.1669776852449974, time 728.0, rides 123\n",
      "Initial State is  [3, 20, 6]\n",
      "episode 1789, reward 849.0, memory_length 2000, epsilon 0.1668107075597524, time 727.0, rides 124\n",
      "Initial State is  [3, 19, 1]\n",
      "episode 1790, reward 894.0, memory_length 2000, epsilon 0.16664389685219266, time 729.0, rides 135\n",
      "Initial State is  [4, 23, 5]\n",
      "episode 1791, reward 740.0, memory_length 2000, epsilon 0.16647725295534047, time 726.0, rides 115\n",
      "Initial State is  [1, 16, 4]\n",
      "episode 1792, reward 468.0, memory_length 2000, epsilon 0.16631077570238512, time 729.0, rides 122\n",
      "Initial State is  [1, 1, 4]\n",
      "episode 1793, reward 993.0, memory_length 2000, epsilon 0.16614446492668272, time 729.0, rides 119\n",
      "Initial State is  [3, 6, 2]\n",
      "episode 1794, reward 692.0, memory_length 2000, epsilon 0.16597832046175603, time 725.0, rides 129\n",
      "Initial State is  [0, 14, 1]\n",
      "episode 1795, reward 915.0, memory_length 2000, epsilon 0.16581234214129428, time 733.0, rides 129\n",
      "Initial State is  [3, 8, 5]\n",
      "episode 1796, reward 655.0, memory_length 2000, epsilon 0.16564652979915298, time 729.0, rides 122\n",
      "Initial State is  [2, 17, 3]\n",
      "episode 1797, reward 1028.0, memory_length 2000, epsilon 0.16548088326935384, time 722.0, rides 122\n",
      "Initial State is  [1, 22, 0]\n",
      "episode 1798, reward 715.0, memory_length 2000, epsilon 0.1653154023860845, time 728.0, rides 127\n",
      "Initial State is  [1, 8, 0]\n",
      "episode 1799, reward 659.0, memory_length 2000, epsilon 0.1651500869836984, time 734.0, rides 120\n",
      "Initial State is  [4, 23, 3]\n",
      "episode 1800, reward 680.0, memory_length 2000, epsilon 0.1649849368967147, time 725.0, rides 135\n",
      "Initial State is  [0, 18, 6]\n",
      "episode 1801, reward 679.0, memory_length 2000, epsilon 0.164819951959818, time 724.0, rides 136\n",
      "Initial State is  [0, 10, 2]\n",
      "episode 1802, reward 657.0, memory_length 2000, epsilon 0.16465513200785817, time 729.0, rides 129\n",
      "Initial State is  [0, 23, 0]\n",
      "episode 1803, reward 523.0, memory_length 2000, epsilon 0.1644904768758503, time 727.0, rides 124\n",
      "Initial State is  [0, 17, 3]\n",
      "episode 1804, reward 768.0, memory_length 2000, epsilon 0.16432598639897444, time 736.0, rides 133\n",
      "Initial State is  [2, 22, 0]\n",
      "episode 1805, reward 581.0, memory_length 2000, epsilon 0.16416166041257546, time 729.0, rides 120\n",
      "Initial State is  [4, 5, 6]\n",
      "episode 1806, reward 694.0, memory_length 2000, epsilon 0.16399749875216288, time 728.0, rides 137\n",
      "Initial State is  [3, 16, 1]\n",
      "episode 1807, reward 529.0, memory_length 2000, epsilon 0.16383350125341073, time 730.0, rides 129\n",
      "Initial State is  [2, 22, 3]\n",
      "episode 1808, reward 905.0, memory_length 2000, epsilon 0.16366966775215733, time 727.0, rides 121\n",
      "Initial State is  [4, 21, 5]\n",
      "episode 1809, reward 684.0, memory_length 2000, epsilon 0.16350599808440516, time 723.0, rides 121\n",
      "Initial State is  [3, 12, 5]\n",
      "episode 1810, reward 813.0, memory_length 2000, epsilon 0.16334249208632076, time 722.0, rides 128\n",
      "Initial State is  [4, 18, 3]\n",
      "episode 1811, reward 580.0, memory_length 2000, epsilon 0.16317914959423443, time 736.0, rides 124\n",
      "Initial State is  [1, 15, 5]\n",
      "episode 1812, reward 563.0, memory_length 2000, epsilon 0.1630159704446402, time 730.0, rides 126\n",
      "Initial State is  [4, 21, 6]\n",
      "episode 1813, reward 863.0, memory_length 2000, epsilon 0.16285295447419557, time 733.0, rides 125\n",
      "Initial State is  [1, 17, 4]\n",
      "episode 1814, reward 763.0, memory_length 2000, epsilon 0.16269010151972138, time 727.0, rides 130\n",
      "Initial State is  [0, 12, 6]\n",
      "episode 1815, reward 381.0, memory_length 2000, epsilon 0.16252741141820165, time 733.0, rides 120\n",
      "Initial State is  [1, 3, 4]\n",
      "episode 1816, reward 796.0, memory_length 2000, epsilon 0.16236488400678345, time 727.0, rides 130\n",
      "Initial State is  [3, 14, 2]\n",
      "episode 1817, reward 773.0, memory_length 2000, epsilon 0.16220251912277667, time 734.0, rides 126\n",
      "Initial State is  [2, 14, 4]\n",
      "episode 1818, reward 1041.0, memory_length 2000, epsilon 0.1620403166036539, time 727.0, rides 113\n",
      "Initial State is  [0, 4, 6]\n",
      "episode 1819, reward 498.0, memory_length 2000, epsilon 0.16187827628705023, time 724.0, rides 122\n",
      "Initial State is  [1, 12, 5]\n",
      "episode 1820, reward 819.0, memory_length 2000, epsilon 0.16171639801076318, time 734.0, rides 139\n",
      "Initial State is  [2, 15, 3]\n",
      "episode 1821, reward 685.0, memory_length 2000, epsilon 0.1615546816127524, time 733.0, rides 123\n",
      "Initial State is  [4, 20, 1]\n",
      "episode 1822, reward 731.0, memory_length 2000, epsilon 0.16139312693113966, time 727.0, rides 116\n",
      "Initial State is  [2, 8, 6]\n",
      "episode 1823, reward 858.0, memory_length 2000, epsilon 0.16123173380420852, time 731.0, rides 127\n",
      "Initial State is  [2, 11, 0]\n",
      "episode 1824, reward 1011.0, memory_length 2000, epsilon 0.16107050207040433, time 730.0, rides 138\n",
      "Initial State is  [1, 16, 4]\n",
      "episode 1825, reward 654.0, memory_length 2000, epsilon 0.16090943156833393, time 726.0, rides 130\n",
      "Initial State is  [0, 20, 1]\n",
      "episode 1826, reward 1026.0, memory_length 2000, epsilon 0.1607485221367656, time 725.0, rides 131\n",
      "Initial State is  [0, 14, 0]\n",
      "episode 1827, reward 1043.0, memory_length 2000, epsilon 0.16058777361462884, time 736.0, rides 140\n",
      "Initial State is  [2, 21, 2]\n",
      "episode 1828, reward 820.0, memory_length 2000, epsilon 0.1604271858410142, time 730.0, rides 123\n",
      "Initial State is  [4, 13, 0]\n",
      "episode 1829, reward 629.0, memory_length 2000, epsilon 0.16026675865517317, time 734.0, rides 129\n",
      "Initial State is  [2, 14, 5]\n",
      "episode 1830, reward 423.0, memory_length 2000, epsilon 0.160106491896518, time 726.0, rides 113\n",
      "Initial State is  [4, 3, 5]\n",
      "episode 1831, reward 665.0, memory_length 2000, epsilon 0.15994638540462147, time 724.0, rides 135\n",
      "Initial State is  [3, 1, 6]\n",
      "episode 1832, reward 883.0, memory_length 2000, epsilon 0.15978643901921685, time 730.0, rides 115\n",
      "Initial State is  [3, 2, 1]\n",
      "episode 1833, reward 595.0, memory_length 2000, epsilon 0.15962665258019765, time 732.0, rides 129\n",
      "Initial State is  [1, 6, 5]\n",
      "episode 1834, reward 472.0, memory_length 2000, epsilon 0.15946702592761744, time 724.0, rides 115\n",
      "Initial State is  [2, 10, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 1835, reward 893.0, memory_length 2000, epsilon 0.15930755890168982, time 726.0, rides 135\n",
      "Initial State is  [1, 16, 5]\n",
      "episode 1836, reward 395.0, memory_length 2000, epsilon 0.15914825134278812, time 726.0, rides 120\n",
      "Initial State is  [1, 23, 3]\n",
      "episode 1837, reward 599.0, memory_length 2000, epsilon 0.15898910309144534, time 735.0, rides 125\n",
      "Initial State is  [0, 13, 1]\n",
      "episode 1838, reward 682.0, memory_length 2000, epsilon 0.1588301139883539, time 729.0, rides 126\n",
      "Initial State is  [3, 13, 1]\n",
      "episode 1839, reward 489.0, memory_length 2000, epsilon 0.15867128387436555, time 726.0, rides 135\n",
      "Initial State is  [4, 9, 2]\n",
      "episode 1840, reward 825.0, memory_length 2000, epsilon 0.15851261259049118, time 722.0, rides 122\n",
      "Initial State is  [2, 3, 4]\n",
      "episode 1841, reward 491.0, memory_length 2000, epsilon 0.1583540999779007, time 733.0, rides 136\n",
      "Initial State is  [0, 8, 2]\n",
      "episode 1842, reward 617.0, memory_length 2000, epsilon 0.1581957458779228, time 728.0, rides 132\n",
      "Initial State is  [1, 16, 5]\n",
      "episode 1843, reward 1070.0, memory_length 2000, epsilon 0.15803755013204487, time 730.0, rides 133\n",
      "Initial State is  [4, 1, 1]\n",
      "episode 1844, reward 644.0, memory_length 2000, epsilon 0.15787951258191282, time 728.0, rides 131\n",
      "Initial State is  [3, 21, 3]\n",
      "episode 1845, reward 627.0, memory_length 2000, epsilon 0.1577216330693309, time 723.0, rides 122\n",
      "Initial State is  [1, 20, 6]\n",
      "episode 1846, reward 478.0, memory_length 2000, epsilon 0.15756391143626158, time 725.0, rides 129\n",
      "Initial State is  [1, 23, 0]\n",
      "episode 1847, reward 999.0, memory_length 2000, epsilon 0.15740634752482532, time 725.0, rides 128\n",
      "Initial State is  [2, 11, 5]\n",
      "episode 1848, reward 733.0, memory_length 2000, epsilon 0.1572489411773005, time 733.0, rides 131\n",
      "Initial State is  [1, 12, 6]\n",
      "episode 1849, reward 924.0, memory_length 2000, epsilon 0.1570916922361232, time 731.0, rides 134\n",
      "Initial State is  [4, 0, 2]\n",
      "episode 1850, reward 570.0, memory_length 2000, epsilon 0.15693460054388708, time 725.0, rides 127\n",
      "Initial State is  [4, 17, 1]\n",
      "episode 1851, reward 301.0, memory_length 2000, epsilon 0.1567776659433432, time 730.0, rides 122\n",
      "Initial State is  [3, 17, 4]\n",
      "episode 1852, reward 611.0, memory_length 2000, epsilon 0.15662088827739984, time 726.0, rides 117\n",
      "Initial State is  [1, 0, 4]\n",
      "episode 1853, reward 540.0, memory_length 2000, epsilon 0.15646426738912245, time 730.0, rides 131\n",
      "Initial State is  [0, 14, 5]\n",
      "episode 1854, reward 520.0, memory_length 2000, epsilon 0.1563078031217333, time 734.0, rides 123\n",
      "Initial State is  [4, 20, 0]\n",
      "episode 1855, reward 523.0, memory_length 2000, epsilon 0.15615149531861158, time 726.0, rides 117\n",
      "Initial State is  [2, 13, 1]\n",
      "episode 1856, reward 735.0, memory_length 2000, epsilon 0.15599534382329297, time 730.0, rides 129\n",
      "Initial State is  [1, 22, 6]\n",
      "episode 1857, reward 923.0, memory_length 2000, epsilon 0.15583934847946967, time 727.0, rides 132\n",
      "Initial State is  [1, 7, 4]\n",
      "episode 1858, reward 814.0, memory_length 2000, epsilon 0.15568350913099022, time 741.0, rides 120\n",
      "Initial State is  [2, 14, 0]\n",
      "episode 1859, reward 714.0, memory_length 2000, epsilon 0.15552782562185924, time 729.0, rides 131\n",
      "Initial State is  [4, 10, 1]\n",
      "episode 1860, reward 580.0, memory_length 2000, epsilon 0.15537229779623737, time 733.0, rides 121\n",
      "Initial State is  [2, 18, 4]\n",
      "episode 1861, reward 629.0, memory_length 2000, epsilon 0.15521692549844113, time 732.0, rides 135\n",
      "Initial State is  [1, 12, 6]\n",
      "episode 1862, reward 743.0, memory_length 2000, epsilon 0.15506170857294269, time 734.0, rides 142\n",
      "Initial State is  [3, 4, 3]\n",
      "episode 1863, reward 664.0, memory_length 2000, epsilon 0.15490664686436975, time 730.0, rides 129\n",
      "Initial State is  [2, 22, 2]\n",
      "episode 1864, reward 500.0, memory_length 2000, epsilon 0.15475174021750537, time 745.0, rides 136\n",
      "Initial State is  [0, 9, 2]\n",
      "episode 1865, reward 731.0, memory_length 2000, epsilon 0.15459698847728787, time 731.0, rides 126\n",
      "Initial State is  [3, 18, 2]\n",
      "episode 1866, reward 630.0, memory_length 2000, epsilon 0.1544423914888106, time 725.0, rides 137\n",
      "Initial State is  [4, 12, 6]\n",
      "episode 1867, reward 534.0, memory_length 2000, epsilon 0.15428794909732177, time 726.0, rides 127\n",
      "Initial State is  [4, 14, 1]\n",
      "episode 1868, reward 582.0, memory_length 2000, epsilon 0.15413366114822444, time 735.0, rides 125\n",
      "Initial State is  [4, 18, 5]\n",
      "episode 1869, reward 475.0, memory_length 2000, epsilon 0.15397952748707622, time 728.0, rides 136\n",
      "Initial State is  [4, 17, 0]\n",
      "episode 1870, reward 437.0, memory_length 2000, epsilon 0.15382554795958914, time 730.0, rides 118\n",
      "Initial State is  [3, 14, 6]\n",
      "episode 1871, reward 412.0, memory_length 2000, epsilon 0.15367172241162955, time 730.0, rides 121\n",
      "Initial State is  [2, 5, 4]\n",
      "episode 1872, reward 689.0, memory_length 2000, epsilon 0.15351805068921792, time 734.0, rides 121\n",
      "Initial State is  [1, 20, 4]\n",
      "episode 1873, reward 870.0, memory_length 2000, epsilon 0.1533645326385287, time 732.0, rides 128\n",
      "Initial State is  [4, 3, 4]\n",
      "episode 1874, reward 605.0, memory_length 2000, epsilon 0.15321116810589017, time 725.0, rides 133\n",
      "Initial State is  [2, 5, 5]\n",
      "episode 1875, reward 533.0, memory_length 2000, epsilon 0.1530579569377843, time 728.0, rides 128\n",
      "Initial State is  [4, 13, 3]\n",
      "episode 1876, reward 1096.0, memory_length 2000, epsilon 0.1529048989808465, time 732.0, rides 129\n",
      "Initial State is  [1, 11, 5]\n",
      "episode 1877, reward 662.0, memory_length 2000, epsilon 0.15275199408186566, time 725.0, rides 134\n",
      "Initial State is  [0, 6, 6]\n",
      "episode 1878, reward 643.0, memory_length 2000, epsilon 0.15259924208778378, time 725.0, rides 135\n",
      "Initial State is  [0, 10, 5]\n",
      "episode 1879, reward 548.0, memory_length 2000, epsilon 0.15244664284569598, time 725.0, rides 140\n",
      "Initial State is  [0, 8, 3]\n",
      "episode 1880, reward 772.0, memory_length 2000, epsilon 0.15229419620285028, time 731.0, rides 127\n",
      "Initial State is  [4, 6, 4]\n",
      "episode 1881, reward 585.0, memory_length 2000, epsilon 0.15214190200664743, time 728.0, rides 133\n",
      "Initial State is  [4, 5, 3]\n",
      "episode 1882, reward 846.0, memory_length 2000, epsilon 0.15198976010464077, time 730.0, rides 131\n",
      "Initial State is  [0, 2, 6]\n",
      "episode 1883, reward 698.0, memory_length 2000, epsilon 0.15183777034453613, time 724.0, rides 139\n",
      "Initial State is  [1, 23, 5]\n",
      "episode 1884, reward 146.0, memory_length 2000, epsilon 0.1516859325741916, time 731.0, rides 120\n",
      "Initial State is  [1, 5, 5]\n",
      "episode 1885, reward 697.0, memory_length 2000, epsilon 0.1515342466416174, time 726.0, rides 143\n",
      "Initial State is  [1, 3, 0]\n",
      "episode 1886, reward 670.0, memory_length 2000, epsilon 0.1513827123949758, time 724.0, rides 137\n",
      "Initial State is  [1, 6, 2]\n",
      "episode 1887, reward 691.0, memory_length 2000, epsilon 0.15123132968258082, time 725.0, rides 122\n",
      "Initial State is  [4, 3, 6]\n",
      "episode 1888, reward 618.0, memory_length 2000, epsilon 0.15108009835289823, time 724.0, rides 121\n",
      "Initial State is  [1, 17, 5]\n",
      "episode 1889, reward 778.0, memory_length 2000, epsilon 0.15092901825454533, time 730.0, rides 130\n",
      "Initial State is  [0, 14, 5]\n",
      "episode 1890, reward 757.0, memory_length 2000, epsilon 0.15077808923629077, time 732.0, rides 118\n",
      "Initial State is  [1, 22, 3]\n",
      "episode 1891, reward 892.0, memory_length 2000, epsilon 0.15062731114705447, time 724.0, rides 130\n",
      "Initial State is  [2, 0, 0]\n",
      "episode 1892, reward 690.0, memory_length 2000, epsilon 0.1504766838359074, time 730.0, rides 117\n",
      "Initial State is  [0, 11, 3]\n",
      "episode 1893, reward 488.0, memory_length 2000, epsilon 0.15032620715207148, time 739.0, rides 108\n",
      "Initial State is  [0, 14, 4]\n",
      "episode 1894, reward 514.0, memory_length 2000, epsilon 0.1501758809449194, time 736.0, rides 122\n",
      "Initial State is  [3, 21, 3]\n",
      "episode 1895, reward 788.0, memory_length 2000, epsilon 0.15002570506397447, time 732.0, rides 134\n",
      "Initial State is  [0, 22, 1]\n",
      "episode 1896, reward 473.0, memory_length 2000, epsilon 0.1498756793589105, time 725.0, rides 127\n",
      "Initial State is  [0, 16, 6]\n",
      "episode 1897, reward 748.0, memory_length 2000, epsilon 0.14972580367955157, time 731.0, rides 134\n",
      "Initial State is  [1, 1, 4]\n",
      "episode 1898, reward 619.0, memory_length 2000, epsilon 0.14957607787587202, time 728.0, rides 127\n",
      "Initial State is  [0, 2, 3]\n",
      "episode 1899, reward 662.0, memory_length 2000, epsilon 0.14942650179799613, time 727.0, rides 125\n",
      "Initial State is  [3, 0, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 1900, reward 774.0, memory_length 2000, epsilon 0.14927707529619813, time 730.0, rides 127\n",
      "Initial State is  [2, 21, 3]\n",
      "episode 1901, reward 530.0, memory_length 2000, epsilon 0.14912779822090194, time 734.0, rides 126\n",
      "Initial State is  [4, 3, 1]\n",
      "episode 1902, reward 796.0, memory_length 2000, epsilon 0.14897867042268104, time 722.0, rides 138\n",
      "Initial State is  [0, 20, 0]\n",
      "episode 1903, reward 568.0, memory_length 2000, epsilon 0.14882969175225835, time 721.0, rides 108\n",
      "Initial State is  [2, 6, 4]\n",
      "episode 1904, reward 687.0, memory_length 2000, epsilon 0.1486808620605061, time 728.0, rides 133\n",
      "Initial State is  [0, 11, 1]\n",
      "episode 1905, reward 738.0, memory_length 2000, epsilon 0.14853218119844558, time 733.0, rides 121\n",
      "Initial State is  [1, 0, 0]\n",
      "episode 1906, reward 398.0, memory_length 2000, epsilon 0.14838364901724713, time 730.0, rides 120\n",
      "Initial State is  [2, 3, 1]\n",
      "episode 1907, reward 651.0, memory_length 2000, epsilon 0.14823526536822987, time 730.0, rides 132\n",
      "Initial State is  [2, 6, 0]\n",
      "episode 1908, reward 486.0, memory_length 2000, epsilon 0.14808703010286164, time 730.0, rides 139\n",
      "Initial State is  [0, 8, 3]\n",
      "episode 1909, reward 725.0, memory_length 2000, epsilon 0.1479389430727588, time 731.0, rides 131\n",
      "Initial State is  [4, 15, 0]\n",
      "episode 1910, reward 763.0, memory_length 2000, epsilon 0.14779100412968604, time 728.0, rides 128\n",
      "Initial State is  [0, 16, 6]\n",
      "episode 1911, reward 690.0, memory_length 2000, epsilon 0.14764321312555637, time 730.0, rides 126\n",
      "Initial State is  [0, 14, 3]\n",
      "episode 1912, reward 633.0, memory_length 2000, epsilon 0.14749556991243082, time 738.0, rides 127\n",
      "Initial State is  [1, 0, 3]\n",
      "episode 1913, reward 539.0, memory_length 2000, epsilon 0.1473480743425184, time 722.0, rides 124\n",
      "Initial State is  [3, 15, 5]\n",
      "episode 1914, reward 736.0, memory_length 2000, epsilon 0.14720072626817587, time 729.0, rides 137\n",
      "Initial State is  [2, 3, 4]\n",
      "episode 1915, reward 825.0, memory_length 2000, epsilon 0.1470535255419077, time 734.0, rides 119\n",
      "Initial State is  [4, 23, 2]\n",
      "episode 1916, reward 374.0, memory_length 2000, epsilon 0.1469064720163658, time 730.0, rides 123\n",
      "Initial State is  [1, 13, 0]\n",
      "episode 1917, reward 490.0, memory_length 2000, epsilon 0.14675956554434944, time 727.0, rides 120\n",
      "Initial State is  [4, 7, 4]\n",
      "episode 1918, reward 832.0, memory_length 2000, epsilon 0.14661280597880508, time 730.0, rides 124\n",
      "Initial State is  [1, 5, 3]\n",
      "episode 1919, reward 852.0, memory_length 2000, epsilon 0.14646619317282628, time 723.0, rides 130\n",
      "Initial State is  [3, 11, 2]\n",
      "episode 1920, reward 909.0, memory_length 2000, epsilon 0.14631972697965345, time 732.0, rides 126\n",
      "Initial State is  [1, 23, 3]\n",
      "episode 1921, reward 894.0, memory_length 2000, epsilon 0.1461734072526738, time 728.0, rides 122\n",
      "Initial State is  [1, 22, 3]\n",
      "episode 1922, reward 939.0, memory_length 2000, epsilon 0.14602723384542113, time 732.0, rides 142\n",
      "Initial State is  [0, 9, 4]\n",
      "episode 1923, reward 632.0, memory_length 2000, epsilon 0.14588120661157572, time 726.0, rides 133\n",
      "Initial State is  [4, 9, 3]\n",
      "episode 1924, reward 431.0, memory_length 2000, epsilon 0.14573532540496414, time 733.0, rides 127\n",
      "Initial State is  [2, 9, 1]\n",
      "episode 1925, reward 666.0, memory_length 2000, epsilon 0.14558959007955918, time 723.0, rides 130\n",
      "Initial State is  [3, 0, 0]\n",
      "episode 1926, reward 750.0, memory_length 2000, epsilon 0.1454440004894796, time 731.0, rides 136\n",
      "Initial State is  [2, 22, 0]\n",
      "episode 1927, reward 710.0, memory_length 2000, epsilon 0.14529855648899012, time 733.0, rides 145\n",
      "Initial State is  [4, 10, 3]\n",
      "episode 1928, reward 769.0, memory_length 2000, epsilon 0.14515325793250114, time 722.0, rides 126\n",
      "Initial State is  [0, 1, 1]\n",
      "episode 1929, reward 649.0, memory_length 2000, epsilon 0.14500810467456865, time 730.0, rides 130\n",
      "Initial State is  [2, 0, 2]\n",
      "episode 1930, reward 912.0, memory_length 2000, epsilon 0.14486309656989407, time 728.0, rides 124\n",
      "Initial State is  [2, 17, 6]\n",
      "episode 1931, reward 780.0, memory_length 2000, epsilon 0.1447182334733242, time 723.0, rides 125\n",
      "Initial State is  [2, 16, 3]\n",
      "episode 1932, reward 787.0, memory_length 2000, epsilon 0.14457351523985087, time 725.0, rides 121\n",
      "Initial State is  [1, 4, 5]\n",
      "episode 1933, reward 603.0, memory_length 2000, epsilon 0.14442894172461102, time 728.0, rides 114\n",
      "Initial State is  [4, 6, 2]\n",
      "episode 1934, reward 891.0, memory_length 2000, epsilon 0.1442845127828864, time 729.0, rides 121\n",
      "Initial State is  [3, 14, 6]\n",
      "episode 1935, reward 701.0, memory_length 2000, epsilon 0.1441402282701035, time 735.0, rides 116\n",
      "Initial State is  [0, 21, 5]\n",
      "episode 1936, reward 718.0, memory_length 2000, epsilon 0.1439960880418334, time 730.0, rides 132\n",
      "Initial State is  [1, 6, 3]\n",
      "episode 1937, reward 407.0, memory_length 2000, epsilon 0.14385209195379156, time 724.0, rides 118\n",
      "Initial State is  [3, 19, 0]\n",
      "episode 1938, reward 788.0, memory_length 2000, epsilon 0.14370823986183776, time 734.0, rides 126\n",
      "Initial State is  [3, 8, 0]\n",
      "episode 1939, reward 528.0, memory_length 2000, epsilon 0.1435645316219759, time 723.0, rides 123\n",
      "Initial State is  [1, 13, 0]\n",
      "episode 1940, reward 708.0, memory_length 2000, epsilon 0.14342096709035393, time 734.0, rides 133\n",
      "Initial State is  [0, 10, 4]\n",
      "episode 1941, reward 844.0, memory_length 2000, epsilon 0.14327754612326357, time 734.0, rides 131\n",
      "Initial State is  [1, 13, 2]\n",
      "episode 1942, reward 607.0, memory_length 2000, epsilon 0.14313426857714032, time 731.0, rides 131\n",
      "Initial State is  [0, 9, 0]\n",
      "episode 1943, reward 693.0, memory_length 2000, epsilon 0.14299113430856317, time 735.0, rides 126\n",
      "Initial State is  [3, 7, 0]\n",
      "episode 1944, reward 685.0, memory_length 2000, epsilon 0.1428481431742546, time 729.0, rides 124\n",
      "Initial State is  [1, 9, 5]\n",
      "episode 1945, reward 622.0, memory_length 2000, epsilon 0.14270529503108034, time 726.0, rides 119\n",
      "Initial State is  [1, 8, 3]\n",
      "episode 1946, reward 632.0, memory_length 2000, epsilon 0.14256258973604927, time 729.0, rides 129\n",
      "Initial State is  [2, 15, 1]\n",
      "episode 1947, reward 786.0, memory_length 2000, epsilon 0.14242002714631322, time 730.0, rides 118\n",
      "Initial State is  [0, 11, 4]\n",
      "episode 1948, reward 557.0, memory_length 2000, epsilon 0.1422776071191669, time 731.0, rides 128\n",
      "Initial State is  [4, 21, 2]\n",
      "episode 1949, reward 754.0, memory_length 2000, epsilon 0.14213532951204774, time 727.0, rides 132\n",
      "Initial State is  [4, 7, 4]\n",
      "episode 1950, reward 959.0, memory_length 2000, epsilon 0.1419931941825357, time 728.0, rides 141\n",
      "Initial State is  [3, 0, 2]\n",
      "episode 1951, reward 706.0, memory_length 2000, epsilon 0.14185120098835316, time 731.0, rides 125\n",
      "Initial State is  [2, 16, 6]\n",
      "episode 1952, reward 706.0, memory_length 2000, epsilon 0.1417093497873648, time 732.0, rides 128\n",
      "Initial State is  [0, 16, 3]\n",
      "episode 1953, reward 799.0, memory_length 2000, epsilon 0.14156764043757744, time 726.0, rides 120\n",
      "Initial State is  [2, 13, 4]\n",
      "episode 1954, reward 802.0, memory_length 2000, epsilon 0.14142607279713987, time 731.0, rides 126\n",
      "Initial State is  [1, 8, 1]\n",
      "episode 1955, reward 1039.0, memory_length 2000, epsilon 0.14128464672434274, time 724.0, rides 124\n",
      "Initial State is  [3, 22, 2]\n",
      "episode 1956, reward 568.0, memory_length 2000, epsilon 0.1411433620776184, time 734.0, rides 120\n",
      "Initial State is  [2, 8, 2]\n",
      "episode 1957, reward 633.0, memory_length 2000, epsilon 0.14100221871554078, time 732.0, rides 115\n",
      "Initial State is  [0, 13, 1]\n",
      "episode 1958, reward 624.0, memory_length 2000, epsilon 0.14086121649682523, time 732.0, rides 140\n",
      "Initial State is  [4, 21, 2]\n",
      "episode 1959, reward 816.0, memory_length 2000, epsilon 0.14072035528032842, time 738.0, rides 138\n",
      "Initial State is  [3, 20, 4]\n",
      "episode 1960, reward 788.0, memory_length 2000, epsilon 0.1405796349250481, time 742.0, rides 146\n",
      "Initial State is  [4, 4, 4]\n",
      "episode 1961, reward 536.0, memory_length 2000, epsilon 0.14043905529012304, time 728.0, rides 117\n",
      "Initial State is  [0, 21, 3]\n",
      "episode 1962, reward 702.0, memory_length 2000, epsilon 0.14029861623483292, time 731.0, rides 125\n",
      "Initial State is  [0, 17, 3]\n",
      "episode 1963, reward 676.0, memory_length 2000, epsilon 0.1401583176185981, time 728.0, rides 123\n",
      "Initial State is  [0, 18, 5]\n",
      "episode 1964, reward 981.0, memory_length 2000, epsilon 0.1400181593009795, time 727.0, rides 123\n",
      "Initial State is  [3, 2, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 1965, reward 905.0, memory_length 2000, epsilon 0.1398781411416785, time 733.0, rides 129\n",
      "Initial State is  [3, 22, 3]\n",
      "episode 1966, reward 588.0, memory_length 2000, epsilon 0.13973826300053682, time 732.0, rides 123\n",
      "Initial State is  [2, 2, 4]\n",
      "episode 1967, reward 480.0, memory_length 2000, epsilon 0.13959852473753628, time 725.0, rides 116\n",
      "Initial State is  [0, 2, 3]\n",
      "episode 1968, reward 658.0, memory_length 2000, epsilon 0.13945892621279873, time 726.0, rides 117\n",
      "Initial State is  [2, 10, 4]\n",
      "episode 1969, reward 676.0, memory_length 2000, epsilon 0.13931946728658592, time 731.0, rides 127\n",
      "Initial State is  [1, 16, 6]\n",
      "episode 1970, reward 693.0, memory_length 2000, epsilon 0.13918014781929933, time 730.0, rides 124\n",
      "Initial State is  [2, 12, 5]\n",
      "episode 1971, reward 729.0, memory_length 2000, epsilon 0.13904096767148003, time 733.0, rides 123\n",
      "Initial State is  [0, 17, 4]\n",
      "episode 1972, reward 469.0, memory_length 2000, epsilon 0.13890192670380855, time 726.0, rides 120\n",
      "Initial State is  [3, 18, 4]\n",
      "episode 1973, reward 601.0, memory_length 2000, epsilon 0.13876302477710473, time 722.0, rides 124\n",
      "Initial State is  [3, 5, 2]\n",
      "episode 1974, reward 745.0, memory_length 2000, epsilon 0.13862426175232762, time 738.0, rides 124\n",
      "Initial State is  [2, 23, 4]\n",
      "episode 1975, reward 739.0, memory_length 2000, epsilon 0.1384856374905753, time 736.0, rides 126\n",
      "Initial State is  [4, 18, 2]\n",
      "episode 1976, reward 568.0, memory_length 2000, epsilon 0.13834715185308472, time 726.0, rides 123\n",
      "Initial State is  [4, 6, 6]\n",
      "episode 1977, reward 573.0, memory_length 2000, epsilon 0.13820880470123162, time 725.0, rides 121\n",
      "Initial State is  [2, 6, 2]\n",
      "episode 1978, reward 834.0, memory_length 2000, epsilon 0.1380705958965304, time 728.0, rides 124\n",
      "Initial State is  [1, 11, 5]\n",
      "episode 1979, reward 481.0, memory_length 2000, epsilon 0.13793252530063388, time 728.0, rides 111\n",
      "Initial State is  [2, 17, 2]\n",
      "episode 1980, reward 555.0, memory_length 2000, epsilon 0.13779459277533324, time 724.0, rides 114\n",
      "Initial State is  [0, 3, 2]\n",
      "episode 1981, reward 1040.0, memory_length 2000, epsilon 0.1376567981825579, time 730.0, rides 127\n",
      "Initial State is  [0, 8, 5]\n",
      "episode 1982, reward 799.0, memory_length 2000, epsilon 0.13751914138437535, time 729.0, rides 129\n",
      "Initial State is  [4, 16, 4]\n",
      "episode 1983, reward 867.0, memory_length 2000, epsilon 0.13738162224299097, time 727.0, rides 123\n",
      "Initial State is  [0, 21, 3]\n",
      "episode 1984, reward 772.0, memory_length 2000, epsilon 0.13724424062074797, time 736.0, rides 129\n",
      "Initial State is  [0, 18, 4]\n",
      "episode 1985, reward 539.0, memory_length 2000, epsilon 0.13710699638012722, time 732.0, rides 123\n",
      "Initial State is  [3, 15, 1]\n",
      "episode 1986, reward 1075.0, memory_length 2000, epsilon 0.1369698893837471, time 726.0, rides 124\n",
      "Initial State is  [4, 12, 1]\n",
      "episode 1987, reward 904.0, memory_length 2000, epsilon 0.13683291949436335, time 737.0, rides 129\n",
      "Initial State is  [2, 21, 5]\n",
      "episode 1988, reward 837.0, memory_length 2000, epsilon 0.13669608657486898, time 730.0, rides 120\n",
      "Initial State is  [3, 0, 2]\n",
      "episode 1989, reward 798.0, memory_length 2000, epsilon 0.1365593904882941, time 722.0, rides 120\n",
      "Initial State is  [2, 11, 1]\n",
      "episode 1990, reward 892.0, memory_length 2000, epsilon 0.1364228310978058, time 723.0, rides 132\n",
      "Initial State is  [4, 1, 3]\n",
      "episode 1991, reward 783.0, memory_length 2000, epsilon 0.13628640826670801, time 722.0, rides 120\n",
      "Initial State is  [2, 2, 4]\n",
      "episode 1992, reward 815.0, memory_length 2000, epsilon 0.1361501218584413, time 725.0, rides 126\n",
      "Initial State is  [2, 4, 5]\n",
      "episode 1993, reward 429.0, memory_length 2000, epsilon 0.13601397173658286, time 726.0, rides 146\n",
      "Initial State is  [4, 11, 0]\n",
      "episode 1994, reward 751.0, memory_length 2000, epsilon 0.1358779577648463, time 723.0, rides 130\n",
      "Initial State is  [2, 14, 3]\n",
      "episode 1995, reward 630.0, memory_length 2000, epsilon 0.13574207980708144, time 730.0, rides 121\n",
      "Initial State is  [3, 17, 2]\n",
      "episode 1996, reward 644.0, memory_length 2000, epsilon 0.13560633772727437, time 732.0, rides 141\n",
      "Initial State is  [2, 15, 3]\n",
      "episode 1997, reward 830.0, memory_length 2000, epsilon 0.1354707313895471, time 725.0, rides 123\n",
      "Initial State is  [1, 23, 5]\n",
      "episode 1998, reward 904.0, memory_length 2000, epsilon 0.13533526065815754, time 734.0, rides 132\n",
      "Initial State is  [1, 5, 2]\n",
      "episode 1999, reward 695.0, memory_length 2000, epsilon 0.1351999253974994, time 732.0, rides 114\n",
      "Initial State is  [3, 3, 2]\n",
      "episode 2000, reward 633.0, memory_length 2000, epsilon 0.13506472547210188, time 737.0, rides 122\n",
      "Initial State is  [4, 6, 1]\n",
      "episode 2001, reward 494.0, memory_length 2000, epsilon 0.1349296607466298, time 728.0, rides 137\n",
      "Initial State is  [2, 22, 6]\n",
      "episode 2002, reward 812.0, memory_length 2000, epsilon 0.13479473108588316, time 725.0, rides 133\n",
      "Initial State is  [4, 23, 6]\n",
      "episode 2003, reward 999.0, memory_length 2000, epsilon 0.13465993635479728, time 730.0, rides 134\n",
      "Initial State is  [1, 2, 1]\n",
      "episode 2004, reward 544.0, memory_length 2000, epsilon 0.13452527641844247, time 730.0, rides 119\n",
      "Initial State is  [3, 2, 3]\n",
      "episode 2005, reward 692.0, memory_length 2000, epsilon 0.13439075114202403, time 732.0, rides 128\n",
      "Initial State is  [3, 21, 6]\n",
      "episode 2006, reward 701.0, memory_length 2000, epsilon 0.134256360390882, time 726.0, rides 125\n",
      "Initial State is  [1, 18, 5]\n",
      "episode 2007, reward 603.0, memory_length 2000, epsilon 0.13412210403049113, time 734.0, rides 140\n",
      "Initial State is  [3, 5, 4]\n",
      "episode 2008, reward 668.0, memory_length 2000, epsilon 0.13398798192646064, time 728.0, rides 134\n",
      "Initial State is  [4, 7, 1]\n",
      "episode 2009, reward 670.0, memory_length 2000, epsilon 0.13385399394453418, time 728.0, rides 129\n",
      "Initial State is  [1, 14, 5]\n",
      "episode 2010, reward 376.0, memory_length 2000, epsilon 0.13372013995058965, time 726.0, rides 129\n",
      "Initial State is  [2, 2, 1]\n",
      "episode 2011, reward 785.0, memory_length 2000, epsilon 0.13358641981063907, time 724.0, rides 123\n",
      "Initial State is  [3, 20, 2]\n",
      "episode 2012, reward 634.0, memory_length 2000, epsilon 0.13345283339082842, time 729.0, rides 114\n",
      "Initial State is  [0, 15, 0]\n",
      "episode 2013, reward 708.0, memory_length 2000, epsilon 0.13331938055743758, time 735.0, rides 116\n",
      "Initial State is  [1, 10, 5]\n",
      "episode 2014, reward 531.0, memory_length 2000, epsilon 0.13318606117688014, time 729.0, rides 130\n",
      "Initial State is  [2, 15, 4]\n",
      "episode 2015, reward 951.0, memory_length 2000, epsilon 0.13305287511570327, time 739.0, rides 138\n",
      "Initial State is  [2, 19, 5]\n",
      "episode 2016, reward 922.0, memory_length 2000, epsilon 0.13291982224058757, time 722.0, rides 138\n",
      "Initial State is  [2, 21, 2]\n",
      "episode 2017, reward 606.0, memory_length 2000, epsilon 0.13278690241834698, time 725.0, rides 129\n",
      "Initial State is  [1, 4, 4]\n",
      "episode 2018, reward 836.0, memory_length 2000, epsilon 0.13265411551592862, time 724.0, rides 126\n",
      "Initial State is  [2, 16, 4]\n",
      "episode 2019, reward 938.0, memory_length 2000, epsilon 0.13252146140041268, time 739.0, rides 126\n",
      "Initial State is  [0, 16, 3]\n",
      "episode 2020, reward 717.0, memory_length 2000, epsilon 0.13238893993901227, time 730.0, rides 144\n",
      "Initial State is  [2, 17, 6]\n",
      "episode 2021, reward 690.0, memory_length 2000, epsilon 0.13225655099907327, time 742.0, rides 131\n",
      "Initial State is  [1, 22, 6]\n",
      "episode 2022, reward 664.0, memory_length 2000, epsilon 0.1321242944480742, time 735.0, rides 120\n",
      "Initial State is  [3, 13, 6]\n",
      "episode 2023, reward 581.0, memory_length 2000, epsilon 0.13199217015362613, time 730.0, rides 123\n",
      "Initial State is  [2, 12, 3]\n",
      "episode 2024, reward 750.0, memory_length 2000, epsilon 0.1318601779834725, time 729.0, rides 131\n",
      "Initial State is  [2, 3, 3]\n",
      "episode 2025, reward 376.0, memory_length 2000, epsilon 0.13172831780548902, time 731.0, rides 118\n",
      "Initial State is  [3, 21, 2]\n",
      "episode 2026, reward 777.0, memory_length 2000, epsilon 0.13159658948768352, time 729.0, rides 130\n",
      "Initial State is  [1, 21, 6]\n",
      "episode 2027, reward 617.0, memory_length 2000, epsilon 0.13146499289819583, time 727.0, rides 131\n",
      "Initial State is  [0, 16, 0]\n",
      "episode 2028, reward 672.0, memory_length 2000, epsilon 0.13133352790529762, time 739.0, rides 125\n",
      "Initial State is  [4, 6, 0]\n",
      "episode 2029, reward 1063.0, memory_length 2000, epsilon 0.13120219437739233, time 733.0, rides 141\n",
      "Initial State is  [1, 13, 2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 2030, reward 770.0, memory_length 2000, epsilon 0.13107099218301493, time 732.0, rides 140\n",
      "Initial State is  [2, 21, 6]\n",
      "episode 2031, reward 625.0, memory_length 2000, epsilon 0.13093992119083192, time 723.0, rides 112\n",
      "Initial State is  [3, 21, 1]\n",
      "episode 2032, reward 831.0, memory_length 2000, epsilon 0.13080898126964108, time 729.0, rides 124\n",
      "Initial State is  [1, 2, 4]\n",
      "episode 2033, reward 608.0, memory_length 2000, epsilon 0.13067817228837145, time 727.0, rides 121\n",
      "Initial State is  [2, 4, 1]\n",
      "episode 2034, reward 447.0, memory_length 2000, epsilon 0.13054749411608307, time 731.0, rides 119\n",
      "Initial State is  [4, 12, 1]\n",
      "episode 2035, reward 562.0, memory_length 2000, epsilon 0.13041694662196698, time 736.0, rides 127\n",
      "Initial State is  [0, 15, 2]\n",
      "episode 2036, reward 906.0, memory_length 2000, epsilon 0.13028652967534501, time 734.0, rides 125\n",
      "Initial State is  [0, 23, 1]\n",
      "episode 2037, reward 963.0, memory_length 2000, epsilon 0.13015624314566968, time 733.0, rides 146\n",
      "Initial State is  [1, 21, 0]\n",
      "episode 2038, reward 786.0, memory_length 2000, epsilon 0.130026086902524, time 723.0, rides 125\n",
      "Initial State is  [2, 14, 1]\n",
      "episode 2039, reward 776.0, memory_length 2000, epsilon 0.1298960608156215, time 729.0, rides 119\n",
      "Initial State is  [2, 8, 2]\n",
      "episode 2040, reward 655.0, memory_length 2000, epsilon 0.12976616475480587, time 726.0, rides 118\n",
      "Initial State is  [1, 4, 0]\n",
      "episode 2041, reward 1116.0, memory_length 2000, epsilon 0.12963639859005108, time 726.0, rides 127\n",
      "Initial State is  [4, 15, 5]\n",
      "episode 2042, reward 618.0, memory_length 2000, epsilon 0.12950676219146104, time 727.0, rides 112\n",
      "Initial State is  [2, 13, 3]\n",
      "episode 2043, reward 567.0, memory_length 2000, epsilon 0.1293772554292696, time 728.0, rides 134\n",
      "Initial State is  [3, 19, 1]\n",
      "episode 2044, reward 695.0, memory_length 2000, epsilon 0.1292478781738403, time 728.0, rides 127\n",
      "Initial State is  [4, 18, 5]\n",
      "episode 2045, reward 828.0, memory_length 2000, epsilon 0.12911863029566648, time 732.0, rides 133\n",
      "Initial State is  [3, 23, 4]\n",
      "episode 2046, reward 939.0, memory_length 2000, epsilon 0.12898951166537082, time 732.0, rides 124\n",
      "Initial State is  [3, 16, 5]\n",
      "episode 2047, reward 864.0, memory_length 2000, epsilon 0.12886052215370544, time 732.0, rides 123\n",
      "Initial State is  [3, 0, 2]\n",
      "episode 2048, reward 773.0, memory_length 2000, epsilon 0.12873166163155172, time 727.0, rides 115\n",
      "Initial State is  [2, 2, 5]\n",
      "episode 2049, reward 826.0, memory_length 2000, epsilon 0.12860292996992018, time 736.0, rides 126\n",
      "Initial State is  [4, 11, 2]\n",
      "episode 2050, reward 561.0, memory_length 2000, epsilon 0.12847432703995026, time 725.0, rides 122\n",
      "Initial State is  [3, 8, 6]\n",
      "episode 2051, reward 798.0, memory_length 2000, epsilon 0.12834585271291032, time 725.0, rides 122\n",
      "Initial State is  [1, 0, 5]\n",
      "episode 2052, reward 820.0, memory_length 2000, epsilon 0.12821750686019742, time 731.0, rides 140\n",
      "Initial State is  [4, 9, 6]\n",
      "episode 2053, reward 889.0, memory_length 2000, epsilon 0.12808928935333722, time 731.0, rides 142\n",
      "Initial State is  [4, 0, 2]\n",
      "episode 2054, reward 582.0, memory_length 2000, epsilon 0.12796120006398387, time 734.0, rides 126\n",
      "Initial State is  [2, 13, 5]\n",
      "episode 2055, reward 489.0, memory_length 2000, epsilon 0.1278332388639199, time 724.0, rides 124\n",
      "Initial State is  [1, 5, 5]\n",
      "episode 2056, reward 324.0, memory_length 2000, epsilon 0.12770540562505597, time 730.0, rides 130\n",
      "Initial State is  [4, 6, 4]\n",
      "episode 2057, reward 638.0, memory_length 2000, epsilon 0.1275777002194309, time 733.0, rides 142\n",
      "Initial State is  [0, 15, 4]\n",
      "episode 2058, reward 813.0, memory_length 2000, epsilon 0.12745012251921148, time 730.0, rides 136\n",
      "Initial State is  [0, 4, 3]\n",
      "episode 2059, reward 885.0, memory_length 2000, epsilon 0.12732267239669226, time 738.0, rides 125\n",
      "Initial State is  [0, 12, 1]\n",
      "episode 2060, reward 902.0, memory_length 2000, epsilon 0.12719534972429558, time 734.0, rides 112\n",
      "Initial State is  [4, 8, 0]\n",
      "episode 2061, reward 631.0, memory_length 2000, epsilon 0.12706815437457128, time 721.0, rides 120\n",
      "Initial State is  [0, 23, 1]\n",
      "episode 2062, reward 822.0, memory_length 2000, epsilon 0.1269410862201967, time 730.0, rides 129\n",
      "Initial State is  [4, 12, 6]\n",
      "episode 2063, reward 555.0, memory_length 2000, epsilon 0.1268141451339765, time 733.0, rides 136\n",
      "Initial State is  [1, 14, 1]\n",
      "episode 2064, reward 614.0, memory_length 2000, epsilon 0.12668733098884255, time 729.0, rides 118\n",
      "Initial State is  [2, 17, 4]\n",
      "episode 2065, reward 777.0, memory_length 2000, epsilon 0.12656064365785372, time 733.0, rides 129\n",
      "Initial State is  [1, 20, 5]\n",
      "episode 2066, reward 642.0, memory_length 2000, epsilon 0.12643408301419587, time 730.0, rides 125\n",
      "Initial State is  [2, 15, 0]\n",
      "episode 2067, reward 656.0, memory_length 2000, epsilon 0.12630764893118168, time 723.0, rides 134\n",
      "Initial State is  [3, 10, 5]\n",
      "episode 2068, reward 845.0, memory_length 2000, epsilon 0.1261813412822505, time 731.0, rides 123\n",
      "Initial State is  [1, 10, 4]\n",
      "episode 2069, reward 418.0, memory_length 2000, epsilon 0.12605515994096825, time 723.0, rides 119\n",
      "Initial State is  [1, 3, 5]\n",
      "episode 2070, reward 843.0, memory_length 2000, epsilon 0.12592910478102728, time 723.0, rides 135\n",
      "Initial State is  [3, 1, 3]\n",
      "episode 2071, reward 755.0, memory_length 2000, epsilon 0.12580317567624627, time 738.0, rides 135\n",
      "Initial State is  [0, 8, 0]\n",
      "episode 2072, reward 632.0, memory_length 2000, epsilon 0.12567737250057, time 724.0, rides 114\n",
      "Initial State is  [4, 0, 2]\n",
      "episode 2073, reward 785.0, memory_length 2000, epsilon 0.12555169512806943, time 725.0, rides 121\n",
      "Initial State is  [2, 19, 4]\n",
      "episode 2074, reward 826.0, memory_length 2000, epsilon 0.12542614343294137, time 738.0, rides 124\n",
      "Initial State is  [3, 18, 3]\n",
      "episode 2075, reward 693.0, memory_length 2000, epsilon 0.12530071728950842, time 726.0, rides 126\n",
      "Initial State is  [0, 9, 5]\n",
      "episode 2076, reward 744.0, memory_length 2000, epsilon 0.12517541657221892, time 736.0, rides 119\n",
      "Initial State is  [0, 10, 0]\n",
      "episode 2077, reward 807.0, memory_length 2000, epsilon 0.1250502411556467, time 731.0, rides 120\n",
      "Initial State is  [2, 22, 3]\n",
      "episode 2078, reward 767.0, memory_length 2000, epsilon 0.12492519091449104, time 726.0, rides 132\n",
      "Initial State is  [3, 2, 0]\n",
      "episode 2079, reward 603.0, memory_length 2000, epsilon 0.12480026572357655, time 729.0, rides 112\n",
      "Initial State is  [3, 21, 4]\n",
      "episode 2080, reward 728.0, memory_length 2000, epsilon 0.12467546545785298, time 724.0, rides 118\n",
      "Initial State is  [3, 22, 5]\n",
      "episode 2081, reward 840.0, memory_length 2000, epsilon 0.12455078999239512, time 725.0, rides 135\n",
      "Initial State is  [0, 15, 6]\n",
      "episode 2082, reward 984.0, memory_length 2000, epsilon 0.12442623920240273, time 728.0, rides 124\n",
      "Initial State is  [0, 17, 5]\n",
      "episode 2083, reward 615.0, memory_length 2000, epsilon 0.12430181296320032, time 727.0, rides 136\n",
      "Initial State is  [0, 9, 5]\n",
      "episode 2084, reward 753.0, memory_length 2000, epsilon 0.12417751115023712, time 732.0, rides 120\n",
      "Initial State is  [2, 1, 4]\n",
      "episode 2085, reward 873.0, memory_length 2000, epsilon 0.12405333363908688, time 742.0, rides 123\n",
      "Initial State is  [2, 7, 0]\n",
      "episode 2086, reward 1008.0, memory_length 2000, epsilon 0.1239292803054478, time 734.0, rides 131\n",
      "Initial State is  [3, 8, 3]\n",
      "episode 2087, reward 620.0, memory_length 2000, epsilon 0.12380535102514234, time 734.0, rides 118\n",
      "Initial State is  [3, 16, 6]\n",
      "episode 2088, reward 624.0, memory_length 2000, epsilon 0.1236815456741172, time 723.0, rides 121\n",
      "Initial State is  [4, 13, 2]\n",
      "episode 2089, reward 790.0, memory_length 2000, epsilon 0.12355786412844308, time 727.0, rides 117\n",
      "Initial State is  [2, 23, 1]\n",
      "episode 2090, reward 646.0, memory_length 2000, epsilon 0.12343430626431463, time 724.0, rides 124\n",
      "Initial State is  [4, 11, 3]\n",
      "episode 2091, reward 489.0, memory_length 2000, epsilon 0.12331087195805032, time 734.0, rides 118\n",
      "Initial State is  [2, 13, 3]\n",
      "episode 2092, reward 826.0, memory_length 2000, epsilon 0.12318756108609227, time 724.0, rides 128\n",
      "Initial State is  [1, 14, 0]\n",
      "episode 2093, reward 656.0, memory_length 2000, epsilon 0.12306437352500618, time 723.0, rides 117\n",
      "Initial State is  [2, 10, 2]\n",
      "episode 2094, reward 831.0, memory_length 2000, epsilon 0.12294130915148117, time 729.0, rides 126\n",
      "Initial State is  [1, 12, 1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 2095, reward 636.0, memory_length 2000, epsilon 0.12281836784232969, time 731.0, rides 127\n",
      "Initial State is  [4, 3, 4]\n",
      "episode 2096, reward 664.0, memory_length 2000, epsilon 0.12269554947448735, time 727.0, rides 118\n",
      "Initial State is  [4, 1, 0]\n",
      "episode 2097, reward 392.0, memory_length 2000, epsilon 0.12257285392501287, time 725.0, rides 118\n",
      "Initial State is  [3, 19, 1]\n",
      "episode 2098, reward 1014.0, memory_length 2000, epsilon 0.12245028107108785, time 722.0, rides 120\n",
      "Initial State is  [2, 20, 5]\n",
      "episode 2099, reward 620.0, memory_length 2000, epsilon 0.12232783079001676, time 728.0, rides 127\n",
      "Initial State is  [2, 10, 3]\n",
      "episode 2100, reward 549.0, memory_length 2000, epsilon 0.12220550295922675, time 732.0, rides 129\n",
      "Initial State is  [0, 6, 3]\n",
      "episode 2101, reward 893.0, memory_length 2000, epsilon 0.12208329745626752, time 728.0, rides 122\n",
      "Initial State is  [4, 0, 5]\n",
      "episode 2102, reward 1040.0, memory_length 2000, epsilon 0.12196121415881125, time 721.0, rides 122\n",
      "Initial State is  [4, 18, 1]\n",
      "episode 2103, reward 787.0, memory_length 2000, epsilon 0.12183925294465245, time 728.0, rides 137\n",
      "Initial State is  [3, 8, 6]\n",
      "episode 2104, reward 768.0, memory_length 2000, epsilon 0.1217174136917078, time 737.0, rides 119\n",
      "Initial State is  [4, 15, 5]\n",
      "episode 2105, reward 990.0, memory_length 2000, epsilon 0.12159569627801609, time 724.0, rides 123\n",
      "Initial State is  [3, 21, 4]\n",
      "episode 2106, reward 820.0, memory_length 2000, epsilon 0.12147410058173808, time 734.0, rides 125\n",
      "Initial State is  [4, 7, 0]\n",
      "episode 2107, reward 709.0, memory_length 2000, epsilon 0.12135262648115634, time 734.0, rides 123\n",
      "Initial State is  [0, 18, 0]\n",
      "episode 2108, reward 584.0, memory_length 2000, epsilon 0.12123127385467519, time 724.0, rides 132\n",
      "Initial State is  [0, 1, 6]\n",
      "episode 2109, reward 791.0, memory_length 2000, epsilon 0.12111004258082052, time 725.0, rides 130\n",
      "Initial State is  [3, 1, 1]\n",
      "episode 2110, reward 663.0, memory_length 2000, epsilon 0.1209889325382397, time 726.0, rides 133\n",
      "Initial State is  [1, 11, 2]\n",
      "episode 2111, reward 595.0, memory_length 2000, epsilon 0.12086794360570145, time 727.0, rides 127\n",
      "Initial State is  [1, 19, 4]\n",
      "episode 2112, reward 769.0, memory_length 2000, epsilon 0.12074707566209575, time 731.0, rides 128\n",
      "Initial State is  [1, 21, 4]\n",
      "episode 2113, reward 514.0, memory_length 2000, epsilon 0.12062632858643366, time 733.0, rides 125\n",
      "Initial State is  [1, 14, 5]\n",
      "episode 2114, reward 715.0, memory_length 2000, epsilon 0.12050570225784722, time 741.0, rides 127\n",
      "Initial State is  [0, 12, 1]\n",
      "episode 2115, reward 764.0, memory_length 2000, epsilon 0.12038519655558938, time 725.0, rides 129\n",
      "Initial State is  [3, 4, 0]\n",
      "episode 2116, reward 775.0, memory_length 2000, epsilon 0.12026481135903379, time 722.0, rides 127\n",
      "Initial State is  [4, 1, 5]\n",
      "episode 2117, reward 793.0, memory_length 2000, epsilon 0.12014454654767476, time 726.0, rides 128\n",
      "Initial State is  [1, 5, 4]\n",
      "episode 2118, reward 656.0, memory_length 2000, epsilon 0.12002440200112709, time 728.0, rides 128\n",
      "Initial State is  [0, 14, 2]\n",
      "episode 2119, reward 829.0, memory_length 2000, epsilon 0.11990437759912596, time 735.0, rides 144\n",
      "Initial State is  [1, 4, 2]\n",
      "episode 2120, reward 608.0, memory_length 2000, epsilon 0.11978447322152684, time 723.0, rides 130\n",
      "Initial State is  [1, 9, 2]\n",
      "episode 2121, reward 851.0, memory_length 2000, epsilon 0.11966468874830531, time 730.0, rides 129\n",
      "Initial State is  [1, 8, 6]\n",
      "episode 2122, reward 779.0, memory_length 2000, epsilon 0.11954502405955701, time 726.0, rides 123\n",
      "Initial State is  [1, 23, 5]\n",
      "episode 2123, reward 611.0, memory_length 2000, epsilon 0.11942547903549745, time 730.0, rides 131\n",
      "Initial State is  [0, 7, 4]\n",
      "episode 2124, reward 952.0, memory_length 2000, epsilon 0.11930605355646196, time 728.0, rides 117\n",
      "Initial State is  [4, 18, 1]\n",
      "episode 2125, reward 664.0, memory_length 2000, epsilon 0.11918674750290549, time 724.0, rides 116\n",
      "Initial State is  [3, 10, 1]\n",
      "episode 2126, reward 558.0, memory_length 2000, epsilon 0.11906756075540259, time 727.0, rides 130\n",
      "Initial State is  [1, 3, 6]\n",
      "episode 2127, reward 579.0, memory_length 2000, epsilon 0.11894849319464719, time 725.0, rides 117\n",
      "Initial State is  [3, 18, 6]\n",
      "episode 2128, reward 1086.0, memory_length 2000, epsilon 0.11882954470145254, time 733.0, rides 120\n",
      "Initial State is  [2, 4, 3]\n",
      "episode 2129, reward 1006.0, memory_length 2000, epsilon 0.11871071515675109, time 740.0, rides 113\n",
      "Initial State is  [0, 14, 1]\n",
      "episode 2130, reward 592.0, memory_length 2000, epsilon 0.11859200444159435, time 727.0, rides 125\n",
      "Initial State is  [4, 18, 1]\n",
      "episode 2131, reward 828.0, memory_length 2000, epsilon 0.11847341243715274, time 733.0, rides 129\n",
      "Initial State is  [0, 20, 2]\n",
      "episode 2132, reward 608.0, memory_length 2000, epsilon 0.1183549390247156, time 726.0, rides 127\n",
      "Initial State is  [2, 8, 0]\n",
      "episode 2133, reward 440.0, memory_length 2000, epsilon 0.11823658408569088, time 735.0, rides 122\n",
      "Initial State is  [2, 6, 5]\n",
      "episode 2134, reward 921.0, memory_length 2000, epsilon 0.11811834750160519, time 727.0, rides 133\n",
      "Initial State is  [3, 5, 3]\n",
      "episode 2135, reward 1050.0, memory_length 2000, epsilon 0.11800022915410358, time 728.0, rides 131\n",
      "Initial State is  [3, 2, 1]\n",
      "episode 2136, reward 691.0, memory_length 2000, epsilon 0.11788222892494947, time 735.0, rides 119\n",
      "Initial State is  [2, 7, 1]\n",
      "episode 2137, reward 558.0, memory_length 2000, epsilon 0.11776434669602452, time 725.0, rides 129\n",
      "Initial State is  [1, 22, 2]\n",
      "episode 2138, reward 700.0, memory_length 2000, epsilon 0.1176465823493285, time 728.0, rides 120\n",
      "Initial State is  [0, 3, 6]\n",
      "episode 2139, reward 752.0, memory_length 2000, epsilon 0.11752893576697916, time 725.0, rides 121\n",
      "Initial State is  [3, 22, 2]\n",
      "episode 2140, reward 688.0, memory_length 2000, epsilon 0.11741140683121218, time 723.0, rides 109\n",
      "Initial State is  [2, 15, 1]\n",
      "episode 2141, reward 624.0, memory_length 2000, epsilon 0.11729399542438097, time 722.0, rides 114\n",
      "Initial State is  [2, 16, 0]\n",
      "episode 2142, reward 580.0, memory_length 2000, epsilon 0.11717670142895659, time 725.0, rides 120\n",
      "Initial State is  [4, 14, 3]\n",
      "episode 2143, reward 502.0, memory_length 2000, epsilon 0.11705952472752763, time 729.0, rides 130\n",
      "Initial State is  [4, 16, 1]\n",
      "episode 2144, reward 783.0, memory_length 2000, epsilon 0.1169424652028001, time 723.0, rides 134\n",
      "Initial State is  [4, 9, 6]\n",
      "episode 2145, reward 1201.0, memory_length 2000, epsilon 0.1168255227375973, time 722.0, rides 135\n",
      "Initial State is  [4, 20, 2]\n",
      "episode 2146, reward 731.0, memory_length 2000, epsilon 0.1167086972148597, time 726.0, rides 132\n",
      "Initial State is  [2, 23, 3]\n",
      "episode 2147, reward 865.0, memory_length 2000, epsilon 0.11659198851764484, time 728.0, rides 115\n",
      "Initial State is  [0, 9, 2]\n",
      "episode 2148, reward 849.0, memory_length 2000, epsilon 0.1164753965291272, time 726.0, rides 138\n",
      "Initial State is  [0, 4, 0]\n",
      "episode 2149, reward 674.0, memory_length 2000, epsilon 0.11635892113259806, time 732.0, rides 134\n",
      "Initial State is  [4, 1, 5]\n",
      "episode 2150, reward 526.0, memory_length 2000, epsilon 0.11624256221146546, time 734.0, rides 126\n",
      "Initial State is  [1, 23, 6]\n",
      "episode 2151, reward 964.0, memory_length 2000, epsilon 0.116126319649254, time 727.0, rides 119\n",
      "Initial State is  [3, 1, 4]\n",
      "episode 2152, reward 699.0, memory_length 2000, epsilon 0.11601019332960474, time 727.0, rides 129\n",
      "Initial State is  [2, 2, 0]\n",
      "episode 2153, reward 760.0, memory_length 2000, epsilon 0.11589418313627514, time 732.0, rides 117\n",
      "Initial State is  [0, 5, 0]\n",
      "episode 2154, reward 649.0, memory_length 2000, epsilon 0.11577828895313887, time 735.0, rides 120\n",
      "Initial State is  [1, 17, 6]\n",
      "episode 2155, reward 1104.0, memory_length 2000, epsilon 0.11566251066418573, time 735.0, rides 121\n",
      "Initial State is  [4, 12, 2]\n",
      "episode 2156, reward 850.0, memory_length 2000, epsilon 0.11554684815352155, time 739.0, rides 124\n",
      "Initial State is  [0, 17, 6]\n",
      "episode 2157, reward 588.0, memory_length 2000, epsilon 0.11543130130536802, time 727.0, rides 111\n",
      "Initial State is  [2, 1, 0]\n",
      "episode 2158, reward 536.0, memory_length 2000, epsilon 0.11531587000406265, time 727.0, rides 121\n",
      "Initial State is  [1, 22, 4]\n",
      "episode 2159, reward 728.0, memory_length 2000, epsilon 0.1152005541340586, time 728.0, rides 131\n",
      "Initial State is  [4, 22, 2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 2160, reward 955.0, memory_length 2000, epsilon 0.11508535357992454, time 730.0, rides 121\n",
      "Initial State is  [1, 9, 4]\n",
      "episode 2161, reward 502.0, memory_length 2000, epsilon 0.11497026822634461, time 728.0, rides 118\n",
      "Initial State is  [2, 17, 4]\n",
      "episode 2162, reward 709.0, memory_length 2000, epsilon 0.11485529795811826, time 733.0, rides 119\n",
      "Initial State is  [2, 20, 6]\n",
      "episode 2163, reward 790.0, memory_length 2000, epsilon 0.11474044266016015, time 728.0, rides 131\n",
      "Initial State is  [1, 11, 3]\n",
      "episode 2164, reward 881.0, memory_length 2000, epsilon 0.11462570221749999, time 736.0, rides 138\n",
      "Initial State is  [1, 12, 1]\n",
      "episode 2165, reward 886.0, memory_length 2000, epsilon 0.11451107651528249, time 743.0, rides 115\n",
      "Initial State is  [3, 1, 3]\n",
      "episode 2166, reward 668.0, memory_length 2000, epsilon 0.11439656543876721, time 729.0, rides 129\n",
      "Initial State is  [1, 23, 1]\n",
      "episode 2167, reward 1065.0, memory_length 2000, epsilon 0.11428216887332844, time 735.0, rides 128\n",
      "Initial State is  [0, 12, 4]\n",
      "episode 2168, reward 496.0, memory_length 2000, epsilon 0.11416788670445512, time 737.0, rides 119\n",
      "Initial State is  [4, 23, 2]\n",
      "episode 2169, reward 688.0, memory_length 2000, epsilon 0.11405371881775066, time 722.0, rides 124\n",
      "Initial State is  [0, 20, 1]\n",
      "episode 2170, reward 731.0, memory_length 2000, epsilon 0.11393966509893291, time 732.0, rides 124\n",
      "Initial State is  [3, 21, 6]\n",
      "episode 2171, reward 535.0, memory_length 2000, epsilon 0.11382572543383399, time 725.0, rides 114\n",
      "Initial State is  [3, 8, 4]\n",
      "episode 2172, reward 833.0, memory_length 2000, epsilon 0.11371189970840015, time 728.0, rides 132\n",
      "Initial State is  [1, 11, 4]\n",
      "episode 2173, reward 647.0, memory_length 2000, epsilon 0.11359818780869176, time 728.0, rides 137\n",
      "Initial State is  [1, 0, 0]\n",
      "episode 2174, reward 729.0, memory_length 2000, epsilon 0.11348458962088306, time 729.0, rides 125\n",
      "Initial State is  [0, 15, 2]\n",
      "episode 2175, reward 686.0, memory_length 2000, epsilon 0.11337110503126219, time 732.0, rides 132\n",
      "Initial State is  [1, 8, 0]\n",
      "episode 2176, reward 665.0, memory_length 2000, epsilon 0.11325773392623092, time 729.0, rides 124\n",
      "Initial State is  [0, 4, 4]\n",
      "episode 2177, reward 859.0, memory_length 2000, epsilon 0.1131444761923047, time 728.0, rides 118\n",
      "Initial State is  [1, 2, 1]\n",
      "episode 2178, reward 452.0, memory_length 2000, epsilon 0.11303133171611239, time 727.0, rides 128\n",
      "Initial State is  [0, 3, 2]\n",
      "episode 2179, reward 870.0, memory_length 2000, epsilon 0.11291830038439628, time 731.0, rides 125\n",
      "Initial State is  [2, 4, 4]\n",
      "episode 2180, reward 784.0, memory_length 2000, epsilon 0.11280538208401188, time 722.0, rides 126\n",
      "Initial State is  [2, 3, 2]\n",
      "episode 2181, reward 575.0, memory_length 2000, epsilon 0.11269257670192787, time 729.0, rides 112\n",
      "Initial State is  [0, 1, 6]\n",
      "episode 2182, reward 730.0, memory_length 2000, epsilon 0.11257988412522593, time 730.0, rides 129\n",
      "Initial State is  [2, 10, 3]\n",
      "episode 2183, reward 301.0, memory_length 2000, epsilon 0.1124673042411007, time 732.0, rides 122\n",
      "Initial State is  [0, 13, 5]\n",
      "episode 2184, reward 833.0, memory_length 2000, epsilon 0.1123548369368596, time 728.0, rides 119\n",
      "Initial State is  [0, 9, 1]\n",
      "episode 2185, reward 636.0, memory_length 2000, epsilon 0.11224248209992274, time 725.0, rides 126\n",
      "Initial State is  [3, 19, 0]\n",
      "episode 2186, reward 819.0, memory_length 2000, epsilon 0.11213023961782281, time 727.0, rides 119\n",
      "Initial State is  [3, 22, 1]\n",
      "episode 2187, reward 612.0, memory_length 2000, epsilon 0.11201810937820499, time 730.0, rides 134\n",
      "Initial State is  [4, 1, 0]\n",
      "episode 2188, reward 773.0, memory_length 2000, epsilon 0.11190609126882678, time 730.0, rides 118\n",
      "Initial State is  [3, 11, 6]\n",
      "episode 2189, reward 1042.0, memory_length 2000, epsilon 0.11179418517755796, time 732.0, rides 123\n",
      "Initial State is  [1, 13, 4]\n",
      "episode 2190, reward 810.0, memory_length 2000, epsilon 0.1116823909923804, time 729.0, rides 129\n",
      "Initial State is  [1, 5, 0]\n",
      "episode 2191, reward 751.0, memory_length 2000, epsilon 0.11157070860138803, time 727.0, rides 142\n",
      "Initial State is  [2, 9, 4]\n",
      "episode 2192, reward 670.0, memory_length 2000, epsilon 0.11145913789278664, time 740.0, rides 130\n",
      "Initial State is  [1, 9, 2]\n",
      "episode 2193, reward 834.0, memory_length 2000, epsilon 0.11134767875489385, time 728.0, rides 134\n",
      "Initial State is  [4, 2, 1]\n",
      "episode 2194, reward 649.0, memory_length 2000, epsilon 0.11123633107613895, time 723.0, rides 133\n",
      "Initial State is  [0, 12, 2]\n",
      "episode 2195, reward 647.0, memory_length 2000, epsilon 0.11112509474506281, time 727.0, rides 128\n",
      "Initial State is  [1, 14, 3]\n",
      "episode 2196, reward 765.0, memory_length 2000, epsilon 0.11101396965031775, time 728.0, rides 128\n",
      "Initial State is  [1, 0, 4]\n",
      "episode 2197, reward 683.0, memory_length 2000, epsilon 0.11090295568066744, time 727.0, rides 125\n",
      "Initial State is  [3, 1, 0]\n",
      "episode 2198, reward 456.0, memory_length 2000, epsilon 0.11079205272498677, time 729.0, rides 112\n",
      "Initial State is  [2, 7, 3]\n",
      "episode 2199, reward 1078.0, memory_length 2000, epsilon 0.11068126067226178, time 722.0, rides 123\n",
      "Initial State is  [2, 9, 6]\n",
      "episode 2200, reward 1264.0, memory_length 2000, epsilon 0.11057057941158951, time 731.0, rides 133\n",
      "Initial State is  [1, 17, 2]\n",
      "episode 2201, reward 719.0, memory_length 2000, epsilon 0.11046000883217792, time 726.0, rides 130\n",
      "Initial State is  [0, 19, 5]\n",
      "episode 2202, reward 854.0, memory_length 2000, epsilon 0.11034954882334574, time 727.0, rides 126\n",
      "Initial State is  [4, 5, 4]\n",
      "episode 2203, reward 977.0, memory_length 2000, epsilon 0.1102391992745224, time 730.0, rides 119\n",
      "Initial State is  [0, 6, 6]\n",
      "episode 2204, reward 878.0, memory_length 2000, epsilon 0.11012896007524788, time 729.0, rides 136\n",
      "Initial State is  [3, 8, 6]\n",
      "episode 2205, reward 366.0, memory_length 2000, epsilon 0.11001883111517263, time 727.0, rides 121\n",
      "Initial State is  [4, 2, 1]\n",
      "episode 2206, reward 643.0, memory_length 2000, epsilon 0.10990881228405747, time 734.0, rides 137\n",
      "Initial State is  [0, 3, 6]\n",
      "episode 2207, reward 642.0, memory_length 2000, epsilon 0.10979890347177342, time 735.0, rides 121\n",
      "Initial State is  [0, 5, 4]\n",
      "episode 2208, reward 949.0, memory_length 2000, epsilon 0.10968910456830164, time 734.0, rides 128\n",
      "Initial State is  [1, 18, 5]\n",
      "episode 2209, reward 883.0, memory_length 2000, epsilon 0.10957941546373333, time 729.0, rides 130\n",
      "Initial State is  [3, 15, 5]\n",
      "episode 2210, reward 667.0, memory_length 2000, epsilon 0.1094698360482696, time 732.0, rides 126\n",
      "Initial State is  [0, 2, 1]\n",
      "episode 2211, reward 959.0, memory_length 2000, epsilon 0.10936036621222132, time 733.0, rides 123\n",
      "Initial State is  [4, 8, 0]\n",
      "episode 2212, reward 852.0, memory_length 2000, epsilon 0.10925100584600911, time 731.0, rides 125\n",
      "Initial State is  [1, 9, 1]\n",
      "episode 2213, reward 797.0, memory_length 2000, epsilon 0.1091417548401631, time 731.0, rides 117\n",
      "Initial State is  [2, 12, 1]\n",
      "episode 2214, reward 526.0, memory_length 2000, epsilon 0.10903261308532293, time 732.0, rides 116\n",
      "Initial State is  [1, 12, 3]\n",
      "episode 2215, reward 821.0, memory_length 2000, epsilon 0.1089235804722376, time 733.0, rides 112\n",
      "Initial State is  [1, 15, 0]\n",
      "episode 2216, reward 530.0, memory_length 2000, epsilon 0.10881465689176537, time 734.0, rides 119\n",
      "Initial State is  [3, 2, 1]\n",
      "episode 2217, reward 634.0, memory_length 2000, epsilon 0.1087058422348736, time 736.0, rides 120\n",
      "Initial State is  [2, 22, 6]\n",
      "episode 2218, reward 651.0, memory_length 2000, epsilon 0.10859713639263872, time 733.0, rides 128\n",
      "Initial State is  [4, 22, 0]\n",
      "episode 2219, reward 415.0, memory_length 2000, epsilon 0.10848853925624609, time 724.0, rides 118\n",
      "Initial State is  [1, 1, 2]\n",
      "episode 2220, reward 856.0, memory_length 2000, epsilon 0.10838005071698985, time 725.0, rides 129\n",
      "Initial State is  [4, 14, 5]\n",
      "episode 2221, reward 696.0, memory_length 2000, epsilon 0.10827167066627286, time 731.0, rides 124\n",
      "Initial State is  [1, 8, 0]\n",
      "episode 2222, reward 493.0, memory_length 2000, epsilon 0.10816339899560658, time 731.0, rides 120\n",
      "Initial State is  [2, 23, 0]\n",
      "episode 2223, reward 730.0, memory_length 2000, epsilon 0.10805523559661097, time 724.0, rides 121\n",
      "Initial State is  [3, 7, 2]\n",
      "episode 2224, reward 855.0, memory_length 2000, epsilon 0.10794718036101436, time 727.0, rides 129\n",
      "Initial State is  [2, 22, 3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 2225, reward 763.0, memory_length 2000, epsilon 0.10783923318065335, time 733.0, rides 120\n",
      "Initial State is  [4, 6, 4]\n",
      "episode 2226, reward 525.0, memory_length 2000, epsilon 0.10773139394747269, time 726.0, rides 136\n",
      "Initial State is  [3, 18, 4]\n",
      "episode 2227, reward 725.0, memory_length 2000, epsilon 0.10762366255352522, time 733.0, rides 122\n",
      "Initial State is  [4, 20, 4]\n",
      "episode 2228, reward 832.0, memory_length 2000, epsilon 0.1075160388909717, time 728.0, rides 124\n",
      "Initial State is  [4, 17, 3]\n",
      "episode 2229, reward 535.0, memory_length 2000, epsilon 0.10740852285208072, time 736.0, rides 118\n",
      "Initial State is  [0, 15, 6]\n",
      "episode 2230, reward 487.0, memory_length 2000, epsilon 0.10730111432922863, time 728.0, rides 124\n",
      "Initial State is  [3, 3, 5]\n",
      "episode 2231, reward 604.0, memory_length 2000, epsilon 0.1071938132148994, time 732.0, rides 121\n",
      "Initial State is  [2, 3, 5]\n",
      "episode 2232, reward 689.0, memory_length 2000, epsilon 0.1070866194016845, time 728.0, rides 129\n",
      "Initial State is  [4, 1, 4]\n",
      "episode 2233, reward 526.0, memory_length 2000, epsilon 0.10697953278228281, time 729.0, rides 117\n",
      "Initial State is  [0, 5, 0]\n",
      "episode 2234, reward 619.0, memory_length 2000, epsilon 0.10687255324950053, time 733.0, rides 106\n",
      "Initial State is  [3, 9, 4]\n",
      "episode 2235, reward 454.0, memory_length 2000, epsilon 0.10676568069625103, time 731.0, rides 137\n",
      "Initial State is  [0, 3, 3]\n",
      "episode 2236, reward 777.0, memory_length 2000, epsilon 0.10665891501555477, time 726.0, rides 131\n",
      "Initial State is  [0, 4, 1]\n",
      "episode 2237, reward 981.0, memory_length 2000, epsilon 0.10655225610053921, time 729.0, rides 120\n",
      "Initial State is  [3, 17, 0]\n",
      "episode 2238, reward 735.0, memory_length 2000, epsilon 0.10644570384443867, time 728.0, rides 123\n",
      "Initial State is  [1, 18, 6]\n",
      "episode 2239, reward 783.0, memory_length 2000, epsilon 0.10633925814059424, time 729.0, rides 132\n",
      "Initial State is  [2, 14, 2]\n",
      "episode 2240, reward 745.0, memory_length 2000, epsilon 0.10623291888245365, time 731.0, rides 128\n",
      "Initial State is  [2, 6, 6]\n",
      "episode 2241, reward 805.0, memory_length 2000, epsilon 0.1061266859635712, time 725.0, rides 130\n",
      "Initial State is  [1, 18, 3]\n",
      "episode 2242, reward 466.0, memory_length 2000, epsilon 0.10602055927760762, time 724.0, rides 132\n",
      "Initial State is  [3, 1, 4]\n",
      "episode 2243, reward 871.0, memory_length 2000, epsilon 0.10591453871833001, time 729.0, rides 122\n",
      "Initial State is  [0, 6, 5]\n",
      "episode 2244, reward 1055.0, memory_length 2000, epsilon 0.10580862417961168, time 725.0, rides 130\n",
      "Initial State is  [0, 17, 6]\n",
      "episode 2245, reward 686.0, memory_length 2000, epsilon 0.10570281555543207, time 728.0, rides 124\n",
      "Initial State is  [0, 17, 6]\n",
      "episode 2246, reward 548.0, memory_length 2000, epsilon 0.10559711273987664, time 733.0, rides 131\n",
      "Initial State is  [2, 0, 6]\n",
      "episode 2247, reward 876.0, memory_length 2000, epsilon 0.10549151562713677, time 727.0, rides 123\n",
      "Initial State is  [3, 18, 2]\n",
      "episode 2248, reward 594.0, memory_length 2000, epsilon 0.10538602411150963, time 732.0, rides 135\n",
      "Initial State is  [4, 23, 0]\n",
      "episode 2249, reward 745.0, memory_length 2000, epsilon 0.10528063808739813, time 723.0, rides 128\n",
      "Initial State is  [3, 23, 3]\n",
      "episode 2250, reward 1001.0, memory_length 2000, epsilon 0.10517535744931072, time 728.0, rides 129\n",
      "Initial State is  [2, 22, 4]\n",
      "episode 2251, reward 791.0, memory_length 2000, epsilon 0.10507018209186142, time 726.0, rides 122\n",
      "Initial State is  [1, 9, 4]\n",
      "episode 2252, reward 820.0, memory_length 2000, epsilon 0.10496511190976955, time 729.0, rides 131\n",
      "Initial State is  [1, 0, 4]\n",
      "episode 2253, reward 746.0, memory_length 2000, epsilon 0.10486014679785978, time 725.0, rides 133\n",
      "Initial State is  [4, 5, 5]\n",
      "episode 2254, reward 760.0, memory_length 2000, epsilon 0.10475528665106192, time 728.0, rides 126\n",
      "Initial State is  [0, 9, 5]\n",
      "episode 2255, reward 863.0, memory_length 2000, epsilon 0.10465053136441085, time 730.0, rides 130\n",
      "Initial State is  [0, 16, 0]\n",
      "episode 2256, reward 490.0, memory_length 2000, epsilon 0.10454588083304645, time 727.0, rides 128\n",
      "Initial State is  [2, 5, 2]\n",
      "episode 2257, reward 857.0, memory_length 2000, epsilon 0.1044413349522134, time 729.0, rides 127\n",
      "Initial State is  [2, 0, 2]\n",
      "episode 2258, reward 533.0, memory_length 2000, epsilon 0.10433689361726119, time 731.0, rides 132\n",
      "Initial State is  [1, 16, 3]\n",
      "episode 2259, reward 439.0, memory_length 2000, epsilon 0.10423255672364393, time 730.0, rides 136\n",
      "Initial State is  [3, 22, 2]\n",
      "episode 2260, reward 832.0, memory_length 2000, epsilon 0.10412832416692028, time 724.0, rides 136\n",
      "Initial State is  [4, 3, 0]\n",
      "episode 2261, reward 928.0, memory_length 2000, epsilon 0.10402419584275335, time 732.0, rides 133\n",
      "Initial State is  [0, 8, 6]\n",
      "episode 2262, reward 605.0, memory_length 2000, epsilon 0.1039201716469106, time 735.0, rides 122\n",
      "Initial State is  [1, 20, 3]\n",
      "episode 2263, reward 394.0, memory_length 2000, epsilon 0.10381625147526369, time 723.0, rides 132\n",
      "Initial State is  [2, 17, 4]\n",
      "episode 2264, reward 730.0, memory_length 2000, epsilon 0.10371243522378842, time 730.0, rides 132\n",
      "Initial State is  [3, 18, 0]\n",
      "episode 2265, reward 708.0, memory_length 2000, epsilon 0.10360872278856463, time 728.0, rides 128\n",
      "Initial State is  [1, 12, 0]\n",
      "episode 2266, reward 762.0, memory_length 2000, epsilon 0.10350511406577606, time 741.0, rides 131\n",
      "Initial State is  [1, 10, 0]\n",
      "episode 2267, reward 716.0, memory_length 2000, epsilon 0.10340160895171029, time 729.0, rides 122\n",
      "Initial State is  [2, 9, 5]\n",
      "episode 2268, reward 631.0, memory_length 2000, epsilon 0.10329820734275857, time 725.0, rides 126\n",
      "Initial State is  [0, 21, 2]\n",
      "episode 2269, reward 760.0, memory_length 2000, epsilon 0.10319490913541582, time 728.0, rides 123\n",
      "Initial State is  [0, 22, 4]\n",
      "episode 2270, reward 861.0, memory_length 2000, epsilon 0.1030917142262804, time 731.0, rides 125\n",
      "Initial State is  [0, 11, 6]\n",
      "episode 2271, reward 822.0, memory_length 2000, epsilon 0.10298862251205412, time 732.0, rides 120\n",
      "Initial State is  [1, 4, 0]\n",
      "episode 2272, reward 480.0, memory_length 2000, epsilon 0.10288563388954207, time 720.0, rides 144\n",
      "Initial State is  [1, 20, 1]\n",
      "episode 2273, reward 837.0, memory_length 2000, epsilon 0.10278274825565252, time 729.0, rides 139\n",
      "Initial State is  [2, 14, 3]\n",
      "episode 2274, reward 652.0, memory_length 2000, epsilon 0.10267996550739687, time 725.0, rides 130\n",
      "Initial State is  [3, 0, 4]\n",
      "episode 2275, reward 619.0, memory_length 2000, epsilon 0.10257728554188948, time 732.0, rides 122\n",
      "Initial State is  [2, 10, 4]\n",
      "episode 2276, reward 754.0, memory_length 2000, epsilon 0.1024747082563476, time 727.0, rides 123\n",
      "Initial State is  [0, 17, 4]\n",
      "episode 2277, reward 473.0, memory_length 2000, epsilon 0.10237223354809125, time 732.0, rides 126\n",
      "Initial State is  [1, 17, 2]\n",
      "episode 2278, reward 999.0, memory_length 2000, epsilon 0.10226986131454316, time 724.0, rides 122\n",
      "Initial State is  [3, 4, 2]\n",
      "episode 2279, reward 458.0, memory_length 2000, epsilon 0.10216759145322861, time 727.0, rides 123\n",
      "Initial State is  [3, 2, 4]\n",
      "episode 2280, reward 674.0, memory_length 2000, epsilon 0.10206542386177538, time 733.0, rides 126\n",
      "Initial State is  [1, 4, 1]\n",
      "episode 2281, reward 638.0, memory_length 2000, epsilon 0.1019633584379136, time 731.0, rides 119\n",
      "Initial State is  [4, 0, 5]\n",
      "episode 2282, reward 413.0, memory_length 2000, epsilon 0.10186139507947568, time 734.0, rides 144\n",
      "Initial State is  [4, 13, 0]\n",
      "episode 2283, reward 725.0, memory_length 2000, epsilon 0.1017595336843962, time 723.0, rides 128\n",
      "Initial State is  [1, 15, 6]\n",
      "episode 2284, reward 629.0, memory_length 2000, epsilon 0.10165777415071181, time 735.0, rides 139\n",
      "Initial State is  [0, 4, 4]\n",
      "episode 2285, reward 788.0, memory_length 2000, epsilon 0.1015561163765611, time 731.0, rides 131\n",
      "Initial State is  [1, 9, 1]\n",
      "episode 2286, reward 796.0, memory_length 2000, epsilon 0.10145456026018454, time 727.0, rides 138\n",
      "Initial State is  [2, 6, 3]\n",
      "episode 2287, reward 758.0, memory_length 2000, epsilon 0.10135310569992435, time 728.0, rides 114\n",
      "Initial State is  [2, 23, 5]\n",
      "episode 2288, reward 911.0, memory_length 2000, epsilon 0.10125175259422443, time 727.0, rides 123\n",
      "Initial State is  [4, 0, 3]\n",
      "episode 2289, reward 738.0, memory_length 2000, epsilon 0.10115050084163021, time 721.0, rides 127\n",
      "Initial State is  [4, 16, 1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 2290, reward 680.0, memory_length 2000, epsilon 0.10104935034078859, time 725.0, rides 119\n",
      "Initial State is  [2, 3, 3]\n",
      "episode 2291, reward 544.0, memory_length 2000, epsilon 0.1009483009904478, time 726.0, rides 119\n",
      "Initial State is  [2, 8, 3]\n",
      "episode 2292, reward 521.0, memory_length 2000, epsilon 0.10084735268945735, time 735.0, rides 128\n",
      "Initial State is  [4, 16, 2]\n",
      "episode 2293, reward 1030.0, memory_length 2000, epsilon 0.10074650533676789, time 729.0, rides 130\n",
      "Initial State is  [3, 13, 1]\n",
      "episode 2294, reward 827.0, memory_length 2000, epsilon 0.10064575883143112, time 728.0, rides 132\n",
      "Initial State is  [3, 11, 6]\n",
      "episode 2295, reward 493.0, memory_length 2000, epsilon 0.1005451130725997, time 734.0, rides 132\n",
      "Initial State is  [0, 3, 4]\n",
      "episode 2296, reward 770.0, memory_length 2000, epsilon 0.1004445679595271, time 727.0, rides 136\n",
      "Initial State is  [2, 15, 0]\n",
      "episode 2297, reward 724.0, memory_length 2000, epsilon 0.10034412339156756, time 723.0, rides 124\n",
      "Initial State is  [0, 20, 6]\n",
      "episode 2298, reward 805.0, memory_length 2000, epsilon 0.100243779268176, time 731.0, rides 119\n",
      "Initial State is  [3, 8, 4]\n",
      "episode 2299, reward 831.0, memory_length 2000, epsilon 0.10014353548890782, time 724.0, rides 117\n",
      "Initial State is  [4, 23, 4]\n",
      "episode 2300, reward 679.0, memory_length 2000, epsilon 0.10004339195341891, time 735.0, rides 114\n",
      "Initial State is  [4, 1, 3]\n",
      "episode 2301, reward 688.0, memory_length 2000, epsilon 0.09994334856146549, time 727.0, rides 128\n",
      "Initial State is  [0, 12, 4]\n",
      "episode 2302, reward 568.0, memory_length 2000, epsilon 0.09984340521290402, time 724.0, rides 137\n",
      "Initial State is  [0, 5, 1]\n",
      "episode 2303, reward 661.0, memory_length 2000, epsilon 0.09974356180769112, time 725.0, rides 131\n",
      "Initial State is  [3, 13, 1]\n",
      "episode 2304, reward 535.0, memory_length 2000, epsilon 0.09964381824588342, time 725.0, rides 131\n",
      "Initial State is  [3, 18, 4]\n",
      "episode 2305, reward 588.0, memory_length 2000, epsilon 0.09954417442763754, time 735.0, rides 138\n",
      "Initial State is  [4, 21, 1]\n",
      "episode 2306, reward 848.0, memory_length 2000, epsilon 0.0994446302532099, time 724.0, rides 130\n",
      "Initial State is  [2, 13, 2]\n",
      "episode 2307, reward 735.0, memory_length 2000, epsilon 0.09934518562295669, time 723.0, rides 129\n",
      "Initial State is  [0, 23, 5]\n",
      "episode 2308, reward 901.0, memory_length 2000, epsilon 0.09924584043733373, time 730.0, rides 135\n",
      "Initial State is  [0, 6, 0]\n",
      "episode 2309, reward 752.0, memory_length 2000, epsilon 0.0991465945968964, time 727.0, rides 125\n",
      "Initial State is  [4, 16, 3]\n",
      "episode 2310, reward 800.0, memory_length 2000, epsilon 0.0990474480022995, time 733.0, rides 139\n",
      "Initial State is  [0, 3, 4]\n",
      "episode 2311, reward 793.0, memory_length 2000, epsilon 0.09894840055429721, time 731.0, rides 133\n",
      "Initial State is  [3, 4, 3]\n",
      "episode 2312, reward 651.0, memory_length 2000, epsilon 0.09884945215374291, time 734.0, rides 135\n",
      "Initial State is  [4, 2, 3]\n",
      "episode 2313, reward 707.0, memory_length 2000, epsilon 0.09875060270158917, time 723.0, rides 124\n",
      "Initial State is  [3, 1, 3]\n",
      "episode 2314, reward 925.0, memory_length 2000, epsilon 0.09865185209888758, time 731.0, rides 127\n",
      "Initial State is  [3, 17, 4]\n",
      "episode 2315, reward 684.0, memory_length 2000, epsilon 0.0985532002467887, time 728.0, rides 124\n",
      "Initial State is  [0, 12, 1]\n",
      "episode 2316, reward 780.0, memory_length 2000, epsilon 0.0984546470465419, time 731.0, rides 124\n",
      "Initial State is  [4, 3, 2]\n",
      "episode 2317, reward 1071.0, memory_length 2000, epsilon 0.09835619239949536, time 723.0, rides 138\n",
      "Initial State is  [1, 13, 5]\n",
      "episode 2318, reward 721.0, memory_length 2000, epsilon 0.09825783620709587, time 729.0, rides 119\n",
      "Initial State is  [0, 12, 0]\n",
      "episode 2319, reward 735.0, memory_length 2000, epsilon 0.09815957837088878, time 738.0, rides 127\n",
      "Initial State is  [2, 17, 2]\n",
      "episode 2320, reward 658.0, memory_length 2000, epsilon 0.09806141879251788, time 730.0, rides 127\n",
      "Initial State is  [1, 20, 0]\n",
      "episode 2321, reward 610.0, memory_length 2000, epsilon 0.09796335737372537, time 725.0, rides 136\n",
      "Initial State is  [4, 21, 3]\n",
      "episode 2322, reward 1172.0, memory_length 2000, epsilon 0.09786539401635165, time 723.0, rides 133\n",
      "Initial State is  [2, 20, 2]\n",
      "episode 2323, reward 761.0, memory_length 2000, epsilon 0.0977675286223353, time 723.0, rides 128\n",
      "Initial State is  [1, 9, 2]\n",
      "episode 2324, reward 823.0, memory_length 2000, epsilon 0.09766976109371296, time 726.0, rides 121\n",
      "Initial State is  [3, 2, 0]\n",
      "episode 2325, reward 867.0, memory_length 2000, epsilon 0.09757209133261925, time 725.0, rides 129\n",
      "Initial State is  [3, 3, 6]\n",
      "episode 2326, reward 721.0, memory_length 2000, epsilon 0.09747451924128663, time 723.0, rides 134\n",
      "Initial State is  [0, 21, 2]\n",
      "episode 2327, reward 939.0, memory_length 2000, epsilon 0.09737704472204534, time 724.0, rides 123\n",
      "Initial State is  [3, 13, 2]\n",
      "episode 2328, reward 827.0, memory_length 2000, epsilon 0.0972796676773233, time 728.0, rides 134\n",
      "Initial State is  [2, 23, 1]\n",
      "episode 2329, reward 738.0, memory_length 2000, epsilon 0.09718238800964597, time 729.0, rides 129\n",
      "Initial State is  [2, 14, 5]\n",
      "episode 2330, reward 986.0, memory_length 2000, epsilon 0.09708520562163633, time 727.0, rides 121\n",
      "Initial State is  [1, 14, 1]\n",
      "episode 2331, reward 739.0, memory_length 2000, epsilon 0.09698812041601469, time 731.0, rides 125\n",
      "Initial State is  [2, 20, 5]\n",
      "episode 2332, reward 845.0, memory_length 2000, epsilon 0.09689113229559868, time 725.0, rides 130\n",
      "Initial State is  [1, 12, 5]\n",
      "episode 2333, reward 713.0, memory_length 2000, epsilon 0.09679424116330308, time 725.0, rides 123\n",
      "Initial State is  [2, 16, 2]\n",
      "episode 2334, reward 844.0, memory_length 2000, epsilon 0.09669744692213977, time 728.0, rides 118\n",
      "Initial State is  [1, 19, 0]\n",
      "episode 2335, reward 953.0, memory_length 2000, epsilon 0.09660074947521763, time 739.0, rides 131\n",
      "Initial State is  [3, 23, 1]\n",
      "episode 2336, reward 691.0, memory_length 2000, epsilon 0.09650414872574241, time 736.0, rides 121\n",
      "Initial State is  [0, 7, 0]\n",
      "episode 2337, reward 1169.0, memory_length 2000, epsilon 0.09640764457701667, time 726.0, rides 141\n",
      "Initial State is  [4, 22, 2]\n",
      "episode 2338, reward 778.0, memory_length 2000, epsilon 0.09631123693243965, time 730.0, rides 123\n",
      "Initial State is  [1, 18, 5]\n",
      "episode 2339, reward 753.0, memory_length 2000, epsilon 0.09621492569550721, time 730.0, rides 112\n",
      "Initial State is  [2, 1, 6]\n",
      "episode 2340, reward 841.0, memory_length 2000, epsilon 0.09611871076981171, time 728.0, rides 134\n",
      "Initial State is  [0, 4, 6]\n",
      "episode 2341, reward 821.0, memory_length 2000, epsilon 0.0960225920590419, time 725.0, rides 120\n",
      "Initial State is  [1, 4, 6]\n",
      "episode 2342, reward 750.0, memory_length 2000, epsilon 0.09592656946698286, time 733.0, rides 118\n",
      "Initial State is  [3, 13, 1]\n",
      "episode 2343, reward 861.0, memory_length 2000, epsilon 0.09583064289751587, time 734.0, rides 128\n",
      "Initial State is  [0, 23, 2]\n",
      "episode 2344, reward 649.0, memory_length 2000, epsilon 0.09573481225461836, time 733.0, rides 121\n",
      "Initial State is  [1, 20, 0]\n",
      "episode 2345, reward 409.0, memory_length 2000, epsilon 0.09563907744236373, time 722.0, rides 135\n",
      "Initial State is  [1, 18, 1]\n",
      "episode 2346, reward 589.0, memory_length 2000, epsilon 0.09554343836492137, time 743.0, rides 121\n",
      "Initial State is  [3, 12, 5]\n",
      "episode 2347, reward 855.0, memory_length 2000, epsilon 0.09544789492655645, time 723.0, rides 129\n",
      "Initial State is  [1, 7, 1]\n",
      "episode 2348, reward 1065.0, memory_length 2000, epsilon 0.09535244703162989, time 735.0, rides 127\n",
      "Initial State is  [4, 23, 2]\n",
      "episode 2349, reward 847.0, memory_length 2000, epsilon 0.09525709458459826, time 735.0, rides 125\n",
      "Initial State is  [1, 23, 1]\n",
      "episode 2350, reward 961.0, memory_length 2000, epsilon 0.09516183749001367, time 730.0, rides 132\n",
      "Initial State is  [1, 15, 3]\n",
      "episode 2351, reward 994.0, memory_length 2000, epsilon 0.09506667565252365, time 721.0, rides 154\n",
      "Initial State is  [3, 19, 4]\n",
      "episode 2352, reward 638.0, memory_length 2000, epsilon 0.09497160897687112, time 735.0, rides 124\n",
      "Initial State is  [0, 17, 5]\n",
      "episode 2353, reward 863.0, memory_length 2000, epsilon 0.09487663736789426, time 724.0, rides 124\n",
      "Initial State is  [0, 20, 2]\n",
      "episode 2354, reward 890.0, memory_length 2000, epsilon 0.09478176073052637, time 728.0, rides 121\n",
      "Initial State is  [1, 17, 3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 2355, reward 837.0, memory_length 2000, epsilon 0.09468697896979585, time 723.0, rides 147\n",
      "Initial State is  [2, 15, 5]\n",
      "episode 2356, reward 754.0, memory_length 2000, epsilon 0.09459229199082606, time 726.0, rides 145\n",
      "Initial State is  [3, 16, 0]\n",
      "episode 2357, reward 853.0, memory_length 2000, epsilon 0.09449769969883523, time 727.0, rides 122\n",
      "Initial State is  [1, 13, 5]\n",
      "episode 2358, reward 617.0, memory_length 2000, epsilon 0.09440320199913639, time 736.0, rides 112\n",
      "Initial State is  [3, 8, 4]\n",
      "episode 2359, reward 808.0, memory_length 2000, epsilon 0.09430879879713726, time 732.0, rides 120\n",
      "Initial State is  [2, 12, 1]\n",
      "episode 2360, reward 822.0, memory_length 2000, epsilon 0.09421448999834012, time 730.0, rides 124\n",
      "Initial State is  [3, 1, 1]\n",
      "episode 2361, reward 679.0, memory_length 2000, epsilon 0.09412027550834177, time 733.0, rides 129\n",
      "Initial State is  [2, 15, 4]\n",
      "episode 2362, reward 531.0, memory_length 2000, epsilon 0.09402615523283343, time 728.0, rides 131\n",
      "Initial State is  [0, 6, 4]\n",
      "episode 2363, reward 912.0, memory_length 2000, epsilon 0.0939321290776006, time 735.0, rides 132\n",
      "Initial State is  [0, 10, 5]\n",
      "episode 2364, reward 819.0, memory_length 2000, epsilon 0.093838196948523, time 723.0, rides 126\n",
      "Initial State is  [3, 10, 1]\n",
      "episode 2365, reward 663.0, memory_length 2000, epsilon 0.09374435875157447, time 725.0, rides 126\n",
      "Initial State is  [0, 20, 0]\n",
      "episode 2366, reward 815.0, memory_length 2000, epsilon 0.09365061439282289, time 725.0, rides 132\n",
      "Initial State is  [0, 17, 2]\n",
      "episode 2367, reward 663.0, memory_length 2000, epsilon 0.09355696377843006, time 727.0, rides 124\n",
      "Initial State is  [3, 4, 1]\n",
      "episode 2368, reward 749.0, memory_length 2000, epsilon 0.09346340681465164, time 730.0, rides 121\n",
      "Initial State is  [4, 13, 5]\n",
      "episode 2369, reward 746.0, memory_length 2000, epsilon 0.09336994340783698, time 733.0, rides 128\n",
      "Initial State is  [3, 21, 5]\n",
      "episode 2370, reward 483.0, memory_length 2000, epsilon 0.09327657346442915, time 736.0, rides 124\n",
      "Initial State is  [1, 18, 1]\n",
      "episode 2371, reward 726.0, memory_length 2000, epsilon 0.09318329689096472, time 731.0, rides 130\n",
      "Initial State is  [2, 21, 0]\n",
      "episode 2372, reward 1133.0, memory_length 2000, epsilon 0.09309011359407375, time 735.0, rides 112\n",
      "Initial State is  [1, 0, 6]\n",
      "episode 2373, reward 543.0, memory_length 2000, epsilon 0.09299702348047968, time 732.0, rides 123\n",
      "Initial State is  [3, 12, 1]\n",
      "episode 2374, reward 816.0, memory_length 2000, epsilon 0.0929040264569992, time 730.0, rides 118\n",
      "Initial State is  [3, 8, 2]\n",
      "episode 2375, reward 753.0, memory_length 2000, epsilon 0.09281112243054221, time 726.0, rides 121\n",
      "Initial State is  [4, 12, 1]\n",
      "episode 2376, reward 882.0, memory_length 2000, epsilon 0.09271831130811167, time 739.0, rides 131\n",
      "Initial State is  [1, 0, 5]\n",
      "episode 2377, reward 1050.0, memory_length 2000, epsilon 0.09262559299680356, time 729.0, rides 129\n",
      "Initial State is  [4, 3, 3]\n",
      "episode 2378, reward 771.0, memory_length 2000, epsilon 0.09253296740380676, time 728.0, rides 124\n",
      "Initial State is  [0, 3, 4]\n",
      "episode 2379, reward 809.0, memory_length 2000, epsilon 0.09244043443640296, time 725.0, rides 131\n",
      "Initial State is  [1, 19, 0]\n",
      "episode 2380, reward 934.0, memory_length 2000, epsilon 0.09234799400196655, time 731.0, rides 132\n",
      "Initial State is  [1, 4, 6]\n",
      "episode 2381, reward 811.0, memory_length 2000, epsilon 0.09225564600796458, time 723.0, rides 125\n",
      "Initial State is  [4, 2, 3]\n",
      "episode 2382, reward 902.0, memory_length 2000, epsilon 0.09216339036195662, time 730.0, rides 127\n",
      "Initial State is  [2, 20, 3]\n",
      "episode 2383, reward 902.0, memory_length 2000, epsilon 0.09207122697159466, time 728.0, rides 136\n",
      "Initial State is  [1, 12, 4]\n",
      "episode 2384, reward 671.0, memory_length 2000, epsilon 0.09197915574462306, time 721.0, rides 142\n",
      "Initial State is  [4, 23, 0]\n",
      "episode 2385, reward 806.0, memory_length 2000, epsilon 0.09188717658887845, time 730.0, rides 125\n",
      "Initial State is  [1, 21, 1]\n",
      "episode 2386, reward 788.0, memory_length 2000, epsilon 0.09179528941228957, time 732.0, rides 141\n",
      "Initial State is  [4, 6, 4]\n",
      "episode 2387, reward 817.0, memory_length 2000, epsilon 0.09170349412287727, time 725.0, rides 122\n",
      "Initial State is  [0, 14, 0]\n",
      "episode 2388, reward 805.0, memory_length 2000, epsilon 0.0916117906287544, time 730.0, rides 123\n",
      "Initial State is  [1, 6, 6]\n",
      "episode 2389, reward 823.0, memory_length 2000, epsilon 0.09152017883812565, time 733.0, rides 122\n",
      "Initial State is  [4, 6, 5]\n",
      "episode 2390, reward 804.0, memory_length 2000, epsilon 0.09142865865928752, time 729.0, rides 124\n",
      "Initial State is  [3, 23, 2]\n",
      "episode 2391, reward 701.0, memory_length 2000, epsilon 0.09133723000062824, time 727.0, rides 130\n",
      "Initial State is  [3, 6, 6]\n",
      "episode 2392, reward 945.0, memory_length 2000, epsilon 0.09124589277062761, time 724.0, rides 138\n",
      "Initial State is  [1, 9, 2]\n",
      "episode 2393, reward 796.0, memory_length 2000, epsilon 0.09115464687785699, time 724.0, rides 129\n",
      "Initial State is  [3, 3, 1]\n",
      "episode 2394, reward 1046.0, memory_length 2000, epsilon 0.09106349223097913, time 724.0, rides 129\n",
      "Initial State is  [3, 1, 0]\n",
      "episode 2395, reward 836.0, memory_length 2000, epsilon 0.09097242873874815, time 727.0, rides 129\n",
      "Initial State is  [0, 14, 6]\n",
      "episode 2396, reward 690.0, memory_length 2000, epsilon 0.0908814563100094, time 727.0, rides 138\n",
      "Initial State is  [3, 6, 1]\n",
      "episode 2397, reward 823.0, memory_length 2000, epsilon 0.0907905748536994, time 729.0, rides 127\n",
      "Initial State is  [4, 4, 2]\n",
      "episode 2398, reward 837.0, memory_length 2000, epsilon 0.0906997842788457, time 726.0, rides 139\n",
      "Initial State is  [0, 15, 6]\n",
      "episode 2399, reward 791.0, memory_length 2000, epsilon 0.09060908449456685, time 728.0, rides 125\n",
      "Initial State is  [1, 17, 6]\n",
      "episode 2400, reward 868.0, memory_length 2000, epsilon 0.09051847541007228, time 728.0, rides 130\n",
      "Initial State is  [2, 18, 3]\n",
      "episode 2401, reward 873.0, memory_length 2000, epsilon 0.09042795693466221, time 730.0, rides 125\n",
      "Initial State is  [1, 8, 2]\n",
      "episode 2402, reward 536.0, memory_length 2000, epsilon 0.09033752897772755, time 727.0, rides 134\n",
      "Initial State is  [4, 7, 2]\n",
      "episode 2403, reward 719.0, memory_length 2000, epsilon 0.09024719144874982, time 747.0, rides 133\n",
      "Initial State is  [1, 14, 6]\n",
      "episode 2404, reward 708.0, memory_length 2000, epsilon 0.09015694425730107, time 730.0, rides 121\n",
      "Initial State is  [3, 19, 4]\n",
      "episode 2405, reward 587.0, memory_length 2000, epsilon 0.09006678731304377, time 735.0, rides 127\n",
      "Initial State is  [2, 23, 1]\n",
      "episode 2406, reward 1052.0, memory_length 2000, epsilon 0.08997672052573072, time 730.0, rides 129\n",
      "Initial State is  [4, 2, 1]\n",
      "episode 2407, reward 685.0, memory_length 2000, epsilon 0.08988674380520499, time 725.0, rides 136\n",
      "Initial State is  [4, 11, 5]\n",
      "episode 2408, reward 771.0, memory_length 2000, epsilon 0.08979685706139978, time 728.0, rides 129\n",
      "Initial State is  [1, 7, 4]\n",
      "episode 2409, reward 814.0, memory_length 2000, epsilon 0.08970706020433838, time 727.0, rides 117\n",
      "Initial State is  [4, 2, 4]\n",
      "episode 2410, reward 739.0, memory_length 2000, epsilon 0.08961735314413405, time 724.0, rides 130\n",
      "Initial State is  [3, 21, 3]\n",
      "episode 2411, reward 756.0, memory_length 2000, epsilon 0.08952773579098991, time 734.0, rides 134\n",
      "Initial State is  [2, 16, 1]\n",
      "episode 2412, reward 665.0, memory_length 2000, epsilon 0.08943820805519892, time 723.0, rides 127\n",
      "Initial State is  [2, 23, 5]\n",
      "episode 2413, reward 759.0, memory_length 2000, epsilon 0.08934876984714372, time 725.0, rides 132\n",
      "Initial State is  [4, 0, 2]\n",
      "episode 2414, reward 652.0, memory_length 2000, epsilon 0.08925942107729658, time 729.0, rides 125\n",
      "Initial State is  [1, 15, 4]\n",
      "episode 2415, reward 791.0, memory_length 2000, epsilon 0.08917016165621929, time 735.0, rides 121\n",
      "Initial State is  [0, 8, 5]\n",
      "episode 2416, reward 775.0, memory_length 2000, epsilon 0.08908099149456307, time 727.0, rides 127\n",
      "Initial State is  [3, 2, 5]\n",
      "episode 2417, reward 1074.0, memory_length 2000, epsilon 0.08899191050306851, time 726.0, rides 137\n",
      "Initial State is  [4, 17, 2]\n",
      "episode 2418, reward 957.0, memory_length 2000, epsilon 0.08890291859256544, time 725.0, rides 127\n",
      "Initial State is  [1, 4, 1]\n",
      "episode 2419, reward 605.0, memory_length 2000, epsilon 0.08881401567397287, time 730.0, rides 134\n",
      "Initial State is  [2, 6, 3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 2420, reward 717.0, memory_length 2000, epsilon 0.0887252016582989, time 731.0, rides 130\n",
      "Initial State is  [4, 12, 6]\n",
      "episode 2421, reward 946.0, memory_length 2000, epsilon 0.0886364764566406, time 724.0, rides 136\n",
      "Initial State is  [0, 8, 2]\n",
      "episode 2422, reward 638.0, memory_length 2000, epsilon 0.08854783998018396, time 727.0, rides 132\n",
      "Initial State is  [0, 2, 1]\n",
      "episode 2423, reward 708.0, memory_length 2000, epsilon 0.08845929214020377, time 725.0, rides 131\n",
      "Initial State is  [0, 0, 3]\n",
      "episode 2424, reward 855.0, memory_length 2000, epsilon 0.08837083284806357, time 734.0, rides 137\n",
      "Initial State is  [0, 3, 6]\n",
      "episode 2425, reward 708.0, memory_length 2000, epsilon 0.08828246201521552, time 730.0, rides 141\n",
      "Initial State is  [1, 19, 5]\n",
      "episode 2426, reward 889.0, memory_length 2000, epsilon 0.0881941795532003, time 722.0, rides 125\n",
      "Initial State is  [4, 7, 6]\n",
      "episode 2427, reward 1192.0, memory_length 2000, epsilon 0.0881059853736471, time 733.0, rides 138\n",
      "Initial State is  [2, 15, 6]\n",
      "episode 2428, reward 869.0, memory_length 2000, epsilon 0.08801787938827345, time 728.0, rides 152\n",
      "Initial State is  [1, 11, 5]\n",
      "episode 2429, reward 975.0, memory_length 2000, epsilon 0.08792986150888517, time 724.0, rides 143\n",
      "Initial State is  [0, 0, 4]\n",
      "episode 2430, reward 826.0, memory_length 2000, epsilon 0.08784193164737629, time 730.0, rides 128\n",
      "Initial State is  [1, 20, 2]\n",
      "episode 2431, reward 726.0, memory_length 2000, epsilon 0.08775408971572891, time 727.0, rides 120\n",
      "Initial State is  [1, 10, 2]\n",
      "episode 2432, reward 827.0, memory_length 2000, epsilon 0.08766633562601318, time 737.0, rides 136\n",
      "Initial State is  [1, 11, 6]\n",
      "episode 2433, reward 749.0, memory_length 2000, epsilon 0.08757866929038717, time 729.0, rides 130\n",
      "Initial State is  [0, 6, 3]\n",
      "episode 2434, reward 767.0, memory_length 2000, epsilon 0.08749109062109678, time 730.0, rides 129\n",
      "Initial State is  [3, 16, 6]\n",
      "episode 2435, reward 765.0, memory_length 2000, epsilon 0.08740359953047568, time 729.0, rides 129\n",
      "Initial State is  [3, 2, 3]\n",
      "episode 2436, reward 916.0, memory_length 2000, epsilon 0.0873161959309452, time 724.0, rides 130\n",
      "Initial State is  [3, 20, 3]\n",
      "episode 2437, reward 753.0, memory_length 2000, epsilon 0.08722887973501425, time 727.0, rides 131\n",
      "Initial State is  [1, 19, 6]\n",
      "episode 2438, reward 803.0, memory_length 2000, epsilon 0.08714165085527924, time 728.0, rides 149\n",
      "Initial State is  [4, 9, 6]\n",
      "episode 2439, reward 388.0, memory_length 2000, epsilon 0.08705450920442395, time 720.0, rides 127\n",
      "Initial State is  [2, 7, 1]\n",
      "episode 2440, reward 835.0, memory_length 2000, epsilon 0.08696745469521953, time 725.0, rides 139\n",
      "Initial State is  [3, 3, 1]\n",
      "episode 2441, reward 827.0, memory_length 2000, epsilon 0.08688048724052432, time 728.0, rides 126\n",
      "Initial State is  [0, 1, 2]\n",
      "episode 2442, reward 787.0, memory_length 2000, epsilon 0.08679360675328379, time 733.0, rides 131\n",
      "Initial State is  [2, 5, 6]\n",
      "episode 2443, reward 706.0, memory_length 2000, epsilon 0.0867068131465305, time 726.0, rides 126\n",
      "Initial State is  [4, 21, 1]\n",
      "episode 2444, reward 643.0, memory_length 2000, epsilon 0.08662010633338398, time 725.0, rides 136\n",
      "Initial State is  [0, 17, 1]\n",
      "episode 2445, reward 945.0, memory_length 2000, epsilon 0.0865334862270506, time 742.0, rides 135\n",
      "Initial State is  [0, 8, 5]\n",
      "episode 2446, reward 875.0, memory_length 2000, epsilon 0.08644695274082355, time 737.0, rides 132\n",
      "Initial State is  [0, 18, 3]\n",
      "episode 2447, reward 1013.0, memory_length 2000, epsilon 0.08636050578808273, time 727.0, rides 121\n",
      "Initial State is  [2, 21, 6]\n",
      "episode 2448, reward 902.0, memory_length 2000, epsilon 0.08627414528229464, time 734.0, rides 122\n",
      "Initial State is  [0, 14, 2]\n",
      "episode 2449, reward 917.0, memory_length 2000, epsilon 0.08618787113701235, time 726.0, rides 127\n",
      "Initial State is  [2, 13, 6]\n",
      "episode 2450, reward 1076.0, memory_length 2000, epsilon 0.08610168326587533, time 735.0, rides 131\n",
      "Initial State is  [2, 16, 3]\n",
      "episode 2451, reward 838.0, memory_length 2000, epsilon 0.08601558158260945, time 731.0, rides 129\n",
      "Initial State is  [2, 6, 2]\n",
      "episode 2452, reward 891.0, memory_length 2000, epsilon 0.08592956600102684, time 733.0, rides 128\n",
      "Initial State is  [2, 17, 4]\n",
      "episode 2453, reward 860.0, memory_length 2000, epsilon 0.08584363643502582, time 730.0, rides 114\n",
      "Initial State is  [1, 4, 6]\n",
      "episode 2454, reward 730.0, memory_length 2000, epsilon 0.08575779279859079, time 727.0, rides 120\n",
      "Initial State is  [4, 18, 1]\n",
      "episode 2455, reward 819.0, memory_length 2000, epsilon 0.0856720350057922, time 733.0, rides 116\n",
      "Initial State is  [0, 6, 3]\n",
      "episode 2456, reward 769.0, memory_length 2000, epsilon 0.0855863629707864, time 733.0, rides 121\n",
      "Initial State is  [4, 4, 0]\n",
      "episode 2457, reward 926.0, memory_length 2000, epsilon 0.0855007766078156, time 727.0, rides 120\n",
      "Initial State is  [4, 22, 2]\n",
      "episode 2458, reward 511.0, memory_length 2000, epsilon 0.08541527583120778, time 724.0, rides 131\n",
      "Initial State is  [0, 4, 1]\n",
      "episode 2459, reward 693.0, memory_length 2000, epsilon 0.08532986055537657, time 742.0, rides 132\n",
      "Initial State is  [2, 11, 6]\n",
      "episode 2460, reward 729.0, memory_length 2000, epsilon 0.0852445306948212, time 735.0, rides 122\n",
      "Initial State is  [4, 6, 4]\n",
      "episode 2461, reward 786.0, memory_length 2000, epsilon 0.08515928616412638, time 729.0, rides 131\n",
      "Initial State is  [1, 11, 4]\n",
      "episode 2462, reward 856.0, memory_length 2000, epsilon 0.08507412687796226, time 726.0, rides 134\n",
      "Initial State is  [4, 8, 2]\n",
      "episode 2463, reward 935.0, memory_length 2000, epsilon 0.0849890527510843, time 725.0, rides 125\n",
      "Initial State is  [3, 20, 4]\n",
      "episode 2464, reward 978.0, memory_length 2000, epsilon 0.08490406369833321, time 731.0, rides 129\n",
      "Initial State is  [0, 6, 5]\n",
      "episode 2465, reward 813.0, memory_length 2000, epsilon 0.08481915963463488, time 726.0, rides 126\n",
      "Initial State is  [4, 13, 4]\n",
      "episode 2466, reward 576.0, memory_length 2000, epsilon 0.08473434047500024, time 725.0, rides 127\n",
      "Initial State is  [2, 20, 3]\n",
      "episode 2467, reward 632.0, memory_length 2000, epsilon 0.08464960613452524, time 725.0, rides 132\n",
      "Initial State is  [0, 9, 1]\n",
      "episode 2468, reward 879.0, memory_length 2000, epsilon 0.08456495652839072, time 729.0, rides 143\n",
      "Initial State is  [2, 10, 0]\n",
      "episode 2469, reward 621.0, memory_length 2000, epsilon 0.08448039157186232, time 736.0, rides 136\n",
      "Initial State is  [1, 1, 3]\n",
      "episode 2470, reward 799.0, memory_length 2000, epsilon 0.08439591118029047, time 736.0, rides 136\n",
      "Initial State is  [3, 8, 2]\n",
      "episode 2471, reward 629.0, memory_length 2000, epsilon 0.08431151526911018, time 726.0, rides 128\n",
      "Initial State is  [0, 22, 3]\n",
      "episode 2472, reward 941.0, memory_length 2000, epsilon 0.08422720375384107, time 735.0, rides 127\n",
      "Initial State is  [0, 14, 6]\n",
      "episode 2473, reward 943.0, memory_length 2000, epsilon 0.08414297655008723, time 730.0, rides 127\n",
      "Initial State is  [2, 15, 1]\n",
      "episode 2474, reward 910.0, memory_length 2000, epsilon 0.08405883357353715, time 726.0, rides 130\n",
      "Initial State is  [3, 22, 1]\n",
      "episode 2475, reward 728.0, memory_length 2000, epsilon 0.0839747747399636, time 729.0, rides 141\n",
      "Initial State is  [4, 10, 5]\n",
      "episode 2476, reward 1090.0, memory_length 2000, epsilon 0.08389079996522364, time 729.0, rides 127\n",
      "Initial State is  [2, 0, 4]\n",
      "episode 2477, reward 871.0, memory_length 2000, epsilon 0.08380690916525842, time 726.0, rides 124\n",
      "Initial State is  [4, 4, 5]\n",
      "episode 2478, reward 927.0, memory_length 2000, epsilon 0.08372310225609315, time 725.0, rides 146\n",
      "Initial State is  [2, 14, 2]\n",
      "episode 2479, reward 708.0, memory_length 2000, epsilon 0.08363937915383707, time 727.0, rides 126\n",
      "Initial State is  [4, 10, 3]\n",
      "episode 2480, reward 485.0, memory_length 2000, epsilon 0.08355573977468322, time 727.0, rides 127\n",
      "Initial State is  [2, 7, 1]\n",
      "episode 2481, reward 595.0, memory_length 2000, epsilon 0.08347218403490854, time 736.0, rides 120\n",
      "Initial State is  [3, 21, 5]\n",
      "episode 2482, reward 951.0, memory_length 2000, epsilon 0.08338871185087363, time 732.0, rides 119\n",
      "Initial State is  [0, 15, 6]\n",
      "episode 2483, reward 694.0, memory_length 2000, epsilon 0.08330532313902275, time 726.0, rides 116\n",
      "Initial State is  [3, 20, 5]\n",
      "episode 2484, reward 678.0, memory_length 2000, epsilon 0.08322201781588373, time 724.0, rides 125\n",
      "Initial State is  [3, 15, 5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 2485, reward 503.0, memory_length 2000, epsilon 0.08313879579806785, time 731.0, rides 116\n",
      "Initial State is  [2, 7, 1]\n",
      "episode 2486, reward 703.0, memory_length 2000, epsilon 0.08305565700226979, time 725.0, rides 128\n",
      "Initial State is  [3, 11, 2]\n",
      "episode 2487, reward 588.0, memory_length 2000, epsilon 0.08297260134526752, time 729.0, rides 134\n",
      "Initial State is  [1, 16, 2]\n",
      "episode 2488, reward 932.0, memory_length 2000, epsilon 0.08288962874392225, time 729.0, rides 122\n",
      "Initial State is  [1, 1, 2]\n",
      "episode 2489, reward 887.0, memory_length 2000, epsilon 0.08280673911517833, time 720.0, rides 127\n",
      "Initial State is  [0, 17, 2]\n",
      "episode 2490, reward 334.0, memory_length 2000, epsilon 0.08272393237606315, time 728.0, rides 122\n",
      "Initial State is  [0, 5, 3]\n",
      "episode 2491, reward 803.0, memory_length 2000, epsilon 0.0826412084436871, time 734.0, rides 130\n",
      "Initial State is  [2, 19, 1]\n",
      "episode 2492, reward 861.0, memory_length 2000, epsilon 0.08255856723524341, time 729.0, rides 130\n",
      "Initial State is  [3, 1, 6]\n",
      "episode 2493, reward 684.0, memory_length 2000, epsilon 0.08247600866800817, time 730.0, rides 128\n",
      "Initial State is  [4, 1, 6]\n",
      "episode 2494, reward 590.0, memory_length 2000, epsilon 0.08239353265934016, time 733.0, rides 128\n",
      "Initial State is  [2, 8, 2]\n",
      "episode 2495, reward 1002.0, memory_length 2000, epsilon 0.08231113912668082, time 724.0, rides 132\n",
      "Initial State is  [0, 21, 1]\n",
      "episode 2496, reward 813.0, memory_length 2000, epsilon 0.08222882798755414, time 725.0, rides 124\n",
      "Initial State is  [0, 10, 3]\n",
      "episode 2497, reward 833.0, memory_length 2000, epsilon 0.08214659915956658, time 733.0, rides 122\n",
      "Initial State is  [2, 6, 4]\n",
      "episode 2498, reward 1082.0, memory_length 2000, epsilon 0.08206445256040702, time 745.0, rides 133\n",
      "Initial State is  [4, 22, 2]\n",
      "episode 2499, reward 1116.0, memory_length 2000, epsilon 0.08198238810784661, time 734.0, rides 129\n",
      "Initial State is  [1, 1, 6]\n",
      "episode 2500, reward 783.0, memory_length 2000, epsilon 0.08190040571973876, time 720.0, rides 122\n",
      "Initial State is  [2, 6, 0]\n",
      "episode 2501, reward 789.0, memory_length 2000, epsilon 0.08181850531401902, time 728.0, rides 139\n",
      "Initial State is  [4, 13, 0]\n",
      "episode 2502, reward 469.0, memory_length 2000, epsilon 0.081736686808705, time 720.0, rides 132\n",
      "Initial State is  [4, 18, 4]\n",
      "episode 2503, reward 784.0, memory_length 2000, epsilon 0.08165495012189629, time 725.0, rides 122\n",
      "Initial State is  [2, 3, 0]\n",
      "episode 2504, reward 479.0, memory_length 2000, epsilon 0.08157329517177439, time 726.0, rides 114\n",
      "Initial State is  [3, 19, 6]\n",
      "episode 2505, reward 773.0, memory_length 2000, epsilon 0.08149172187660261, time 728.0, rides 133\n",
      "Initial State is  [3, 1, 3]\n",
      "episode 2506, reward 764.0, memory_length 2000, epsilon 0.08141023015472601, time 727.0, rides 130\n",
      "Initial State is  [1, 5, 1]\n",
      "episode 2507, reward 740.0, memory_length 2000, epsilon 0.08132881992457128, time 730.0, rides 134\n",
      "Initial State is  [3, 9, 2]\n",
      "episode 2508, reward 956.0, memory_length 2000, epsilon 0.0812474911046467, time 729.0, rides 128\n",
      "Initial State is  [2, 5, 4]\n",
      "episode 2509, reward 726.0, memory_length 2000, epsilon 0.08116624361354205, time 728.0, rides 139\n",
      "Initial State is  [0, 12, 5]\n",
      "episode 2510, reward 629.0, memory_length 2000, epsilon 0.08108507736992851, time 729.0, rides 127\n",
      "Initial State is  [1, 14, 3]\n",
      "episode 2511, reward 1000.0, memory_length 2000, epsilon 0.08100399229255859, time 727.0, rides 137\n",
      "Initial State is  [1, 3, 2]\n",
      "episode 2512, reward 1161.0, memory_length 2000, epsilon 0.08092298830026602, time 728.0, rides 131\n",
      "Initial State is  [4, 2, 2]\n",
      "episode 2513, reward 831.0, memory_length 2000, epsilon 0.08084206531196575, time 731.0, rides 126\n",
      "Initial State is  [3, 15, 1]\n",
      "episode 2514, reward 588.0, memory_length 2000, epsilon 0.08076122324665379, time 728.0, rides 132\n",
      "Initial State is  [3, 8, 4]\n",
      "episode 2515, reward 716.0, memory_length 2000, epsilon 0.08068046202340713, time 733.0, rides 128\n",
      "Initial State is  [1, 0, 3]\n",
      "episode 2516, reward 809.0, memory_length 2000, epsilon 0.08059978156138373, time 723.0, rides 134\n",
      "Initial State is  [2, 2, 4]\n",
      "episode 2517, reward 969.0, memory_length 2000, epsilon 0.08051918177982234, time 733.0, rides 126\n",
      "Initial State is  [2, 0, 4]\n",
      "episode 2518, reward 725.0, memory_length 2000, epsilon 0.08043866259804251, time 734.0, rides 121\n",
      "Initial State is  [1, 14, 1]\n",
      "episode 2519, reward 770.0, memory_length 2000, epsilon 0.08035822393544446, time 723.0, rides 116\n",
      "Initial State is  [1, 17, 3]\n",
      "episode 2520, reward 848.0, memory_length 2000, epsilon 0.08027786571150902, time 726.0, rides 134\n",
      "Initial State is  [1, 16, 0]\n",
      "episode 2521, reward 717.0, memory_length 2000, epsilon 0.08019758784579752, time 728.0, rides 123\n",
      "Initial State is  [1, 11, 0]\n",
      "episode 2522, reward 1104.0, memory_length 2000, epsilon 0.08011739025795171, time 721.0, rides 142\n",
      "Initial State is  [4, 9, 0]\n",
      "episode 2523, reward 1138.0, memory_length 2000, epsilon 0.08003727286769376, time 727.0, rides 130\n",
      "Initial State is  [0, 16, 3]\n",
      "episode 2524, reward 549.0, memory_length 2000, epsilon 0.07995723559482606, time 737.0, rides 136\n",
      "Initial State is  [1, 20, 5]\n",
      "episode 2525, reward 907.0, memory_length 2000, epsilon 0.07987727835923124, time 731.0, rides 135\n",
      "Initial State is  [1, 8, 3]\n",
      "episode 2526, reward 578.0, memory_length 2000, epsilon 0.07979740108087201, time 730.0, rides 132\n",
      "Initial State is  [1, 16, 4]\n",
      "episode 2527, reward 799.0, memory_length 2000, epsilon 0.07971760367979114, time 726.0, rides 129\n",
      "Initial State is  [1, 16, 6]\n",
      "episode 2528, reward 627.0, memory_length 2000, epsilon 0.07963788607611134, time 728.0, rides 145\n",
      "Initial State is  [0, 14, 2]\n",
      "episode 2529, reward 882.0, memory_length 2000, epsilon 0.07955824819003524, time 732.0, rides 132\n",
      "Initial State is  [0, 10, 0]\n",
      "episode 2530, reward 1021.0, memory_length 2000, epsilon 0.0794786899418452, time 726.0, rides 133\n",
      "Initial State is  [3, 7, 1]\n",
      "episode 2531, reward 740.0, memory_length 2000, epsilon 0.07939921125190336, time 724.0, rides 130\n",
      "Initial State is  [3, 8, 6]\n",
      "episode 2532, reward 750.0, memory_length 2000, epsilon 0.07931981204065146, time 725.0, rides 122\n",
      "Initial State is  [2, 13, 6]\n",
      "episode 2533, reward 944.0, memory_length 2000, epsilon 0.0792404922286108, time 723.0, rides 132\n",
      "Initial State is  [3, 18, 2]\n",
      "episode 2534, reward 861.0, memory_length 2000, epsilon 0.0791612517363822, time 726.0, rides 142\n",
      "Initial State is  [4, 7, 6]\n",
      "episode 2535, reward 842.0, memory_length 2000, epsilon 0.07908209048464582, time 723.0, rides 125\n",
      "Initial State is  [0, 8, 3]\n",
      "episode 2536, reward 896.0, memory_length 2000, epsilon 0.07900300839416118, time 730.0, rides 122\n",
      "Initial State is  [1, 18, 2]\n",
      "episode 2537, reward 942.0, memory_length 2000, epsilon 0.07892400538576702, time 724.0, rides 120\n",
      "Initial State is  [1, 18, 1]\n",
      "episode 2538, reward 556.0, memory_length 2000, epsilon 0.07884508138038125, time 727.0, rides 127\n",
      "Initial State is  [3, 12, 3]\n",
      "episode 2539, reward 799.0, memory_length 2000, epsilon 0.07876623629900087, time 725.0, rides 127\n",
      "Initial State is  [1, 12, 6]\n",
      "episode 2540, reward 934.0, memory_length 2000, epsilon 0.07868747006270187, time 736.0, rides 127\n",
      "Initial State is  [4, 13, 3]\n",
      "episode 2541, reward 1125.0, memory_length 2000, epsilon 0.07860878259263916, time 725.0, rides 136\n",
      "Initial State is  [4, 1, 1]\n",
      "episode 2542, reward 963.0, memory_length 2000, epsilon 0.07853017381004652, time 728.0, rides 126\n",
      "Initial State is  [2, 16, 1]\n",
      "episode 2543, reward 923.0, memory_length 2000, epsilon 0.07845164363623648, time 725.0, rides 121\n",
      "Initial State is  [3, 13, 0]\n",
      "episode 2544, reward 1086.0, memory_length 2000, epsilon 0.07837319199260025, time 726.0, rides 136\n",
      "Initial State is  [3, 17, 3]\n",
      "episode 2545, reward 750.0, memory_length 2000, epsilon 0.07829481880060765, time 734.0, rides 117\n",
      "Initial State is  [2, 10, 6]\n",
      "episode 2546, reward 970.0, memory_length 2000, epsilon 0.07821652398180705, time 735.0, rides 135\n",
      "Initial State is  [3, 11, 2]\n",
      "episode 2547, reward 880.0, memory_length 2000, epsilon 0.07813830745782524, time 725.0, rides 124\n",
      "Initial State is  [4, 10, 2]\n",
      "episode 2548, reward 656.0, memory_length 2000, epsilon 0.07806016915036741, time 734.0, rides 122\n",
      "Initial State is  [0, 4, 5]\n",
      "episode 2549, reward 865.0, memory_length 2000, epsilon 0.07798210898121705, time 729.0, rides 130\n",
      "Initial State is  [4, 6, 4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 2550, reward 746.0, memory_length 2000, epsilon 0.07790412687223583, time 728.0, rides 134\n",
      "Initial State is  [0, 9, 6]\n",
      "episode 2551, reward 809.0, memory_length 2000, epsilon 0.07782622274536359, time 728.0, rides 136\n",
      "Initial State is  [0, 4, 3]\n",
      "episode 2552, reward 646.0, memory_length 2000, epsilon 0.07774839652261822, time 729.0, rides 137\n",
      "Initial State is  [1, 20, 4]\n",
      "episode 2553, reward 715.0, memory_length 2000, epsilon 0.0776706481260956, time 724.0, rides 136\n",
      "Initial State is  [0, 20, 1]\n",
      "episode 2554, reward 761.0, memory_length 2000, epsilon 0.0775929774779695, time 732.0, rides 130\n",
      "Initial State is  [0, 8, 1]\n",
      "episode 2555, reward 1165.0, memory_length 2000, epsilon 0.07751538450049153, time 723.0, rides 133\n",
      "Initial State is  [3, 2, 0]\n",
      "episode 2556, reward 743.0, memory_length 2000, epsilon 0.07743786911599104, time 729.0, rides 127\n",
      "Initial State is  [4, 22, 6]\n",
      "episode 2557, reward 846.0, memory_length 2000, epsilon 0.07736043124687506, time 725.0, rides 134\n",
      "Initial State is  [0, 4, 5]\n",
      "episode 2558, reward 623.0, memory_length 2000, epsilon 0.07728307081562819, time 732.0, rides 121\n",
      "Initial State is  [2, 18, 0]\n",
      "episode 2559, reward 553.0, memory_length 2000, epsilon 0.07720578774481256, time 725.0, rides 133\n",
      "Initial State is  [4, 5, 4]\n",
      "episode 2560, reward 540.0, memory_length 2000, epsilon 0.07712858195706775, time 727.0, rides 136\n",
      "Initial State is  [0, 12, 1]\n",
      "episode 2561, reward 730.0, memory_length 2000, epsilon 0.07705145337511068, time 729.0, rides 122\n",
      "Initial State is  [1, 20, 5]\n",
      "episode 2562, reward 940.0, memory_length 2000, epsilon 0.07697440192173557, time 740.0, rides 133\n",
      "Initial State is  [3, 6, 5]\n",
      "episode 2563, reward 1057.0, memory_length 2000, epsilon 0.07689742751981384, time 726.0, rides 133\n",
      "Initial State is  [1, 4, 1]\n",
      "episode 2564, reward 769.0, memory_length 2000, epsilon 0.07682053009229402, time 731.0, rides 121\n",
      "Initial State is  [0, 21, 0]\n",
      "episode 2565, reward 947.0, memory_length 2000, epsilon 0.07674370956220172, time 735.0, rides 134\n",
      "Initial State is  [2, 4, 3]\n",
      "episode 2566, reward 787.0, memory_length 2000, epsilon 0.07666696585263952, time 720.0, rides 137\n",
      "Initial State is  [4, 17, 4]\n",
      "episode 2567, reward 933.0, memory_length 2000, epsilon 0.07659029888678688, time 727.0, rides 120\n",
      "Initial State is  [1, 16, 1]\n",
      "episode 2568, reward 645.0, memory_length 2000, epsilon 0.07651370858790009, time 727.0, rides 139\n",
      "Initial State is  [3, 16, 1]\n",
      "episode 2569, reward 649.0, memory_length 2000, epsilon 0.07643719487931219, time 728.0, rides 125\n",
      "Initial State is  [3, 7, 5]\n",
      "episode 2570, reward 711.0, memory_length 2000, epsilon 0.07636075768443289, time 721.0, rides 118\n",
      "Initial State is  [4, 22, 2]\n",
      "episode 2571, reward 886.0, memory_length 2000, epsilon 0.07628439692674846, time 730.0, rides 128\n",
      "Initial State is  [3, 5, 3]\n",
      "episode 2572, reward 761.0, memory_length 2000, epsilon 0.07620811252982171, time 727.0, rides 118\n",
      "Initial State is  [2, 22, 4]\n",
      "episode 2573, reward 1028.0, memory_length 2000, epsilon 0.07613190441729188, time 727.0, rides 131\n",
      "Initial State is  [3, 0, 5]\n",
      "episode 2574, reward 915.0, memory_length 2000, epsilon 0.07605577251287458, time 729.0, rides 116\n",
      "Initial State is  [3, 10, 1]\n",
      "episode 2575, reward 940.0, memory_length 2000, epsilon 0.0759797167403617, time 728.0, rides 131\n",
      "Initial State is  [3, 10, 1]\n",
      "episode 2576, reward 790.0, memory_length 2000, epsilon 0.07590373702362134, time 726.0, rides 129\n",
      "Initial State is  [2, 14, 6]\n",
      "episode 2577, reward 850.0, memory_length 2000, epsilon 0.07582783328659772, time 729.0, rides 134\n",
      "Initial State is  [0, 10, 5]\n",
      "episode 2578, reward 766.0, memory_length 2000, epsilon 0.07575200545331112, time 725.0, rides 132\n",
      "Initial State is  [0, 17, 5]\n",
      "episode 2579, reward 857.0, memory_length 2000, epsilon 0.0756762534478578, time 736.0, rides 118\n",
      "Initial State is  [4, 12, 5]\n",
      "episode 2580, reward 785.0, memory_length 2000, epsilon 0.07560057719440995, time 729.0, rides 130\n",
      "Initial State is  [2, 7, 6]\n",
      "episode 2581, reward 579.0, memory_length 2000, epsilon 0.07552497661721554, time 731.0, rides 117\n",
      "Initial State is  [2, 18, 4]\n",
      "episode 2582, reward 785.0, memory_length 2000, epsilon 0.07544945164059833, time 725.0, rides 131\n",
      "Initial State is  [4, 22, 6]\n",
      "episode 2583, reward 686.0, memory_length 2000, epsilon 0.07537400218895772, time 731.0, rides 119\n",
      "Initial State is  [1, 6, 2]\n",
      "episode 2584, reward 593.0, memory_length 2000, epsilon 0.07529862818676876, time 733.0, rides 112\n",
      "Initial State is  [4, 17, 4]\n",
      "episode 2585, reward 636.0, memory_length 2000, epsilon 0.07522332955858199, time 730.0, rides 133\n",
      "Initial State is  [4, 4, 0]\n",
      "episode 2586, reward 673.0, memory_length 2000, epsilon 0.0751481062290234, time 722.0, rides 119\n",
      "Initial State is  [4, 10, 1]\n",
      "episode 2587, reward 886.0, memory_length 2000, epsilon 0.07507295812279438, time 731.0, rides 139\n",
      "Initial State is  [0, 16, 5]\n",
      "episode 2588, reward 644.0, memory_length 2000, epsilon 0.07499788516467158, time 728.0, rides 139\n",
      "Initial State is  [2, 12, 6]\n",
      "episode 2589, reward 850.0, memory_length 2000, epsilon 0.07492288727950691, time 734.0, rides 118\n",
      "Initial State is  [4, 4, 6]\n",
      "episode 2590, reward 875.0, memory_length 2000, epsilon 0.0748479643922274, time 735.0, rides 122\n",
      "Initial State is  [2, 21, 5]\n",
      "episode 2591, reward 962.0, memory_length 2000, epsilon 0.07477311642783518, time 730.0, rides 132\n",
      "Initial State is  [2, 10, 1]\n",
      "episode 2592, reward 886.0, memory_length 2000, epsilon 0.07469834331140734, time 721.0, rides 124\n",
      "Initial State is  [3, 21, 6]\n",
      "episode 2593, reward 629.0, memory_length 2000, epsilon 0.07462364496809594, time 730.0, rides 129\n",
      "Initial State is  [4, 17, 2]\n",
      "episode 2594, reward 735.0, memory_length 2000, epsilon 0.07454902132312784, time 722.0, rides 128\n",
      "Initial State is  [2, 12, 3]\n",
      "episode 2595, reward 783.0, memory_length 2000, epsilon 0.0744744723018047, time 724.0, rides 115\n",
      "Initial State is  [2, 0, 0]\n",
      "episode 2596, reward 849.0, memory_length 2000, epsilon 0.0743999978295029, time 724.0, rides 126\n",
      "Initial State is  [0, 19, 6]\n",
      "episode 2597, reward 608.0, memory_length 2000, epsilon 0.0743255978316734, time 729.0, rides 129\n",
      "Initial State is  [3, 5, 5]\n",
      "episode 2598, reward 1017.0, memory_length 2000, epsilon 0.07425127223384173, time 743.0, rides 138\n",
      "Initial State is  [2, 12, 1]\n",
      "episode 2599, reward 780.0, memory_length 2000, epsilon 0.07417702096160789, time 733.0, rides 125\n",
      "Initial State is  [3, 21, 2]\n",
      "episode 2600, reward 845.0, memory_length 2000, epsilon 0.07410284394064628, time 730.0, rides 122\n",
      "Initial State is  [3, 14, 6]\n",
      "episode 2601, reward 749.0, memory_length 2000, epsilon 0.07402874109670564, time 724.0, rides 119\n",
      "Initial State is  [2, 15, 5]\n",
      "episode 2602, reward 963.0, memory_length 2000, epsilon 0.07395471235560894, time 731.0, rides 125\n",
      "Initial State is  [4, 4, 5]\n",
      "episode 2603, reward 847.0, memory_length 2000, epsilon 0.07388075764325333, time 734.0, rides 121\n",
      "Initial State is  [0, 15, 6]\n",
      "episode 2604, reward 681.0, memory_length 2000, epsilon 0.07380687688561008, time 721.0, rides 122\n",
      "Initial State is  [4, 14, 1]\n",
      "episode 2605, reward 934.0, memory_length 2000, epsilon 0.07373307000872446, time 729.0, rides 125\n",
      "Initial State is  [4, 3, 1]\n",
      "episode 2606, reward 1081.0, memory_length 2000, epsilon 0.07365933693871574, time 722.0, rides 147\n",
      "Initial State is  [3, 19, 5]\n",
      "episode 2607, reward 471.0, memory_length 2000, epsilon 0.07358567760177702, time 724.0, rides 134\n",
      "Initial State is  [3, 11, 0]\n",
      "episode 2608, reward 739.0, memory_length 2000, epsilon 0.07351209192417524, time 731.0, rides 126\n",
      "Initial State is  [2, 5, 2]\n",
      "episode 2609, reward 1215.0, memory_length 2000, epsilon 0.07343857983225106, time 731.0, rides 140\n",
      "Initial State is  [3, 9, 0]\n",
      "episode 2610, reward 432.0, memory_length 2000, epsilon 0.07336514125241882, time 724.0, rides 146\n",
      "Initial State is  [0, 9, 0]\n",
      "episode 2611, reward 500.0, memory_length 2000, epsilon 0.0732917761111664, time 728.0, rides 121\n",
      "Initial State is  [0, 8, 6]\n",
      "episode 2612, reward 749.0, memory_length 2000, epsilon 0.07321848433505523, time 728.0, rides 127\n",
      "Initial State is  [0, 21, 0]\n",
      "episode 2613, reward 845.0, memory_length 2000, epsilon 0.07314526585072018, time 725.0, rides 135\n",
      "Initial State is  [1, 4, 3]\n",
      "episode 2614, reward 958.0, memory_length 2000, epsilon 0.07307212058486945, time 728.0, rides 132\n",
      "Initial State is  [4, 7, 1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 2615, reward 896.0, memory_length 2000, epsilon 0.07299904846428458, time 727.0, rides 125\n",
      "Initial State is  [3, 19, 5]\n",
      "episode 2616, reward 855.0, memory_length 2000, epsilon 0.0729260494158203, time 730.0, rides 126\n",
      "Initial State is  [3, 21, 5]\n",
      "episode 2617, reward 891.0, memory_length 2000, epsilon 0.07285312336640448, time 737.0, rides 143\n",
      "Initial State is  [2, 6, 6]\n",
      "episode 2618, reward 528.0, memory_length 2000, epsilon 0.07278027024303808, time 728.0, rides 130\n",
      "Initial State is  [2, 20, 5]\n",
      "episode 2619, reward 847.0, memory_length 2000, epsilon 0.07270748997279504, time 724.0, rides 150\n",
      "Initial State is  [2, 4, 4]\n",
      "episode 2620, reward 1077.0, memory_length 2000, epsilon 0.07263478248282225, time 722.0, rides 135\n",
      "Initial State is  [3, 10, 6]\n",
      "episode 2621, reward 1032.0, memory_length 2000, epsilon 0.07256214770033943, time 727.0, rides 129\n",
      "Initial State is  [2, 15, 1]\n",
      "episode 2622, reward 798.0, memory_length 2000, epsilon 0.0724895855526391, time 730.0, rides 133\n",
      "Initial State is  [3, 13, 2]\n",
      "episode 2623, reward 672.0, memory_length 2000, epsilon 0.07241709596708645, time 723.0, rides 135\n",
      "Initial State is  [1, 7, 2]\n",
      "episode 2624, reward 766.0, memory_length 2000, epsilon 0.07234467887111937, time 724.0, rides 121\n",
      "Initial State is  [0, 3, 5]\n",
      "episode 2625, reward 1169.0, memory_length 2000, epsilon 0.07227233419224825, time 727.0, rides 125\n",
      "Initial State is  [1, 2, 1]\n",
      "episode 2626, reward 1230.0, memory_length 2000, epsilon 0.07220006185805601, time 731.0, rides 132\n",
      "Initial State is  [0, 3, 0]\n",
      "episode 2627, reward 895.0, memory_length 2000, epsilon 0.07212786179619796, time 730.0, rides 117\n",
      "Initial State is  [2, 0, 6]\n",
      "episode 2628, reward 818.0, memory_length 2000, epsilon 0.07205573393440176, time 734.0, rides 124\n",
      "Initial State is  [0, 18, 1]\n",
      "episode 2629, reward 676.0, memory_length 2000, epsilon 0.07198367820046736, time 731.0, rides 125\n",
      "Initial State is  [3, 2, 6]\n",
      "episode 2630, reward 952.0, memory_length 2000, epsilon 0.0719116945222669, time 726.0, rides 116\n",
      "Initial State is  [0, 15, 4]\n",
      "episode 2631, reward 799.0, memory_length 2000, epsilon 0.07183978282774463, time 727.0, rides 126\n",
      "Initial State is  [1, 20, 5]\n",
      "episode 2632, reward 628.0, memory_length 2000, epsilon 0.07176794304491688, time 730.0, rides 130\n",
      "Initial State is  [4, 4, 4]\n",
      "episode 2633, reward 903.0, memory_length 2000, epsilon 0.07169617510187196, time 732.0, rides 130\n",
      "Initial State is  [2, 4, 5]\n",
      "episode 2634, reward 1023.0, memory_length 2000, epsilon 0.07162447892677008, time 728.0, rides 116\n",
      "Initial State is  [4, 19, 3]\n",
      "episode 2635, reward 775.0, memory_length 2000, epsilon 0.07155285444784332, time 727.0, rides 129\n",
      "Initial State is  [3, 23, 3]\n",
      "episode 2636, reward 828.0, memory_length 2000, epsilon 0.07148130159339547, time 725.0, rides 114\n",
      "Initial State is  [3, 20, 5]\n",
      "episode 2637, reward 599.0, memory_length 2000, epsilon 0.07140982029180207, time 736.0, rides 115\n",
      "Initial State is  [0, 18, 4]\n",
      "episode 2638, reward 726.0, memory_length 2000, epsilon 0.07133841047151027, time 730.0, rides 130\n",
      "Initial State is  [4, 18, 0]\n",
      "episode 2639, reward 1082.0, memory_length 2000, epsilon 0.07126707206103876, time 730.0, rides 147\n",
      "Initial State is  [3, 16, 2]\n",
      "episode 2640, reward 962.0, memory_length 2000, epsilon 0.07119580498897772, time 737.0, rides 130\n",
      "Initial State is  [1, 8, 6]\n",
      "episode 2641, reward 426.0, memory_length 2000, epsilon 0.07112460918398875, time 727.0, rides 131\n",
      "Initial State is  [0, 16, 4]\n",
      "episode 2642, reward 920.0, memory_length 2000, epsilon 0.07105348457480476, time 731.0, rides 132\n",
      "Initial State is  [1, 18, 3]\n",
      "episode 2643, reward 904.0, memory_length 2000, epsilon 0.07098243109022996, time 723.0, rides 142\n",
      "Initial State is  [4, 18, 2]\n",
      "episode 2644, reward 982.0, memory_length 2000, epsilon 0.07091144865913973, time 734.0, rides 127\n",
      "Initial State is  [2, 4, 0]\n",
      "episode 2645, reward 865.0, memory_length 2000, epsilon 0.07084053721048059, time 726.0, rides 138\n",
      "Initial State is  [4, 1, 0]\n",
      "episode 2646, reward 796.0, memory_length 2000, epsilon 0.0707696966732701, time 728.0, rides 123\n",
      "Initial State is  [1, 13, 1]\n",
      "episode 2647, reward 875.0, memory_length 2000, epsilon 0.07069892697659683, time 728.0, rides 127\n",
      "Initial State is  [1, 18, 2]\n",
      "episode 2648, reward 704.0, memory_length 2000, epsilon 0.07062822804962023, time 731.0, rides 137\n",
      "Initial State is  [0, 20, 2]\n",
      "episode 2649, reward 878.0, memory_length 2000, epsilon 0.0705575998215706, time 729.0, rides 143\n",
      "Initial State is  [1, 23, 4]\n",
      "episode 2650, reward 662.0, memory_length 2000, epsilon 0.07048704222174904, time 724.0, rides 133\n",
      "Initial State is  [0, 13, 6]\n",
      "episode 2651, reward 776.0, memory_length 2000, epsilon 0.07041655517952729, time 724.0, rides 125\n",
      "Initial State is  [1, 1, 6]\n",
      "episode 2652, reward 916.0, memory_length 2000, epsilon 0.07034613862434776, time 727.0, rides 153\n",
      "Initial State is  [3, 23, 1]\n",
      "episode 2653, reward 852.0, memory_length 2000, epsilon 0.07027579248572341, time 731.0, rides 136\n",
      "Initial State is  [1, 22, 3]\n",
      "episode 2654, reward 748.0, memory_length 2000, epsilon 0.07020551669323769, time 728.0, rides 126\n",
      "Initial State is  [2, 16, 2]\n",
      "episode 2655, reward 781.0, memory_length 2000, epsilon 0.07013531117654445, time 733.0, rides 127\n",
      "Initial State is  [4, 6, 4]\n",
      "episode 2656, reward 1091.0, memory_length 2000, epsilon 0.07006517586536791, time 726.0, rides 141\n",
      "Initial State is  [1, 11, 3]\n",
      "episode 2657, reward 614.0, memory_length 2000, epsilon 0.06999511068950254, time 723.0, rides 129\n",
      "Initial State is  [0, 4, 2]\n",
      "episode 2658, reward 853.0, memory_length 2000, epsilon 0.06992511557881304, time 732.0, rides 126\n",
      "Initial State is  [1, 5, 0]\n",
      "episode 2659, reward 840.0, memory_length 2000, epsilon 0.06985519046323423, time 725.0, rides 134\n",
      "Initial State is  [1, 11, 0]\n",
      "episode 2660, reward 812.0, memory_length 2000, epsilon 0.06978533527277099, time 728.0, rides 131\n",
      "Initial State is  [1, 20, 0]\n",
      "episode 2661, reward 609.0, memory_length 2000, epsilon 0.06971554993749822, time 728.0, rides 121\n",
      "Initial State is  [2, 19, 2]\n",
      "episode 2662, reward 979.0, memory_length 2000, epsilon 0.06964583438756072, time 734.0, rides 120\n",
      "Initial State is  [1, 7, 4]\n",
      "episode 2663, reward 1080.0, memory_length 2000, epsilon 0.06957618855317316, time 731.0, rides 123\n",
      "Initial State is  [4, 1, 4]\n",
      "episode 2664, reward 617.0, memory_length 2000, epsilon 0.06950661236461998, time 727.0, rides 128\n",
      "Initial State is  [2, 15, 3]\n",
      "episode 2665, reward 642.0, memory_length 2000, epsilon 0.06943710575225537, time 728.0, rides 148\n",
      "Initial State is  [2, 8, 4]\n",
      "episode 2666, reward 984.0, memory_length 2000, epsilon 0.06936766864650311, time 725.0, rides 124\n",
      "Initial State is  [4, 2, 6]\n",
      "episode 2667, reward 759.0, memory_length 2000, epsilon 0.06929830097785661, time 733.0, rides 113\n",
      "Initial State is  [3, 9, 2]\n",
      "episode 2668, reward 825.0, memory_length 2000, epsilon 0.06922900267687875, time 735.0, rides 137\n",
      "Initial State is  [3, 4, 2]\n",
      "episode 2669, reward 823.0, memory_length 2000, epsilon 0.06915977367420187, time 727.0, rides 132\n",
      "Initial State is  [1, 7, 1]\n",
      "episode 2670, reward 534.0, memory_length 2000, epsilon 0.06909061390052766, time 726.0, rides 133\n",
      "Initial State is  [0, 5, 6]\n",
      "episode 2671, reward 925.0, memory_length 2000, epsilon 0.06902152328662714, time 727.0, rides 121\n",
      "Initial State is  [2, 13, 4]\n",
      "episode 2672, reward 961.0, memory_length 2000, epsilon 0.06895250176334052, time 733.0, rides 138\n",
      "Initial State is  [4, 8, 1]\n",
      "episode 2673, reward 748.0, memory_length 2000, epsilon 0.06888354926157718, time 744.0, rides 125\n",
      "Initial State is  [1, 2, 3]\n",
      "episode 2674, reward 715.0, memory_length 2000, epsilon 0.0688146657123156, time 731.0, rides 148\n",
      "Initial State is  [2, 17, 5]\n",
      "episode 2675, reward 1077.0, memory_length 2000, epsilon 0.06874585104660329, time 724.0, rides 131\n",
      "Initial State is  [3, 13, 1]\n",
      "episode 2676, reward 659.0, memory_length 2000, epsilon 0.06867710519555668, time 733.0, rides 133\n",
      "Initial State is  [2, 13, 2]\n",
      "episode 2677, reward 727.0, memory_length 2000, epsilon 0.06860842809036113, time 730.0, rides 125\n",
      "Initial State is  [4, 23, 1]\n",
      "episode 2678, reward 945.0, memory_length 2000, epsilon 0.06853981966227077, time 726.0, rides 137\n",
      "Initial State is  [3, 16, 0]\n",
      "episode 2679, reward 1161.0, memory_length 2000, epsilon 0.06847127984260849, time 729.0, rides 134\n",
      "Initial State is  [0, 19, 1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 2680, reward 866.0, memory_length 2000, epsilon 0.06840280856276588, time 722.0, rides 119\n",
      "Initial State is  [0, 6, 4]\n",
      "episode 2681, reward 624.0, memory_length 2000, epsilon 0.06833440575420312, time 734.0, rides 142\n",
      "Initial State is  [0, 12, 6]\n",
      "episode 2682, reward 885.0, memory_length 2000, epsilon 0.06826607134844892, time 733.0, rides 138\n",
      "Initial State is  [2, 6, 3]\n",
      "episode 2683, reward 775.0, memory_length 2000, epsilon 0.06819780527710047, time 733.0, rides 139\n",
      "Initial State is  [3, 7, 5]\n",
      "episode 2684, reward 727.0, memory_length 2000, epsilon 0.06812960747182337, time 731.0, rides 127\n",
      "Initial State is  [2, 9, 4]\n",
      "episode 2685, reward 585.0, memory_length 2000, epsilon 0.06806147786435154, time 729.0, rides 139\n",
      "Initial State is  [2, 22, 5]\n",
      "episode 2686, reward 775.0, memory_length 2000, epsilon 0.06799341638648719, time 733.0, rides 130\n",
      "Initial State is  [1, 16, 2]\n",
      "episode 2687, reward 780.0, memory_length 2000, epsilon 0.06792542297010071, time 734.0, rides 135\n",
      "Initial State is  [2, 8, 6]\n",
      "episode 2688, reward 767.0, memory_length 2000, epsilon 0.06785749754713061, time 731.0, rides 113\n",
      "Initial State is  [2, 17, 2]\n",
      "episode 2689, reward 924.0, memory_length 2000, epsilon 0.06778964004958347, time 732.0, rides 136\n",
      "Initial State is  [2, 9, 2]\n",
      "episode 2690, reward 727.0, memory_length 2000, epsilon 0.06772185040953389, time 736.0, rides 126\n",
      "Initial State is  [2, 9, 3]\n",
      "episode 2691, reward 1076.0, memory_length 2000, epsilon 0.06765412855912437, time 725.0, rides 123\n",
      "Initial State is  [1, 11, 5]\n",
      "episode 2692, reward 1124.0, memory_length 2000, epsilon 0.06758647443056524, time 725.0, rides 140\n",
      "Initial State is  [4, 8, 4]\n",
      "episode 2693, reward 1132.0, memory_length 2000, epsilon 0.06751888795613468, time 728.0, rides 132\n",
      "Initial State is  [0, 23, 5]\n",
      "episode 2694, reward 855.0, memory_length 2000, epsilon 0.06745136906817856, time 725.0, rides 132\n",
      "Initial State is  [3, 11, 5]\n",
      "episode 2695, reward 1003.0, memory_length 2000, epsilon 0.06738391769911038, time 740.0, rides 139\n",
      "Initial State is  [2, 6, 3]\n",
      "episode 2696, reward 1023.0, memory_length 2000, epsilon 0.06731653378141127, time 733.0, rides 127\n",
      "Initial State is  [3, 0, 1]\n",
      "episode 2697, reward 955.0, memory_length 2000, epsilon 0.06724921724762986, time 730.0, rides 138\n",
      "Initial State is  [4, 19, 4]\n",
      "episode 2698, reward 867.0, memory_length 2000, epsilon 0.06718196803038223, time 724.0, rides 129\n",
      "Initial State is  [4, 2, 1]\n",
      "episode 2699, reward 695.0, memory_length 2000, epsilon 0.06711478606235186, time 727.0, rides 120\n",
      "Initial State is  [3, 20, 5]\n",
      "episode 2700, reward 903.0, memory_length 2000, epsilon 0.06704767127628951, time 730.0, rides 142\n",
      "Initial State is  [2, 0, 0]\n",
      "episode 2701, reward 1182.0, memory_length 2000, epsilon 0.06698062360501322, time 728.0, rides 135\n",
      "Initial State is  [0, 9, 6]\n",
      "episode 2702, reward 768.0, memory_length 2000, epsilon 0.06691364298140821, time 728.0, rides 128\n",
      "Initial State is  [2, 11, 0]\n",
      "episode 2703, reward 1043.0, memory_length 2000, epsilon 0.06684672933842681, time 728.0, rides 128\n",
      "Initial State is  [2, 17, 5]\n",
      "episode 2704, reward 553.0, memory_length 2000, epsilon 0.06677988260908838, time 723.0, rides 132\n",
      "Initial State is  [0, 23, 1]\n",
      "episode 2705, reward 808.0, memory_length 2000, epsilon 0.06671310272647929, time 729.0, rides 136\n",
      "Initial State is  [4, 0, 4]\n",
      "episode 2706, reward 778.0, memory_length 2000, epsilon 0.06664638962375281, time 725.0, rides 136\n",
      "Initial State is  [2, 14, 1]\n",
      "episode 2707, reward 836.0, memory_length 2000, epsilon 0.06657974323412906, time 727.0, rides 143\n",
      "Initial State is  [1, 13, 1]\n",
      "episode 2708, reward 784.0, memory_length 2000, epsilon 0.06651316349089494, time 731.0, rides 144\n",
      "Initial State is  [2, 5, 5]\n",
      "episode 2709, reward 1036.0, memory_length 2000, epsilon 0.06644665032740404, time 729.0, rides 124\n",
      "Initial State is  [0, 5, 4]\n",
      "episode 2710, reward 557.0, memory_length 2000, epsilon 0.06638020367707664, time 727.0, rides 122\n",
      "Initial State is  [1, 23, 6]\n",
      "episode 2711, reward 773.0, memory_length 2000, epsilon 0.06631382347339956, time 728.0, rides 133\n",
      "Initial State is  [4, 1, 5]\n",
      "episode 2712, reward 771.0, memory_length 2000, epsilon 0.06624750964992616, time 737.0, rides 138\n",
      "Initial State is  [3, 2, 1]\n",
      "episode 2713, reward 899.0, memory_length 2000, epsilon 0.06618126214027624, time 728.0, rides 124\n",
      "Initial State is  [4, 2, 0]\n",
      "episode 2714, reward 958.0, memory_length 2000, epsilon 0.06611508087813596, time 731.0, rides 126\n",
      "Initial State is  [4, 11, 2]\n",
      "episode 2715, reward 982.0, memory_length 2000, epsilon 0.06604896579725783, time 723.0, rides 131\n",
      "Initial State is  [2, 13, 6]\n",
      "episode 2716, reward 1108.0, memory_length 2000, epsilon 0.06598291683146057, time 723.0, rides 124\n",
      "Initial State is  [3, 13, 2]\n",
      "episode 2717, reward 1196.0, memory_length 2000, epsilon 0.0659169339146291, time 730.0, rides 129\n",
      "Initial State is  [0, 3, 6]\n",
      "episode 2718, reward 904.0, memory_length 2000, epsilon 0.06585101698071448, time 727.0, rides 137\n",
      "Initial State is  [2, 19, 5]\n",
      "episode 2719, reward 708.0, memory_length 2000, epsilon 0.06578516596373377, time 727.0, rides 111\n",
      "Initial State is  [1, 15, 2]\n",
      "episode 2720, reward 668.0, memory_length 2000, epsilon 0.06571938079777004, time 732.0, rides 120\n",
      "Initial State is  [4, 16, 3]\n",
      "episode 2721, reward 724.0, memory_length 2000, epsilon 0.06565366141697226, time 729.0, rides 127\n",
      "Initial State is  [2, 5, 1]\n",
      "episode 2722, reward 708.0, memory_length 2000, epsilon 0.06558800775555529, time 730.0, rides 134\n",
      "Initial State is  [4, 1, 5]\n",
      "episode 2723, reward 993.0, memory_length 2000, epsilon 0.06552241974779974, time 730.0, rides 128\n",
      "Initial State is  [1, 5, 2]\n",
      "episode 2724, reward 578.0, memory_length 2000, epsilon 0.06545689732805193, time 723.0, rides 121\n",
      "Initial State is  [1, 6, 0]\n",
      "episode 2725, reward 779.0, memory_length 2000, epsilon 0.06539144043072388, time 723.0, rides 138\n",
      "Initial State is  [0, 19, 1]\n",
      "episode 2726, reward 872.0, memory_length 2000, epsilon 0.06532604899029316, time 727.0, rides 135\n",
      "Initial State is  [2, 7, 1]\n",
      "episode 2727, reward 660.0, memory_length 2000, epsilon 0.06526072294130288, time 723.0, rides 129\n",
      "Initial State is  [0, 12, 4]\n",
      "episode 2728, reward 880.0, memory_length 2000, epsilon 0.06519546221836157, time 741.0, rides 123\n",
      "Initial State is  [2, 18, 5]\n",
      "episode 2729, reward 581.0, memory_length 2000, epsilon 0.06513026675614321, time 735.0, rides 117\n",
      "Initial State is  [1, 14, 3]\n",
      "episode 2730, reward 853.0, memory_length 2000, epsilon 0.06506513648938707, time 736.0, rides 130\n",
      "Initial State is  [2, 13, 4]\n",
      "episode 2731, reward 667.0, memory_length 2000, epsilon 0.06500007135289768, time 739.0, rides 114\n",
      "Initial State is  [3, 12, 2]\n",
      "episode 2732, reward 596.0, memory_length 2000, epsilon 0.06493507128154478, time 726.0, rides 132\n",
      "Initial State is  [2, 23, 0]\n",
      "episode 2733, reward 907.0, memory_length 2000, epsilon 0.06487013621026325, time 725.0, rides 122\n",
      "Initial State is  [1, 10, 3]\n",
      "episode 2734, reward 1171.0, memory_length 2000, epsilon 0.06480526607405299, time 725.0, rides 141\n",
      "Initial State is  [2, 2, 6]\n",
      "episode 2735, reward 968.0, memory_length 2000, epsilon 0.06474046080797893, time 725.0, rides 127\n",
      "Initial State is  [4, 3, 6]\n",
      "episode 2736, reward 868.0, memory_length 2000, epsilon 0.06467572034717095, time 726.0, rides 132\n",
      "Initial State is  [1, 21, 1]\n",
      "episode 2737, reward 785.0, memory_length 2000, epsilon 0.06461104462682378, time 723.0, rides 129\n",
      "Initial State is  [3, 17, 3]\n",
      "episode 2738, reward 713.0, memory_length 2000, epsilon 0.06454643358219696, time 727.0, rides 133\n",
      "Initial State is  [4, 14, 4]\n",
      "episode 2739, reward 554.0, memory_length 2000, epsilon 0.06448188714861476, time 726.0, rides 130\n",
      "Initial State is  [2, 16, 6]\n",
      "episode 2740, reward 685.0, memory_length 2000, epsilon 0.06441740526146615, time 730.0, rides 126\n",
      "Initial State is  [3, 4, 6]\n",
      "episode 2741, reward 987.0, memory_length 2000, epsilon 0.06435298785620469, time 731.0, rides 122\n",
      "Initial State is  [4, 3, 2]\n",
      "episode 2742, reward 1102.0, memory_length 2000, epsilon 0.06428863486834849, time 734.0, rides 117\n",
      "Initial State is  [0, 0, 3]\n",
      "episode 2743, reward 861.0, memory_length 2000, epsilon 0.06422434623348014, time 729.0, rides 135\n",
      "Initial State is  [0, 2, 3]\n",
      "episode 2744, reward 952.0, memory_length 2000, epsilon 0.06416012188724667, time 728.0, rides 138\n",
      "Initial State is  [1, 0, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 2745, reward 919.0, memory_length 2000, epsilon 0.06409596176535942, time 725.0, rides 133\n",
      "Initial State is  [0, 1, 6]\n",
      "episode 2746, reward 939.0, memory_length 2000, epsilon 0.06403186580359406, time 725.0, rides 130\n",
      "Initial State is  [0, 12, 0]\n",
      "episode 2747, reward 915.0, memory_length 2000, epsilon 0.06396783393779047, time 728.0, rides 126\n",
      "Initial State is  [2, 8, 4]\n",
      "episode 2748, reward 809.0, memory_length 2000, epsilon 0.06390386610385268, time 732.0, rides 119\n",
      "Initial State is  [2, 20, 4]\n",
      "episode 2749, reward 808.0, memory_length 2000, epsilon 0.06383996223774882, time 725.0, rides 127\n",
      "Initial State is  [4, 0, 0]\n",
      "episode 2750, reward 803.0, memory_length 2000, epsilon 0.06377612227551108, time 733.0, rides 144\n",
      "Initial State is  [0, 9, 3]\n",
      "episode 2751, reward 804.0, memory_length 2000, epsilon 0.06371234615323557, time 726.0, rides 144\n",
      "Initial State is  [2, 20, 6]\n",
      "episode 2752, reward 846.0, memory_length 2000, epsilon 0.06364863380708233, time 732.0, rides 131\n",
      "Initial State is  [4, 13, 5]\n",
      "episode 2753, reward 806.0, memory_length 2000, epsilon 0.06358498517327525, time 729.0, rides 136\n",
      "Initial State is  [0, 16, 4]\n",
      "episode 2754, reward 726.0, memory_length 2000, epsilon 0.06352140018810197, time 726.0, rides 127\n",
      "Initial State is  [2, 18, 2]\n",
      "episode 2755, reward 850.0, memory_length 2000, epsilon 0.06345787878791387, time 729.0, rides 125\n",
      "Initial State is  [4, 16, 3]\n",
      "episode 2756, reward 860.0, memory_length 2000, epsilon 0.06339442090912595, time 725.0, rides 116\n",
      "Initial State is  [0, 15, 5]\n",
      "episode 2757, reward 787.0, memory_length 2000, epsilon 0.06333102648821683, time 728.0, rides 125\n",
      "Initial State is  [0, 20, 4]\n",
      "episode 2758, reward 768.0, memory_length 2000, epsilon 0.06326769546172861, time 730.0, rides 126\n",
      "Initial State is  [4, 18, 2]\n",
      "episode 2759, reward 711.0, memory_length 2000, epsilon 0.06320442776626688, time 728.0, rides 135\n",
      "Initial State is  [3, 17, 3]\n",
      "episode 2760, reward 1223.0, memory_length 2000, epsilon 0.06314122333850061, time 731.0, rides 145\n",
      "Initial State is  [3, 12, 2]\n",
      "episode 2761, reward 848.0, memory_length 2000, epsilon 0.06307808211516211, time 728.0, rides 121\n",
      "Initial State is  [0, 3, 5]\n",
      "episode 2762, reward 598.0, memory_length 2000, epsilon 0.06301500403304695, time 727.0, rides 132\n",
      "Initial State is  [4, 23, 0]\n",
      "episode 2763, reward 1087.0, memory_length 2000, epsilon 0.0629519890290139, time 731.0, rides 131\n",
      "Initial State is  [1, 10, 6]\n",
      "episode 2764, reward 973.0, memory_length 2000, epsilon 0.06288903703998489, time 727.0, rides 140\n",
      "Initial State is  [2, 7, 0]\n",
      "episode 2765, reward 796.0, memory_length 2000, epsilon 0.0628261480029449, time 727.0, rides 136\n",
      "Initial State is  [2, 4, 4]\n",
      "episode 2766, reward 963.0, memory_length 2000, epsilon 0.06276332185494196, time 727.0, rides 141\n",
      "Initial State is  [3, 1, 5]\n",
      "episode 2767, reward 840.0, memory_length 2000, epsilon 0.06270055853308702, time 723.0, rides 130\n",
      "Initial State is  [0, 2, 5]\n",
      "episode 2768, reward 1067.0, memory_length 2000, epsilon 0.06263785797455393, time 730.0, rides 132\n",
      "Initial State is  [4, 16, 4]\n",
      "episode 2769, reward 983.0, memory_length 2000, epsilon 0.06257522011657937, time 728.0, rides 126\n",
      "Initial State is  [1, 17, 2]\n",
      "episode 2770, reward 608.0, memory_length 2000, epsilon 0.0625126448964628, time 723.0, rides 117\n",
      "Initial State is  [2, 20, 6]\n",
      "episode 2771, reward 1265.0, memory_length 2000, epsilon 0.062450132251566336, time 728.0, rides 133\n",
      "Initial State is  [3, 14, 1]\n",
      "episode 2772, reward 923.0, memory_length 2000, epsilon 0.062387682119314766, time 731.0, rides 123\n",
      "Initial State is  [2, 17, 6]\n",
      "episode 2773, reward 643.0, memory_length 2000, epsilon 0.06232529443719545, time 735.0, rides 134\n",
      "Initial State is  [2, 5, 0]\n",
      "episode 2774, reward 757.0, memory_length 2000, epsilon 0.06226296914275825, time 723.0, rides 124\n",
      "Initial State is  [4, 10, 2]\n",
      "episode 2775, reward 893.0, memory_length 2000, epsilon 0.06220070617361549, time 724.0, rides 129\n",
      "Initial State is  [2, 21, 4]\n",
      "episode 2776, reward 848.0, memory_length 2000, epsilon 0.062138505467441874, time 734.0, rides 129\n",
      "Initial State is  [4, 16, 1]\n",
      "episode 2777, reward 801.0, memory_length 2000, epsilon 0.06207636696197443, time 724.0, rides 118\n",
      "Initial State is  [0, 20, 3]\n",
      "episode 2778, reward 659.0, memory_length 2000, epsilon 0.062014290595012456, time 735.0, rides 119\n",
      "Initial State is  [4, 11, 5]\n",
      "episode 2779, reward 978.0, memory_length 2000, epsilon 0.06195227630441744, time 730.0, rides 130\n",
      "Initial State is  [0, 23, 4]\n",
      "episode 2780, reward 742.0, memory_length 2000, epsilon 0.06189032402811302, time 731.0, rides 130\n",
      "Initial State is  [1, 1, 2]\n",
      "episode 2781, reward 577.0, memory_length 2000, epsilon 0.06182843370408491, time 724.0, rides 121\n",
      "Initial State is  [0, 3, 3]\n",
      "episode 2782, reward 508.0, memory_length 2000, epsilon 0.061766605270380824, time 728.0, rides 134\n",
      "Initial State is  [3, 19, 6]\n",
      "episode 2783, reward 997.0, memory_length 2000, epsilon 0.061704838665110444, time 727.0, rides 139\n",
      "Initial State is  [2, 1, 5]\n",
      "episode 2784, reward 660.0, memory_length 2000, epsilon 0.06164313382644533, time 735.0, rides 133\n",
      "Initial State is  [1, 1, 4]\n",
      "episode 2785, reward 513.0, memory_length 2000, epsilon 0.06158149069261889, time 734.0, rides 126\n",
      "Initial State is  [0, 3, 5]\n",
      "episode 2786, reward 918.0, memory_length 2000, epsilon 0.06151990920192627, time 728.0, rides 127\n",
      "Initial State is  [4, 20, 1]\n",
      "episode 2787, reward 761.0, memory_length 2000, epsilon 0.06145838929272435, time 720.0, rides 126\n",
      "Initial State is  [2, 16, 6]\n",
      "episode 2788, reward 1041.0, memory_length 2000, epsilon 0.061396930903431624, time 733.0, rides 135\n",
      "Initial State is  [4, 11, 6]\n",
      "episode 2789, reward 537.0, memory_length 2000, epsilon 0.061335533972528195, time 729.0, rides 132\n",
      "Initial State is  [1, 10, 0]\n",
      "episode 2790, reward 869.0, memory_length 2000, epsilon 0.061274198438555666, time 726.0, rides 128\n",
      "Initial State is  [0, 4, 3]\n",
      "episode 2791, reward 817.0, memory_length 2000, epsilon 0.06121292424011711, time 731.0, rides 138\n",
      "Initial State is  [4, 21, 5]\n",
      "episode 2792, reward 752.0, memory_length 2000, epsilon 0.06115171131587699, time 724.0, rides 127\n",
      "Initial State is  [0, 9, 5]\n",
      "episode 2793, reward 501.0, memory_length 2000, epsilon 0.06109055960456111, time 723.0, rides 130\n",
      "Initial State is  [3, 4, 1]\n",
      "episode 2794, reward 865.0, memory_length 2000, epsilon 0.06102946904495655, time 737.0, rides 132\n",
      "Initial State is  [0, 8, 0]\n",
      "episode 2795, reward 861.0, memory_length 2000, epsilon 0.06096843957591159, time 729.0, rides 120\n",
      "Initial State is  [4, 10, 0]\n",
      "episode 2796, reward 807.0, memory_length 2000, epsilon 0.06090747113633568, time 726.0, rides 129\n",
      "Initial State is  [4, 10, 6]\n",
      "episode 2797, reward 740.0, memory_length 2000, epsilon 0.06084656366519934, time 732.0, rides 124\n",
      "Initial State is  [2, 10, 2]\n",
      "episode 2798, reward 726.0, memory_length 2000, epsilon 0.06078571710153414, time 728.0, rides 123\n",
      "Initial State is  [0, 18, 3]\n",
      "episode 2799, reward 760.0, memory_length 2000, epsilon 0.06072493138443261, time 733.0, rides 130\n",
      "Initial State is  [3, 6, 1]\n",
      "episode 2800, reward 987.0, memory_length 2000, epsilon 0.060664206453048174, time 722.0, rides 131\n",
      "Initial State is  [4, 10, 3]\n",
      "episode 2801, reward 998.0, memory_length 2000, epsilon 0.06060354224659512, time 729.0, rides 125\n",
      "Initial State is  [0, 20, 6]\n",
      "episode 2802, reward 899.0, memory_length 2000, epsilon 0.06054293870434853, time 732.0, rides 126\n",
      "Initial State is  [3, 20, 5]\n",
      "episode 2803, reward 646.0, memory_length 2000, epsilon 0.06048239576564418, time 725.0, rides 116\n",
      "Initial State is  [1, 12, 2]\n",
      "episode 2804, reward 1007.0, memory_length 2000, epsilon 0.06042191336987854, time 731.0, rides 138\n",
      "Initial State is  [0, 21, 3]\n",
      "episode 2805, reward 881.0, memory_length 2000, epsilon 0.06036149145650866, time 732.0, rides 116\n",
      "Initial State is  [4, 12, 3]\n",
      "episode 2806, reward 478.0, memory_length 2000, epsilon 0.06030112996505215, time 727.0, rides 121\n",
      "Initial State is  [2, 18, 5]\n",
      "episode 2807, reward 652.0, memory_length 2000, epsilon 0.0602408288350871, time 737.0, rides 131\n",
      "Initial State is  [4, 6, 3]\n",
      "episode 2808, reward 665.0, memory_length 2000, epsilon 0.06018058800625201, time 725.0, rides 128\n",
      "Initial State is  [3, 10, 3]\n",
      "episode 2809, reward 875.0, memory_length 2000, epsilon 0.06012040741824576, time 730.0, rides 132\n",
      "Initial State is  [3, 11, 2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 2810, reward 820.0, memory_length 2000, epsilon 0.06006028701082751, time 726.0, rides 127\n",
      "Initial State is  [3, 4, 5]\n",
      "episode 2811, reward 948.0, memory_length 2000, epsilon 0.060000226723816684, time 728.0, rides 119\n",
      "Initial State is  [1, 5, 4]\n",
      "episode 2812, reward 768.0, memory_length 2000, epsilon 0.059940226497092866, time 735.0, rides 135\n",
      "Initial State is  [2, 18, 4]\n",
      "episode 2813, reward 940.0, memory_length 2000, epsilon 0.05988028627059577, time 723.0, rides 126\n",
      "Initial State is  [2, 5, 4]\n",
      "episode 2814, reward 862.0, memory_length 2000, epsilon 0.05982040598432518, time 729.0, rides 144\n",
      "Initial State is  [3, 7, 6]\n",
      "episode 2815, reward 478.0, memory_length 2000, epsilon 0.05976058557834085, time 726.0, rides 124\n",
      "Initial State is  [0, 18, 6]\n",
      "episode 2816, reward 866.0, memory_length 2000, epsilon 0.05970082499276251, time 726.0, rides 132\n",
      "Initial State is  [0, 2, 5]\n",
      "episode 2817, reward 744.0, memory_length 2000, epsilon 0.059641124167769746, time 732.0, rides 128\n",
      "Initial State is  [0, 2, 0]\n",
      "episode 2818, reward 730.0, memory_length 2000, epsilon 0.059581483043601974, time 727.0, rides 131\n",
      "Initial State is  [3, 9, 1]\n",
      "episode 2819, reward 830.0, memory_length 2000, epsilon 0.05952190156055837, time 729.0, rides 138\n",
      "Initial State is  [1, 7, 4]\n",
      "episode 2820, reward 955.0, memory_length 2000, epsilon 0.05946237965899782, time 723.0, rides 135\n",
      "Initial State is  [0, 18, 3]\n",
      "episode 2821, reward 687.0, memory_length 2000, epsilon 0.05940291727933882, time 727.0, rides 120\n",
      "Initial State is  [2, 5, 4]\n",
      "episode 2822, reward 979.0, memory_length 2000, epsilon 0.05934351436205949, time 725.0, rides 137\n",
      "Initial State is  [4, 19, 4]\n",
      "episode 2823, reward 698.0, memory_length 2000, epsilon 0.05928417084769743, time 726.0, rides 123\n",
      "Initial State is  [0, 3, 1]\n",
      "episode 2824, reward 914.0, memory_length 2000, epsilon 0.05922488667684973, time 737.0, rides 137\n",
      "Initial State is  [2, 4, 1]\n",
      "episode 2825, reward 691.0, memory_length 2000, epsilon 0.05916566179017289, time 728.0, rides 136\n",
      "Initial State is  [2, 0, 3]\n",
      "episode 2826, reward 949.0, memory_length 2000, epsilon 0.059106496128382716, time 730.0, rides 137\n",
      "Initial State is  [3, 23, 4]\n",
      "episode 2827, reward 736.0, memory_length 2000, epsilon 0.05904738963225433, time 733.0, rides 128\n",
      "Initial State is  [1, 3, 4]\n",
      "episode 2828, reward 634.0, memory_length 2000, epsilon 0.058988342242622074, time 730.0, rides 129\n",
      "Initial State is  [1, 20, 0]\n",
      "episode 2829, reward 892.0, memory_length 2000, epsilon 0.058929353900379455, time 731.0, rides 129\n",
      "Initial State is  [2, 15, 4]\n",
      "episode 2830, reward 889.0, memory_length 2000, epsilon 0.058870424546479075, time 722.0, rides 137\n",
      "Initial State is  [3, 6, 1]\n",
      "episode 2831, reward 596.0, memory_length 2000, epsilon 0.0588115541219326, time 727.0, rides 132\n",
      "Initial State is  [1, 10, 6]\n",
      "episode 2832, reward 1030.0, memory_length 2000, epsilon 0.05875274256781066, time 733.0, rides 141\n",
      "Initial State is  [1, 20, 0]\n",
      "episode 2833, reward 679.0, memory_length 2000, epsilon 0.058693989825242855, time 734.0, rides 152\n",
      "Initial State is  [4, 13, 6]\n",
      "episode 2834, reward 878.0, memory_length 2000, epsilon 0.058635295835417614, time 732.0, rides 136\n",
      "Initial State is  [3, 19, 6]\n",
      "episode 2835, reward 935.0, memory_length 2000, epsilon 0.058576660539582194, time 730.0, rides 122\n",
      "Initial State is  [2, 21, 0]\n",
      "episode 2836, reward 811.0, memory_length 2000, epsilon 0.058518083879042615, time 726.0, rides 115\n",
      "Initial State is  [2, 17, 2]\n",
      "episode 2837, reward 732.0, memory_length 2000, epsilon 0.05845956579516357, time 725.0, rides 139\n",
      "Initial State is  [0, 11, 2]\n",
      "episode 2838, reward 606.0, memory_length 2000, epsilon 0.05840110622936841, time 730.0, rides 139\n",
      "Initial State is  [4, 10, 4]\n",
      "episode 2839, reward 862.0, memory_length 2000, epsilon 0.05834270512313904, time 725.0, rides 138\n",
      "Initial State is  [4, 3, 6]\n",
      "episode 2840, reward 783.0, memory_length 2000, epsilon 0.058284362418015906, time 725.0, rides 119\n",
      "Initial State is  [0, 6, 2]\n",
      "episode 2841, reward 782.0, memory_length 2000, epsilon 0.05822607805559789, time 732.0, rides 130\n",
      "Initial State is  [2, 22, 1]\n",
      "episode 2842, reward 812.0, memory_length 2000, epsilon 0.05816785197754229, time 725.0, rides 129\n",
      "Initial State is  [1, 17, 2]\n",
      "episode 2843, reward 915.0, memory_length 2000, epsilon 0.05810968412556475, time 728.0, rides 116\n",
      "Initial State is  [0, 11, 4]\n",
      "episode 2844, reward 916.0, memory_length 2000, epsilon 0.058051574441439185, time 733.0, rides 120\n",
      "Initial State is  [3, 20, 5]\n",
      "episode 2845, reward 1014.0, memory_length 2000, epsilon 0.05799352286699774, time 728.0, rides 135\n",
      "Initial State is  [4, 11, 2]\n",
      "episode 2846, reward 949.0, memory_length 2000, epsilon 0.057935529344130744, time 721.0, rides 132\n",
      "Initial State is  [4, 19, 1]\n",
      "episode 2847, reward 612.0, memory_length 2000, epsilon 0.05787759381478661, time 724.0, rides 123\n",
      "Initial State is  [4, 11, 3]\n",
      "episode 2848, reward 728.0, memory_length 2000, epsilon 0.057819716220971824, time 725.0, rides 130\n",
      "Initial State is  [1, 9, 0]\n",
      "episode 2849, reward 757.0, memory_length 2000, epsilon 0.05776189650475085, time 736.0, rides 121\n",
      "Initial State is  [2, 1, 2]\n",
      "episode 2850, reward 866.0, memory_length 2000, epsilon 0.0577041346082461, time 727.0, rides 121\n",
      "Initial State is  [3, 14, 6]\n",
      "episode 2851, reward 742.0, memory_length 2000, epsilon 0.05764643047363786, time 723.0, rides 113\n",
      "Initial State is  [1, 20, 5]\n",
      "episode 2852, reward 869.0, memory_length 2000, epsilon 0.05758878404316422, time 733.0, rides 131\n",
      "Initial State is  [4, 8, 5]\n",
      "episode 2853, reward 822.0, memory_length 2000, epsilon 0.05753119525912106, time 731.0, rides 122\n",
      "Initial State is  [1, 0, 6]\n",
      "episode 2854, reward 656.0, memory_length 2000, epsilon 0.05747366406386194, time 737.0, rides 119\n",
      "Initial State is  [1, 14, 3]\n",
      "episode 2855, reward 779.0, memory_length 2000, epsilon 0.057416190399798075, time 734.0, rides 137\n",
      "Initial State is  [4, 1, 3]\n",
      "episode 2856, reward 935.0, memory_length 2000, epsilon 0.05735877420939828, time 730.0, rides 114\n",
      "Initial State is  [1, 20, 0]\n",
      "episode 2857, reward 675.0, memory_length 2000, epsilon 0.05730141543518888, time 724.0, rides 122\n",
      "Initial State is  [1, 23, 5]\n",
      "episode 2858, reward 798.0, memory_length 2000, epsilon 0.05724411401975369, time 733.0, rides 132\n",
      "Initial State is  [4, 6, 6]\n",
      "episode 2859, reward 881.0, memory_length 2000, epsilon 0.057186869905733934, time 730.0, rides 126\n",
      "Initial State is  [0, 6, 5]\n",
      "episode 2860, reward 500.0, memory_length 2000, epsilon 0.0571296830358282, time 732.0, rides 133\n",
      "Initial State is  [2, 10, 0]\n",
      "episode 2861, reward 399.0, memory_length 2000, epsilon 0.05707255335279238, time 730.0, rides 125\n",
      "Initial State is  [2, 1, 2]\n",
      "episode 2862, reward 739.0, memory_length 2000, epsilon 0.057015480799439584, time 727.0, rides 120\n",
      "Initial State is  [2, 5, 2]\n",
      "episode 2863, reward 849.0, memory_length 2000, epsilon 0.05695846531864014, time 722.0, rides 121\n",
      "Initial State is  [0, 23, 5]\n",
      "episode 2864, reward 781.0, memory_length 2000, epsilon 0.0569015068533215, time 733.0, rides 132\n",
      "Initial State is  [1, 1, 4]\n",
      "episode 2865, reward 681.0, memory_length 2000, epsilon 0.05684460534646818, time 733.0, rides 122\n",
      "Initial State is  [1, 10, 0]\n",
      "episode 2866, reward 789.0, memory_length 2000, epsilon 0.05678776074112171, time 730.0, rides 132\n",
      "Initial State is  [1, 22, 3]\n",
      "episode 2867, reward 833.0, memory_length 2000, epsilon 0.056730972980380594, time 729.0, rides 117\n",
      "Initial State is  [2, 15, 4]\n",
      "episode 2868, reward 1086.0, memory_length 2000, epsilon 0.05667424200740021, time 731.0, rides 113\n",
      "Initial State is  [3, 9, 4]\n",
      "episode 2869, reward 763.0, memory_length 2000, epsilon 0.05661756776539281, time 729.0, rides 127\n",
      "Initial State is  [2, 15, 5]\n",
      "episode 2870, reward 943.0, memory_length 2000, epsilon 0.05656095019762742, time 724.0, rides 124\n",
      "Initial State is  [0, 14, 3]\n",
      "episode 2871, reward 812.0, memory_length 2000, epsilon 0.056504389247429794, time 720.0, rides 122\n",
      "Initial State is  [0, 8, 2]\n",
      "episode 2872, reward 791.0, memory_length 2000, epsilon 0.05644788485818236, time 734.0, rides 125\n",
      "Initial State is  [4, 3, 6]\n",
      "episode 2873, reward 1037.0, memory_length 2000, epsilon 0.05639143697332418, time 725.0, rides 127\n",
      "Initial State is  [1, 23, 4]\n",
      "episode 2874, reward 920.0, memory_length 2000, epsilon 0.05633504553635086, time 727.0, rides 134\n",
      "Initial State is  [0, 17, 1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 2875, reward 839.0, memory_length 2000, epsilon 0.05627871049081451, time 727.0, rides 137\n",
      "Initial State is  [1, 3, 2]\n",
      "episode 2876, reward 704.0, memory_length 2000, epsilon 0.0562224317803237, time 730.0, rides 123\n",
      "Initial State is  [0, 12, 4]\n",
      "episode 2877, reward 774.0, memory_length 2000, epsilon 0.056166209348543376, time 725.0, rides 136\n",
      "Initial State is  [3, 18, 6]\n",
      "episode 2878, reward 836.0, memory_length 2000, epsilon 0.056110043139194835, time 728.0, rides 123\n",
      "Initial State is  [3, 3, 5]\n",
      "episode 2879, reward 674.0, memory_length 2000, epsilon 0.05605393309605564, time 736.0, rides 124\n",
      "Initial State is  [2, 1, 6]\n",
      "episode 2880, reward 827.0, memory_length 2000, epsilon 0.055997879162959584, time 729.0, rides 126\n",
      "Initial State is  [1, 22, 3]\n",
      "episode 2881, reward 766.0, memory_length 2000, epsilon 0.055941881283796624, time 730.0, rides 118\n",
      "Initial State is  [0, 23, 0]\n",
      "episode 2882, reward 813.0, memory_length 2000, epsilon 0.055885939402512824, time 729.0, rides 128\n",
      "Initial State is  [1, 5, 5]\n",
      "episode 2883, reward 987.0, memory_length 2000, epsilon 0.05583005346311031, time 735.0, rides 125\n",
      "Initial State is  [1, 8, 3]\n",
      "episode 2884, reward 630.0, memory_length 2000, epsilon 0.0557742234096472, time 728.0, rides 120\n",
      "Initial State is  [1, 11, 4]\n",
      "episode 2885, reward 990.0, memory_length 2000, epsilon 0.055718449186237556, time 730.0, rides 119\n",
      "Initial State is  [3, 19, 1]\n",
      "episode 2886, reward 746.0, memory_length 2000, epsilon 0.055662730737051316, time 730.0, rides 121\n",
      "Initial State is  [3, 21, 1]\n",
      "episode 2887, reward 923.0, memory_length 2000, epsilon 0.055607068006314264, time 730.0, rides 139\n",
      "Initial State is  [4, 5, 4]\n",
      "episode 2888, reward 968.0, memory_length 2000, epsilon 0.05555146093830795, time 727.0, rides 129\n",
      "Initial State is  [4, 17, 1]\n",
      "episode 2889, reward 647.0, memory_length 2000, epsilon 0.05549590947736964, time 728.0, rides 118\n",
      "Initial State is  [4, 19, 4]\n",
      "episode 2890, reward 862.0, memory_length 2000, epsilon 0.05544041356789227, time 726.0, rides 125\n",
      "Initial State is  [1, 10, 6]\n",
      "episode 2891, reward 952.0, memory_length 2000, epsilon 0.05538497315432438, time 734.0, rides 131\n",
      "Initial State is  [2, 9, 0]\n",
      "episode 2892, reward 986.0, memory_length 2000, epsilon 0.05532958818117005, time 727.0, rides 142\n",
      "Initial State is  [0, 6, 0]\n",
      "episode 2893, reward 788.0, memory_length 2000, epsilon 0.05527425859298888, time 726.0, rides 124\n",
      "Initial State is  [2, 19, 3]\n",
      "episode 2894, reward 1077.0, memory_length 2000, epsilon 0.05521898433439589, time 735.0, rides 133\n",
      "Initial State is  [0, 13, 1]\n",
      "episode 2895, reward 695.0, memory_length 2000, epsilon 0.0551637653500615, time 734.0, rides 131\n",
      "Initial State is  [2, 2, 0]\n",
      "episode 2896, reward 656.0, memory_length 2000, epsilon 0.05510860158471143, time 734.0, rides 135\n",
      "Initial State is  [1, 9, 0]\n",
      "episode 2897, reward 607.0, memory_length 2000, epsilon 0.05505349298312672, time 729.0, rides 127\n",
      "Initial State is  [0, 4, 6]\n",
      "episode 2898, reward 1152.0, memory_length 2000, epsilon 0.054998439490143596, time 731.0, rides 126\n",
      "Initial State is  [0, 4, 2]\n",
      "episode 2899, reward 757.0, memory_length 2000, epsilon 0.05494344105065345, time 728.0, rides 126\n",
      "Initial State is  [4, 9, 3]\n",
      "episode 2900, reward 779.0, memory_length 2000, epsilon 0.05488849760960279, time 726.0, rides 137\n",
      "Initial State is  [2, 21, 1]\n",
      "episode 2901, reward 734.0, memory_length 2000, epsilon 0.05483360911199319, time 725.0, rides 126\n",
      "Initial State is  [2, 0, 0]\n",
      "episode 2902, reward 767.0, memory_length 2000, epsilon 0.0547787755028812, time 721.0, rides 131\n",
      "Initial State is  [0, 17, 3]\n",
      "episode 2903, reward 931.0, memory_length 2000, epsilon 0.054723996727378314, time 727.0, rides 143\n",
      "Initial State is  [2, 16, 1]\n",
      "episode 2904, reward 638.0, memory_length 2000, epsilon 0.054669272730650934, time 733.0, rides 124\n",
      "Initial State is  [1, 0, 1]\n",
      "episode 2905, reward 902.0, memory_length 2000, epsilon 0.05461460345792028, time 728.0, rides 117\n",
      "Initial State is  [0, 15, 5]\n",
      "episode 2906, reward 1057.0, memory_length 2000, epsilon 0.054559988854462366, time 736.0, rides 135\n",
      "Initial State is  [4, 12, 3]\n",
      "episode 2907, reward 595.0, memory_length 2000, epsilon 0.054505428865607906, time 728.0, rides 127\n",
      "Initial State is  [0, 4, 1]\n",
      "episode 2908, reward 824.0, memory_length 2000, epsilon 0.0544509234367423, time 724.0, rides 133\n",
      "Initial State is  [2, 11, 3]\n",
      "episode 2909, reward 848.0, memory_length 2000, epsilon 0.05439647251330556, time 730.0, rides 123\n",
      "Initial State is  [1, 18, 3]\n",
      "episode 2910, reward 961.0, memory_length 2000, epsilon 0.054342076040792255, time 731.0, rides 114\n",
      "Initial State is  [3, 4, 4]\n",
      "episode 2911, reward 787.0, memory_length 2000, epsilon 0.054287733964751464, time 731.0, rides 124\n",
      "Initial State is  [0, 0, 4]\n",
      "episode 2912, reward 1115.0, memory_length 2000, epsilon 0.054233446230786714, time 741.0, rides 136\n",
      "Initial State is  [1, 23, 1]\n",
      "episode 2913, reward 848.0, memory_length 2000, epsilon 0.054179212784555925, time 725.0, rides 134\n",
      "Initial State is  [4, 17, 2]\n",
      "episode 2914, reward 839.0, memory_length 2000, epsilon 0.05412503357177137, time 726.0, rides 143\n",
      "Initial State is  [3, 23, 1]\n",
      "episode 2915, reward 857.0, memory_length 2000, epsilon 0.0540709085381996, time 722.0, rides 136\n",
      "Initial State is  [4, 6, 4]\n",
      "episode 2916, reward 823.0, memory_length 2000, epsilon 0.0540168376296614, time 732.0, rides 123\n",
      "Initial State is  [4, 21, 4]\n",
      "episode 2917, reward 524.0, memory_length 2000, epsilon 0.053962820792031733, time 727.0, rides 135\n",
      "Initial State is  [3, 23, 6]\n",
      "episode 2918, reward 825.0, memory_length 2000, epsilon 0.053908857971239704, time 726.0, rides 140\n",
      "Initial State is  [2, 7, 2]\n",
      "episode 2919, reward 932.0, memory_length 2000, epsilon 0.05385494911326846, time 726.0, rides 131\n",
      "Initial State is  [1, 1, 5]\n",
      "episode 2920, reward 996.0, memory_length 2000, epsilon 0.0538010941641552, time 731.0, rides 108\n",
      "Initial State is  [3, 5, 1]\n",
      "episode 2921, reward 857.0, memory_length 2000, epsilon 0.05374729306999104, time 729.0, rides 142\n",
      "Initial State is  [0, 8, 3]\n",
      "episode 2922, reward 890.0, memory_length 2000, epsilon 0.05369354577692105, time 727.0, rides 126\n",
      "Initial State is  [3, 12, 4]\n",
      "episode 2923, reward 735.0, memory_length 2000, epsilon 0.053639852231144126, time 725.0, rides 140\n",
      "Initial State is  [4, 22, 5]\n",
      "episode 2924, reward 893.0, memory_length 2000, epsilon 0.053586212378912985, time 724.0, rides 149\n",
      "Initial State is  [4, 5, 3]\n",
      "episode 2925, reward 755.0, memory_length 2000, epsilon 0.05353262616653407, time 723.0, rides 131\n",
      "Initial State is  [2, 13, 6]\n",
      "episode 2926, reward 940.0, memory_length 2000, epsilon 0.053479093540367534, time 724.0, rides 133\n",
      "Initial State is  [0, 17, 3]\n",
      "episode 2927, reward 783.0, memory_length 2000, epsilon 0.05342561444682717, time 726.0, rides 131\n",
      "Initial State is  [0, 2, 1]\n",
      "episode 2928, reward 600.0, memory_length 2000, epsilon 0.05337218883238034, time 730.0, rides 119\n",
      "Initial State is  [1, 18, 5]\n",
      "episode 2929, reward 971.0, memory_length 2000, epsilon 0.053318816643547956, time 728.0, rides 123\n",
      "Initial State is  [3, 18, 6]\n",
      "episode 2930, reward 892.0, memory_length 2000, epsilon 0.05326549782690441, time 726.0, rides 128\n",
      "Initial State is  [2, 6, 2]\n",
      "episode 2931, reward 692.0, memory_length 2000, epsilon 0.053212232329077506, time 722.0, rides 126\n",
      "Initial State is  [1, 7, 6]\n",
      "episode 2932, reward 657.0, memory_length 2000, epsilon 0.05315902009674843, time 730.0, rides 143\n",
      "Initial State is  [3, 14, 2]\n",
      "episode 2933, reward 616.0, memory_length 2000, epsilon 0.05310586107665168, time 727.0, rides 138\n",
      "Initial State is  [3, 12, 5]\n",
      "episode 2934, reward 922.0, memory_length 2000, epsilon 0.05305275521557503, time 723.0, rides 135\n",
      "Initial State is  [0, 9, 5]\n",
      "episode 2935, reward 822.0, memory_length 2000, epsilon 0.052999702460359455, time 733.0, rides 129\n",
      "Initial State is  [0, 11, 3]\n",
      "episode 2936, reward 731.0, memory_length 2000, epsilon 0.052946702757899096, time 724.0, rides 127\n",
      "Initial State is  [3, 4, 3]\n",
      "episode 2937, reward 805.0, memory_length 2000, epsilon 0.0528937560551412, time 734.0, rides 137\n",
      "Initial State is  [3, 15, 4]\n",
      "episode 2938, reward 855.0, memory_length 2000, epsilon 0.05284086229908606, time 725.0, rides 145\n",
      "Initial State is  [1, 2, 5]\n",
      "episode 2939, reward 944.0, memory_length 2000, epsilon 0.05278802143678697, time 726.0, rides 129\n",
      "Initial State is  [3, 3, 3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 2940, reward 846.0, memory_length 2000, epsilon 0.052735233415350184, time 734.0, rides 124\n",
      "Initial State is  [2, 1, 6]\n",
      "episode 2941, reward 591.0, memory_length 2000, epsilon 0.052682498181934836, time 725.0, rides 127\n",
      "Initial State is  [4, 4, 4]\n",
      "episode 2942, reward 940.0, memory_length 2000, epsilon 0.052629815683752905, time 743.0, rides 120\n",
      "Initial State is  [0, 12, 4]\n",
      "episode 2943, reward 980.0, memory_length 2000, epsilon 0.05257718586806915, time 723.0, rides 133\n",
      "Initial State is  [0, 5, 0]\n",
      "episode 2944, reward 960.0, memory_length 2000, epsilon 0.05252460868220108, time 731.0, rides 134\n",
      "Initial State is  [1, 17, 3]\n",
      "episode 2945, reward 923.0, memory_length 2000, epsilon 0.05247208407351888, time 725.0, rides 122\n",
      "Initial State is  [1, 11, 2]\n",
      "episode 2946, reward 824.0, memory_length 2000, epsilon 0.052419611989445364, time 735.0, rides 131\n",
      "Initial State is  [3, 6, 5]\n",
      "episode 2947, reward 924.0, memory_length 2000, epsilon 0.05236719237745592, time 728.0, rides 133\n",
      "Initial State is  [1, 3, 4]\n",
      "episode 2948, reward 841.0, memory_length 2000, epsilon 0.05231482518507846, time 731.0, rides 141\n",
      "Initial State is  [0, 2, 3]\n",
      "episode 2949, reward 635.0, memory_length 2000, epsilon 0.052262510359893384, time 727.0, rides 139\n",
      "Initial State is  [1, 11, 6]\n",
      "episode 2950, reward 1052.0, memory_length 2000, epsilon 0.05221024784953349, time 736.0, rides 139\n",
      "Initial State is  [3, 17, 0]\n",
      "episode 2951, reward 836.0, memory_length 2000, epsilon 0.05215803760168396, time 730.0, rides 148\n",
      "Initial State is  [2, 23, 3]\n",
      "episode 2952, reward 759.0, memory_length 2000, epsilon 0.052105879564082275, time 733.0, rides 140\n",
      "Initial State is  [4, 20, 2]\n",
      "episode 2953, reward 720.0, memory_length 2000, epsilon 0.05205377368451819, time 732.0, rides 135\n",
      "Initial State is  [0, 6, 4]\n",
      "episode 2954, reward 615.0, memory_length 2000, epsilon 0.052001719910833674, time 725.0, rides 132\n",
      "Initial State is  [0, 3, 1]\n",
      "episode 2955, reward 946.0, memory_length 2000, epsilon 0.05194971819092284, time 737.0, rides 138\n",
      "Initial State is  [4, 12, 3]\n",
      "episode 2956, reward 1136.0, memory_length 2000, epsilon 0.05189776847273191, time 723.0, rides 144\n",
      "Initial State is  [0, 5, 5]\n",
      "episode 2957, reward 873.0, memory_length 2000, epsilon 0.05184587070425918, time 726.0, rides 126\n",
      "Initial State is  [2, 4, 5]\n",
      "episode 2958, reward 865.0, memory_length 2000, epsilon 0.05179402483355492, time 726.0, rides 125\n",
      "Initial State is  [1, 0, 2]\n",
      "episode 2959, reward 718.0, memory_length 2000, epsilon 0.05174223080872137, time 735.0, rides 126\n",
      "Initial State is  [2, 18, 5]\n",
      "episode 2960, reward 574.0, memory_length 2000, epsilon 0.05169048857791265, time 728.0, rides 132\n",
      "Initial State is  [1, 17, 6]\n",
      "episode 2961, reward 733.0, memory_length 2000, epsilon 0.051638798089334734, time 724.0, rides 130\n",
      "Initial State is  [3, 11, 4]\n",
      "episode 2962, reward 890.0, memory_length 2000, epsilon 0.051587159291245396, time 725.0, rides 137\n",
      "Initial State is  [0, 4, 5]\n",
      "episode 2963, reward 807.0, memory_length 2000, epsilon 0.05153557213195415, time 724.0, rides 138\n",
      "Initial State is  [3, 15, 5]\n",
      "episode 2964, reward 1166.0, memory_length 2000, epsilon 0.0514840365598222, time 730.0, rides 137\n",
      "Initial State is  [3, 5, 1]\n",
      "episode 2965, reward 700.0, memory_length 2000, epsilon 0.05143255252326238, time 727.0, rides 129\n",
      "Initial State is  [4, 12, 1]\n",
      "episode 2966, reward 1082.0, memory_length 2000, epsilon 0.05138111997073912, time 727.0, rides 134\n",
      "Initial State is  [3, 8, 2]\n",
      "episode 2967, reward 892.0, memory_length 2000, epsilon 0.05132973885076838, time 730.0, rides 134\n",
      "Initial State is  [3, 16, 4]\n",
      "episode 2968, reward 877.0, memory_length 2000, epsilon 0.05127840911191761, time 721.0, rides 135\n",
      "Initial State is  [1, 21, 1]\n",
      "episode 2969, reward 879.0, memory_length 2000, epsilon 0.05122713070280569, time 727.0, rides 135\n",
      "Initial State is  [3, 22, 2]\n",
      "episode 2970, reward 830.0, memory_length 2000, epsilon 0.05117590357210289, time 730.0, rides 130\n",
      "Initial State is  [3, 14, 4]\n",
      "episode 2971, reward 666.0, memory_length 2000, epsilon 0.05112472766853079, time 727.0, rides 140\n",
      "Initial State is  [1, 18, 1]\n",
      "episode 2972, reward 820.0, memory_length 2000, epsilon 0.05107360294086226, time 735.0, rides 137\n",
      "Initial State is  [4, 8, 5]\n",
      "episode 2973, reward 579.0, memory_length 2000, epsilon 0.051022529337921396, time 729.0, rides 104\n",
      "Initial State is  [1, 2, 5]\n",
      "episode 2974, reward 791.0, memory_length 2000, epsilon 0.05097150680858348, time 723.0, rides 134\n",
      "Initial State is  [2, 13, 0]\n",
      "episode 2975, reward 794.0, memory_length 2000, epsilon 0.05092053530177489, time 727.0, rides 131\n",
      "Initial State is  [2, 0, 1]\n",
      "episode 2976, reward 958.0, memory_length 2000, epsilon 0.050869614766473115, time 726.0, rides 143\n",
      "Initial State is  [2, 17, 2]\n",
      "episode 2977, reward 879.0, memory_length 2000, epsilon 0.05081874515170664, time 722.0, rides 144\n",
      "Initial State is  [1, 17, 0]\n",
      "episode 2978, reward 986.0, memory_length 2000, epsilon 0.050767926406554933, time 721.0, rides 142\n",
      "Initial State is  [0, 5, 4]\n",
      "episode 2979, reward 823.0, memory_length 2000, epsilon 0.05071715848014838, time 731.0, rides 126\n",
      "Initial State is  [1, 21, 6]\n",
      "episode 2980, reward 913.0, memory_length 2000, epsilon 0.050666441321668226, time 726.0, rides 140\n",
      "Initial State is  [2, 13, 4]\n",
      "episode 2981, reward 580.0, memory_length 2000, epsilon 0.050615774880346555, time 720.0, rides 129\n",
      "Initial State is  [4, 3, 5]\n",
      "episode 2982, reward 811.0, memory_length 2000, epsilon 0.05056515910546621, time 730.0, rides 126\n",
      "Initial State is  [3, 0, 0]\n",
      "episode 2983, reward 742.0, memory_length 2000, epsilon 0.05051459394636074, time 728.0, rides 120\n",
      "Initial State is  [3, 7, 1]\n",
      "episode 2984, reward 795.0, memory_length 2000, epsilon 0.050464079352414384, time 737.0, rides 125\n",
      "Initial State is  [4, 19, 5]\n",
      "episode 2985, reward 694.0, memory_length 2000, epsilon 0.05041361527306197, time 725.0, rides 123\n",
      "Initial State is  [2, 0, 3]\n",
      "episode 2986, reward 851.0, memory_length 2000, epsilon 0.05036320165778891, time 729.0, rides 139\n",
      "Initial State is  [0, 17, 6]\n",
      "episode 2987, reward 838.0, memory_length 2000, epsilon 0.05031283845613112, time 728.0, rides 132\n",
      "Initial State is  [0, 1, 3]\n",
      "episode 2988, reward 763.0, memory_length 2000, epsilon 0.05026252561767499, time 727.0, rides 136\n",
      "Initial State is  [1, 12, 1]\n",
      "episode 2989, reward 552.0, memory_length 2000, epsilon 0.05021226309205731, time 735.0, rides 133\n",
      "Initial State is  [4, 21, 3]\n",
      "episode 2990, reward 711.0, memory_length 2000, epsilon 0.05016205082896526, time 731.0, rides 119\n",
      "Initial State is  [4, 6, 1]\n",
      "episode 2991, reward 791.0, memory_length 2000, epsilon 0.05011188877813629, time 729.0, rides 132\n",
      "Initial State is  [0, 1, 6]\n",
      "episode 2992, reward 595.0, memory_length 2000, epsilon 0.05006177688935816, time 721.0, rides 143\n",
      "Initial State is  [4, 9, 1]\n",
      "episode 2993, reward 791.0, memory_length 2000, epsilon 0.0500117151124688, time 733.0, rides 143\n",
      "Initial State is  [3, 9, 3]\n",
      "episode 2994, reward 698.0, memory_length 2000, epsilon 0.04996170339735633, time 730.0, rides 138\n",
      "Initial State is  [2, 13, 6]\n",
      "episode 2995, reward 731.0, memory_length 2000, epsilon 0.049911741693958976, time 724.0, rides 129\n",
      "Initial State is  [2, 17, 5]\n",
      "episode 2996, reward 479.0, memory_length 2000, epsilon 0.04986182995226502, time 722.0, rides 123\n",
      "Initial State is  [3, 2, 4]\n",
      "episode 2997, reward 586.0, memory_length 2000, epsilon 0.049811968122312755, time 730.0, rides 134\n",
      "Initial State is  [1, 13, 6]\n",
      "episode 2998, reward 753.0, memory_length 2000, epsilon 0.04976215615419044, time 733.0, rides 141\n",
      "Initial State is  [1, 1, 0]\n",
      "episode 2999, reward 1008.0, memory_length 2000, epsilon 0.04971239399803625, time 725.0, rides 124\n",
      "Initial State is  [1, 12, 0]\n",
      "episode 3000, reward 679.0, memory_length 2000, epsilon 0.049662681604038215, time 728.0, rides 135\n",
      "Initial State is  [3, 20, 2]\n",
      "episode 3001, reward 513.0, memory_length 2000, epsilon 0.04961301892243418, time 730.0, rides 135\n",
      "Initial State is  [3, 17, 2]\n",
      "episode 3002, reward 799.0, memory_length 2000, epsilon 0.04956340590351174, time 727.0, rides 141\n",
      "Initial State is  [2, 21, 3]\n",
      "episode 3003, reward 619.0, memory_length 2000, epsilon 0.04951384249760823, time 737.0, rides 137\n",
      "Initial State is  [2, 0, 4]\n",
      "episode 3004, reward 653.0, memory_length 2000, epsilon 0.04946432865511062, time 728.0, rides 125\n",
      "Initial State is  [0, 19, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 3005, reward 1155.0, memory_length 2000, epsilon 0.04941486432645551, time 724.0, rides 138\n",
      "Initial State is  [2, 18, 0]\n",
      "episode 3006, reward 971.0, memory_length 2000, epsilon 0.049365449462129056, time 728.0, rides 133\n",
      "Initial State is  [3, 2, 0]\n",
      "episode 3007, reward 782.0, memory_length 2000, epsilon 0.049316084012666926, time 730.0, rides 134\n",
      "Initial State is  [1, 2, 4]\n",
      "episode 3008, reward 814.0, memory_length 2000, epsilon 0.04926676792865426, time 724.0, rides 128\n",
      "Initial State is  [2, 7, 5]\n",
      "episode 3009, reward 771.0, memory_length 2000, epsilon 0.0492175011607256, time 726.0, rides 135\n",
      "Initial State is  [3, 0, 1]\n",
      "episode 3010, reward 774.0, memory_length 2000, epsilon 0.049168283659564875, time 727.0, rides 136\n",
      "Initial State is  [3, 19, 6]\n",
      "episode 3011, reward 1006.0, memory_length 2000, epsilon 0.04911911537590531, time 729.0, rides 126\n",
      "Initial State is  [0, 13, 2]\n",
      "episode 3012, reward 744.0, memory_length 2000, epsilon 0.04906999626052941, time 734.0, rides 131\n",
      "Initial State is  [1, 12, 5]\n",
      "episode 3013, reward 854.0, memory_length 2000, epsilon 0.04902092626426888, time 732.0, rides 133\n",
      "Initial State is  [4, 9, 6]\n",
      "episode 3014, reward 831.0, memory_length 2000, epsilon 0.04897190533800461, time 722.0, rides 132\n",
      "Initial State is  [3, 5, 5]\n",
      "episode 3015, reward 723.0, memory_length 2000, epsilon 0.0489229334326666, time 727.0, rides 118\n",
      "Initial State is  [4, 13, 5]\n",
      "episode 3016, reward 841.0, memory_length 2000, epsilon 0.048874010499233934, time 728.0, rides 131\n",
      "Initial State is  [1, 9, 5]\n",
      "episode 3017, reward 810.0, memory_length 2000, epsilon 0.048825136488734704, time 735.0, rides 143\n",
      "Initial State is  [4, 4, 2]\n",
      "episode 3018, reward 673.0, memory_length 2000, epsilon 0.04877631135224597, time 725.0, rides 128\n",
      "Initial State is  [0, 1, 6]\n",
      "episode 3019, reward 1049.0, memory_length 2000, epsilon 0.04872753504089373, time 728.0, rides 140\n",
      "Initial State is  [2, 12, 0]\n",
      "episode 3020, reward 574.0, memory_length 2000, epsilon 0.048678807505852836, time 724.0, rides 116\n",
      "Initial State is  [4, 12, 5]\n",
      "episode 3021, reward 539.0, memory_length 2000, epsilon 0.048630128698346986, time 727.0, rides 128\n",
      "Initial State is  [4, 17, 0]\n",
      "episode 3022, reward 797.0, memory_length 2000, epsilon 0.04858149856964864, time 734.0, rides 148\n",
      "Initial State is  [3, 18, 0]\n",
      "episode 3023, reward 902.0, memory_length 2000, epsilon 0.04853291707107899, time 732.0, rides 123\n",
      "Initial State is  [1, 9, 5]\n",
      "episode 3024, reward 930.0, memory_length 2000, epsilon 0.048484384154007916, time 724.0, rides 129\n",
      "Initial State is  [0, 2, 0]\n",
      "episode 3025, reward 850.0, memory_length 2000, epsilon 0.04843589976985391, time 735.0, rides 130\n",
      "Initial State is  [4, 15, 2]\n",
      "episode 3026, reward 1084.0, memory_length 2000, epsilon 0.04838746387008405, time 720.0, rides 121\n",
      "Initial State is  [1, 17, 4]\n",
      "episode 3027, reward 1065.0, memory_length 2000, epsilon 0.04833907640621397, time 729.0, rides 129\n",
      "Initial State is  [2, 23, 0]\n",
      "episode 3028, reward 797.0, memory_length 2000, epsilon 0.04829073732980775, time 730.0, rides 125\n",
      "Initial State is  [0, 7, 6]\n",
      "episode 3029, reward 865.0, memory_length 2000, epsilon 0.04824244659247794, time 732.0, rides 133\n",
      "Initial State is  [4, 0, 3]\n",
      "episode 3030, reward 890.0, memory_length 2000, epsilon 0.04819420414588547, time 734.0, rides 123\n",
      "Initial State is  [0, 12, 3]\n",
      "episode 3031, reward 624.0, memory_length 2000, epsilon 0.048146009941739586, time 732.0, rides 132\n",
      "Initial State is  [1, 10, 3]\n",
      "episode 3032, reward 1002.0, memory_length 2000, epsilon 0.048097863931797845, time 725.0, rides 105\n",
      "Initial State is  [1, 12, 1]\n",
      "episode 3033, reward 920.0, memory_length 2000, epsilon 0.04804976606786605, time 731.0, rides 126\n",
      "Initial State is  [0, 5, 5]\n",
      "episode 3034, reward 765.0, memory_length 2000, epsilon 0.048001716301798183, time 728.0, rides 127\n",
      "Initial State is  [1, 1, 0]\n",
      "episode 3035, reward 782.0, memory_length 2000, epsilon 0.04795371458549638, time 728.0, rides 139\n",
      "Initial State is  [4, 4, 1]\n",
      "episode 3036, reward 929.0, memory_length 2000, epsilon 0.047905760870910884, time 722.0, rides 123\n",
      "Initial State is  [2, 1, 2]\n",
      "episode 3037, reward 1039.0, memory_length 2000, epsilon 0.047857855110039975, time 730.0, rides 134\n",
      "Initial State is  [3, 8, 1]\n",
      "episode 3038, reward 815.0, memory_length 2000, epsilon 0.04780999725492994, time 726.0, rides 130\n",
      "Initial State is  [2, 6, 4]\n",
      "episode 3039, reward 812.0, memory_length 2000, epsilon 0.04776218725767501, time 727.0, rides 135\n",
      "Initial State is  [0, 15, 3]\n",
      "episode 3040, reward 744.0, memory_length 2000, epsilon 0.047714425070417336, time 732.0, rides 135\n",
      "Initial State is  [0, 7, 6]\n",
      "episode 3041, reward 1052.0, memory_length 2000, epsilon 0.047666710645346916, time 728.0, rides 142\n",
      "Initial State is  [2, 6, 5]\n",
      "episode 3042, reward 753.0, memory_length 2000, epsilon 0.047619043934701566, time 721.0, rides 125\n",
      "Initial State is  [0, 14, 4]\n",
      "episode 3043, reward 721.0, memory_length 2000, epsilon 0.047571424890766864, time 724.0, rides 137\n",
      "Initial State is  [1, 11, 2]\n",
      "episode 3044, reward 846.0, memory_length 2000, epsilon 0.0475238534658761, time 726.0, rides 120\n",
      "Initial State is  [4, 18, 1]\n",
      "episode 3045, reward 741.0, memory_length 2000, epsilon 0.047476329612410224, time 728.0, rides 132\n",
      "Initial State is  [3, 16, 6]\n",
      "episode 3046, reward 777.0, memory_length 2000, epsilon 0.04742885328279781, time 723.0, rides 132\n",
      "Initial State is  [3, 1, 0]\n",
      "episode 3047, reward 387.0, memory_length 2000, epsilon 0.04738142442951501, time 724.0, rides 118\n",
      "Initial State is  [1, 11, 5]\n",
      "episode 3048, reward 938.0, memory_length 2000, epsilon 0.047334043005085494, time 728.0, rides 134\n",
      "Initial State is  [0, 18, 3]\n",
      "episode 3049, reward 921.0, memory_length 2000, epsilon 0.047286708962080405, time 722.0, rides 132\n",
      "Initial State is  [3, 13, 6]\n",
      "episode 3050, reward 899.0, memory_length 2000, epsilon 0.04723942225311833, time 738.0, rides 129\n",
      "Initial State is  [3, 20, 5]\n",
      "episode 3051, reward 1022.0, memory_length 2000, epsilon 0.04719218283086521, time 724.0, rides 128\n",
      "Initial State is  [3, 20, 6]\n",
      "episode 3052, reward 892.0, memory_length 2000, epsilon 0.047144990648034346, time 725.0, rides 130\n",
      "Initial State is  [0, 22, 2]\n",
      "episode 3053, reward 901.0, memory_length 2000, epsilon 0.04709784565738631, time 731.0, rides 125\n",
      "Initial State is  [0, 4, 6]\n",
      "episode 3054, reward 874.0, memory_length 2000, epsilon 0.047050747811728924, time 727.0, rides 131\n",
      "Initial State is  [0, 17, 6]\n",
      "episode 3055, reward 805.0, memory_length 2000, epsilon 0.047003697063917195, time 725.0, rides 131\n",
      "Initial State is  [4, 14, 2]\n",
      "episode 3056, reward 909.0, memory_length 2000, epsilon 0.04695669336685328, time 721.0, rides 128\n",
      "Initial State is  [1, 5, 1]\n",
      "episode 3057, reward 629.0, memory_length 2000, epsilon 0.04690973667348643, time 730.0, rides 132\n",
      "Initial State is  [2, 9, 4]\n",
      "episode 3058, reward 767.0, memory_length 2000, epsilon 0.04686282693681294, time 727.0, rides 134\n",
      "Initial State is  [3, 8, 5]\n",
      "episode 3059, reward 880.0, memory_length 2000, epsilon 0.04681596410987613, time 726.0, rides 138\n",
      "Initial State is  [3, 0, 6]\n",
      "episode 3060, reward 771.0, memory_length 2000, epsilon 0.04676914814576626, time 724.0, rides 133\n",
      "Initial State is  [0, 11, 4]\n",
      "episode 3061, reward 829.0, memory_length 2000, epsilon 0.04672237899762049, time 730.0, rides 119\n",
      "Initial State is  [3, 19, 3]\n",
      "episode 3062, reward 828.0, memory_length 2000, epsilon 0.046675656618622864, time 724.0, rides 133\n",
      "Initial State is  [3, 0, 2]\n",
      "episode 3063, reward 900.0, memory_length 2000, epsilon 0.04662898096200424, time 726.0, rides 117\n",
      "Initial State is  [3, 6, 5]\n",
      "episode 3064, reward 982.0, memory_length 2000, epsilon 0.046582351981042235, time 723.0, rides 134\n",
      "Initial State is  [1, 22, 3]\n",
      "episode 3065, reward 1023.0, memory_length 2000, epsilon 0.04653576962906119, time 722.0, rides 127\n",
      "Initial State is  [2, 20, 3]\n",
      "episode 3066, reward 789.0, memory_length 2000, epsilon 0.04648923385943213, time 732.0, rides 124\n",
      "Initial State is  [0, 16, 4]\n",
      "episode 3067, reward 770.0, memory_length 2000, epsilon 0.04644274462557269, time 735.0, rides 136\n",
      "Initial State is  [1, 2, 6]\n",
      "episode 3068, reward 643.0, memory_length 2000, epsilon 0.04639630188094712, time 731.0, rides 121\n",
      "Initial State is  [2, 3, 2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 3069, reward 952.0, memory_length 2000, epsilon 0.046349905579066174, time 736.0, rides 128\n",
      "Initial State is  [1, 0, 3]\n",
      "episode 3070, reward 750.0, memory_length 2000, epsilon 0.04630355567348711, time 726.0, rides 116\n",
      "Initial State is  [1, 6, 6]\n",
      "episode 3071, reward 701.0, memory_length 2000, epsilon 0.046257252117813626, time 730.0, rides 131\n",
      "Initial State is  [4, 6, 5]\n",
      "episode 3072, reward 809.0, memory_length 2000, epsilon 0.04621099486569581, time 725.0, rides 129\n",
      "Initial State is  [0, 15, 1]\n",
      "episode 3073, reward 946.0, memory_length 2000, epsilon 0.04616478387083012, time 726.0, rides 134\n",
      "Initial State is  [2, 15, 1]\n",
      "episode 3074, reward 859.0, memory_length 2000, epsilon 0.04611861908695929, time 731.0, rides 130\n",
      "Initial State is  [2, 20, 0]\n",
      "episode 3075, reward 1052.0, memory_length 2000, epsilon 0.04607250046787233, time 724.0, rides 130\n",
      "Initial State is  [4, 1, 0]\n",
      "episode 3076, reward 857.0, memory_length 2000, epsilon 0.04602642796740446, time 727.0, rides 131\n",
      "Initial State is  [3, 0, 6]\n",
      "episode 3077, reward 788.0, memory_length 2000, epsilon 0.045980401539437055, time 723.0, rides 117\n",
      "Initial State is  [0, 7, 4]\n",
      "episode 3078, reward 835.0, memory_length 2000, epsilon 0.04593442113789762, time 738.0, rides 134\n",
      "Initial State is  [0, 2, 3]\n",
      "episode 3079, reward 650.0, memory_length 2000, epsilon 0.04588848671675972, time 731.0, rides 142\n",
      "Initial State is  [3, 19, 4]\n",
      "episode 3080, reward 482.0, memory_length 2000, epsilon 0.04584259823004296, time 742.0, rides 127\n",
      "Initial State is  [0, 7, 0]\n",
      "episode 3081, reward 900.0, memory_length 2000, epsilon 0.045796755631812916, time 723.0, rides 139\n",
      "Initial State is  [2, 2, 6]\n",
      "episode 3082, reward 1058.0, memory_length 2000, epsilon 0.0457509588761811, time 738.0, rides 125\n",
      "Initial State is  [4, 4, 4]\n",
      "episode 3083, reward 776.0, memory_length 2000, epsilon 0.04570520791730492, time 730.0, rides 119\n",
      "Initial State is  [1, 18, 4]\n",
      "episode 3084, reward 1067.0, memory_length 2000, epsilon 0.04565950270938761, time 727.0, rides 131\n",
      "Initial State is  [2, 4, 4]\n",
      "episode 3085, reward 999.0, memory_length 2000, epsilon 0.045613843206678224, time 731.0, rides 127\n",
      "Initial State is  [0, 7, 4]\n",
      "episode 3086, reward 567.0, memory_length 2000, epsilon 0.045568229363471546, time 727.0, rides 128\n",
      "Initial State is  [1, 3, 4]\n",
      "episode 3087, reward 805.0, memory_length 2000, epsilon 0.04552266113410808, time 729.0, rides 124\n",
      "Initial State is  [4, 1, 1]\n",
      "episode 3088, reward 754.0, memory_length 2000, epsilon 0.04547713847297397, time 741.0, rides 124\n",
      "Initial State is  [1, 14, 6]\n",
      "episode 3089, reward 655.0, memory_length 2000, epsilon 0.045431661334501, time 727.0, rides 124\n",
      "Initial State is  [0, 21, 2]\n",
      "episode 3090, reward 845.0, memory_length 2000, epsilon 0.0453862296731665, time 724.0, rides 120\n",
      "Initial State is  [3, 19, 4]\n",
      "episode 3091, reward 636.0, memory_length 2000, epsilon 0.04534084344349334, time 729.0, rides 142\n",
      "Initial State is  [0, 2, 2]\n",
      "episode 3092, reward 692.0, memory_length 2000, epsilon 0.045295502600049845, time 725.0, rides 123\n",
      "Initial State is  [2, 6, 3]\n",
      "episode 3093, reward 704.0, memory_length 2000, epsilon 0.04525020709744979, time 738.0, rides 129\n",
      "Initial State is  [0, 17, 0]\n",
      "episode 3094, reward 665.0, memory_length 2000, epsilon 0.045204956890352345, time 729.0, rides 122\n",
      "Initial State is  [0, 1, 1]\n",
      "episode 3095, reward 407.0, memory_length 2000, epsilon 0.04515975193346199, time 735.0, rides 128\n",
      "Initial State is  [3, 21, 0]\n",
      "episode 3096, reward 814.0, memory_length 2000, epsilon 0.04511459218152853, time 728.0, rides 122\n",
      "Initial State is  [2, 14, 4]\n",
      "episode 3097, reward 543.0, memory_length 2000, epsilon 0.045069477589347, time 735.0, rides 121\n",
      "Initial State is  [4, 14, 3]\n",
      "episode 3098, reward 1042.0, memory_length 2000, epsilon 0.045024408111757654, time 732.0, rides 128\n",
      "Initial State is  [0, 14, 4]\n",
      "episode 3099, reward 1127.0, memory_length 2000, epsilon 0.044979383703645896, time 725.0, rides 133\n",
      "Initial State is  [2, 19, 2]\n",
      "episode 3100, reward 663.0, memory_length 2000, epsilon 0.04493440431994225, time 730.0, rides 127\n",
      "Initial State is  [1, 18, 0]\n",
      "episode 3101, reward 692.0, memory_length 2000, epsilon 0.04488946991562231, time 738.0, rides 138\n",
      "Initial State is  [0, 10, 2]\n",
      "episode 3102, reward 1002.0, memory_length 2000, epsilon 0.04484458044570669, time 733.0, rides 116\n",
      "Initial State is  [2, 11, 5]\n",
      "episode 3103, reward 771.0, memory_length 2000, epsilon 0.04479973586526098, time 723.0, rides 126\n",
      "Initial State is  [4, 2, 2]\n",
      "episode 3104, reward 884.0, memory_length 2000, epsilon 0.04475493612939572, time 729.0, rides 124\n",
      "Initial State is  [0, 0, 2]\n",
      "episode 3105, reward 673.0, memory_length 2000, epsilon 0.04471018119326632, time 727.0, rides 114\n",
      "Initial State is  [2, 22, 5]\n",
      "episode 3106, reward 1073.0, memory_length 2000, epsilon 0.044665471012073056, time 727.0, rides 128\n",
      "Initial State is  [1, 11, 1]\n",
      "episode 3107, reward 865.0, memory_length 2000, epsilon 0.04462080554106098, time 730.0, rides 136\n",
      "Initial State is  [0, 10, 2]\n",
      "episode 3108, reward 1201.0, memory_length 2000, epsilon 0.04457618473551992, time 726.0, rides 129\n",
      "Initial State is  [3, 7, 1]\n",
      "episode 3109, reward 798.0, memory_length 2000, epsilon 0.0445316085507844, time 727.0, rides 130\n",
      "Initial State is  [2, 4, 4]\n",
      "episode 3110, reward 837.0, memory_length 2000, epsilon 0.04448707694223361, time 731.0, rides 132\n",
      "Initial State is  [4, 17, 4]\n",
      "episode 3111, reward 973.0, memory_length 2000, epsilon 0.04444258986529138, time 726.0, rides 122\n",
      "Initial State is  [3, 23, 0]\n",
      "episode 3112, reward 813.0, memory_length 2000, epsilon 0.044398147275426084, time 727.0, rides 119\n",
      "Initial State is  [3, 5, 2]\n",
      "episode 3113, reward 778.0, memory_length 2000, epsilon 0.044353749128150655, time 725.0, rides 132\n",
      "Initial State is  [2, 13, 0]\n",
      "episode 3114, reward 883.0, memory_length 2000, epsilon 0.0443093953790225, time 735.0, rides 126\n",
      "Initial State is  [4, 21, 2]\n",
      "episode 3115, reward 836.0, memory_length 2000, epsilon 0.04426508598364348, time 738.0, rides 129\n",
      "Initial State is  [3, 15, 4]\n",
      "episode 3116, reward 996.0, memory_length 2000, epsilon 0.044220820897659836, time 730.0, rides 133\n",
      "Initial State is  [4, 23, 5]\n",
      "episode 3117, reward 797.0, memory_length 2000, epsilon 0.044176600076762176, time 731.0, rides 127\n",
      "Initial State is  [0, 20, 5]\n",
      "episode 3118, reward 979.0, memory_length 2000, epsilon 0.04413242347668542, time 734.0, rides 120\n",
      "Initial State is  [1, 19, 3]\n",
      "episode 3119, reward 1093.0, memory_length 2000, epsilon 0.04408829105320873, time 725.0, rides 136\n",
      "Initial State is  [3, 20, 5]\n",
      "episode 3120, reward 939.0, memory_length 2000, epsilon 0.044044202762155524, time 731.0, rides 135\n",
      "Initial State is  [4, 0, 4]\n",
      "episode 3121, reward 704.0, memory_length 2000, epsilon 0.04400015855939337, time 733.0, rides 123\n",
      "Initial State is  [2, 18, 4]\n",
      "episode 3122, reward 731.0, memory_length 2000, epsilon 0.04395615840083398, time 730.0, rides 127\n",
      "Initial State is  [3, 23, 1]\n",
      "episode 3123, reward 736.0, memory_length 2000, epsilon 0.04391220224243315, time 727.0, rides 131\n",
      "Initial State is  [3, 9, 6]\n",
      "episode 3124, reward 964.0, memory_length 2000, epsilon 0.043868290040190716, time 727.0, rides 146\n",
      "Initial State is  [3, 9, 6]\n",
      "episode 3125, reward 891.0, memory_length 2000, epsilon 0.04382442175015053, time 736.0, rides 141\n",
      "Initial State is  [0, 20, 2]\n",
      "episode 3126, reward 663.0, memory_length 2000, epsilon 0.043780597328400374, time 726.0, rides 140\n",
      "Initial State is  [4, 19, 5]\n",
      "episode 3127, reward 958.0, memory_length 2000, epsilon 0.04373681673107197, time 724.0, rides 136\n",
      "Initial State is  [4, 7, 5]\n",
      "episode 3128, reward 891.0, memory_length 2000, epsilon 0.0436930799143409, time 732.0, rides 122\n",
      "Initial State is  [4, 13, 1]\n",
      "episode 3129, reward 894.0, memory_length 2000, epsilon 0.043649386834426554, time 725.0, rides 125\n",
      "Initial State is  [1, 15, 0]\n",
      "episode 3130, reward 611.0, memory_length 2000, epsilon 0.04360573744759213, time 726.0, rides 137\n",
      "Initial State is  [1, 13, 4]\n",
      "episode 3131, reward 809.0, memory_length 2000, epsilon 0.04356213171014454, time 729.0, rides 135\n",
      "Initial State is  [0, 20, 4]\n",
      "episode 3132, reward 857.0, memory_length 2000, epsilon 0.04351856957843439, time 731.0, rides 139\n",
      "Initial State is  [0, 19, 2]\n",
      "episode 3133, reward 742.0, memory_length 2000, epsilon 0.04347505100885596, time 741.0, rides 127\n",
      "Initial State is  [4, 12, 1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 3134, reward 756.0, memory_length 2000, epsilon 0.0434315759578471, time 729.0, rides 143\n",
      "Initial State is  [0, 16, 2]\n",
      "episode 3135, reward 729.0, memory_length 2000, epsilon 0.04338814438188925, time 725.0, rides 137\n",
      "Initial State is  [4, 21, 1]\n",
      "episode 3136, reward 1065.0, memory_length 2000, epsilon 0.043344756237507366, time 734.0, rides 141\n",
      "Initial State is  [1, 22, 0]\n",
      "episode 3137, reward 1020.0, memory_length 2000, epsilon 0.043301411481269855, time 728.0, rides 123\n",
      "Initial State is  [0, 3, 5]\n",
      "episode 3138, reward 956.0, memory_length 2000, epsilon 0.04325811006978859, time 732.0, rides 135\n",
      "Initial State is  [0, 8, 4]\n",
      "episode 3139, reward 651.0, memory_length 2000, epsilon 0.043214851959718796, time 724.0, rides 148\n",
      "Initial State is  [0, 22, 2]\n",
      "episode 3140, reward 773.0, memory_length 2000, epsilon 0.04317163710775908, time 723.0, rides 125\n",
      "Initial State is  [0, 15, 2]\n",
      "episode 3141, reward 899.0, memory_length 2000, epsilon 0.04312846547065132, time 732.0, rides 139\n",
      "Initial State is  [3, 1, 3]\n",
      "episode 3142, reward 687.0, memory_length 2000, epsilon 0.04308533700518067, time 729.0, rides 129\n",
      "Initial State is  [0, 10, 0]\n",
      "episode 3143, reward 861.0, memory_length 2000, epsilon 0.04304225166817549, time 725.0, rides 129\n",
      "Initial State is  [1, 0, 4]\n",
      "episode 3144, reward 686.0, memory_length 2000, epsilon 0.04299920941650732, time 734.0, rides 142\n",
      "Initial State is  [2, 23, 3]\n",
      "episode 3145, reward 551.0, memory_length 2000, epsilon 0.04295621020709081, time 728.0, rides 131\n",
      "Initial State is  [3, 13, 2]\n",
      "episode 3146, reward 980.0, memory_length 2000, epsilon 0.04291325399688372, time 728.0, rides 125\n",
      "Initial State is  [3, 7, 1]\n",
      "episode 3147, reward 903.0, memory_length 2000, epsilon 0.042870340742886835, time 729.0, rides 125\n",
      "Initial State is  [1, 17, 4]\n",
      "episode 3148, reward 870.0, memory_length 2000, epsilon 0.04282747040214395, time 727.0, rides 128\n",
      "Initial State is  [0, 21, 0]\n",
      "episode 3149, reward 916.0, memory_length 2000, epsilon 0.042784642931741806, time 733.0, rides 143\n",
      "Initial State is  [3, 8, 4]\n",
      "episode 3150, reward 1112.0, memory_length 2000, epsilon 0.04274185828881007, time 730.0, rides 126\n",
      "Initial State is  [0, 8, 3]\n",
      "episode 3151, reward 839.0, memory_length 2000, epsilon 0.04269911643052126, time 733.0, rides 135\n",
      "Initial State is  [3, 11, 3]\n",
      "episode 3152, reward 699.0, memory_length 2000, epsilon 0.04265641731409074, time 726.0, rides 133\n",
      "Initial State is  [0, 3, 6]\n",
      "episode 3153, reward 821.0, memory_length 2000, epsilon 0.04261376089677665, time 734.0, rides 145\n",
      "Initial State is  [1, 18, 5]\n",
      "episode 3154, reward 1067.0, memory_length 2000, epsilon 0.042571147135879873, time 734.0, rides 142\n",
      "Initial State is  [0, 6, 1]\n",
      "episode 3155, reward 474.0, memory_length 2000, epsilon 0.042528575988743995, time 725.0, rides 137\n",
      "Initial State is  [1, 8, 0]\n",
      "episode 3156, reward 676.0, memory_length 2000, epsilon 0.04248604741275525, time 735.0, rides 123\n",
      "Initial State is  [2, 7, 5]\n",
      "episode 3157, reward 1031.0, memory_length 2000, epsilon 0.0424435613653425, time 740.0, rides 129\n",
      "Initial State is  [3, 12, 5]\n",
      "episode 3158, reward 1078.0, memory_length 2000, epsilon 0.04240111780397716, time 731.0, rides 124\n",
      "Initial State is  [1, 23, 1]\n",
      "episode 3159, reward 987.0, memory_length 2000, epsilon 0.04235871668617318, time 725.0, rides 116\n",
      "Initial State is  [0, 21, 0]\n",
      "episode 3160, reward 957.0, memory_length 2000, epsilon 0.042316357969487, time 730.0, rides 135\n",
      "Initial State is  [2, 15, 3]\n",
      "episode 3161, reward 783.0, memory_length 2000, epsilon 0.042274041611517515, time 726.0, rides 136\n",
      "Initial State is  [4, 14, 0]\n",
      "episode 3162, reward 1125.0, memory_length 2000, epsilon 0.042231767569906, time 723.0, rides 131\n",
      "Initial State is  [1, 11, 3]\n",
      "episode 3163, reward 569.0, memory_length 2000, epsilon 0.042189535802336094, time 731.0, rides 116\n",
      "Initial State is  [0, 15, 6]\n",
      "episode 3164, reward 934.0, memory_length 2000, epsilon 0.042147346266533756, time 731.0, rides 141\n",
      "Initial State is  [1, 13, 4]\n",
      "episode 3165, reward 852.0, memory_length 2000, epsilon 0.04210519892026722, time 729.0, rides 118\n",
      "Initial State is  [0, 11, 0]\n",
      "episode 3166, reward 968.0, memory_length 2000, epsilon 0.04206309372134695, time 736.0, rides 109\n",
      "Initial State is  [2, 16, 3]\n",
      "episode 3167, reward 700.0, memory_length 2000, epsilon 0.042021030627625605, time 725.0, rides 128\n",
      "Initial State is  [3, 2, 5]\n",
      "episode 3168, reward 678.0, memory_length 2000, epsilon 0.041979009596997977, time 733.0, rides 133\n",
      "Initial State is  [3, 17, 3]\n",
      "episode 3169, reward 809.0, memory_length 2000, epsilon 0.041937030587400975, time 727.0, rides 130\n",
      "Initial State is  [1, 2, 6]\n",
      "episode 3170, reward 699.0, memory_length 2000, epsilon 0.041895093556813576, time 734.0, rides 136\n",
      "Initial State is  [3, 22, 1]\n",
      "episode 3171, reward 945.0, memory_length 2000, epsilon 0.04185319846325676, time 727.0, rides 130\n",
      "Initial State is  [0, 20, 0]\n",
      "episode 3172, reward 1067.0, memory_length 2000, epsilon 0.041811345264793506, time 731.0, rides 142\n",
      "Initial State is  [4, 13, 2]\n",
      "episode 3173, reward 696.0, memory_length 2000, epsilon 0.041769533919528715, time 724.0, rides 132\n",
      "Initial State is  [1, 7, 4]\n",
      "episode 3174, reward 733.0, memory_length 2000, epsilon 0.041727764385609184, time 730.0, rides 128\n",
      "Initial State is  [1, 12, 1]\n",
      "episode 3175, reward 939.0, memory_length 2000, epsilon 0.041686036621223575, time 732.0, rides 137\n",
      "Initial State is  [3, 20, 4]\n",
      "episode 3176, reward 902.0, memory_length 2000, epsilon 0.04164435058460235, time 727.0, rides 131\n",
      "Initial State is  [1, 14, 4]\n",
      "episode 3177, reward 604.0, memory_length 2000, epsilon 0.041602706234017746, time 726.0, rides 129\n",
      "Initial State is  [4, 10, 4]\n",
      "episode 3178, reward 538.0, memory_length 2000, epsilon 0.04156110352778373, time 726.0, rides 133\n",
      "Initial State is  [4, 4, 4]\n",
      "episode 3179, reward 837.0, memory_length 2000, epsilon 0.04151954242425594, time 731.0, rides 129\n",
      "Initial State is  [2, 1, 4]\n",
      "episode 3180, reward 513.0, memory_length 2000, epsilon 0.04147802288183169, time 728.0, rides 141\n",
      "Initial State is  [3, 6, 1]\n",
      "episode 3181, reward 819.0, memory_length 2000, epsilon 0.041436544858949854, time 725.0, rides 128\n",
      "Initial State is  [1, 6, 3]\n",
      "episode 3182, reward 672.0, memory_length 2000, epsilon 0.041395108314090906, time 727.0, rides 120\n",
      "Initial State is  [0, 2, 1]\n",
      "episode 3183, reward 904.0, memory_length 2000, epsilon 0.04135371320577681, time 725.0, rides 114\n",
      "Initial State is  [0, 12, 0]\n",
      "episode 3184, reward 813.0, memory_length 2000, epsilon 0.04131235949257103, time 729.0, rides 115\n",
      "Initial State is  [0, 5, 0]\n",
      "episode 3185, reward 874.0, memory_length 2000, epsilon 0.04127104713307846, time 723.0, rides 125\n",
      "Initial State is  [0, 13, 2]\n",
      "episode 3186, reward 833.0, memory_length 2000, epsilon 0.041229776085945385, time 731.0, rides 130\n",
      "Initial State is  [3, 19, 1]\n",
      "episode 3187, reward 1141.0, memory_length 2000, epsilon 0.04118854630985944, time 734.0, rides 131\n",
      "Initial State is  [2, 2, 4]\n",
      "episode 3188, reward 677.0, memory_length 2000, epsilon 0.04114735776354959, time 728.0, rides 130\n",
      "Initial State is  [0, 4, 2]\n",
      "episode 3189, reward 905.0, memory_length 2000, epsilon 0.04110621040578604, time 734.0, rides 118\n",
      "Initial State is  [0, 5, 4]\n",
      "episode 3190, reward 975.0, memory_length 2000, epsilon 0.04106510419538025, time 724.0, rides 125\n",
      "Initial State is  [4, 13, 2]\n",
      "episode 3191, reward 787.0, memory_length 2000, epsilon 0.04102403909118487, time 731.0, rides 132\n",
      "Initial State is  [2, 14, 1]\n",
      "episode 3192, reward 894.0, memory_length 2000, epsilon 0.04098301505209369, time 726.0, rides 139\n",
      "Initial State is  [2, 19, 6]\n",
      "episode 3193, reward 1056.0, memory_length 2000, epsilon 0.040942032037041595, time 726.0, rides 132\n",
      "Initial State is  [1, 22, 6]\n",
      "episode 3194, reward 870.0, memory_length 2000, epsilon 0.04090109000500455, time 743.0, rides 132\n",
      "Initial State is  [0, 8, 0]\n",
      "episode 3195, reward 932.0, memory_length 2000, epsilon 0.04086018891499955, time 726.0, rides 120\n",
      "Initial State is  [4, 6, 5]\n",
      "episode 3196, reward 1016.0, memory_length 2000, epsilon 0.04081932872608455, time 728.0, rides 134\n",
      "Initial State is  [2, 11, 6]\n",
      "episode 3197, reward 855.0, memory_length 2000, epsilon 0.04077850939735846, time 725.0, rides 131\n",
      "Initial State is  [4, 8, 5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 3198, reward 764.0, memory_length 2000, epsilon 0.04073773088796111, time 729.0, rides 127\n",
      "Initial State is  [0, 14, 1]\n",
      "episode 3199, reward 1117.0, memory_length 2000, epsilon 0.04069699315707315, time 736.0, rides 125\n",
      "Initial State is  [2, 22, 4]\n",
      "episode 3200, reward 1038.0, memory_length 2000, epsilon 0.04065629616391608, time 729.0, rides 125\n",
      "Initial State is  [4, 0, 3]\n",
      "episode 3201, reward 779.0, memory_length 2000, epsilon 0.040615639867752164, time 728.0, rides 137\n",
      "Initial State is  [4, 16, 6]\n",
      "episode 3202, reward 951.0, memory_length 2000, epsilon 0.040575024227884414, time 730.0, rides 129\n",
      "Initial State is  [1, 2, 3]\n",
      "episode 3203, reward 891.0, memory_length 2000, epsilon 0.04053444920365653, time 727.0, rides 127\n",
      "Initial State is  [4, 1, 6]\n",
      "episode 3204, reward 950.0, memory_length 2000, epsilon 0.040493914754452874, time 729.0, rides 127\n",
      "Initial State is  [1, 9, 6]\n",
      "episode 3205, reward 844.0, memory_length 2000, epsilon 0.04045342083969842, time 730.0, rides 144\n",
      "Initial State is  [1, 2, 3]\n",
      "episode 3206, reward 955.0, memory_length 2000, epsilon 0.04041296741885872, time 731.0, rides 124\n",
      "Initial State is  [3, 0, 1]\n",
      "episode 3207, reward 589.0, memory_length 2000, epsilon 0.04037255445143986, time 728.0, rides 122\n",
      "Initial State is  [1, 20, 0]\n",
      "episode 3208, reward 902.0, memory_length 2000, epsilon 0.04033218189698842, time 733.0, rides 140\n",
      "Initial State is  [3, 4, 2]\n",
      "episode 3209, reward 718.0, memory_length 2000, epsilon 0.040291849715091435, time 735.0, rides 131\n",
      "Initial State is  [2, 11, 2]\n",
      "episode 3210, reward 699.0, memory_length 2000, epsilon 0.04025155786537634, time 732.0, rides 125\n",
      "Initial State is  [0, 16, 6]\n",
      "episode 3211, reward 779.0, memory_length 2000, epsilon 0.04021130630751096, time 733.0, rides 123\n",
      "Initial State is  [0, 10, 2]\n",
      "episode 3212, reward 417.0, memory_length 2000, epsilon 0.04017109500120345, time 739.0, rides 115\n",
      "Initial State is  [4, 5, 5]\n",
      "episode 3213, reward 712.0, memory_length 2000, epsilon 0.040130923906202244, time 723.0, rides 129\n",
      "Initial State is  [2, 9, 6]\n",
      "episode 3214, reward 945.0, memory_length 2000, epsilon 0.04009079298229604, time 728.0, rides 135\n",
      "Initial State is  [4, 4, 6]\n",
      "episode 3215, reward 871.0, memory_length 2000, epsilon 0.040050702189313746, time 722.0, rides 130\n",
      "Initial State is  [0, 3, 4]\n",
      "episode 3216, reward 1016.0, memory_length 2000, epsilon 0.04001065148712443, time 740.0, rides 127\n",
      "Initial State is  [4, 10, 4]\n",
      "episode 3217, reward 956.0, memory_length 2000, epsilon 0.03997064083563731, time 731.0, rides 121\n",
      "Initial State is  [0, 2, 6]\n",
      "episode 3218, reward 1114.0, memory_length 2000, epsilon 0.039930670194801676, time 726.0, rides 122\n",
      "Initial State is  [3, 2, 4]\n",
      "episode 3219, reward 921.0, memory_length 2000, epsilon 0.03989073952460687, time 730.0, rides 133\n",
      "Initial State is  [2, 3, 5]\n",
      "episode 3220, reward 1145.0, memory_length 2000, epsilon 0.039850848785082264, time 734.0, rides 130\n",
      "Initial State is  [2, 21, 3]\n",
      "episode 3221, reward 644.0, memory_length 2000, epsilon 0.03981099793629718, time 729.0, rides 127\n",
      "Initial State is  [3, 3, 5]\n",
      "episode 3222, reward 873.0, memory_length 2000, epsilon 0.03977118693836088, time 731.0, rides 115\n",
      "Initial State is  [1, 11, 1]\n",
      "episode 3223, reward 925.0, memory_length 2000, epsilon 0.03973141575142252, time 729.0, rides 131\n",
      "Initial State is  [4, 9, 4]\n",
      "episode 3224, reward 1078.0, memory_length 2000, epsilon 0.0396916843356711, time 724.0, rides 126\n",
      "Initial State is  [2, 9, 6]\n",
      "episode 3225, reward 1180.0, memory_length 2000, epsilon 0.03965199265133543, time 727.0, rides 122\n",
      "Initial State is  [1, 8, 6]\n",
      "episode 3226, reward 1135.0, memory_length 2000, epsilon 0.03961234065868409, time 727.0, rides 133\n",
      "Initial State is  [4, 6, 3]\n",
      "episode 3227, reward 755.0, memory_length 2000, epsilon 0.039572728318025406, time 735.0, rides 120\n",
      "Initial State is  [1, 5, 0]\n",
      "episode 3228, reward 845.0, memory_length 2000, epsilon 0.03953315558970738, time 732.0, rides 127\n",
      "Initial State is  [3, 2, 6]\n",
      "episode 3229, reward 938.0, memory_length 2000, epsilon 0.03949362243411767, time 731.0, rides 120\n",
      "Initial State is  [4, 0, 5]\n",
      "episode 3230, reward 1121.0, memory_length 2000, epsilon 0.03945412881168355, time 727.0, rides 120\n",
      "Initial State is  [4, 11, 4]\n",
      "episode 3231, reward 1047.0, memory_length 2000, epsilon 0.03941467468287187, time 732.0, rides 122\n",
      "Initial State is  [1, 14, 0]\n",
      "episode 3232, reward 757.0, memory_length 2000, epsilon 0.03937526000818899, time 729.0, rides 128\n",
      "Initial State is  [0, 8, 2]\n",
      "episode 3233, reward 1032.0, memory_length 2000, epsilon 0.039335884748180804, time 730.0, rides 122\n",
      "Initial State is  [2, 10, 1]\n",
      "episode 3234, reward 868.0, memory_length 2000, epsilon 0.039296548863432625, time 727.0, rides 127\n",
      "Initial State is  [3, 12, 6]\n",
      "episode 3235, reward 883.0, memory_length 2000, epsilon 0.03925725231456919, time 731.0, rides 125\n",
      "Initial State is  [2, 13, 5]\n",
      "episode 3236, reward 723.0, memory_length 2000, epsilon 0.039217995062254624, time 729.0, rides 124\n",
      "Initial State is  [3, 5, 5]\n",
      "episode 3237, reward 797.0, memory_length 2000, epsilon 0.03917877706719237, time 724.0, rides 116\n",
      "Initial State is  [1, 15, 4]\n",
      "episode 3238, reward 834.0, memory_length 2000, epsilon 0.03913959829012518, time 724.0, rides 129\n",
      "Initial State is  [0, 20, 5]\n",
      "episode 3239, reward 886.0, memory_length 2000, epsilon 0.03910045869183505, time 740.0, rides 130\n",
      "Initial State is  [4, 23, 3]\n",
      "episode 3240, reward 991.0, memory_length 2000, epsilon 0.03906135823314322, time 726.0, rides 125\n",
      "Initial State is  [4, 19, 2]\n",
      "episode 3241, reward 721.0, memory_length 2000, epsilon 0.03902229687491007, time 730.0, rides 133\n",
      "Initial State is  [4, 21, 5]\n",
      "episode 3242, reward 876.0, memory_length 2000, epsilon 0.03898327457803516, time 737.0, rides 136\n",
      "Initial State is  [4, 14, 0]\n",
      "episode 3243, reward 697.0, memory_length 2000, epsilon 0.038944291303457126, time 727.0, rides 139\n",
      "Initial State is  [3, 5, 3]\n",
      "episode 3244, reward 757.0, memory_length 2000, epsilon 0.03890534701215367, time 727.0, rides 127\n",
      "Initial State is  [2, 10, 2]\n",
      "episode 3245, reward 998.0, memory_length 2000, epsilon 0.03886644166514151, time 725.0, rides 137\n",
      "Initial State is  [0, 9, 1]\n",
      "episode 3246, reward 790.0, memory_length 2000, epsilon 0.03882757522347637, time 727.0, rides 131\n",
      "Initial State is  [3, 15, 3]\n",
      "episode 3247, reward 896.0, memory_length 2000, epsilon 0.038788747648252894, time 731.0, rides 125\n",
      "Initial State is  [2, 21, 2]\n",
      "episode 3248, reward 700.0, memory_length 2000, epsilon 0.03874995890060464, time 735.0, rides 132\n",
      "Initial State is  [2, 2, 1]\n",
      "episode 3249, reward 554.0, memory_length 2000, epsilon 0.03871120894170404, time 721.0, rides 135\n",
      "Initial State is  [1, 9, 5]\n",
      "episode 3250, reward 1215.0, memory_length 2000, epsilon 0.03867249773276234, time 727.0, rides 129\n",
      "Initial State is  [1, 10, 4]\n",
      "episode 3251, reward 882.0, memory_length 2000, epsilon 0.038633825235029576, time 726.0, rides 137\n",
      "Initial State is  [2, 8, 0]\n",
      "episode 3252, reward 1037.0, memory_length 2000, epsilon 0.038595191409794546, time 736.0, rides 130\n",
      "Initial State is  [4, 10, 2]\n",
      "episode 3253, reward 981.0, memory_length 2000, epsilon 0.03855659621838475, time 726.0, rides 126\n",
      "Initial State is  [0, 8, 2]\n",
      "episode 3254, reward 516.0, memory_length 2000, epsilon 0.03851803962216637, time 726.0, rides 134\n",
      "Initial State is  [3, 16, 0]\n",
      "episode 3255, reward 823.0, memory_length 2000, epsilon 0.0384795215825442, time 732.0, rides 135\n",
      "Initial State is  [1, 16, 0]\n",
      "episode 3256, reward 749.0, memory_length 2000, epsilon 0.03844104206096166, time 724.0, rides 122\n",
      "Initial State is  [4, 14, 1]\n",
      "episode 3257, reward 827.0, memory_length 2000, epsilon 0.0384026010189007, time 728.0, rides 128\n",
      "Initial State is  [1, 3, 5]\n",
      "episode 3258, reward 836.0, memory_length 2000, epsilon 0.0383641984178818, time 737.0, rides 130\n",
      "Initial State is  [3, 10, 3]\n",
      "episode 3259, reward 829.0, memory_length 2000, epsilon 0.03832583421946392, time 734.0, rides 122\n",
      "Initial State is  [0, 7, 1]\n",
      "episode 3260, reward 1137.0, memory_length 2000, epsilon 0.03828750838524446, time 727.0, rides 124\n",
      "Initial State is  [3, 8, 4]\n",
      "episode 3261, reward 809.0, memory_length 2000, epsilon 0.03824922087685922, time 727.0, rides 133\n",
      "Initial State is  [4, 9, 5]\n",
      "episode 3262, reward 558.0, memory_length 2000, epsilon 0.038210971655982355, time 724.0, rides 120\n",
      "Initial State is  [2, 23, 3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 3263, reward 847.0, memory_length 2000, epsilon 0.03817276068432637, time 731.0, rides 125\n",
      "Initial State is  [2, 23, 0]\n",
      "episode 3264, reward 568.0, memory_length 2000, epsilon 0.03813458792364205, time 726.0, rides 125\n",
      "Initial State is  [0, 8, 0]\n",
      "episode 3265, reward 861.0, memory_length 2000, epsilon 0.03809645333571841, time 729.0, rides 119\n",
      "Initial State is  [2, 0, 5]\n",
      "episode 3266, reward 849.0, memory_length 2000, epsilon 0.03805835688238269, time 727.0, rides 126\n",
      "Initial State is  [0, 17, 4]\n",
      "episode 3267, reward 1166.0, memory_length 2000, epsilon 0.038020298525500304, time 731.0, rides 120\n",
      "Initial State is  [0, 8, 3]\n",
      "episode 3268, reward 1074.0, memory_length 2000, epsilon 0.0379822782269748, time 724.0, rides 127\n",
      "Initial State is  [3, 16, 5]\n",
      "episode 3269, reward 691.0, memory_length 2000, epsilon 0.037944295948747826, time 727.0, rides 123\n",
      "Initial State is  [1, 5, 6]\n",
      "episode 3270, reward 700.0, memory_length 2000, epsilon 0.03790635165279908, time 729.0, rides 132\n",
      "Initial State is  [2, 3, 4]\n",
      "episode 3271, reward 858.0, memory_length 2000, epsilon 0.037868445301146275, time 724.0, rides 131\n",
      "Initial State is  [4, 11, 1]\n",
      "episode 3272, reward 943.0, memory_length 2000, epsilon 0.03783057685584513, time 729.0, rides 132\n",
      "Initial State is  [1, 9, 6]\n",
      "episode 3273, reward 1202.0, memory_length 2000, epsilon 0.037792746278989285, time 726.0, rides 135\n",
      "Initial State is  [2, 21, 3]\n",
      "episode 3274, reward 726.0, memory_length 2000, epsilon 0.03775495353271029, time 725.0, rides 120\n",
      "Initial State is  [1, 0, 1]\n",
      "episode 3275, reward 698.0, memory_length 2000, epsilon 0.03771719857917758, time 735.0, rides 136\n",
      "Initial State is  [1, 11, 4]\n",
      "episode 3276, reward 922.0, memory_length 2000, epsilon 0.0376794813805984, time 722.0, rides 136\n",
      "Initial State is  [0, 12, 3]\n",
      "episode 3277, reward 953.0, memory_length 2000, epsilon 0.0376418018992178, time 727.0, rides 134\n",
      "Initial State is  [3, 21, 3]\n",
      "episode 3278, reward 911.0, memory_length 2000, epsilon 0.03760416009731858, time 725.0, rides 123\n",
      "Initial State is  [4, 12, 3]\n",
      "episode 3279, reward 769.0, memory_length 2000, epsilon 0.03756655593722126, time 731.0, rides 124\n",
      "Initial State is  [4, 6, 4]\n",
      "episode 3280, reward 1035.0, memory_length 2000, epsilon 0.03752898938128404, time 722.0, rides 123\n",
      "Initial State is  [2, 15, 4]\n",
      "episode 3281, reward 691.0, memory_length 2000, epsilon 0.03749146039190275, time 732.0, rides 131\n",
      "Initial State is  [0, 14, 2]\n",
      "episode 3282, reward 593.0, memory_length 2000, epsilon 0.037453968931510845, time 728.0, rides 110\n",
      "Initial State is  [3, 15, 4]\n",
      "episode 3283, reward 660.0, memory_length 2000, epsilon 0.037416514962579334, time 725.0, rides 114\n",
      "Initial State is  [2, 17, 2]\n",
      "episode 3284, reward 905.0, memory_length 2000, epsilon 0.037379098447616756, time 725.0, rides 134\n",
      "Initial State is  [2, 15, 6]\n",
      "episode 3285, reward 1042.0, memory_length 2000, epsilon 0.03734171934916914, time 728.0, rides 129\n",
      "Initial State is  [2, 7, 5]\n",
      "episode 3286, reward 951.0, memory_length 2000, epsilon 0.037304377629819974, time 721.0, rides 132\n",
      "Initial State is  [0, 14, 6]\n",
      "episode 3287, reward 1007.0, memory_length 2000, epsilon 0.037267073252190155, time 738.0, rides 126\n",
      "Initial State is  [3, 12, 2]\n",
      "episode 3288, reward 674.0, memory_length 2000, epsilon 0.037229806178937966, time 731.0, rides 129\n",
      "Initial State is  [1, 23, 3]\n",
      "episode 3289, reward 934.0, memory_length 2000, epsilon 0.03719257637275903, time 723.0, rides 140\n",
      "Initial State is  [2, 9, 1]\n",
      "episode 3290, reward 746.0, memory_length 2000, epsilon 0.03715538379638627, time 723.0, rides 116\n",
      "Initial State is  [1, 21, 5]\n",
      "episode 3291, reward 738.0, memory_length 2000, epsilon 0.03711822841258988, time 728.0, rides 126\n",
      "Initial State is  [4, 14, 0]\n",
      "episode 3292, reward 900.0, memory_length 2000, epsilon 0.03708111018417729, time 725.0, rides 130\n",
      "Initial State is  [0, 5, 0]\n",
      "episode 3293, reward 711.0, memory_length 2000, epsilon 0.037044029073993116, time 731.0, rides 115\n",
      "Initial State is  [2, 20, 5]\n",
      "episode 3294, reward 847.0, memory_length 2000, epsilon 0.03700698504491912, time 722.0, rides 129\n",
      "Initial State is  [3, 16, 4]\n",
      "episode 3295, reward 791.0, memory_length 2000, epsilon 0.0369699780598742, time 732.0, rides 121\n",
      "Initial State is  [3, 13, 4]\n",
      "episode 3296, reward 687.0, memory_length 2000, epsilon 0.03693300808181433, time 734.0, rides 127\n",
      "Initial State is  [0, 16, 4]\n",
      "episode 3297, reward 942.0, memory_length 2000, epsilon 0.036896075073732514, time 727.0, rides 130\n",
      "Initial State is  [1, 11, 2]\n",
      "episode 3298, reward 687.0, memory_length 2000, epsilon 0.03685917899865878, time 728.0, rides 129\n",
      "Initial State is  [0, 6, 1]\n",
      "episode 3299, reward 946.0, memory_length 2000, epsilon 0.036822319819660124, time 729.0, rides 124\n",
      "Initial State is  [1, 8, 0]\n",
      "episode 3300, reward 958.0, memory_length 2000, epsilon 0.03678549749984046, time 725.0, rides 123\n",
      "Initial State is  [4, 0, 4]\n",
      "episode 3301, reward 955.0, memory_length 2000, epsilon 0.036748712002340624, time 729.0, rides 135\n",
      "Initial State is  [4, 8, 5]\n",
      "episode 3302, reward 940.0, memory_length 2000, epsilon 0.03671196329033828, time 729.0, rides 138\n",
      "Initial State is  [3, 1, 3]\n",
      "episode 3303, reward 862.0, memory_length 2000, epsilon 0.03667525132704794, time 730.0, rides 128\n",
      "Initial State is  [2, 16, 4]\n",
      "episode 3304, reward 565.0, memory_length 2000, epsilon 0.036638576075720894, time 731.0, rides 132\n",
      "Initial State is  [0, 12, 4]\n",
      "episode 3305, reward 717.0, memory_length 2000, epsilon 0.036601937499645174, time 722.0, rides 116\n",
      "Initial State is  [1, 8, 2]\n",
      "episode 3306, reward 527.0, memory_length 2000, epsilon 0.03656533556214553, time 732.0, rides 126\n",
      "Initial State is  [4, 7, 5]\n",
      "episode 3307, reward 819.0, memory_length 2000, epsilon 0.036528770226583386, time 731.0, rides 129\n",
      "Initial State is  [4, 0, 3]\n",
      "episode 3308, reward 969.0, memory_length 2000, epsilon 0.0364922414563568, time 727.0, rides 133\n",
      "Initial State is  [0, 11, 6]\n",
      "episode 3309, reward 840.0, memory_length 2000, epsilon 0.03645574921490045, time 727.0, rides 130\n",
      "Initial State is  [0, 19, 6]\n",
      "episode 3310, reward 839.0, memory_length 2000, epsilon 0.036419293465685544, time 728.0, rides 126\n",
      "Initial State is  [0, 9, 6]\n",
      "episode 3311, reward 941.0, memory_length 2000, epsilon 0.03638287417221986, time 727.0, rides 121\n",
      "Initial State is  [1, 3, 4]\n",
      "episode 3312, reward 802.0, memory_length 2000, epsilon 0.03634649129804764, time 735.0, rides 119\n",
      "Initial State is  [4, 0, 1]\n",
      "episode 3313, reward 662.0, memory_length 2000, epsilon 0.036310144806749586, time 725.0, rides 127\n",
      "Initial State is  [3, 0, 2]\n",
      "episode 3314, reward 694.0, memory_length 2000, epsilon 0.03627383466194284, time 720.0, rides 119\n",
      "Initial State is  [2, 21, 1]\n",
      "episode 3315, reward 991.0, memory_length 2000, epsilon 0.0362375608272809, time 734.0, rides 144\n",
      "Initial State is  [0, 11, 3]\n",
      "episode 3316, reward 500.0, memory_length 2000, epsilon 0.03620132326645362, time 732.0, rides 131\n",
      "Initial State is  [0, 10, 2]\n",
      "episode 3317, reward 907.0, memory_length 2000, epsilon 0.03616512194318716, time 726.0, rides 129\n",
      "Initial State is  [3, 16, 1]\n",
      "episode 3318, reward 884.0, memory_length 2000, epsilon 0.03612895682124397, time 726.0, rides 126\n",
      "Initial State is  [3, 3, 4]\n",
      "episode 3319, reward 1133.0, memory_length 2000, epsilon 0.03609282786442273, time 729.0, rides 134\n",
      "Initial State is  [3, 0, 6]\n",
      "episode 3320, reward 1024.0, memory_length 2000, epsilon 0.036056735036558304, time 728.0, rides 124\n",
      "Initial State is  [2, 20, 1]\n",
      "episode 3321, reward 595.0, memory_length 2000, epsilon 0.036020678301521745, time 731.0, rides 121\n",
      "Initial State is  [4, 17, 1]\n",
      "episode 3322, reward 961.0, memory_length 2000, epsilon 0.03598465762322022, time 736.0, rides 119\n",
      "Initial State is  [3, 6, 1]\n",
      "episode 3323, reward 638.0, memory_length 2000, epsilon 0.035948672965597, time 730.0, rides 122\n",
      "Initial State is  [0, 9, 0]\n",
      "episode 3324, reward 978.0, memory_length 2000, epsilon 0.035912724292631405, time 729.0, rides 130\n",
      "Initial State is  [4, 14, 5]\n",
      "episode 3325, reward 944.0, memory_length 2000, epsilon 0.03587681156833877, time 728.0, rides 138\n",
      "Initial State is  [0, 21, 4]\n",
      "episode 3326, reward 1055.0, memory_length 2000, epsilon 0.035840934756770436, time 736.0, rides 126\n",
      "Initial State is  [3, 16, 2]\n",
      "episode 3327, reward 752.0, memory_length 2000, epsilon 0.03580509382201367, time 729.0, rides 129\n",
      "Initial State is  [1, 22, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 3328, reward 952.0, memory_length 2000, epsilon 0.035769288728191656, time 731.0, rides 129\n",
      "Initial State is  [1, 20, 5]\n",
      "episode 3329, reward 1050.0, memory_length 2000, epsilon 0.035733519439463464, time 721.0, rides 119\n",
      "Initial State is  [2, 12, 3]\n",
      "episode 3330, reward 819.0, memory_length 2000, epsilon 0.035697785920024, time 730.0, rides 135\n",
      "Initial State is  [1, 1, 2]\n",
      "episode 3331, reward 692.0, memory_length 2000, epsilon 0.03566208813410397, time 727.0, rides 144\n",
      "Initial State is  [2, 7, 1]\n",
      "episode 3332, reward 755.0, memory_length 2000, epsilon 0.03562642604596987, time 723.0, rides 130\n",
      "Initial State is  [1, 19, 4]\n",
      "episode 3333, reward 1018.0, memory_length 2000, epsilon 0.035590799619923896, time 722.0, rides 134\n",
      "Initial State is  [3, 15, 4]\n",
      "episode 3334, reward 906.0, memory_length 2000, epsilon 0.035555208820303975, time 728.0, rides 127\n",
      "Initial State is  [3, 18, 6]\n",
      "episode 3335, reward 534.0, memory_length 2000, epsilon 0.035519653611483674, time 731.0, rides 121\n",
      "Initial State is  [1, 14, 5]\n",
      "episode 3336, reward 954.0, memory_length 2000, epsilon 0.03548413395787219, time 730.0, rides 130\n",
      "Initial State is  [2, 19, 5]\n",
      "episode 3337, reward 578.0, memory_length 2000, epsilon 0.03544864982391432, time 732.0, rides 125\n",
      "Initial State is  [0, 4, 2]\n",
      "episode 3338, reward 1031.0, memory_length 2000, epsilon 0.0354132011740904, time 731.0, rides 138\n",
      "Initial State is  [2, 20, 4]\n",
      "episode 3339, reward 867.0, memory_length 2000, epsilon 0.03537778797291631, time 727.0, rides 113\n",
      "Initial State is  [1, 2, 2]\n",
      "episode 3340, reward 912.0, memory_length 2000, epsilon 0.03534241018494339, time 730.0, rides 129\n",
      "Initial State is  [1, 11, 0]\n",
      "episode 3341, reward 594.0, memory_length 2000, epsilon 0.03530706777475845, time 724.0, rides 113\n",
      "Initial State is  [3, 21, 2]\n",
      "episode 3342, reward 584.0, memory_length 2000, epsilon 0.03527176070698369, time 728.0, rides 120\n",
      "Initial State is  [1, 11, 3]\n",
      "episode 3343, reward 644.0, memory_length 2000, epsilon 0.035236488946276706, time 728.0, rides 131\n",
      "Initial State is  [1, 18, 2]\n",
      "episode 3344, reward 683.0, memory_length 2000, epsilon 0.03520125245733043, time 728.0, rides 131\n",
      "Initial State is  [2, 22, 6]\n",
      "episode 3345, reward 1002.0, memory_length 2000, epsilon 0.0351660512048731, time 727.0, rides 122\n",
      "Initial State is  [2, 0, 2]\n",
      "episode 3346, reward 816.0, memory_length 2000, epsilon 0.03513088515366823, time 732.0, rides 120\n",
      "Initial State is  [1, 6, 5]\n",
      "episode 3347, reward 832.0, memory_length 2000, epsilon 0.035095754268514565, time 733.0, rides 134\n",
      "Initial State is  [1, 14, 6]\n",
      "episode 3348, reward 1128.0, memory_length 2000, epsilon 0.03506065851424605, time 729.0, rides 136\n",
      "Initial State is  [0, 4, 5]\n",
      "episode 3349, reward 833.0, memory_length 2000, epsilon 0.0350255978557318, time 729.0, rides 124\n",
      "Initial State is  [4, 21, 0]\n",
      "episode 3350, reward 855.0, memory_length 2000, epsilon 0.03499057225787607, time 726.0, rides 124\n",
      "Initial State is  [1, 3, 2]\n",
      "episode 3351, reward 852.0, memory_length 2000, epsilon 0.034955581685618194, time 729.0, rides 125\n",
      "Initial State is  [4, 13, 2]\n",
      "episode 3352, reward 856.0, memory_length 2000, epsilon 0.034920626103932574, time 723.0, rides 139\n",
      "Initial State is  [1, 21, 6]\n",
      "episode 3353, reward 968.0, memory_length 2000, epsilon 0.03488570547782864, time 730.0, rides 115\n",
      "Initial State is  [0, 2, 3]\n",
      "episode 3354, reward 757.0, memory_length 2000, epsilon 0.03485081977235081, time 729.0, rides 123\n",
      "Initial State is  [0, 17, 1]\n",
      "episode 3355, reward 892.0, memory_length 2000, epsilon 0.03481596895257846, time 732.0, rides 136\n",
      "Initial State is  [2, 21, 3]\n",
      "episode 3356, reward 678.0, memory_length 2000, epsilon 0.034781152983625885, time 723.0, rides 123\n",
      "Initial State is  [3, 7, 0]\n",
      "episode 3357, reward 1045.0, memory_length 2000, epsilon 0.03474637183064226, time 727.0, rides 128\n",
      "Initial State is  [2, 7, 0]\n",
      "episode 3358, reward 1046.0, memory_length 2000, epsilon 0.03471162545881162, time 731.0, rides 125\n",
      "Initial State is  [2, 7, 4]\n",
      "episode 3359, reward 884.0, memory_length 2000, epsilon 0.03467691383335281, time 731.0, rides 140\n",
      "Initial State is  [1, 3, 4]\n",
      "episode 3360, reward 802.0, memory_length 2000, epsilon 0.034642236919519453, time 730.0, rides 130\n",
      "Initial State is  [1, 5, 1]\n",
      "episode 3361, reward 837.0, memory_length 2000, epsilon 0.03460759468259993, time 729.0, rides 133\n",
      "Initial State is  [4, 21, 0]\n",
      "episode 3362, reward 700.0, memory_length 2000, epsilon 0.03457298708791733, time 734.0, rides 140\n",
      "Initial State is  [3, 4, 0]\n",
      "episode 3363, reward 604.0, memory_length 2000, epsilon 0.03453841410082942, time 728.0, rides 115\n",
      "Initial State is  [1, 3, 3]\n",
      "episode 3364, reward 734.0, memory_length 2000, epsilon 0.03450387568672859, time 732.0, rides 137\n",
      "Initial State is  [3, 0, 4]\n",
      "episode 3365, reward 840.0, memory_length 2000, epsilon 0.03446937181104186, time 734.0, rides 118\n",
      "Initial State is  [2, 3, 2]\n",
      "episode 3366, reward 907.0, memory_length 2000, epsilon 0.034434902439230815, time 732.0, rides 120\n",
      "Initial State is  [2, 5, 1]\n",
      "episode 3367, reward 746.0, memory_length 2000, epsilon 0.03440046753679158, time 731.0, rides 122\n",
      "Initial State is  [1, 11, 1]\n",
      "episode 3368, reward 829.0, memory_length 2000, epsilon 0.03436606706925479, time 726.0, rides 124\n",
      "Initial State is  [2, 12, 0]\n",
      "episode 3369, reward 639.0, memory_length 2000, epsilon 0.034331701002185536, time 731.0, rides 127\n",
      "Initial State is  [2, 2, 4]\n",
      "episode 3370, reward 999.0, memory_length 2000, epsilon 0.03429736930118335, time 727.0, rides 127\n",
      "Initial State is  [2, 3, 3]\n",
      "episode 3371, reward 918.0, memory_length 2000, epsilon 0.03426307193188217, time 738.0, rides 128\n",
      "Initial State is  [0, 18, 3]\n",
      "episode 3372, reward 776.0, memory_length 2000, epsilon 0.034228808859950284, time 731.0, rides 127\n",
      "Initial State is  [4, 20, 2]\n",
      "episode 3373, reward 878.0, memory_length 2000, epsilon 0.034194580051090336, time 729.0, rides 132\n",
      "Initial State is  [0, 0, 4]\n",
      "episode 3374, reward 697.0, memory_length 2000, epsilon 0.034160385471039244, time 725.0, rides 123\n",
      "Initial State is  [2, 0, 0]\n",
      "episode 3375, reward 1005.0, memory_length 2000, epsilon 0.0341262250855682, time 732.0, rides 123\n",
      "Initial State is  [1, 16, 1]\n",
      "episode 3376, reward 751.0, memory_length 2000, epsilon 0.03409209886048263, time 724.0, rides 132\n",
      "Initial State is  [2, 17, 6]\n",
      "episode 3377, reward 863.0, memory_length 2000, epsilon 0.03405800676162215, time 728.0, rides 130\n",
      "Initial State is  [0, 7, 3]\n",
      "episode 3378, reward 892.0, memory_length 2000, epsilon 0.03402394875486053, time 729.0, rides 142\n",
      "Initial State is  [0, 14, 1]\n",
      "episode 3379, reward 1039.0, memory_length 2000, epsilon 0.033989924806105666, time 742.0, rides 131\n",
      "Initial State is  [4, 14, 6]\n",
      "episode 3380, reward 1012.0, memory_length 2000, epsilon 0.03395593488129956, time 730.0, rides 131\n",
      "Initial State is  [4, 19, 1]\n",
      "episode 3381, reward 644.0, memory_length 2000, epsilon 0.03392197894641826, time 721.0, rides 130\n",
      "Initial State is  [1, 16, 0]\n",
      "episode 3382, reward 824.0, memory_length 2000, epsilon 0.033888056967471845, time 728.0, rides 133\n",
      "Initial State is  [4, 21, 6]\n",
      "episode 3383, reward 932.0, memory_length 2000, epsilon 0.03385416891050437, time 736.0, rides 151\n",
      "Initial State is  [2, 9, 2]\n",
      "episode 3384, reward 821.0, memory_length 2000, epsilon 0.03382031474159387, time 739.0, rides 133\n",
      "Initial State is  [2, 3, 3]\n",
      "episode 3385, reward 933.0, memory_length 2000, epsilon 0.033786494426852276, time 724.0, rides 140\n",
      "Initial State is  [3, 9, 3]\n",
      "episode 3386, reward 707.0, memory_length 2000, epsilon 0.03375270793242542, time 724.0, rides 126\n",
      "Initial State is  [1, 6, 5]\n",
      "episode 3387, reward 926.0, memory_length 2000, epsilon 0.033718955224492995, time 732.0, rides 123\n",
      "Initial State is  [3, 2, 0]\n",
      "episode 3388, reward 1119.0, memory_length 2000, epsilon 0.033685236269268504, time 725.0, rides 138\n",
      "Initial State is  [4, 5, 5]\n",
      "episode 3389, reward 1196.0, memory_length 2000, epsilon 0.03365155103299924, time 722.0, rides 132\n",
      "Initial State is  [1, 8, 1]\n",
      "episode 3390, reward 624.0, memory_length 2000, epsilon 0.03361789948196624, time 729.0, rides 137\n",
      "Initial State is  [4, 4, 0]\n",
      "episode 3391, reward 1007.0, memory_length 2000, epsilon 0.03358428158248427, time 730.0, rides 130\n",
      "Initial State is  [2, 12, 4]\n",
      "episode 3392, reward 692.0, memory_length 2000, epsilon 0.03355069730090179, time 726.0, rides 139\n",
      "Initial State is  [4, 5, 3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 3393, reward 1285.0, memory_length 2000, epsilon 0.03351714660360088, time 728.0, rides 130\n",
      "Initial State is  [2, 3, 6]\n",
      "episode 3394, reward 1118.0, memory_length 2000, epsilon 0.03348362945699728, time 726.0, rides 128\n",
      "Initial State is  [4, 1, 3]\n",
      "episode 3395, reward 925.0, memory_length 2000, epsilon 0.03345014582754029, time 727.0, rides 135\n",
      "Initial State is  [3, 16, 6]\n",
      "episode 3396, reward 766.0, memory_length 2000, epsilon 0.033416695681712745, time 727.0, rides 129\n",
      "Initial State is  [1, 0, 0]\n",
      "episode 3397, reward 1045.0, memory_length 2000, epsilon 0.03338327898603103, time 724.0, rides 143\n",
      "Initial State is  [2, 6, 4]\n",
      "episode 3398, reward 483.0, memory_length 2000, epsilon 0.033349895707044996, time 734.0, rides 137\n",
      "Initial State is  [2, 18, 4]\n",
      "episode 3399, reward 769.0, memory_length 2000, epsilon 0.03331654581133795, time 724.0, rides 116\n",
      "Initial State is  [3, 16, 0]\n",
      "episode 3400, reward 1107.0, memory_length 2000, epsilon 0.03328322926552661, time 725.0, rides 127\n",
      "Initial State is  [4, 0, 1]\n",
      "episode 3401, reward 1043.0, memory_length 2000, epsilon 0.03324994603626109, time 725.0, rides 126\n",
      "Initial State is  [4, 6, 5]\n",
      "episode 3402, reward 1089.0, memory_length 2000, epsilon 0.033216696090224825, time 723.0, rides 148\n",
      "Initial State is  [2, 21, 3]\n",
      "episode 3403, reward 943.0, memory_length 2000, epsilon 0.0331834793941346, time 722.0, rides 142\n",
      "Initial State is  [3, 23, 4]\n",
      "episode 3404, reward 1162.0, memory_length 2000, epsilon 0.03315029591474047, time 732.0, rides 143\n",
      "Initial State is  [0, 0, 6]\n",
      "episode 3405, reward 883.0, memory_length 2000, epsilon 0.03311714561882573, time 732.0, rides 130\n",
      "Initial State is  [2, 16, 3]\n",
      "episode 3406, reward 730.0, memory_length 2000, epsilon 0.0330840284732069, time 731.0, rides 133\n",
      "Initial State is  [4, 17, 6]\n",
      "episode 3407, reward 850.0, memory_length 2000, epsilon 0.03305094444473369, time 731.0, rides 126\n",
      "Initial State is  [1, 16, 4]\n",
      "episode 3408, reward 729.0, memory_length 2000, epsilon 0.03301789350028896, time 728.0, rides 136\n",
      "Initial State is  [2, 14, 1]\n",
      "episode 3409, reward 1093.0, memory_length 2000, epsilon 0.03298487560678867, time 732.0, rides 134\n",
      "Initial State is  [2, 17, 0]\n",
      "episode 3410, reward 679.0, memory_length 2000, epsilon 0.03295189073118188, time 722.0, rides 120\n",
      "Initial State is  [1, 5, 1]\n",
      "episode 3411, reward 1196.0, memory_length 2000, epsilon 0.032918938840450704, time 723.0, rides 124\n",
      "Initial State is  [0, 4, 3]\n",
      "episode 3412, reward 1070.0, memory_length 2000, epsilon 0.032886019901610254, time 729.0, rides 134\n",
      "Initial State is  [1, 11, 5]\n",
      "episode 3413, reward 582.0, memory_length 2000, epsilon 0.03285313388170864, time 730.0, rides 127\n",
      "Initial State is  [4, 13, 6]\n",
      "episode 3414, reward 960.0, memory_length 2000, epsilon 0.03282028074782693, time 727.0, rides 128\n",
      "Initial State is  [1, 7, 5]\n",
      "episode 3415, reward 924.0, memory_length 2000, epsilon 0.03278746046707911, time 724.0, rides 132\n",
      "Initial State is  [0, 8, 6]\n",
      "episode 3416, reward 924.0, memory_length 2000, epsilon 0.03275467300661203, time 734.0, rides 146\n",
      "Initial State is  [0, 13, 6]\n",
      "episode 3417, reward 787.0, memory_length 2000, epsilon 0.03272191833360542, time 728.0, rides 134\n",
      "Initial State is  [1, 18, 4]\n",
      "episode 3418, reward 908.0, memory_length 2000, epsilon 0.032689196415271814, time 730.0, rides 143\n",
      "Initial State is  [2, 5, 3]\n",
      "episode 3419, reward 729.0, memory_length 2000, epsilon 0.032656507218856545, time 731.0, rides 143\n",
      "Initial State is  [4, 16, 5]\n",
      "episode 3420, reward 1040.0, memory_length 2000, epsilon 0.03262385071163769, time 730.0, rides 143\n",
      "Initial State is  [0, 2, 0]\n",
      "episode 3421, reward 946.0, memory_length 2000, epsilon 0.032591226860926054, time 731.0, rides 131\n",
      "Initial State is  [2, 4, 0]\n",
      "episode 3422, reward 962.0, memory_length 2000, epsilon 0.03255863563406513, time 730.0, rides 140\n",
      "Initial State is  [0, 5, 0]\n",
      "episode 3423, reward 923.0, memory_length 2000, epsilon 0.03252607699843106, time 727.0, rides 127\n",
      "Initial State is  [0, 18, 2]\n",
      "episode 3424, reward 676.0, memory_length 2000, epsilon 0.03249355092143263, time 728.0, rides 144\n",
      "Initial State is  [1, 20, 2]\n",
      "episode 3425, reward 1143.0, memory_length 2000, epsilon 0.03246105737051119, time 730.0, rides 150\n",
      "Initial State is  [0, 17, 0]\n",
      "episode 3426, reward 1300.0, memory_length 2000, epsilon 0.032428596313140684, time 732.0, rides 146\n",
      "Initial State is  [3, 22, 4]\n",
      "episode 3427, reward 1258.0, memory_length 2000, epsilon 0.032396167716827545, time 725.0, rides 127\n",
      "Initial State is  [2, 14, 5]\n",
      "episode 3428, reward 914.0, memory_length 2000, epsilon 0.032363771549110715, time 730.0, rides 133\n",
      "Initial State is  [2, 19, 3]\n",
      "episode 3429, reward 1077.0, memory_length 2000, epsilon 0.032331407777561605, time 737.0, rides 124\n",
      "Initial State is  [4, 20, 3]\n",
      "episode 3430, reward 1054.0, memory_length 2000, epsilon 0.03229907636978405, time 731.0, rides 134\n",
      "Initial State is  [4, 5, 5]\n",
      "episode 3431, reward 892.0, memory_length 2000, epsilon 0.03226677729341426, time 734.0, rides 132\n",
      "Initial State is  [0, 8, 6]\n",
      "episode 3432, reward 1051.0, memory_length 2000, epsilon 0.03223451051612085, time 725.0, rides 132\n",
      "Initial State is  [2, 23, 5]\n",
      "episode 3433, reward 1036.0, memory_length 2000, epsilon 0.03220227600560473, time 726.0, rides 134\n",
      "Initial State is  [2, 11, 3]\n",
      "episode 3434, reward 1045.0, memory_length 2000, epsilon 0.032170073729599125, time 724.0, rides 132\n",
      "Initial State is  [0, 8, 5]\n",
      "episode 3435, reward 989.0, memory_length 2000, epsilon 0.03213790365586953, time 723.0, rides 137\n",
      "Initial State is  [4, 9, 0]\n",
      "episode 3436, reward 976.0, memory_length 2000, epsilon 0.03210576575221366, time 728.0, rides 138\n",
      "Initial State is  [4, 6, 5]\n",
      "episode 3437, reward 877.0, memory_length 2000, epsilon 0.032073659986461445, time 729.0, rides 141\n",
      "Initial State is  [2, 13, 5]\n",
      "episode 3438, reward 827.0, memory_length 2000, epsilon 0.03204158632647498, time 723.0, rides 122\n",
      "Initial State is  [0, 20, 6]\n",
      "episode 3439, reward 1078.0, memory_length 2000, epsilon 0.032009544740148506, time 727.0, rides 128\n",
      "Initial State is  [0, 18, 1]\n",
      "episode 3440, reward 797.0, memory_length 2000, epsilon 0.03197753519540836, time 728.0, rides 138\n",
      "Initial State is  [1, 3, 2]\n",
      "episode 3441, reward 1055.0, memory_length 2000, epsilon 0.031945557660212946, time 727.0, rides 123\n",
      "Initial State is  [2, 4, 3]\n",
      "episode 3442, reward 1200.0, memory_length 2000, epsilon 0.03191361210255273, time 726.0, rides 138\n",
      "Initial State is  [0, 12, 6]\n",
      "episode 3443, reward 791.0, memory_length 2000, epsilon 0.03188169849045018, time 730.0, rides 131\n",
      "Initial State is  [3, 7, 1]\n",
      "episode 3444, reward 717.0, memory_length 2000, epsilon 0.03184981679195973, time 724.0, rides 129\n",
      "Initial State is  [3, 8, 0]\n",
      "episode 3445, reward 870.0, memory_length 2000, epsilon 0.03181796697516777, time 733.0, rides 137\n",
      "Initial State is  [2, 0, 6]\n",
      "episode 3446, reward 972.0, memory_length 2000, epsilon 0.0317861490081926, time 722.0, rides 136\n",
      "Initial State is  [2, 4, 5]\n",
      "episode 3447, reward 890.0, memory_length 2000, epsilon 0.031754362859184405, time 732.0, rides 133\n",
      "Initial State is  [4, 18, 6]\n",
      "episode 3448, reward 532.0, memory_length 2000, epsilon 0.03172260849632522, time 731.0, rides 143\n",
      "Initial State is  [0, 19, 1]\n",
      "episode 3449, reward 961.0, memory_length 2000, epsilon 0.031690885887828896, time 726.0, rides 131\n",
      "Initial State is  [4, 10, 4]\n",
      "episode 3450, reward 630.0, memory_length 2000, epsilon 0.031659195001941066, time 727.0, rides 134\n",
      "Initial State is  [0, 15, 2]\n",
      "episode 3451, reward 770.0, memory_length 2000, epsilon 0.031627535806939125, time 727.0, rides 126\n",
      "Initial State is  [2, 17, 5]\n",
      "episode 3452, reward 1015.0, memory_length 2000, epsilon 0.031595908271132185, time 731.0, rides 136\n",
      "Initial State is  [3, 14, 5]\n",
      "episode 3453, reward 889.0, memory_length 2000, epsilon 0.03156431236286105, time 722.0, rides 132\n",
      "Initial State is  [0, 10, 1]\n",
      "episode 3454, reward 749.0, memory_length 2000, epsilon 0.03153274805049819, time 723.0, rides 140\n",
      "Initial State is  [1, 22, 0]\n",
      "episode 3455, reward 1207.0, memory_length 2000, epsilon 0.03150121530244769, time 731.0, rides 131\n",
      "Initial State is  [3, 14, 3]\n",
      "episode 3456, reward 936.0, memory_length 2000, epsilon 0.031469714087145245, time 734.0, rides 137\n",
      "Initial State is  [4, 23, 2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 3457, reward 972.0, memory_length 2000, epsilon 0.0314382443730581, time 734.0, rides 134\n",
      "Initial State is  [4, 4, 4]\n",
      "episode 3458, reward 705.0, memory_length 2000, epsilon 0.031406806128685044, time 723.0, rides 143\n",
      "Initial State is  [3, 22, 4]\n",
      "episode 3459, reward 837.0, memory_length 2000, epsilon 0.03137539932255636, time 723.0, rides 127\n",
      "Initial State is  [4, 10, 2]\n",
      "episode 3460, reward 688.0, memory_length 2000, epsilon 0.03134402392323381, time 722.0, rides 130\n",
      "Initial State is  [3, 2, 4]\n",
      "episode 3461, reward 765.0, memory_length 2000, epsilon 0.03131267989931057, time 731.0, rides 137\n",
      "Initial State is  [3, 21, 5]\n",
      "episode 3462, reward 649.0, memory_length 2000, epsilon 0.031281367219411264, time 729.0, rides 118\n",
      "Initial State is  [1, 8, 0]\n",
      "episode 3463, reward 864.0, memory_length 2000, epsilon 0.031250085852191856, time 733.0, rides 127\n",
      "Initial State is  [2, 14, 6]\n",
      "episode 3464, reward 659.0, memory_length 2000, epsilon 0.031218835766339662, time 740.0, rides 126\n",
      "Initial State is  [1, 11, 3]\n",
      "episode 3465, reward 836.0, memory_length 2000, epsilon 0.03118761693057332, time 728.0, rides 127\n",
      "Initial State is  [4, 16, 3]\n",
      "episode 3466, reward 808.0, memory_length 2000, epsilon 0.031156429313642747, time 739.0, rides 136\n",
      "Initial State is  [0, 7, 2]\n",
      "episode 3467, reward 671.0, memory_length 2000, epsilon 0.031125272884329105, time 728.0, rides 116\n",
      "Initial State is  [3, 18, 5]\n",
      "episode 3468, reward 899.0, memory_length 2000, epsilon 0.031094147611444776, time 726.0, rides 120\n",
      "Initial State is  [4, 20, 2]\n",
      "episode 3469, reward 792.0, memory_length 2000, epsilon 0.03106305346383333, time 721.0, rides 139\n",
      "Initial State is  [2, 5, 6]\n",
      "episode 3470, reward 1076.0, memory_length 2000, epsilon 0.031031990410369498, time 729.0, rides 134\n",
      "Initial State is  [3, 12, 4]\n",
      "episode 3471, reward 859.0, memory_length 2000, epsilon 0.03100095841995913, time 731.0, rides 118\n",
      "Initial State is  [4, 10, 4]\n",
      "episode 3472, reward 611.0, memory_length 2000, epsilon 0.03096995746153917, time 732.0, rides 131\n",
      "Initial State is  [1, 16, 6]\n",
      "episode 3473, reward 973.0, memory_length 2000, epsilon 0.03093898750407763, time 727.0, rides 130\n",
      "Initial State is  [0, 22, 1]\n",
      "episode 3474, reward 938.0, memory_length 2000, epsilon 0.030908048516573555, time 728.0, rides 131\n",
      "Initial State is  [2, 15, 3]\n",
      "episode 3475, reward 907.0, memory_length 2000, epsilon 0.030877140468056983, time 728.0, rides 131\n",
      "Initial State is  [0, 10, 0]\n",
      "episode 3476, reward 983.0, memory_length 2000, epsilon 0.030846263327588927, time 729.0, rides 132\n",
      "Initial State is  [3, 20, 6]\n",
      "episode 3477, reward 1075.0, memory_length 2000, epsilon 0.030815417064261337, time 730.0, rides 135\n",
      "Initial State is  [3, 21, 4]\n",
      "episode 3478, reward 753.0, memory_length 2000, epsilon 0.030784601647197075, time 727.0, rides 128\n",
      "Initial State is  [2, 23, 3]\n",
      "episode 3479, reward 851.0, memory_length 2000, epsilon 0.03075381704554988, time 732.0, rides 134\n",
      "Initial State is  [3, 11, 5]\n",
      "episode 3480, reward 676.0, memory_length 2000, epsilon 0.03072306322850433, time 726.0, rides 134\n",
      "Initial State is  [2, 5, 4]\n",
      "episode 3481, reward 796.0, memory_length 2000, epsilon 0.030692340165275823, time 734.0, rides 139\n",
      "Initial State is  [2, 9, 3]\n",
      "episode 3482, reward 841.0, memory_length 2000, epsilon 0.030661647825110546, time 732.0, rides 129\n",
      "Initial State is  [1, 23, 4]\n",
      "episode 3483, reward 715.0, memory_length 2000, epsilon 0.030630986177285435, time 729.0, rides 136\n",
      "Initial State is  [0, 13, 2]\n",
      "episode 3484, reward 935.0, memory_length 2000, epsilon 0.03060035519110815, time 732.0, rides 127\n",
      "Initial State is  [1, 6, 3]\n",
      "episode 3485, reward 641.0, memory_length 2000, epsilon 0.03056975483591704, time 729.0, rides 132\n",
      "Initial State is  [0, 15, 5]\n",
      "episode 3486, reward 825.0, memory_length 2000, epsilon 0.030539185081081124, time 727.0, rides 131\n",
      "Initial State is  [1, 2, 0]\n",
      "episode 3487, reward 825.0, memory_length 2000, epsilon 0.030508645896000042, time 723.0, rides 136\n",
      "Initial State is  [4, 3, 2]\n",
      "episode 3488, reward 1009.0, memory_length 2000, epsilon 0.030478137250104044, time 727.0, rides 132\n",
      "Initial State is  [1, 3, 0]\n",
      "episode 3489, reward 887.0, memory_length 2000, epsilon 0.03044765911285394, time 727.0, rides 119\n",
      "Initial State is  [3, 16, 4]\n",
      "episode 3490, reward 900.0, memory_length 2000, epsilon 0.030417211453741086, time 727.0, rides 121\n",
      "Initial State is  [4, 16, 1]\n",
      "episode 3491, reward 1071.0, memory_length 2000, epsilon 0.030386794242287345, time 724.0, rides 138\n",
      "Initial State is  [4, 2, 2]\n",
      "episode 3492, reward 879.0, memory_length 2000, epsilon 0.030356407448045058, time 729.0, rides 144\n",
      "Initial State is  [0, 6, 4]\n",
      "episode 3493, reward 748.0, memory_length 2000, epsilon 0.03032605104059701, time 732.0, rides 134\n",
      "Initial State is  [2, 20, 1]\n",
      "episode 3494, reward 681.0, memory_length 2000, epsilon 0.030295724989556416, time 727.0, rides 121\n",
      "Initial State is  [4, 6, 6]\n",
      "episode 3495, reward 871.0, memory_length 2000, epsilon 0.030265429264566858, time 725.0, rides 122\n",
      "Initial State is  [2, 19, 1]\n",
      "episode 3496, reward 605.0, memory_length 2000, epsilon 0.03023516383530229, time 733.0, rides 130\n",
      "Initial State is  [2, 3, 1]\n",
      "episode 3497, reward 849.0, memory_length 2000, epsilon 0.03020492867146699, time 728.0, rides 123\n",
      "Initial State is  [2, 11, 4]\n",
      "episode 3498, reward 959.0, memory_length 2000, epsilon 0.03017472374279552, time 723.0, rides 122\n",
      "Initial State is  [4, 22, 0]\n",
      "episode 3499, reward 1002.0, memory_length 2000, epsilon 0.030144549019052724, time 728.0, rides 134\n",
      "Initial State is  [0, 11, 0]\n",
      "episode 3500, reward 910.0, memory_length 2000, epsilon 0.030114404470033673, time 730.0, rides 117\n",
      "Initial State is  [4, 11, 6]\n",
      "episode 3501, reward 1014.0, memory_length 2000, epsilon 0.030084290065563637, time 735.0, rides 136\n",
      "Initial State is  [3, 10, 4]\n",
      "episode 3502, reward 783.0, memory_length 2000, epsilon 0.030054205775498073, time 729.0, rides 126\n",
      "Initial State is  [1, 3, 1]\n",
      "episode 3503, reward 649.0, memory_length 2000, epsilon 0.030024151569722574, time 739.0, rides 132\n",
      "Initial State is  [1, 10, 2]\n",
      "episode 3504, reward 490.0, memory_length 2000, epsilon 0.02999412741815285, time 726.0, rides 132\n",
      "Initial State is  [0, 6, 2]\n",
      "episode 3505, reward 672.0, memory_length 2000, epsilon 0.029964133290734697, time 726.0, rides 134\n",
      "Initial State is  [0, 14, 4]\n",
      "episode 3506, reward 900.0, memory_length 2000, epsilon 0.029934169157443964, time 724.0, rides 138\n",
      "Initial State is  [0, 5, 2]\n",
      "episode 3507, reward 733.0, memory_length 2000, epsilon 0.02990423498828652, time 725.0, rides 126\n",
      "Initial State is  [0, 10, 4]\n",
      "episode 3508, reward 803.0, memory_length 2000, epsilon 0.029874330753298234, time 732.0, rides 106\n",
      "Initial State is  [0, 0, 4]\n",
      "episode 3509, reward 949.0, memory_length 2000, epsilon 0.029844456422544935, time 730.0, rides 119\n",
      "Initial State is  [1, 23, 1]\n",
      "episode 3510, reward 686.0, memory_length 2000, epsilon 0.029814611966122388, time 728.0, rides 113\n",
      "Initial State is  [2, 7, 1]\n",
      "episode 3511, reward 769.0, memory_length 2000, epsilon 0.029784797354156265, time 729.0, rides 121\n",
      "Initial State is  [4, 10, 5]\n",
      "episode 3512, reward 739.0, memory_length 2000, epsilon 0.02975501255680211, time 729.0, rides 120\n",
      "Initial State is  [2, 10, 2]\n",
      "episode 3513, reward 751.0, memory_length 2000, epsilon 0.029725257544245307, time 733.0, rides 136\n",
      "Initial State is  [0, 23, 6]\n",
      "episode 3514, reward 894.0, memory_length 2000, epsilon 0.029695532286701062, time 733.0, rides 138\n",
      "Initial State is  [3, 5, 3]\n",
      "episode 3515, reward 962.0, memory_length 2000, epsilon 0.029665836754414362, time 731.0, rides 120\n",
      "Initial State is  [4, 1, 4]\n",
      "episode 3516, reward 840.0, memory_length 2000, epsilon 0.029636170917659948, time 737.0, rides 123\n",
      "Initial State is  [2, 23, 6]\n",
      "episode 3517, reward 621.0, memory_length 2000, epsilon 0.029606534746742286, time 728.0, rides 136\n",
      "Initial State is  [0, 23, 3]\n",
      "episode 3518, reward 932.0, memory_length 2000, epsilon 0.029576928211995545, time 736.0, rides 131\n",
      "Initial State is  [3, 9, 2]\n",
      "episode 3519, reward 1001.0, memory_length 2000, epsilon 0.029547351283783548, time 722.0, rides 126\n",
      "Initial State is  [2, 3, 2]\n",
      "episode 3520, reward 833.0, memory_length 2000, epsilon 0.029517803932499764, time 725.0, rides 123\n",
      "Initial State is  [0, 19, 6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 3521, reward 1002.0, memory_length 2000, epsilon 0.029488286128567263, time 725.0, rides 142\n",
      "Initial State is  [1, 6, 3]\n",
      "episode 3522, reward 844.0, memory_length 2000, epsilon 0.029458797842438697, time 728.0, rides 128\n",
      "Initial State is  [0, 19, 2]\n",
      "episode 3523, reward 913.0, memory_length 2000, epsilon 0.02942933904459626, time 729.0, rides 117\n",
      "Initial State is  [4, 2, 2]\n",
      "episode 3524, reward 963.0, memory_length 2000, epsilon 0.029399909705551664, time 731.0, rides 127\n",
      "Initial State is  [0, 14, 3]\n",
      "episode 3525, reward 1115.0, memory_length 2000, epsilon 0.02937050979584611, time 727.0, rides 131\n",
      "Initial State is  [1, 18, 5]\n",
      "episode 3526, reward 995.0, memory_length 2000, epsilon 0.029341139286050266, time 729.0, rides 124\n",
      "Initial State is  [4, 20, 3]\n",
      "episode 3527, reward 1017.0, memory_length 2000, epsilon 0.029311798146764215, time 736.0, rides 135\n",
      "Initial State is  [0, 1, 0]\n",
      "episode 3528, reward 496.0, memory_length 2000, epsilon 0.02928248634861745, time 730.0, rides 135\n",
      "Initial State is  [0, 2, 3]\n",
      "episode 3529, reward 770.0, memory_length 2000, epsilon 0.029253203862268835, time 728.0, rides 121\n",
      "Initial State is  [1, 20, 2]\n",
      "episode 3530, reward 969.0, memory_length 2000, epsilon 0.029223950658406567, time 730.0, rides 128\n",
      "Initial State is  [0, 15, 5]\n",
      "episode 3531, reward 1043.0, memory_length 2000, epsilon 0.02919472670774816, time 737.0, rides 121\n",
      "Initial State is  [3, 14, 2]\n",
      "episode 3532, reward 594.0, memory_length 2000, epsilon 0.02916553198104041, time 735.0, rides 128\n",
      "Initial State is  [2, 18, 5]\n",
      "episode 3533, reward 737.0, memory_length 2000, epsilon 0.02913636644905937, time 728.0, rides 128\n",
      "Initial State is  [2, 17, 1]\n",
      "episode 3534, reward 868.0, memory_length 2000, epsilon 0.02910723008261031, time 725.0, rides 127\n",
      "Initial State is  [2, 2, 6]\n",
      "episode 3535, reward 481.0, memory_length 2000, epsilon 0.0290781228525277, time 729.0, rides 115\n",
      "Initial State is  [2, 1, 2]\n",
      "episode 3536, reward 871.0, memory_length 2000, epsilon 0.02904904472967517, time 730.0, rides 127\n",
      "Initial State is  [0, 3, 0]\n",
      "episode 3537, reward 1163.0, memory_length 2000, epsilon 0.029019995684945496, time 722.0, rides 134\n",
      "Initial State is  [2, 22, 4]\n",
      "episode 3538, reward 984.0, memory_length 2000, epsilon 0.02899097568926055, time 728.0, rides 118\n",
      "Initial State is  [4, 18, 0]\n",
      "episode 3539, reward 1059.0, memory_length 2000, epsilon 0.02896198471357129, time 728.0, rides 122\n",
      "Initial State is  [0, 22, 0]\n",
      "episode 3540, reward 900.0, memory_length 2000, epsilon 0.028933022728857716, time 734.0, rides 137\n",
      "Initial State is  [1, 18, 4]\n",
      "episode 3541, reward 874.0, memory_length 2000, epsilon 0.028904089706128858, time 728.0, rides 116\n",
      "Initial State is  [1, 3, 6]\n",
      "episode 3542, reward 913.0, memory_length 2000, epsilon 0.02887518561642273, time 726.0, rides 118\n",
      "Initial State is  [3, 18, 2]\n",
      "episode 3543, reward 1140.0, memory_length 2000, epsilon 0.028846310430806307, time 729.0, rides 131\n",
      "Initial State is  [1, 18, 6]\n",
      "episode 3544, reward 971.0, memory_length 2000, epsilon 0.028817464120375502, time 735.0, rides 131\n",
      "Initial State is  [1, 15, 2]\n",
      "episode 3545, reward 999.0, memory_length 2000, epsilon 0.028788646656255128, time 732.0, rides 115\n",
      "Initial State is  [2, 21, 2]\n",
      "episode 3546, reward 859.0, memory_length 2000, epsilon 0.028759858009598873, time 727.0, rides 120\n",
      "Initial State is  [1, 11, 0]\n",
      "episode 3547, reward 544.0, memory_length 2000, epsilon 0.028731098151589272, time 730.0, rides 121\n",
      "Initial State is  [3, 0, 3]\n",
      "episode 3548, reward 875.0, memory_length 2000, epsilon 0.028702367053437684, time 728.0, rides 126\n",
      "Initial State is  [1, 13, 3]\n",
      "episode 3549, reward 783.0, memory_length 2000, epsilon 0.028673664686384246, time 732.0, rides 135\n",
      "Initial State is  [2, 15, 1]\n",
      "episode 3550, reward 853.0, memory_length 2000, epsilon 0.02864499102169786, time 739.0, rides 130\n",
      "Initial State is  [4, 0, 1]\n",
      "episode 3551, reward 949.0, memory_length 2000, epsilon 0.028616346030676164, time 731.0, rides 131\n",
      "Initial State is  [4, 9, 2]\n",
      "episode 3552, reward 919.0, memory_length 2000, epsilon 0.02858772968464549, time 729.0, rides 120\n",
      "Initial State is  [1, 2, 1]\n",
      "episode 3553, reward 776.0, memory_length 2000, epsilon 0.028559141954960843, time 730.0, rides 122\n",
      "Initial State is  [2, 22, 3]\n",
      "episode 3554, reward 367.0, memory_length 2000, epsilon 0.02853058281300588, time 731.0, rides 134\n",
      "Initial State is  [0, 15, 3]\n",
      "episode 3555, reward 1116.0, memory_length 2000, epsilon 0.028502052230192875, time 726.0, rides 120\n",
      "Initial State is  [4, 14, 0]\n",
      "episode 3556, reward 948.0, memory_length 2000, epsilon 0.028473550177962683, time 726.0, rides 122\n",
      "Initial State is  [1, 10, 0]\n",
      "episode 3557, reward 1101.0, memory_length 2000, epsilon 0.02844507662778472, time 724.0, rides 128\n",
      "Initial State is  [2, 0, 1]\n",
      "episode 3558, reward 699.0, memory_length 2000, epsilon 0.028416631551156934, time 729.0, rides 129\n",
      "Initial State is  [4, 20, 6]\n",
      "episode 3559, reward 691.0, memory_length 2000, epsilon 0.028388214919605775, time 723.0, rides 134\n",
      "Initial State is  [4, 8, 6]\n",
      "episode 3560, reward 658.0, memory_length 2000, epsilon 0.028359826704686168, time 733.0, rides 129\n",
      "Initial State is  [1, 3, 1]\n",
      "episode 3561, reward 938.0, memory_length 2000, epsilon 0.028331466877981482, time 731.0, rides 128\n",
      "Initial State is  [3, 23, 5]\n",
      "episode 3562, reward 923.0, memory_length 2000, epsilon 0.028303135411103502, time 726.0, rides 133\n",
      "Initial State is  [4, 5, 1]\n",
      "episode 3563, reward 1080.0, memory_length 2000, epsilon 0.0282748322756924, time 729.0, rides 124\n",
      "Initial State is  [1, 15, 5]\n",
      "episode 3564, reward 944.0, memory_length 2000, epsilon 0.028246557443416708, time 731.0, rides 122\n",
      "Initial State is  [3, 6, 6]\n",
      "episode 3565, reward 949.0, memory_length 2000, epsilon 0.02821831088597329, time 727.0, rides 116\n",
      "Initial State is  [0, 1, 1]\n",
      "episode 3566, reward 889.0, memory_length 2000, epsilon 0.028190092575087318, time 726.0, rides 130\n",
      "Initial State is  [1, 20, 4]\n",
      "episode 3567, reward 986.0, memory_length 2000, epsilon 0.02816190248251223, time 731.0, rides 126\n",
      "Initial State is  [3, 7, 3]\n",
      "episode 3568, reward 966.0, memory_length 2000, epsilon 0.02813374058002972, time 726.0, rides 126\n",
      "Initial State is  [1, 6, 3]\n",
      "episode 3569, reward 1030.0, memory_length 2000, epsilon 0.02810560683944969, time 724.0, rides 135\n",
      "Initial State is  [4, 8, 4]\n",
      "episode 3570, reward 1064.0, memory_length 2000, epsilon 0.028077501232610238, time 737.0, rides 127\n",
      "Initial State is  [1, 10, 6]\n",
      "episode 3571, reward 975.0, memory_length 2000, epsilon 0.02804942373137763, time 725.0, rides 125\n",
      "Initial State is  [2, 0, 4]\n",
      "episode 3572, reward 731.0, memory_length 2000, epsilon 0.02802137430764625, time 727.0, rides 133\n",
      "Initial State is  [3, 10, 0]\n",
      "episode 3573, reward 950.0, memory_length 2000, epsilon 0.027993352933338603, time 721.0, rides 130\n",
      "Initial State is  [4, 17, 6]\n",
      "episode 3574, reward 771.0, memory_length 2000, epsilon 0.027965359580405264, time 729.0, rides 130\n",
      "Initial State is  [0, 11, 1]\n",
      "episode 3575, reward 853.0, memory_length 2000, epsilon 0.02793739422082486, time 732.0, rides 121\n",
      "Initial State is  [2, 14, 1]\n",
      "episode 3576, reward 829.0, memory_length 2000, epsilon 0.027909456826604034, time 727.0, rides 134\n",
      "Initial State is  [0, 12, 0]\n",
      "episode 3577, reward 980.0, memory_length 2000, epsilon 0.02788154736977743, time 724.0, rides 124\n",
      "Initial State is  [2, 3, 1]\n",
      "episode 3578, reward 737.0, memory_length 2000, epsilon 0.027853665822407652, time 728.0, rides 126\n",
      "Initial State is  [3, 4, 1]\n",
      "episode 3579, reward 787.0, memory_length 2000, epsilon 0.027825812156585243, time 727.0, rides 137\n",
      "Initial State is  [1, 15, 4]\n",
      "episode 3580, reward 1016.0, memory_length 2000, epsilon 0.02779798634442866, time 731.0, rides 127\n",
      "Initial State is  [1, 18, 5]\n",
      "episode 3581, reward 819.0, memory_length 2000, epsilon 0.02777018835808423, time 735.0, rides 125\n",
      "Initial State is  [3, 15, 5]\n",
      "episode 3582, reward 1145.0, memory_length 2000, epsilon 0.027742418169726144, time 726.0, rides 127\n",
      "Initial State is  [4, 16, 6]\n",
      "episode 3583, reward 709.0, memory_length 2000, epsilon 0.02771467575155642, time 729.0, rides 114\n",
      "Initial State is  [1, 15, 3]\n",
      "episode 3584, reward 1024.0, memory_length 2000, epsilon 0.027686961075804862, time 733.0, rides 131\n",
      "Initial State is  [3, 0, 3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 3585, reward 839.0, memory_length 2000, epsilon 0.027659274114729057, time 731.0, rides 131\n",
      "Initial State is  [4, 17, 0]\n",
      "episode 3586, reward 1048.0, memory_length 2000, epsilon 0.027631614840614327, time 730.0, rides 131\n",
      "Initial State is  [1, 20, 6]\n",
      "episode 3587, reward 1086.0, memory_length 2000, epsilon 0.027603983225773714, time 732.0, rides 127\n",
      "Initial State is  [4, 9, 2]\n",
      "episode 3588, reward 1043.0, memory_length 2000, epsilon 0.02757637924254794, time 723.0, rides 142\n",
      "Initial State is  [0, 4, 5]\n",
      "episode 3589, reward 994.0, memory_length 2000, epsilon 0.027548802863305393, time 731.0, rides 138\n",
      "Initial State is  [2, 17, 0]\n",
      "episode 3590, reward 932.0, memory_length 2000, epsilon 0.027521254060442087, time 728.0, rides 124\n",
      "Initial State is  [1, 12, 5]\n",
      "episode 3591, reward 823.0, memory_length 2000, epsilon 0.027493732806381645, time 732.0, rides 146\n",
      "Initial State is  [2, 14, 5]\n",
      "episode 3592, reward 1105.0, memory_length 2000, epsilon 0.027466239073575264, time 730.0, rides 157\n",
      "Initial State is  [0, 13, 5]\n",
      "episode 3593, reward 853.0, memory_length 2000, epsilon 0.02743877283450169, time 734.0, rides 131\n",
      "Initial State is  [0, 2, 0]\n",
      "episode 3594, reward 756.0, memory_length 2000, epsilon 0.027411334061667188, time 730.0, rides 119\n",
      "Initial State is  [4, 0, 5]\n",
      "episode 3595, reward 936.0, memory_length 2000, epsilon 0.02738392272760552, time 725.0, rides 122\n",
      "Initial State is  [3, 2, 0]\n",
      "episode 3596, reward 1118.0, memory_length 2000, epsilon 0.027356538804877914, time 723.0, rides 131\n",
      "Initial State is  [4, 18, 0]\n",
      "episode 3597, reward 753.0, memory_length 2000, epsilon 0.027329182266073036, time 731.0, rides 128\n",
      "Initial State is  [1, 0, 1]\n",
      "episode 3598, reward 919.0, memory_length 2000, epsilon 0.027301853083806962, time 730.0, rides 128\n",
      "Initial State is  [1, 11, 5]\n",
      "episode 3599, reward 1096.0, memory_length 2000, epsilon 0.027274551230723157, time 734.0, rides 140\n",
      "Initial State is  [2, 14, 5]\n",
      "episode 3600, reward 1206.0, memory_length 2000, epsilon 0.027247276679492435, time 727.0, rides 123\n",
      "Initial State is  [1, 5, 1]\n",
      "episode 3601, reward 1001.0, memory_length 2000, epsilon 0.027220029402812942, time 723.0, rides 125\n",
      "Initial State is  [3, 4, 5]\n",
      "episode 3602, reward 1000.0, memory_length 2000, epsilon 0.02719280937341013, time 736.0, rides 118\n",
      "Initial State is  [4, 9, 2]\n",
      "episode 3603, reward 1003.0, memory_length 2000, epsilon 0.02716561656403672, time 729.0, rides 138\n",
      "Initial State is  [3, 6, 2]\n",
      "episode 3604, reward 1020.0, memory_length 2000, epsilon 0.027138450947472685, time 735.0, rides 136\n",
      "Initial State is  [3, 6, 4]\n",
      "episode 3605, reward 799.0, memory_length 2000, epsilon 0.027111312496525212, time 728.0, rides 135\n",
      "Initial State is  [2, 2, 0]\n",
      "episode 3606, reward 975.0, memory_length 2000, epsilon 0.027084201184028687, time 733.0, rides 119\n",
      "Initial State is  [0, 2, 2]\n",
      "episode 3607, reward 1030.0, memory_length 2000, epsilon 0.02705711698284466, time 726.0, rides 119\n",
      "Initial State is  [4, 0, 3]\n",
      "episode 3608, reward 779.0, memory_length 2000, epsilon 0.027030059865861815, time 725.0, rides 122\n",
      "Initial State is  [3, 8, 4]\n",
      "episode 3609, reward 1012.0, memory_length 2000, epsilon 0.027003029805995952, time 720.0, rides 120\n",
      "Initial State is  [2, 12, 5]\n",
      "episode 3610, reward 983.0, memory_length 2000, epsilon 0.026976026776189956, time 730.0, rides 128\n",
      "Initial State is  [4, 16, 0]\n",
      "episode 3611, reward 1182.0, memory_length 2000, epsilon 0.026949050749413766, time 727.0, rides 128\n",
      "Initial State is  [4, 5, 2]\n",
      "episode 3612, reward 1125.0, memory_length 2000, epsilon 0.026922101698664352, time 736.0, rides 135\n",
      "Initial State is  [1, 10, 3]\n",
      "episode 3613, reward 874.0, memory_length 2000, epsilon 0.026895179596965687, time 726.0, rides 128\n",
      "Initial State is  [1, 4, 1]\n",
      "episode 3614, reward 1065.0, memory_length 2000, epsilon 0.026868284417368722, time 733.0, rides 134\n",
      "Initial State is  [4, 2, 4]\n",
      "episode 3615, reward 814.0, memory_length 2000, epsilon 0.026841416132951355, time 726.0, rides 113\n",
      "Initial State is  [0, 9, 2]\n",
      "episode 3616, reward 872.0, memory_length 2000, epsilon 0.026814574716818404, time 732.0, rides 127\n",
      "Initial State is  [4, 6, 4]\n",
      "episode 3617, reward 638.0, memory_length 2000, epsilon 0.026787760142101585, time 730.0, rides 133\n",
      "Initial State is  [0, 5, 5]\n",
      "episode 3618, reward 816.0, memory_length 2000, epsilon 0.026760972381959482, time 731.0, rides 121\n",
      "Initial State is  [4, 18, 0]\n",
      "episode 3619, reward 1149.0, memory_length 2000, epsilon 0.026734211409577522, time 731.0, rides 126\n",
      "Initial State is  [4, 6, 0]\n",
      "episode 3620, reward 972.0, memory_length 2000, epsilon 0.026707477198167944, time 729.0, rides 133\n",
      "Initial State is  [1, 16, 4]\n",
      "episode 3621, reward 1090.0, memory_length 2000, epsilon 0.026680769720969777, time 731.0, rides 138\n",
      "Initial State is  [0, 20, 3]\n",
      "episode 3622, reward 925.0, memory_length 2000, epsilon 0.02665408895124881, time 723.0, rides 131\n",
      "Initial State is  [0, 13, 1]\n",
      "episode 3623, reward 721.0, memory_length 2000, epsilon 0.026627434862297558, time 726.0, rides 115\n",
      "Initial State is  [1, 13, 3]\n",
      "episode 3624, reward 1023.0, memory_length 2000, epsilon 0.02660080742743526, time 731.0, rides 125\n",
      "Initial State is  [2, 4, 2]\n",
      "episode 3625, reward 894.0, memory_length 2000, epsilon 0.026574206620007826, time 725.0, rides 122\n",
      "Initial State is  [0, 18, 6]\n",
      "episode 3626, reward 979.0, memory_length 2000, epsilon 0.02654763241338782, time 731.0, rides 119\n",
      "Initial State is  [3, 19, 3]\n",
      "episode 3627, reward 760.0, memory_length 2000, epsilon 0.02652108478097443, time 730.0, rides 119\n",
      "Initial State is  [0, 4, 3]\n",
      "episode 3628, reward 871.0, memory_length 2000, epsilon 0.026494563696193456, time 731.0, rides 129\n",
      "Initial State is  [1, 8, 3]\n",
      "episode 3629, reward 930.0, memory_length 2000, epsilon 0.026468069132497263, time 724.0, rides 129\n",
      "Initial State is  [3, 1, 4]\n",
      "episode 3630, reward 1008.0, memory_length 2000, epsilon 0.026441601063364767, time 731.0, rides 129\n",
      "Initial State is  [0, 10, 5]\n",
      "episode 3631, reward 780.0, memory_length 2000, epsilon 0.026415159462301403, time 723.0, rides 124\n",
      "Initial State is  [3, 5, 3]\n",
      "episode 3632, reward 717.0, memory_length 2000, epsilon 0.0263887443028391, time 736.0, rides 118\n",
      "Initial State is  [0, 15, 0]\n",
      "episode 3633, reward 1080.0, memory_length 2000, epsilon 0.02636235555853626, time 730.0, rides 139\n",
      "Initial State is  [2, 6, 6]\n",
      "episode 3634, reward 821.0, memory_length 2000, epsilon 0.026335993202977723, time 732.0, rides 120\n",
      "Initial State is  [1, 12, 6]\n",
      "episode 3635, reward 1013.0, memory_length 2000, epsilon 0.026309657209774746, time 729.0, rides 129\n",
      "Initial State is  [1, 16, 1]\n",
      "episode 3636, reward 822.0, memory_length 2000, epsilon 0.02628334755256497, time 729.0, rides 127\n",
      "Initial State is  [2, 21, 6]\n",
      "episode 3637, reward 721.0, memory_length 2000, epsilon 0.026257064205012406, time 728.0, rides 132\n",
      "Initial State is  [1, 22, 1]\n",
      "episode 3638, reward 894.0, memory_length 2000, epsilon 0.026230807140807395, time 728.0, rides 135\n",
      "Initial State is  [2, 11, 3]\n",
      "episode 3639, reward 859.0, memory_length 2000, epsilon 0.026204576333666588, time 730.0, rides 119\n",
      "Initial State is  [4, 9, 2]\n",
      "episode 3640, reward 898.0, memory_length 2000, epsilon 0.02617837175733292, time 721.0, rides 130\n",
      "Initial State is  [4, 17, 0]\n",
      "episode 3641, reward 1032.0, memory_length 2000, epsilon 0.02615219338557559, time 731.0, rides 135\n",
      "Initial State is  [1, 22, 3]\n",
      "episode 3642, reward 768.0, memory_length 2000, epsilon 0.026126041192190013, time 727.0, rides 135\n",
      "Initial State is  [0, 3, 5]\n",
      "episode 3643, reward 942.0, memory_length 2000, epsilon 0.026099915150997823, time 728.0, rides 147\n",
      "Initial State is  [2, 18, 5]\n",
      "episode 3644, reward 1260.0, memory_length 2000, epsilon 0.026073815235846825, time 729.0, rides 125\n",
      "Initial State is  [0, 12, 1]\n",
      "episode 3645, reward 1146.0, memory_length 2000, epsilon 0.02604774142061098, time 735.0, rides 135\n",
      "Initial State is  [1, 0, 2]\n",
      "episode 3646, reward 496.0, memory_length 2000, epsilon 0.02602169367919037, time 740.0, rides 123\n",
      "Initial State is  [0, 8, 2]\n",
      "episode 3647, reward 1060.0, memory_length 2000, epsilon 0.025995671985511178, time 720.0, rides 127\n",
      "Initial State is  [1, 22, 6]\n",
      "episode 3648, reward 915.0, memory_length 2000, epsilon 0.025969676313525668, time 729.0, rides 136\n",
      "Initial State is  [0, 13, 5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 3649, reward 1037.0, memory_length 2000, epsilon 0.02594370663721214, time 731.0, rides 135\n",
      "Initial State is  [0, 23, 0]\n",
      "episode 3650, reward 877.0, memory_length 2000, epsilon 0.02591776293057493, time 726.0, rides 125\n",
      "Initial State is  [1, 8, 3]\n",
      "episode 3651, reward 935.0, memory_length 2000, epsilon 0.025891845167644353, time 723.0, rides 131\n",
      "Initial State is  [1, 20, 6]\n",
      "episode 3652, reward 1264.0, memory_length 2000, epsilon 0.02586595332247671, time 724.0, rides 132\n",
      "Initial State is  [1, 21, 1]\n",
      "episode 3653, reward 980.0, memory_length 2000, epsilon 0.025840087369154233, time 725.0, rides 122\n",
      "Initial State is  [3, 23, 0]\n",
      "episode 3654, reward 911.0, memory_length 2000, epsilon 0.02581424728178508, time 735.0, rides 125\n",
      "Initial State is  [0, 9, 3]\n",
      "episode 3655, reward 885.0, memory_length 2000, epsilon 0.025788433034503296, time 736.0, rides 129\n",
      "Initial State is  [4, 12, 1]\n",
      "episode 3656, reward 1072.0, memory_length 2000, epsilon 0.025762644601468793, time 740.0, rides 141\n",
      "Initial State is  [3, 0, 3]\n",
      "episode 3657, reward 884.0, memory_length 2000, epsilon 0.025736881956867325, time 726.0, rides 121\n",
      "Initial State is  [0, 1, 0]\n",
      "episode 3658, reward 1106.0, memory_length 2000, epsilon 0.025711145074910458, time 727.0, rides 135\n",
      "Initial State is  [4, 12, 1]\n",
      "episode 3659, reward 785.0, memory_length 2000, epsilon 0.025685433929835546, time 724.0, rides 134\n",
      "Initial State is  [2, 1, 0]\n",
      "episode 3660, reward 965.0, memory_length 2000, epsilon 0.025659748495905712, time 737.0, rides 134\n",
      "Initial State is  [2, 2, 5]\n",
      "episode 3661, reward 724.0, memory_length 2000, epsilon 0.025634088747409807, time 725.0, rides 146\n",
      "Initial State is  [3, 17, 0]\n",
      "episode 3662, reward 553.0, memory_length 2000, epsilon 0.025608454658662398, time 724.0, rides 133\n",
      "Initial State is  [3, 13, 5]\n",
      "episode 3663, reward 882.0, memory_length 2000, epsilon 0.025582846204003737, time 734.0, rides 138\n",
      "Initial State is  [3, 9, 1]\n",
      "episode 3664, reward 692.0, memory_length 2000, epsilon 0.025557263357799734, time 729.0, rides 126\n",
      "Initial State is  [4, 20, 1]\n",
      "episode 3665, reward 903.0, memory_length 2000, epsilon 0.025531706094441935, time 731.0, rides 136\n",
      "Initial State is  [3, 18, 1]\n",
      "episode 3666, reward 869.0, memory_length 2000, epsilon 0.025506174388347493, time 731.0, rides 130\n",
      "Initial State is  [2, 16, 5]\n",
      "episode 3667, reward 995.0, memory_length 2000, epsilon 0.025480668213959144, time 726.0, rides 122\n",
      "Initial State is  [0, 10, 4]\n",
      "episode 3668, reward 819.0, memory_length 2000, epsilon 0.025455187545745186, time 730.0, rides 133\n",
      "Initial State is  [1, 17, 3]\n",
      "episode 3669, reward 717.0, memory_length 2000, epsilon 0.02542973235819944, time 730.0, rides 132\n",
      "Initial State is  [1, 11, 1]\n",
      "episode 3670, reward 1048.0, memory_length 2000, epsilon 0.02540430262584124, time 730.0, rides 125\n",
      "Initial State is  [3, 7, 2]\n",
      "episode 3671, reward 750.0, memory_length 2000, epsilon 0.0253788983232154, time 733.0, rides 146\n",
      "Initial State is  [2, 20, 5]\n",
      "episode 3672, reward 586.0, memory_length 2000, epsilon 0.025353519424892185, time 729.0, rides 120\n",
      "Initial State is  [4, 0, 0]\n",
      "episode 3673, reward 819.0, memory_length 2000, epsilon 0.025328165905467295, time 732.0, rides 132\n",
      "Initial State is  [0, 5, 2]\n",
      "episode 3674, reward 634.0, memory_length 2000, epsilon 0.025302837739561827, time 734.0, rides 121\n",
      "Initial State is  [1, 7, 2]\n",
      "episode 3675, reward 557.0, memory_length 2000, epsilon 0.025277534901822264, time 729.0, rides 123\n",
      "Initial State is  [2, 8, 4]\n",
      "episode 3676, reward 511.0, memory_length 2000, epsilon 0.025252257366920442, time 731.0, rides 136\n",
      "Initial State is  [4, 23, 6]\n",
      "episode 3677, reward 738.0, memory_length 2000, epsilon 0.025227005109553523, time 727.0, rides 133\n",
      "Initial State is  [3, 21, 1]\n",
      "episode 3678, reward 980.0, memory_length 2000, epsilon 0.02520177810444397, time 733.0, rides 134\n",
      "Initial State is  [1, 7, 2]\n",
      "episode 3679, reward 1337.0, memory_length 2000, epsilon 0.025176576326339524, time 732.0, rides 131\n",
      "Initial State is  [2, 23, 5]\n",
      "episode 3680, reward 745.0, memory_length 2000, epsilon 0.025151399750013185, time 729.0, rides 137\n",
      "Initial State is  [2, 13, 2]\n",
      "episode 3681, reward 926.0, memory_length 2000, epsilon 0.025126248350263173, time 725.0, rides 130\n",
      "Initial State is  [1, 14, 3]\n",
      "episode 3682, reward 916.0, memory_length 2000, epsilon 0.02510112210191291, time 727.0, rides 121\n",
      "Initial State is  [4, 5, 3]\n",
      "episode 3683, reward 977.0, memory_length 2000, epsilon 0.025076020979810997, time 726.0, rides 124\n",
      "Initial State is  [2, 8, 0]\n",
      "episode 3684, reward 885.0, memory_length 2000, epsilon 0.025050944958831187, time 734.0, rides 122\n",
      "Initial State is  [4, 15, 0]\n",
      "episode 3685, reward 518.0, memory_length 2000, epsilon 0.025025894013872355, time 724.0, rides 124\n",
      "Initial State is  [2, 21, 3]\n",
      "episode 3686, reward 831.0, memory_length 2000, epsilon 0.025000868119858483, time 729.0, rides 127\n",
      "Initial State is  [3, 15, 3]\n",
      "episode 3687, reward 746.0, memory_length 2000, epsilon 0.024975867251738625, time 733.0, rides 129\n",
      "Initial State is  [3, 7, 6]\n",
      "episode 3688, reward 952.0, memory_length 2000, epsilon 0.024950891384486886, time 728.0, rides 126\n",
      "Initial State is  [0, 22, 3]\n",
      "episode 3689, reward 1212.0, memory_length 2000, epsilon 0.0249259404931024, time 736.0, rides 137\n",
      "Initial State is  [3, 5, 0]\n",
      "episode 3690, reward 1207.0, memory_length 2000, epsilon 0.024901014552609298, time 729.0, rides 142\n",
      "Initial State is  [0, 8, 6]\n",
      "episode 3691, reward 849.0, memory_length 2000, epsilon 0.024876113538056688, time 727.0, rides 126\n",
      "Initial State is  [1, 20, 4]\n",
      "episode 3692, reward 934.0, memory_length 2000, epsilon 0.02485123742451863, time 725.0, rides 121\n",
      "Initial State is  [2, 7, 5]\n",
      "episode 3693, reward 854.0, memory_length 2000, epsilon 0.02482638618709411, time 728.0, rides 129\n",
      "Initial State is  [0, 6, 0]\n",
      "episode 3694, reward 764.0, memory_length 2000, epsilon 0.024801559800907015, time 735.0, rides 129\n",
      "Initial State is  [4, 22, 1]\n",
      "episode 3695, reward 881.0, memory_length 2000, epsilon 0.024776758241106107, time 725.0, rides 126\n",
      "Initial State is  [1, 9, 0]\n",
      "episode 3696, reward 742.0, memory_length 2000, epsilon 0.024751981482865, time 738.0, rides 124\n",
      "Initial State is  [4, 1, 6]\n",
      "episode 3697, reward 931.0, memory_length 2000, epsilon 0.024727229501382137, time 724.0, rides 127\n",
      "Initial State is  [0, 5, 2]\n",
      "episode 3698, reward 1023.0, memory_length 2000, epsilon 0.024702502271880755, time 722.0, rides 129\n",
      "Initial State is  [1, 14, 0]\n",
      "episode 3699, reward 739.0, memory_length 2000, epsilon 0.024677799769608873, time 725.0, rides 125\n",
      "Initial State is  [2, 10, 4]\n",
      "episode 3700, reward 1026.0, memory_length 2000, epsilon 0.024653121969839265, time 733.0, rides 129\n",
      "Initial State is  [4, 23, 6]\n",
      "episode 3701, reward 909.0, memory_length 2000, epsilon 0.024628468847869425, time 733.0, rides 131\n",
      "Initial State is  [1, 9, 4]\n",
      "episode 3702, reward 951.0, memory_length 2000, epsilon 0.024603840379021556, time 726.0, rides 126\n",
      "Initial State is  [4, 1, 2]\n",
      "episode 3703, reward 1040.0, memory_length 2000, epsilon 0.024579236538642534, time 729.0, rides 135\n",
      "Initial State is  [2, 6, 6]\n",
      "episode 3704, reward 916.0, memory_length 2000, epsilon 0.02455465730210389, time 735.0, rides 125\n",
      "Initial State is  [3, 8, 0]\n",
      "episode 3705, reward 843.0, memory_length 2000, epsilon 0.024530102644801786, time 727.0, rides 138\n",
      "Initial State is  [2, 9, 1]\n",
      "episode 3706, reward 823.0, memory_length 2000, epsilon 0.024505572542156986, time 724.0, rides 133\n",
      "Initial State is  [1, 7, 2]\n",
      "episode 3707, reward 1112.0, memory_length 2000, epsilon 0.02448106696961483, time 732.0, rides 134\n",
      "Initial State is  [1, 7, 4]\n",
      "episode 3708, reward 895.0, memory_length 2000, epsilon 0.024456585902645215, time 733.0, rides 128\n",
      "Initial State is  [1, 8, 1]\n",
      "episode 3709, reward 770.0, memory_length 2000, epsilon 0.02443212931674257, time 725.0, rides 125\n",
      "Initial State is  [4, 13, 5]\n",
      "episode 3710, reward 892.0, memory_length 2000, epsilon 0.024407697187425827, time 726.0, rides 133\n",
      "Initial State is  [4, 2, 4]\n",
      "episode 3711, reward 1127.0, memory_length 2000, epsilon 0.0243832894902384, time 734.0, rides 131\n",
      "Initial State is  [4, 18, 0]\n",
      "episode 3712, reward 555.0, memory_length 2000, epsilon 0.02435890620074816, time 734.0, rides 126\n",
      "Initial State is  [4, 4, 6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 3713, reward 1050.0, memory_length 2000, epsilon 0.024334547294547412, time 726.0, rides 134\n",
      "Initial State is  [4, 15, 2]\n",
      "episode 3714, reward 961.0, memory_length 2000, epsilon 0.024310212747252865, time 730.0, rides 114\n",
      "Initial State is  [0, 13, 0]\n",
      "episode 3715, reward 1358.0, memory_length 2000, epsilon 0.02428590253450561, time 727.0, rides 135\n",
      "Initial State is  [0, 18, 2]\n",
      "episode 3716, reward 932.0, memory_length 2000, epsilon 0.024261616631971107, time 724.0, rides 140\n",
      "Initial State is  [4, 12, 5]\n",
      "episode 3717, reward 864.0, memory_length 2000, epsilon 0.024237355015339135, time 730.0, rides 126\n",
      "Initial State is  [1, 10, 3]\n",
      "episode 3718, reward 841.0, memory_length 2000, epsilon 0.024213117660323795, time 726.0, rides 131\n",
      "Initial State is  [4, 19, 2]\n",
      "episode 3719, reward 954.0, memory_length 2000, epsilon 0.02418890454266347, time 730.0, rides 143\n",
      "Initial State is  [3, 9, 5]\n",
      "episode 3720, reward 856.0, memory_length 2000, epsilon 0.024164715638120806, time 730.0, rides 138\n",
      "Initial State is  [4, 9, 5]\n",
      "episode 3721, reward 624.0, memory_length 2000, epsilon 0.024140550922482684, time 734.0, rides 125\n",
      "Initial State is  [0, 1, 3]\n",
      "episode 3722, reward 1065.0, memory_length 2000, epsilon 0.0241164103715602, time 728.0, rides 143\n",
      "Initial State is  [4, 9, 6]\n",
      "episode 3723, reward 1008.0, memory_length 2000, epsilon 0.02409229396118864, time 726.0, rides 126\n",
      "Initial State is  [4, 6, 1]\n",
      "episode 3724, reward 862.0, memory_length 2000, epsilon 0.02406820166722745, time 727.0, rides 140\n",
      "Initial State is  [2, 11, 1]\n",
      "episode 3725, reward 1403.0, memory_length 2000, epsilon 0.024044133465560225, time 729.0, rides 135\n",
      "Initial State is  [1, 7, 3]\n",
      "episode 3726, reward 1044.0, memory_length 2000, epsilon 0.024020089332094666, time 726.0, rides 142\n",
      "Initial State is  [2, 21, 5]\n",
      "episode 3727, reward 1049.0, memory_length 2000, epsilon 0.023996069242762572, time 727.0, rides 134\n",
      "Initial State is  [0, 3, 4]\n",
      "episode 3728, reward 1020.0, memory_length 2000, epsilon 0.02397207317351981, time 724.0, rides 151\n",
      "Initial State is  [2, 22, 5]\n",
      "episode 3729, reward 813.0, memory_length 2000, epsilon 0.02394810110034629, time 734.0, rides 143\n",
      "Initial State is  [3, 20, 0]\n",
      "episode 3730, reward 558.0, memory_length 2000, epsilon 0.023924152999245944, time 724.0, rides 123\n",
      "Initial State is  [2, 22, 3]\n",
      "episode 3731, reward 765.0, memory_length 2000, epsilon 0.023900228846246697, time 727.0, rides 133\n",
      "Initial State is  [1, 20, 1]\n",
      "episode 3732, reward 928.0, memory_length 2000, epsilon 0.02387632861740045, time 728.0, rides 141\n",
      "Initial State is  [3, 0, 1]\n",
      "episode 3733, reward 1004.0, memory_length 2000, epsilon 0.02385245228878305, time 726.0, rides 118\n",
      "Initial State is  [0, 0, 2]\n",
      "episode 3734, reward 905.0, memory_length 2000, epsilon 0.023828599836494265, time 729.0, rides 128\n",
      "Initial State is  [1, 14, 6]\n",
      "episode 3735, reward 845.0, memory_length 2000, epsilon 0.023804771236657772, time 728.0, rides 129\n",
      "Initial State is  [2, 10, 1]\n",
      "episode 3736, reward 828.0, memory_length 2000, epsilon 0.023780966465421115, time 731.0, rides 124\n",
      "Initial State is  [1, 16, 4]\n",
      "episode 3737, reward 811.0, memory_length 2000, epsilon 0.023757185498955693, time 723.0, rides 145\n",
      "Initial State is  [3, 16, 0]\n",
      "episode 3738, reward 555.0, memory_length 2000, epsilon 0.023733428313456737, time 730.0, rides 137\n",
      "Initial State is  [4, 3, 2]\n",
      "episode 3739, reward 1069.0, memory_length 2000, epsilon 0.02370969488514328, time 727.0, rides 130\n",
      "Initial State is  [0, 9, 0]\n",
      "episode 3740, reward 1023.0, memory_length 2000, epsilon 0.02368598519025814, time 732.0, rides 142\n",
      "Initial State is  [1, 7, 6]\n",
      "episode 3741, reward 962.0, memory_length 2000, epsilon 0.02366229920506788, time 727.0, rides 130\n",
      "Initial State is  [3, 22, 0]\n",
      "episode 3742, reward 980.0, memory_length 2000, epsilon 0.023638636905862813, time 728.0, rides 138\n",
      "Initial State is  [0, 4, 5]\n",
      "episode 3743, reward 1201.0, memory_length 2000, epsilon 0.02361499826895695, time 732.0, rides 125\n",
      "Initial State is  [0, 7, 2]\n",
      "episode 3744, reward 1052.0, memory_length 2000, epsilon 0.023591383270687993, time 729.0, rides 133\n",
      "Initial State is  [1, 9, 3]\n",
      "episode 3745, reward 889.0, memory_length 2000, epsilon 0.023567791887417304, time 733.0, rides 133\n",
      "Initial State is  [3, 15, 2]\n",
      "episode 3746, reward 1032.0, memory_length 2000, epsilon 0.023544224095529885, time 727.0, rides 127\n",
      "Initial State is  [4, 0, 3]\n",
      "episode 3747, reward 957.0, memory_length 2000, epsilon 0.023520679871434354, time 725.0, rides 123\n",
      "Initial State is  [2, 0, 3]\n",
      "episode 3748, reward 800.0, memory_length 2000, epsilon 0.02349715919156292, time 732.0, rides 143\n",
      "Initial State is  [2, 11, 3]\n",
      "episode 3749, reward 1114.0, memory_length 2000, epsilon 0.023473662032371355, time 728.0, rides 136\n",
      "Initial State is  [3, 19, 4]\n",
      "episode 3750, reward 887.0, memory_length 2000, epsilon 0.023450188370338982, time 723.0, rides 130\n",
      "Initial State is  [4, 10, 6]\n",
      "episode 3751, reward 578.0, memory_length 2000, epsilon 0.023426738181968644, time 725.0, rides 134\n",
      "Initial State is  [2, 10, 3]\n",
      "episode 3752, reward 964.0, memory_length 2000, epsilon 0.023403311443786677, time 733.0, rides 130\n",
      "Initial State is  [2, 9, 3]\n",
      "episode 3753, reward 1059.0, memory_length 2000, epsilon 0.02337990813234289, time 733.0, rides 135\n",
      "Initial State is  [0, 5, 1]\n",
      "episode 3754, reward 920.0, memory_length 2000, epsilon 0.023356528224210547, time 729.0, rides 126\n",
      "Initial State is  [1, 19, 3]\n",
      "episode 3755, reward 675.0, memory_length 2000, epsilon 0.023333171695986338, time 731.0, rides 129\n",
      "Initial State is  [1, 10, 2]\n",
      "episode 3756, reward 769.0, memory_length 2000, epsilon 0.02330983852429035, time 725.0, rides 129\n",
      "Initial State is  [4, 8, 0]\n",
      "episode 3757, reward 822.0, memory_length 2000, epsilon 0.02328652868576606, time 727.0, rides 133\n",
      "Initial State is  [2, 23, 2]\n",
      "episode 3758, reward 941.0, memory_length 2000, epsilon 0.023263242157080293, time 727.0, rides 131\n",
      "Initial State is  [4, 15, 1]\n",
      "episode 3759, reward 907.0, memory_length 2000, epsilon 0.02323997891492321, time 727.0, rides 128\n",
      "Initial State is  [2, 9, 1]\n",
      "episode 3760, reward 939.0, memory_length 2000, epsilon 0.02321673893600829, time 736.0, rides 127\n",
      "Initial State is  [2, 3, 1]\n",
      "episode 3761, reward 1040.0, memory_length 2000, epsilon 0.02319352219707228, time 734.0, rides 131\n",
      "Initial State is  [2, 16, 6]\n",
      "episode 3762, reward 1201.0, memory_length 2000, epsilon 0.023170328674875208, time 728.0, rides 133\n",
      "Initial State is  [1, 20, 3]\n",
      "episode 3763, reward 980.0, memory_length 2000, epsilon 0.023147158346200333, time 732.0, rides 130\n",
      "Initial State is  [4, 5, 3]\n",
      "episode 3764, reward 930.0, memory_length 2000, epsilon 0.02312401118785413, time 727.0, rides 125\n",
      "Initial State is  [1, 22, 6]\n",
      "episode 3765, reward 720.0, memory_length 2000, epsilon 0.023100887176666276, time 731.0, rides 134\n",
      "Initial State is  [2, 13, 6]\n",
      "episode 3766, reward 855.0, memory_length 2000, epsilon 0.02307778628948961, time 729.0, rides 118\n",
      "Initial State is  [4, 2, 4]\n",
      "episode 3767, reward 754.0, memory_length 2000, epsilon 0.02305470850320012, time 738.0, rides 131\n",
      "Initial State is  [1, 15, 1]\n",
      "episode 3768, reward 922.0, memory_length 2000, epsilon 0.023031653794696922, time 727.0, rides 128\n",
      "Initial State is  [1, 11, 3]\n",
      "episode 3769, reward 798.0, memory_length 2000, epsilon 0.023008622140902227, time 725.0, rides 133\n",
      "Initial State is  [1, 10, 2]\n",
      "episode 3770, reward 740.0, memory_length 2000, epsilon 0.022985613518761324, time 729.0, rides 116\n",
      "Initial State is  [0, 5, 6]\n",
      "episode 3771, reward 743.0, memory_length 2000, epsilon 0.022962627905242564, time 730.0, rides 138\n",
      "Initial State is  [3, 21, 2]\n",
      "episode 3772, reward 1014.0, memory_length 2000, epsilon 0.02293966527733732, time 725.0, rides 127\n",
      "Initial State is  [1, 3, 5]\n",
      "episode 3773, reward 796.0, memory_length 2000, epsilon 0.022916725612059985, time 722.0, rides 123\n",
      "Initial State is  [0, 20, 5]\n",
      "episode 3774, reward 1076.0, memory_length 2000, epsilon 0.022893808886447924, time 728.0, rides 127\n",
      "Initial State is  [0, 15, 4]\n",
      "episode 3775, reward 530.0, memory_length 2000, epsilon 0.022870915077561477, time 734.0, rides 134\n",
      "Initial State is  [4, 17, 2]\n",
      "episode 3776, reward 1224.0, memory_length 2000, epsilon 0.022848044162483917, time 732.0, rides 129\n",
      "Initial State is  [1, 23, 3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 3777, reward 887.0, memory_length 2000, epsilon 0.02282519611832143, time 733.0, rides 141\n",
      "Initial State is  [4, 15, 6]\n",
      "episode 3778, reward 829.0, memory_length 2000, epsilon 0.02280237092220311, time 728.0, rides 140\n",
      "Initial State is  [0, 8, 5]\n",
      "episode 3779, reward 1004.0, memory_length 2000, epsilon 0.02277956855128091, time 728.0, rides 128\n",
      "Initial State is  [4, 22, 1]\n",
      "episode 3780, reward 936.0, memory_length 2000, epsilon 0.02275678898272963, time 730.0, rides 134\n",
      "Initial State is  [4, 13, 2]\n",
      "episode 3781, reward 894.0, memory_length 2000, epsilon 0.0227340321937469, time 731.0, rides 125\n",
      "Initial State is  [4, 15, 2]\n",
      "episode 3782, reward 855.0, memory_length 2000, epsilon 0.022711298161553154, time 736.0, rides 131\n",
      "Initial State is  [0, 0, 6]\n",
      "episode 3783, reward 582.0, memory_length 2000, epsilon 0.0226885868633916, time 729.0, rides 120\n",
      "Initial State is  [4, 17, 5]\n",
      "episode 3784, reward 942.0, memory_length 2000, epsilon 0.02266589827652821, time 731.0, rides 120\n",
      "Initial State is  [4, 7, 5]\n",
      "episode 3785, reward 966.0, memory_length 2000, epsilon 0.022643232378251683, time 726.0, rides 130\n",
      "Initial State is  [0, 7, 1]\n",
      "episode 3786, reward 669.0, memory_length 2000, epsilon 0.02262058914587343, time 724.0, rides 128\n",
      "Initial State is  [0, 6, 3]\n",
      "episode 3787, reward 837.0, memory_length 2000, epsilon 0.02259796855672756, time 722.0, rides 122\n",
      "Initial State is  [3, 15, 2]\n",
      "episode 3788, reward 1037.0, memory_length 2000, epsilon 0.022575370588170832, time 731.0, rides 126\n",
      "Initial State is  [1, 4, 0]\n",
      "episode 3789, reward 925.0, memory_length 2000, epsilon 0.02255279521758266, time 725.0, rides 130\n",
      "Initial State is  [1, 17, 3]\n",
      "episode 3790, reward 900.0, memory_length 2000, epsilon 0.02253024242236508, time 728.0, rides 127\n",
      "Initial State is  [3, 17, 2]\n",
      "episode 3791, reward 890.0, memory_length 2000, epsilon 0.022507712179942713, time 726.0, rides 128\n",
      "Initial State is  [4, 13, 3]\n",
      "episode 3792, reward 1135.0, memory_length 2000, epsilon 0.02248520446776277, time 723.0, rides 131\n",
      "Initial State is  [4, 6, 4]\n",
      "episode 3793, reward 782.0, memory_length 2000, epsilon 0.02246271926329501, time 735.0, rides 129\n",
      "Initial State is  [1, 3, 3]\n",
      "episode 3794, reward 911.0, memory_length 2000, epsilon 0.022440256544031714, time 729.0, rides 130\n",
      "Initial State is  [4, 19, 6]\n",
      "episode 3795, reward 904.0, memory_length 2000, epsilon 0.022417816287487683, time 725.0, rides 145\n",
      "Initial State is  [4, 10, 5]\n",
      "episode 3796, reward 937.0, memory_length 2000, epsilon 0.022395398471200194, time 729.0, rides 144\n",
      "Initial State is  [2, 10, 1]\n",
      "episode 3797, reward 789.0, memory_length 2000, epsilon 0.022373003072728992, time 731.0, rides 114\n",
      "Initial State is  [4, 13, 1]\n",
      "episode 3798, reward 1031.0, memory_length 2000, epsilon 0.022350630069656263, time 727.0, rides 142\n",
      "Initial State is  [3, 21, 4]\n",
      "episode 3799, reward 1084.0, memory_length 2000, epsilon 0.022328279439586606, time 723.0, rides 131\n",
      "Initial State is  [0, 5, 6]\n",
      "episode 3800, reward 870.0, memory_length 2000, epsilon 0.022305951160147018, time 726.0, rides 143\n",
      "Initial State is  [2, 3, 0]\n",
      "episode 3801, reward 604.0, memory_length 2000, epsilon 0.02228364520898687, time 728.0, rides 116\n",
      "Initial State is  [1, 5, 6]\n",
      "episode 3802, reward 769.0, memory_length 2000, epsilon 0.022261361563777882, time 723.0, rides 125\n",
      "Initial State is  [4, 14, 3]\n",
      "episode 3803, reward 937.0, memory_length 2000, epsilon 0.022239100202214104, time 728.0, rides 135\n",
      "Initial State is  [1, 17, 6]\n",
      "episode 3804, reward 460.0, memory_length 2000, epsilon 0.02221686110201189, time 734.0, rides 133\n",
      "Initial State is  [0, 17, 1]\n",
      "episode 3805, reward 827.0, memory_length 2000, epsilon 0.02219464424090988, time 735.0, rides 112\n",
      "Initial State is  [3, 1, 1]\n",
      "episode 3806, reward 796.0, memory_length 2000, epsilon 0.022172449596668968, time 731.0, rides 134\n",
      "Initial State is  [4, 2, 6]\n",
      "episode 3807, reward 836.0, memory_length 2000, epsilon 0.022150277147072298, time 730.0, rides 130\n",
      "Initial State is  [4, 15, 5]\n",
      "episode 3808, reward 721.0, memory_length 2000, epsilon 0.022128126869925227, time 725.0, rides 121\n",
      "Initial State is  [4, 20, 2]\n",
      "episode 3809, reward 1009.0, memory_length 2000, epsilon 0.022105998743055303, time 741.0, rides 121\n",
      "Initial State is  [2, 2, 4]\n",
      "episode 3810, reward 982.0, memory_length 2000, epsilon 0.022083892744312248, time 727.0, rides 121\n",
      "Initial State is  [0, 19, 6]\n",
      "episode 3811, reward 1031.0, memory_length 2000, epsilon 0.022061808851567936, time 725.0, rides 141\n",
      "Initial State is  [3, 5, 0]\n",
      "episode 3812, reward 679.0, memory_length 2000, epsilon 0.022039747042716367, time 735.0, rides 111\n",
      "Initial State is  [3, 13, 3]\n",
      "episode 3813, reward 1094.0, memory_length 2000, epsilon 0.02201770729567365, time 740.0, rides 116\n",
      "Initial State is  [1, 23, 0]\n",
      "episode 3814, reward 1009.0, memory_length 2000, epsilon 0.021995689588377977, time 728.0, rides 129\n",
      "Initial State is  [3, 11, 5]\n",
      "episode 3815, reward 969.0, memory_length 2000, epsilon 0.0219736938987896, time 731.0, rides 128\n",
      "Initial State is  [4, 0, 1]\n",
      "episode 3816, reward 875.0, memory_length 2000, epsilon 0.02195172020489081, time 728.0, rides 131\n",
      "Initial State is  [0, 20, 6]\n",
      "episode 3817, reward 724.0, memory_length 2000, epsilon 0.02192976848468592, time 725.0, rides 130\n",
      "Initial State is  [4, 20, 3]\n",
      "episode 3818, reward 972.0, memory_length 2000, epsilon 0.021907838716201233, time 726.0, rides 120\n",
      "Initial State is  [1, 15, 5]\n",
      "episode 3819, reward 677.0, memory_length 2000, epsilon 0.02188593087748503, time 728.0, rides 140\n",
      "Initial State is  [2, 19, 5]\n",
      "episode 3820, reward 872.0, memory_length 2000, epsilon 0.021864044946607545, time 733.0, rides 122\n",
      "Initial State is  [1, 9, 3]\n",
      "episode 3821, reward 684.0, memory_length 2000, epsilon 0.021842180901660936, time 732.0, rides 129\n",
      "Initial State is  [3, 3, 3]\n",
      "episode 3822, reward 930.0, memory_length 2000, epsilon 0.021820338720759277, time 729.0, rides 126\n",
      "Initial State is  [0, 10, 3]\n",
      "episode 3823, reward 900.0, memory_length 2000, epsilon 0.021798518382038518, time 729.0, rides 117\n",
      "Initial State is  [0, 22, 4]\n",
      "episode 3824, reward 860.0, memory_length 2000, epsilon 0.02177671986365648, time 729.0, rides 137\n",
      "Initial State is  [3, 4, 5]\n",
      "episode 3825, reward 965.0, memory_length 2000, epsilon 0.021754943143792824, time 736.0, rides 135\n",
      "Initial State is  [2, 15, 4]\n",
      "episode 3826, reward 812.0, memory_length 2000, epsilon 0.02173318820064903, time 730.0, rides 128\n",
      "Initial State is  [3, 14, 4]\n",
      "episode 3827, reward 1083.0, memory_length 2000, epsilon 0.02171145501244838, time 727.0, rides 126\n",
      "Initial State is  [1, 17, 5]\n",
      "episode 3828, reward 897.0, memory_length 2000, epsilon 0.02168974355743593, time 734.0, rides 137\n",
      "Initial State is  [2, 19, 5]\n",
      "episode 3829, reward 814.0, memory_length 2000, epsilon 0.021668053813878495, time 729.0, rides 137\n",
      "Initial State is  [4, 16, 0]\n",
      "episode 3830, reward 1264.0, memory_length 2000, epsilon 0.021646385760064616, time 725.0, rides 133\n",
      "Initial State is  [4, 15, 4]\n",
      "episode 3831, reward 953.0, memory_length 2000, epsilon 0.021624739374304553, time 724.0, rides 140\n",
      "Initial State is  [2, 9, 1]\n",
      "episode 3832, reward 1141.0, memory_length 2000, epsilon 0.021603114634930247, time 724.0, rides 127\n",
      "Initial State is  [3, 7, 1]\n",
      "episode 3833, reward 1205.0, memory_length 2000, epsilon 0.02158151152029532, time 731.0, rides 146\n",
      "Initial State is  [0, 10, 3]\n",
      "episode 3834, reward 995.0, memory_length 2000, epsilon 0.021559930008775024, time 722.0, rides 144\n",
      "Initial State is  [3, 14, 3]\n",
      "episode 3835, reward 855.0, memory_length 2000, epsilon 0.021538370078766248, time 732.0, rides 147\n",
      "Initial State is  [4, 20, 3]\n",
      "episode 3836, reward 946.0, memory_length 2000, epsilon 0.02151683170868748, time 728.0, rides 132\n",
      "Initial State is  [1, 22, 3]\n",
      "episode 3837, reward 906.0, memory_length 2000, epsilon 0.021495314876978793, time 726.0, rides 137\n",
      "Initial State is  [1, 9, 1]\n",
      "episode 3838, reward 797.0, memory_length 2000, epsilon 0.021473819562101815, time 725.0, rides 136\n",
      "Initial State is  [1, 21, 4]\n",
      "episode 3839, reward 994.0, memory_length 2000, epsilon 0.02145234574253971, time 729.0, rides 132\n",
      "Initial State is  [2, 11, 4]\n",
      "episode 3840, reward 787.0, memory_length 2000, epsilon 0.02143089339679717, time 725.0, rides 120\n",
      "Initial State is  [1, 10, 6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 3841, reward 769.0, memory_length 2000, epsilon 0.021409462503400374, time 728.0, rides 119\n",
      "Initial State is  [3, 9, 4]\n",
      "episode 3842, reward 779.0, memory_length 2000, epsilon 0.021388053040896974, time 722.0, rides 126\n",
      "Initial State is  [0, 14, 2]\n",
      "episode 3843, reward 864.0, memory_length 2000, epsilon 0.021366664987856075, time 730.0, rides 119\n",
      "Initial State is  [2, 11, 1]\n",
      "episode 3844, reward 1038.0, memory_length 2000, epsilon 0.02134529832286822, time 726.0, rides 140\n",
      "Initial State is  [1, 11, 0]\n",
      "episode 3845, reward 1014.0, memory_length 2000, epsilon 0.02132395302454535, time 730.0, rides 140\n",
      "Initial State is  [4, 9, 0]\n",
      "episode 3846, reward 759.0, memory_length 2000, epsilon 0.021302629071520807, time 721.0, rides 133\n",
      "Initial State is  [3, 16, 3]\n",
      "episode 3847, reward 689.0, memory_length 2000, epsilon 0.021281326442449285, time 725.0, rides 114\n",
      "Initial State is  [3, 5, 5]\n",
      "episode 3848, reward 828.0, memory_length 2000, epsilon 0.021260045116006834, time 735.0, rides 122\n",
      "Initial State is  [3, 17, 4]\n",
      "episode 3849, reward 1096.0, memory_length 2000, epsilon 0.021238785070890828, time 731.0, rides 135\n",
      "Initial State is  [1, 6, 2]\n",
      "episode 3850, reward 824.0, memory_length 2000, epsilon 0.021217546285819937, time 730.0, rides 131\n",
      "Initial State is  [2, 19, 4]\n",
      "episode 3851, reward 852.0, memory_length 2000, epsilon 0.02119632873953412, time 728.0, rides 140\n",
      "Initial State is  [4, 12, 1]\n",
      "episode 3852, reward 838.0, memory_length 2000, epsilon 0.021175132410794585, time 730.0, rides 133\n",
      "Initial State is  [2, 10, 1]\n",
      "episode 3853, reward 980.0, memory_length 2000, epsilon 0.02115395727838379, time 727.0, rides 129\n",
      "Initial State is  [0, 18, 2]\n",
      "episode 3854, reward 918.0, memory_length 2000, epsilon 0.021132803321105405, time 728.0, rides 130\n",
      "Initial State is  [2, 22, 2]\n",
      "episode 3855, reward 819.0, memory_length 2000, epsilon 0.0211116705177843, time 730.0, rides 131\n",
      "Initial State is  [1, 16, 1]\n",
      "episode 3856, reward 948.0, memory_length 2000, epsilon 0.021090558847266516, time 725.0, rides 123\n",
      "Initial State is  [3, 3, 3]\n",
      "episode 3857, reward 967.0, memory_length 2000, epsilon 0.02106946828841925, time 726.0, rides 138\n",
      "Initial State is  [4, 22, 2]\n",
      "episode 3858, reward 454.0, memory_length 2000, epsilon 0.02104839882013083, time 729.0, rides 143\n",
      "Initial State is  [3, 18, 2]\n",
      "episode 3859, reward 970.0, memory_length 2000, epsilon 0.0210273504213107, time 723.0, rides 146\n",
      "Initial State is  [1, 4, 5]\n",
      "episode 3860, reward 835.0, memory_length 2000, epsilon 0.02100632307088939, time 727.0, rides 138\n",
      "Initial State is  [2, 1, 6]\n",
      "episode 3861, reward 963.0, memory_length 2000, epsilon 0.0209853167478185, time 731.0, rides 123\n",
      "Initial State is  [3, 16, 3]\n",
      "episode 3862, reward 825.0, memory_length 2000, epsilon 0.02096433143107068, time 734.0, rides 147\n",
      "Initial State is  [2, 4, 3]\n",
      "episode 3863, reward 561.0, memory_length 2000, epsilon 0.020943367099639607, time 725.0, rides 125\n",
      "Initial State is  [0, 7, 1]\n",
      "episode 3864, reward 937.0, memory_length 2000, epsilon 0.020922423732539965, time 728.0, rides 148\n",
      "Initial State is  [0, 7, 1]\n",
      "episode 3865, reward 959.0, memory_length 2000, epsilon 0.020901501308807423, time 726.0, rides 139\n",
      "Initial State is  [2, 9, 1]\n",
      "episode 3866, reward 946.0, memory_length 2000, epsilon 0.020880599807498616, time 724.0, rides 129\n",
      "Initial State is  [4, 1, 5]\n",
      "episode 3867, reward 1022.0, memory_length 2000, epsilon 0.020859719207691117, time 730.0, rides 128\n",
      "Initial State is  [2, 7, 3]\n",
      "episode 3868, reward 878.0, memory_length 2000, epsilon 0.020838859488483425, time 726.0, rides 127\n",
      "Initial State is  [1, 7, 1]\n",
      "episode 3869, reward 904.0, memory_length 2000, epsilon 0.020818020628994943, time 729.0, rides 135\n",
      "Initial State is  [3, 17, 6]\n",
      "episode 3870, reward 963.0, memory_length 2000, epsilon 0.02079720260836595, time 729.0, rides 134\n",
      "Initial State is  [3, 14, 3]\n",
      "episode 3871, reward 914.0, memory_length 2000, epsilon 0.02077640540575758, time 736.0, rides 130\n",
      "Initial State is  [0, 16, 0]\n",
      "episode 3872, reward 841.0, memory_length 2000, epsilon 0.020755629000351824, time 731.0, rides 137\n",
      "Initial State is  [1, 8, 3]\n",
      "episode 3873, reward 730.0, memory_length 2000, epsilon 0.02073487337135147, time 731.0, rides 124\n",
      "Initial State is  [2, 10, 0]\n",
      "episode 3874, reward 994.0, memory_length 2000, epsilon 0.02071413849798012, time 728.0, rides 127\n",
      "Initial State is  [2, 22, 6]\n",
      "episode 3875, reward 1054.0, memory_length 2000, epsilon 0.02069342435948214, time 724.0, rides 126\n",
      "Initial State is  [0, 4, 0]\n",
      "episode 3876, reward 930.0, memory_length 2000, epsilon 0.020672730935122657, time 724.0, rides 125\n",
      "Initial State is  [1, 8, 1]\n",
      "episode 3877, reward 802.0, memory_length 2000, epsilon 0.020652058204187532, time 725.0, rides 141\n",
      "Initial State is  [2, 9, 5]\n",
      "episode 3878, reward 877.0, memory_length 2000, epsilon 0.020631406145983345, time 725.0, rides 118\n",
      "Initial State is  [0, 15, 2]\n",
      "episode 3879, reward 1043.0, memory_length 2000, epsilon 0.02061077473983736, time 732.0, rides 138\n",
      "Initial State is  [4, 6, 2]\n",
      "episode 3880, reward 1008.0, memory_length 2000, epsilon 0.02059016396509752, time 731.0, rides 121\n",
      "Initial State is  [2, 3, 2]\n",
      "episode 3881, reward 886.0, memory_length 2000, epsilon 0.020569573801132425, time 729.0, rides 126\n",
      "Initial State is  [0, 16, 3]\n",
      "episode 3882, reward 1023.0, memory_length 2000, epsilon 0.020549004227331292, time 728.0, rides 141\n",
      "Initial State is  [0, 4, 4]\n",
      "episode 3883, reward 971.0, memory_length 2000, epsilon 0.02052845522310396, time 733.0, rides 127\n",
      "Initial State is  [4, 8, 3]\n",
      "episode 3884, reward 958.0, memory_length 2000, epsilon 0.020507926767880855, time 740.0, rides 130\n",
      "Initial State is  [4, 15, 3]\n",
      "episode 3885, reward 912.0, memory_length 2000, epsilon 0.020487418841112975, time 722.0, rides 124\n",
      "Initial State is  [1, 20, 6]\n",
      "episode 3886, reward 913.0, memory_length 2000, epsilon 0.02046693142227186, time 729.0, rides 132\n",
      "Initial State is  [1, 20, 2]\n",
      "episode 3887, reward 608.0, memory_length 2000, epsilon 0.020446464490849588, time 724.0, rides 119\n",
      "Initial State is  [0, 9, 1]\n",
      "episode 3888, reward 1053.0, memory_length 2000, epsilon 0.02042601802635874, time 730.0, rides 125\n",
      "Initial State is  [2, 2, 1]\n",
      "episode 3889, reward 1092.0, memory_length 2000, epsilon 0.02040559200833238, time 733.0, rides 122\n",
      "Initial State is  [0, 1, 3]\n",
      "episode 3890, reward 1127.0, memory_length 2000, epsilon 0.020385186416324048, time 728.0, rides 136\n",
      "Initial State is  [4, 18, 1]\n",
      "episode 3891, reward 928.0, memory_length 2000, epsilon 0.020364801229907723, time 730.0, rides 130\n",
      "Initial State is  [0, 0, 0]\n",
      "episode 3892, reward 1295.0, memory_length 2000, epsilon 0.020344436428677816, time 727.0, rides 138\n",
      "Initial State is  [3, 14, 4]\n",
      "episode 3893, reward 992.0, memory_length 2000, epsilon 0.02032409199224914, time 735.0, rides 131\n",
      "Initial State is  [1, 1, 0]\n",
      "episode 3894, reward 577.0, memory_length 2000, epsilon 0.02030376790025689, time 724.0, rides 123\n",
      "Initial State is  [4, 6, 4]\n",
      "episode 3895, reward 834.0, memory_length 2000, epsilon 0.020283464132356634, time 729.0, rides 134\n",
      "Initial State is  [4, 3, 2]\n",
      "episode 3896, reward 1167.0, memory_length 2000, epsilon 0.020263180668224277, time 726.0, rides 122\n",
      "Initial State is  [2, 23, 0]\n",
      "episode 3897, reward 1077.0, memory_length 2000, epsilon 0.020242917487556054, time 726.0, rides 130\n",
      "Initial State is  [3, 13, 3]\n",
      "episode 3898, reward 993.0, memory_length 2000, epsilon 0.0202226745700685, time 723.0, rides 128\n",
      "Initial State is  [3, 17, 1]\n",
      "episode 3899, reward 1163.0, memory_length 2000, epsilon 0.02020245189549843, time 731.0, rides 132\n",
      "Initial State is  [1, 2, 4]\n",
      "episode 3900, reward 1195.0, memory_length 2000, epsilon 0.02018224944360293, time 725.0, rides 139\n",
      "Initial State is  [2, 13, 6]\n",
      "episode 3901, reward 1095.0, memory_length 2000, epsilon 0.02016206719415933, time 725.0, rides 124\n",
      "Initial State is  [2, 4, 4]\n",
      "episode 3902, reward 1029.0, memory_length 2000, epsilon 0.02014190512696517, time 730.0, rides 128\n",
      "Initial State is  [2, 8, 0]\n",
      "episode 3903, reward 913.0, memory_length 2000, epsilon 0.020121763221838205, time 734.0, rides 127\n",
      "Initial State is  [1, 15, 5]\n",
      "episode 3904, reward 1052.0, memory_length 2000, epsilon 0.020101641458616367, time 731.0, rides 131\n",
      "Initial State is  [3, 12, 3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 3905, reward 662.0, memory_length 2000, epsilon 0.02008153981715775, time 736.0, rides 121\n",
      "Initial State is  [4, 12, 1]\n",
      "episode 3906, reward 1116.0, memory_length 2000, epsilon 0.020061458277340592, time 740.0, rides 135\n",
      "Initial State is  [1, 17, 0]\n",
      "episode 3907, reward 784.0, memory_length 2000, epsilon 0.02004139681906325, time 734.0, rides 127\n",
      "Initial State is  [1, 0, 4]\n",
      "episode 3908, reward 944.0, memory_length 2000, epsilon 0.02002135542224419, time 721.0, rides 131\n",
      "Initial State is  [2, 6, 2]\n",
      "episode 3909, reward 880.0, memory_length 2000, epsilon 0.020001334066821943, time 729.0, rides 129\n",
      "Initial State is  [3, 21, 3]\n",
      "episode 3910, reward 964.0, memory_length 2000, epsilon 0.01998133273275512, time 727.0, rides 129\n",
      "Initial State is  [3, 6, 0]\n",
      "episode 3911, reward 819.0, memory_length 2000, epsilon 0.019961351400022365, time 730.0, rides 129\n",
      "Initial State is  [0, 18, 6]\n",
      "episode 3912, reward 1079.0, memory_length 2000, epsilon 0.019941390048622342, time 732.0, rides 127\n",
      "Initial State is  [1, 17, 1]\n",
      "episode 3913, reward 1000.0, memory_length 2000, epsilon 0.01992144865857372, time 738.0, rides 133\n",
      "Initial State is  [3, 11, 1]\n",
      "episode 3914, reward 823.0, memory_length 2000, epsilon 0.019901527209915146, time 730.0, rides 124\n",
      "Initial State is  [3, 23, 0]\n",
      "episode 3915, reward 1108.0, memory_length 2000, epsilon 0.019881625682705233, time 730.0, rides 120\n",
      "Initial State is  [4, 15, 3]\n",
      "episode 3916, reward 675.0, memory_length 2000, epsilon 0.019861744057022526, time 726.0, rides 135\n",
      "Initial State is  [1, 15, 6]\n",
      "episode 3917, reward 1016.0, memory_length 2000, epsilon 0.019841882312965502, time 722.0, rides 127\n",
      "Initial State is  [2, 6, 3]\n",
      "episode 3918, reward 986.0, memory_length 2000, epsilon 0.019822040430652537, time 725.0, rides 128\n",
      "Initial State is  [2, 14, 6]\n",
      "episode 3919, reward 780.0, memory_length 2000, epsilon 0.019802218390221886, time 727.0, rides 126\n",
      "Initial State is  [3, 9, 4]\n",
      "episode 3920, reward 1191.0, memory_length 2000, epsilon 0.019782416171831664, time 730.0, rides 125\n",
      "Initial State is  [4, 1, 0]\n",
      "episode 3921, reward 1018.0, memory_length 2000, epsilon 0.01976263375565983, time 733.0, rides 134\n",
      "Initial State is  [2, 15, 5]\n",
      "episode 3922, reward 950.0, memory_length 2000, epsilon 0.019742871121904173, time 722.0, rides 120\n",
      "Initial State is  [1, 18, 1]\n",
      "episode 3923, reward 1027.0, memory_length 2000, epsilon 0.01972312825078227, time 731.0, rides 125\n",
      "Initial State is  [2, 23, 2]\n",
      "episode 3924, reward 942.0, memory_length 2000, epsilon 0.01970340512253149, time 729.0, rides 134\n",
      "Initial State is  [4, 18, 4]\n",
      "episode 3925, reward 1228.0, memory_length 2000, epsilon 0.019683701717408957, time 728.0, rides 129\n",
      "Initial State is  [4, 2, 5]\n",
      "episode 3926, reward 932.0, memory_length 2000, epsilon 0.019664018015691547, time 733.0, rides 128\n",
      "Initial State is  [1, 21, 0]\n",
      "episode 3927, reward 710.0, memory_length 2000, epsilon 0.019644353997675855, time 730.0, rides 132\n",
      "Initial State is  [4, 16, 3]\n",
      "episode 3928, reward 965.0, memory_length 2000, epsilon 0.01962470964367818, time 724.0, rides 129\n",
      "Initial State is  [4, 6, 0]\n",
      "episode 3929, reward 932.0, memory_length 2000, epsilon 0.019605084934034504, time 730.0, rides 121\n",
      "Initial State is  [3, 14, 2]\n",
      "episode 3930, reward 818.0, memory_length 2000, epsilon 0.01958547984910047, time 721.0, rides 119\n",
      "Initial State is  [2, 6, 5]\n",
      "episode 3931, reward 852.0, memory_length 2000, epsilon 0.01956589436925137, time 725.0, rides 123\n",
      "Initial State is  [1, 2, 3]\n",
      "episode 3932, reward 955.0, memory_length 2000, epsilon 0.019546328474882118, time 734.0, rides 126\n",
      "Initial State is  [4, 19, 2]\n",
      "episode 3933, reward 843.0, memory_length 2000, epsilon 0.019526782146407237, time 729.0, rides 135\n",
      "Initial State is  [1, 6, 6]\n",
      "episode 3934, reward 922.0, memory_length 2000, epsilon 0.01950725536426083, time 735.0, rides 126\n",
      "Initial State is  [2, 2, 5]\n",
      "episode 3935, reward 814.0, memory_length 2000, epsilon 0.01948774810889657, time 732.0, rides 127\n",
      "Initial State is  [4, 17, 2]\n",
      "episode 3936, reward 1337.0, memory_length 2000, epsilon 0.019468260360787675, time 725.0, rides 127\n",
      "Initial State is  [0, 23, 6]\n",
      "episode 3937, reward 1113.0, memory_length 2000, epsilon 0.01944879210042689, time 733.0, rides 123\n",
      "Initial State is  [0, 7, 1]\n",
      "episode 3938, reward 1144.0, memory_length 2000, epsilon 0.019429343308326463, time 735.0, rides 126\n",
      "Initial State is  [4, 18, 1]\n",
      "episode 3939, reward 821.0, memory_length 2000, epsilon 0.019409913965018136, time 730.0, rides 132\n",
      "Initial State is  [2, 5, 2]\n",
      "episode 3940, reward 905.0, memory_length 2000, epsilon 0.019390504051053116, time 727.0, rides 126\n",
      "Initial State is  [2, 23, 0]\n",
      "episode 3941, reward 1082.0, memory_length 2000, epsilon 0.019371113547002064, time 731.0, rides 123\n",
      "Initial State is  [4, 12, 2]\n",
      "episode 3942, reward 883.0, memory_length 2000, epsilon 0.01935174243345506, time 732.0, rides 127\n",
      "Initial State is  [1, 7, 0]\n",
      "episode 3943, reward 483.0, memory_length 2000, epsilon 0.019332390691021606, time 732.0, rides 120\n",
      "Initial State is  [1, 16, 1]\n",
      "episode 3944, reward 981.0, memory_length 2000, epsilon 0.019313058300330584, time 724.0, rides 136\n",
      "Initial State is  [3, 13, 6]\n",
      "episode 3945, reward 922.0, memory_length 2000, epsilon 0.019293745242030255, time 726.0, rides 136\n",
      "Initial State is  [2, 20, 2]\n",
      "episode 3946, reward 687.0, memory_length 2000, epsilon 0.019274451496788223, time 723.0, rides 129\n",
      "Initial State is  [3, 16, 3]\n",
      "episode 3947, reward 1215.0, memory_length 2000, epsilon 0.019255177045291436, time 727.0, rides 122\n",
      "Initial State is  [1, 6, 1]\n",
      "episode 3948, reward 1064.0, memory_length 2000, epsilon 0.019235921868246145, time 729.0, rides 134\n",
      "Initial State is  [4, 15, 2]\n",
      "episode 3949, reward 953.0, memory_length 2000, epsilon 0.019216685946377897, time 727.0, rides 120\n",
      "Initial State is  [0, 11, 1]\n",
      "episode 3950, reward 675.0, memory_length 2000, epsilon 0.01919746926043152, time 727.0, rides 135\n",
      "Initial State is  [2, 17, 5]\n",
      "episode 3951, reward 790.0, memory_length 2000, epsilon 0.019178271791171087, time 727.0, rides 138\n",
      "Initial State is  [2, 13, 6]\n",
      "episode 3952, reward 831.0, memory_length 2000, epsilon 0.019159093519379916, time 725.0, rides 126\n",
      "Initial State is  [1, 1, 5]\n",
      "episode 3953, reward 962.0, memory_length 2000, epsilon 0.019139934425860535, time 727.0, rides 138\n",
      "Initial State is  [4, 17, 1]\n",
      "episode 3954, reward 842.0, memory_length 2000, epsilon 0.019120794491434674, time 734.0, rides 136\n",
      "Initial State is  [0, 19, 3]\n",
      "episode 3955, reward 1016.0, memory_length 2000, epsilon 0.01910167369694324, time 732.0, rides 128\n",
      "Initial State is  [1, 23, 0]\n",
      "episode 3956, reward 935.0, memory_length 2000, epsilon 0.019082572023246296, time 728.0, rides 129\n",
      "Initial State is  [1, 20, 2]\n",
      "episode 3957, reward 1108.0, memory_length 2000, epsilon 0.01906348945122305, time 730.0, rides 146\n",
      "Initial State is  [3, 20, 6]\n",
      "episode 3958, reward 892.0, memory_length 2000, epsilon 0.01904442596177183, time 726.0, rides 136\n",
      "Initial State is  [2, 10, 5]\n",
      "episode 3959, reward 952.0, memory_length 2000, epsilon 0.019025381535810057, time 730.0, rides 121\n",
      "Initial State is  [1, 11, 2]\n",
      "episode 3960, reward 1006.0, memory_length 2000, epsilon 0.019006356154274248, time 727.0, rides 133\n",
      "Initial State is  [4, 12, 3]\n",
      "episode 3961, reward 965.0, memory_length 2000, epsilon 0.018987349798119973, time 737.0, rides 135\n",
      "Initial State is  [1, 8, 4]\n",
      "episode 3962, reward 997.0, memory_length 2000, epsilon 0.018968362448321854, time 739.0, rides 129\n",
      "Initial State is  [0, 7, 5]\n",
      "episode 3963, reward 1077.0, memory_length 2000, epsilon 0.018949394085873532, time 730.0, rides 128\n",
      "Initial State is  [2, 19, 3]\n",
      "episode 3964, reward 1078.0, memory_length 2000, epsilon 0.01893044469178766, time 731.0, rides 115\n",
      "Initial State is  [4, 4, 4]\n",
      "episode 3965, reward 980.0, memory_length 2000, epsilon 0.018911514247095872, time 733.0, rides 135\n",
      "Initial State is  [4, 6, 6]\n",
      "episode 3966, reward 903.0, memory_length 2000, epsilon 0.018892602732848776, time 736.0, rides 131\n",
      "Initial State is  [2, 4, 6]\n",
      "episode 3967, reward 1082.0, memory_length 2000, epsilon 0.018873710130115927, time 723.0, rides 140\n",
      "Initial State is  [1, 8, 1]\n",
      "episode 3968, reward 984.0, memory_length 2000, epsilon 0.01885483641998581, time 733.0, rides 132\n",
      "Initial State is  [4, 21, 6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 3969, reward 1008.0, memory_length 2000, epsilon 0.018835981583565822, time 727.0, rides 131\n",
      "Initial State is  [2, 3, 1]\n",
      "episode 3970, reward 896.0, memory_length 2000, epsilon 0.018817145601982256, time 739.0, rides 141\n",
      "Initial State is  [4, 8, 5]\n",
      "episode 3971, reward 1033.0, memory_length 2000, epsilon 0.018798328456380273, time 724.0, rides 127\n",
      "Initial State is  [2, 14, 3]\n",
      "episode 3972, reward 951.0, memory_length 2000, epsilon 0.018779530127923893, time 728.0, rides 146\n",
      "Initial State is  [3, 16, 0]\n",
      "episode 3973, reward 585.0, memory_length 2000, epsilon 0.01876075059779597, time 732.0, rides 124\n",
      "Initial State is  [0, 13, 3]\n",
      "episode 3974, reward 983.0, memory_length 2000, epsilon 0.018741989847198173, time 735.0, rides 133\n",
      "Initial State is  [3, 23, 1]\n",
      "episode 3975, reward 804.0, memory_length 2000, epsilon 0.018723247857350977, time 726.0, rides 134\n",
      "Initial State is  [0, 0, 4]\n",
      "episode 3976, reward 807.0, memory_length 2000, epsilon 0.018704524609493626, time 733.0, rides 137\n",
      "Initial State is  [2, 2, 1]\n",
      "episode 3977, reward 940.0, memory_length 2000, epsilon 0.018685820084884133, time 727.0, rides 133\n",
      "Initial State is  [2, 8, 1]\n",
      "episode 3978, reward 850.0, memory_length 2000, epsilon 0.01866713426479925, time 723.0, rides 130\n",
      "Initial State is  [0, 11, 1]\n",
      "episode 3979, reward 830.0, memory_length 2000, epsilon 0.01864846713053445, time 728.0, rides 134\n",
      "Initial State is  [0, 21, 6]\n",
      "episode 3980, reward 934.0, memory_length 2000, epsilon 0.018629818663403915, time 726.0, rides 141\n",
      "Initial State is  [1, 18, 0]\n",
      "episode 3981, reward 857.0, memory_length 2000, epsilon 0.01861118884474051, time 734.0, rides 123\n",
      "Initial State is  [4, 4, 2]\n",
      "episode 3982, reward 934.0, memory_length 2000, epsilon 0.01859257765589577, time 730.0, rides 133\n",
      "Initial State is  [3, 0, 6]\n",
      "episode 3983, reward 924.0, memory_length 2000, epsilon 0.018573985078239874, time 729.0, rides 132\n",
      "Initial State is  [2, 13, 2]\n",
      "episode 3984, reward 744.0, memory_length 2000, epsilon 0.018555411093161635, time 729.0, rides 128\n",
      "Initial State is  [2, 2, 4]\n",
      "episode 3985, reward 690.0, memory_length 2000, epsilon 0.018536855682068473, time 725.0, rides 128\n",
      "Initial State is  [0, 9, 4]\n",
      "episode 3986, reward 905.0, memory_length 2000, epsilon 0.018518318826386403, time 726.0, rides 134\n",
      "Initial State is  [0, 4, 6]\n",
      "episode 3987, reward 1135.0, memory_length 2000, epsilon 0.018499800507560015, time 733.0, rides 136\n",
      "Initial State is  [0, 11, 6]\n",
      "episode 3988, reward 1022.0, memory_length 2000, epsilon 0.018481300707052454, time 737.0, rides 133\n",
      "Initial State is  [1, 2, 6]\n",
      "episode 3989, reward 997.0, memory_length 2000, epsilon 0.0184628194063454, time 722.0, rides 136\n",
      "Initial State is  [1, 14, 4]\n",
      "episode 3990, reward 751.0, memory_length 2000, epsilon 0.018444356586939055, time 729.0, rides 130\n",
      "Initial State is  [1, 4, 0]\n",
      "episode 3991, reward 1070.0, memory_length 2000, epsilon 0.018425912230352115, time 728.0, rides 132\n",
      "Initial State is  [3, 16, 2]\n",
      "episode 3992, reward 922.0, memory_length 2000, epsilon 0.018407486318121762, time 724.0, rides 123\n",
      "Initial State is  [3, 4, 3]\n",
      "episode 3993, reward 1055.0, memory_length 2000, epsilon 0.01838907883180364, time 724.0, rides 148\n",
      "Initial State is  [0, 6, 6]\n",
      "episode 3994, reward 1042.0, memory_length 2000, epsilon 0.018370689752971837, time 724.0, rides 143\n",
      "Initial State is  [1, 13, 1]\n",
      "episode 3995, reward 877.0, memory_length 2000, epsilon 0.018352319063218867, time 736.0, rides 138\n",
      "Initial State is  [1, 7, 1]\n",
      "episode 3996, reward 966.0, memory_length 2000, epsilon 0.018333966744155647, time 728.0, rides 132\n",
      "Initial State is  [0, 8, 6]\n",
      "episode 3997, reward 1234.0, memory_length 2000, epsilon 0.01831563277741149, time 731.0, rides 142\n",
      "Initial State is  [0, 0, 1]\n",
      "episode 3998, reward 662.0, memory_length 2000, epsilon 0.01829731714463408, time 730.0, rides 134\n",
      "Initial State is  [0, 8, 6]\n",
      "episode 3999, reward 961.0, memory_length 2000, epsilon 0.018279019827489446, time 737.0, rides 139\n",
      "Initial State is  [3, 10, 3]\n",
      "episode 4000, reward 921.0, memory_length 2000, epsilon 0.018260740807661956, time 728.0, rides 130\n",
      "Initial State is  [2, 14, 1]\n",
      "episode 4001, reward 837.0, memory_length 2000, epsilon 0.018242480066854295, time 731.0, rides 130\n",
      "Initial State is  [1, 22, 2]\n",
      "episode 4002, reward 931.0, memory_length 2000, epsilon 0.01822423758678744, time 726.0, rides 131\n",
      "Initial State is  [1, 10, 1]\n",
      "episode 4003, reward 837.0, memory_length 2000, epsilon 0.01820601334920065, time 727.0, rides 126\n",
      "Initial State is  [4, 17, 3]\n",
      "episode 4004, reward 770.0, memory_length 2000, epsilon 0.01818780733585145, time 726.0, rides 131\n",
      "Initial State is  [0, 16, 2]\n",
      "episode 4005, reward 1011.0, memory_length 2000, epsilon 0.0181696195285156, time 738.0, rides 125\n",
      "Initial State is  [1, 19, 6]\n",
      "episode 4006, reward 770.0, memory_length 2000, epsilon 0.018151449908987084, time 725.0, rides 126\n",
      "Initial State is  [1, 10, 6]\n",
      "episode 4007, reward 902.0, memory_length 2000, epsilon 0.018133298459078098, time 726.0, rides 135\n",
      "Initial State is  [3, 0, 2]\n",
      "episode 4008, reward 982.0, memory_length 2000, epsilon 0.01811516516061902, time 728.0, rides 131\n",
      "Initial State is  [1, 3, 4]\n",
      "episode 4009, reward 760.0, memory_length 2000, epsilon 0.0180970499954584, time 725.0, rides 125\n",
      "Initial State is  [0, 15, 4]\n",
      "episode 4010, reward 1155.0, memory_length 2000, epsilon 0.018078952945462943, time 731.0, rides 135\n",
      "Initial State is  [4, 10, 2]\n",
      "episode 4011, reward 957.0, memory_length 2000, epsilon 0.01806087399251748, time 731.0, rides 128\n",
      "Initial State is  [3, 17, 4]\n",
      "episode 4012, reward 763.0, memory_length 2000, epsilon 0.018042813118524962, time 738.0, rides 126\n",
      "Initial State is  [4, 8, 0]\n",
      "episode 4013, reward 936.0, memory_length 2000, epsilon 0.01802477030540644, time 726.0, rides 133\n",
      "Initial State is  [1, 14, 5]\n",
      "episode 4014, reward 879.0, memory_length 2000, epsilon 0.01800674553510103, time 729.0, rides 132\n",
      "Initial State is  [1, 5, 6]\n",
      "episode 4015, reward 1134.0, memory_length 2000, epsilon 0.01798873878956593, time 729.0, rides 139\n",
      "Initial State is  [1, 1, 5]\n",
      "episode 4016, reward 844.0, memory_length 2000, epsilon 0.017970750050776366, time 735.0, rides 139\n",
      "Initial State is  [2, 5, 0]\n",
      "episode 4017, reward 1010.0, memory_length 2000, epsilon 0.01795277930072559, time 731.0, rides 141\n",
      "Initial State is  [2, 3, 1]\n",
      "episode 4018, reward 651.0, memory_length 2000, epsilon 0.017934826521424863, time 722.0, rides 124\n",
      "Initial State is  [0, 21, 5]\n",
      "episode 4019, reward 742.0, memory_length 2000, epsilon 0.017916891694903438, time 727.0, rides 129\n",
      "Initial State is  [3, 6, 6]\n",
      "episode 4020, reward 1077.0, memory_length 2000, epsilon 0.017898974803208532, time 727.0, rides 142\n",
      "Initial State is  [3, 17, 0]\n",
      "episode 4021, reward 1016.0, memory_length 2000, epsilon 0.017881075828405323, time 730.0, rides 134\n",
      "Initial State is  [0, 15, 6]\n",
      "episode 4022, reward 1083.0, memory_length 2000, epsilon 0.017863194752576916, time 733.0, rides 130\n",
      "Initial State is  [1, 10, 5]\n",
      "episode 4023, reward 966.0, memory_length 2000, epsilon 0.017845331557824338, time 732.0, rides 135\n",
      "Initial State is  [0, 21, 6]\n",
      "episode 4024, reward 841.0, memory_length 2000, epsilon 0.017827486226266512, time 722.0, rides 129\n",
      "Initial State is  [4, 7, 6]\n",
      "episode 4025, reward 861.0, memory_length 2000, epsilon 0.017809658740040247, time 728.0, rides 118\n",
      "Initial State is  [1, 19, 6]\n",
      "episode 4026, reward 583.0, memory_length 2000, epsilon 0.017791849081300208, time 726.0, rides 134\n",
      "Initial State is  [4, 22, 4]\n",
      "episode 4027, reward 1073.0, memory_length 2000, epsilon 0.017774057232218907, time 728.0, rides 121\n",
      "Initial State is  [4, 5, 5]\n",
      "episode 4028, reward 630.0, memory_length 2000, epsilon 0.017756283174986686, time 734.0, rides 127\n",
      "Initial State is  [4, 16, 1]\n",
      "episode 4029, reward 1177.0, memory_length 2000, epsilon 0.0177385268918117, time 727.0, rides 130\n",
      "Initial State is  [0, 6, 6]\n",
      "episode 4030, reward 935.0, memory_length 2000, epsilon 0.017720788364919887, time 735.0, rides 124\n",
      "Initial State is  [0, 20, 3]\n",
      "episode 4031, reward 807.0, memory_length 2000, epsilon 0.017703067576554966, time 720.0, rides 132\n",
      "Initial State is  [1, 21, 5]\n",
      "episode 4032, reward 874.0, memory_length 2000, epsilon 0.01768536450897841, time 727.0, rides 138\n",
      "Initial State is  [2, 0, 5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 4033, reward 979.0, memory_length 2000, epsilon 0.01766767914446943, time 732.0, rides 134\n",
      "Initial State is  [1, 10, 2]\n",
      "episode 4034, reward 1269.0, memory_length 2000, epsilon 0.017650011465324963, time 728.0, rides 130\n",
      "Initial State is  [2, 21, 2]\n",
      "episode 4035, reward 857.0, memory_length 2000, epsilon 0.017632361453859637, time 728.0, rides 136\n",
      "Initial State is  [0, 16, 0]\n",
      "episode 4036, reward 895.0, memory_length 2000, epsilon 0.017614729092405777, time 731.0, rides 125\n",
      "Initial State is  [2, 4, 0]\n",
      "episode 4037, reward 970.0, memory_length 2000, epsilon 0.01759711436331337, time 731.0, rides 142\n",
      "Initial State is  [1, 0, 2]\n",
      "episode 4038, reward 1121.0, memory_length 2000, epsilon 0.017579517248950054, time 726.0, rides 128\n",
      "Initial State is  [0, 18, 5]\n",
      "episode 4039, reward 881.0, memory_length 2000, epsilon 0.017561937731701106, time 727.0, rides 137\n",
      "Initial State is  [0, 7, 4]\n",
      "episode 4040, reward 845.0, memory_length 2000, epsilon 0.017544375793969405, time 735.0, rides 125\n",
      "Initial State is  [0, 16, 2]\n",
      "episode 4041, reward 850.0, memory_length 2000, epsilon 0.017526831418175435, time 727.0, rides 126\n",
      "Initial State is  [1, 8, 6]\n",
      "episode 4042, reward 856.0, memory_length 2000, epsilon 0.01750930458675726, time 735.0, rides 121\n",
      "Initial State is  [2, 11, 1]\n",
      "episode 4043, reward 1052.0, memory_length 2000, epsilon 0.017491795282170503, time 729.0, rides 120\n",
      "Initial State is  [4, 8, 0]\n",
      "episode 4044, reward 988.0, memory_length 2000, epsilon 0.017474303486888332, time 730.0, rides 124\n",
      "Initial State is  [1, 9, 2]\n",
      "episode 4045, reward 836.0, memory_length 2000, epsilon 0.017456829183401443, time 726.0, rides 137\n",
      "Initial State is  [3, 16, 1]\n",
      "episode 4046, reward 901.0, memory_length 2000, epsilon 0.017439372354218042, time 735.0, rides 123\n",
      "Initial State is  [2, 6, 1]\n",
      "episode 4047, reward 972.0, memory_length 2000, epsilon 0.017421932981863824, time 736.0, rides 136\n",
      "Initial State is  [4, 22, 5]\n",
      "episode 4048, reward 689.0, memory_length 2000, epsilon 0.01740451104888196, time 728.0, rides 127\n",
      "Initial State is  [2, 14, 2]\n",
      "episode 4049, reward 976.0, memory_length 2000, epsilon 0.017387106537833076, time 727.0, rides 132\n",
      "Initial State is  [1, 9, 0]\n",
      "episode 4050, reward 1008.0, memory_length 2000, epsilon 0.01736971943129524, time 735.0, rides 128\n",
      "Initial State is  [2, 0, 1]\n",
      "episode 4051, reward 731.0, memory_length 2000, epsilon 0.017352349711863946, time 730.0, rides 122\n",
      "Initial State is  [2, 20, 2]\n",
      "episode 4052, reward 1030.0, memory_length 2000, epsilon 0.01733499736215208, time 732.0, rides 136\n",
      "Initial State is  [0, 9, 5]\n",
      "episode 4053, reward 1044.0, memory_length 2000, epsilon 0.01731766236478993, time 723.0, rides 130\n",
      "Initial State is  [1, 9, 6]\n",
      "episode 4054, reward 947.0, memory_length 2000, epsilon 0.017300344702425138, time 728.0, rides 147\n",
      "Initial State is  [3, 18, 6]\n",
      "episode 4055, reward 785.0, memory_length 2000, epsilon 0.017283044357722713, time 730.0, rides 143\n",
      "Initial State is  [3, 14, 4]\n",
      "episode 4056, reward 831.0, memory_length 2000, epsilon 0.01726576131336499, time 728.0, rides 124\n",
      "Initial State is  [2, 16, 1]\n",
      "episode 4057, reward 665.0, memory_length 2000, epsilon 0.017248495552051626, time 724.0, rides 137\n",
      "Initial State is  [2, 18, 4]\n",
      "episode 4058, reward 666.0, memory_length 2000, epsilon 0.017231247056499574, time 730.0, rides 136\n",
      "Initial State is  [4, 7, 3]\n",
      "episode 4059, reward 812.0, memory_length 2000, epsilon 0.017214015809443074, time 737.0, rides 143\n",
      "Initial State is  [4, 13, 0]\n",
      "episode 4060, reward 710.0, memory_length 2000, epsilon 0.017196801793633632, time 735.0, rides 130\n",
      "Initial State is  [3, 10, 6]\n",
      "episode 4061, reward 812.0, memory_length 2000, epsilon 0.01717960499184, time 721.0, rides 138\n",
      "Initial State is  [1, 7, 0]\n",
      "episode 4062, reward 1203.0, memory_length 2000, epsilon 0.01716242538684816, time 728.0, rides 128\n",
      "Initial State is  [3, 11, 1]\n",
      "episode 4063, reward 901.0, memory_length 2000, epsilon 0.01714526296146131, time 729.0, rides 128\n",
      "Initial State is  [3, 16, 0]\n",
      "episode 4064, reward 603.0, memory_length 2000, epsilon 0.01712811769849985, time 725.0, rides 146\n",
      "Initial State is  [3, 7, 5]\n",
      "episode 4065, reward 1026.0, memory_length 2000, epsilon 0.01711098958080135, time 730.0, rides 140\n",
      "Initial State is  [2, 17, 3]\n",
      "episode 4066, reward 1016.0, memory_length 2000, epsilon 0.01709387859122055, time 726.0, rides 129\n",
      "Initial State is  [1, 6, 2]\n",
      "episode 4067, reward 954.0, memory_length 2000, epsilon 0.01707678471262933, time 727.0, rides 144\n",
      "Initial State is  [3, 15, 0]\n",
      "episode 4068, reward 1055.0, memory_length 2000, epsilon 0.017059707927916702, time 726.0, rides 129\n",
      "Initial State is  [2, 22, 0]\n",
      "episode 4069, reward 1175.0, memory_length 2000, epsilon 0.017042648219988785, time 729.0, rides 139\n",
      "Initial State is  [2, 2, 6]\n",
      "episode 4070, reward 1167.0, memory_length 2000, epsilon 0.017025605571768795, time 729.0, rides 131\n",
      "Initial State is  [4, 2, 2]\n",
      "episode 4071, reward 895.0, memory_length 2000, epsilon 0.017008579966197025, time 732.0, rides 129\n",
      "Initial State is  [3, 2, 6]\n",
      "episode 4072, reward 952.0, memory_length 2000, epsilon 0.01699157138623083, time 723.0, rides 139\n",
      "Initial State is  [0, 17, 1]\n",
      "episode 4073, reward 978.0, memory_length 2000, epsilon 0.016974579814844598, time 729.0, rides 125\n",
      "Initial State is  [2, 9, 4]\n",
      "episode 4074, reward 1106.0, memory_length 2000, epsilon 0.016957605235029753, time 727.0, rides 137\n",
      "Initial State is  [0, 19, 2]\n",
      "episode 4075, reward 799.0, memory_length 2000, epsilon 0.016940647629794723, time 729.0, rides 134\n",
      "Initial State is  [0, 10, 1]\n",
      "episode 4076, reward 841.0, memory_length 2000, epsilon 0.01692370698216493, time 732.0, rides 129\n",
      "Initial State is  [1, 15, 2]\n",
      "episode 4077, reward 903.0, memory_length 2000, epsilon 0.016906783275182764, time 729.0, rides 116\n",
      "Initial State is  [1, 23, 4]\n",
      "episode 4078, reward 1058.0, memory_length 2000, epsilon 0.016889876491907582, time 730.0, rides 125\n",
      "Initial State is  [4, 12, 4]\n",
      "episode 4079, reward 934.0, memory_length 2000, epsilon 0.016872986615415676, time 726.0, rides 127\n",
      "Initial State is  [4, 7, 3]\n",
      "episode 4080, reward 897.0, memory_length 2000, epsilon 0.01685611362880026, time 732.0, rides 130\n",
      "Initial State is  [1, 4, 5]\n",
      "episode 4081, reward 723.0, memory_length 2000, epsilon 0.01683925751517146, time 733.0, rides 125\n",
      "Initial State is  [1, 14, 0]\n",
      "episode 4082, reward 1028.0, memory_length 2000, epsilon 0.01682241825765629, time 722.0, rides 141\n",
      "Initial State is  [4, 13, 6]\n",
      "episode 4083, reward 864.0, memory_length 2000, epsilon 0.016805595839398633, time 728.0, rides 134\n",
      "Initial State is  [4, 5, 4]\n",
      "episode 4084, reward 1031.0, memory_length 2000, epsilon 0.016788790243559233, time 731.0, rides 131\n",
      "Initial State is  [3, 6, 4]\n",
      "episode 4085, reward 857.0, memory_length 2000, epsilon 0.016772001453315675, time 727.0, rides 125\n",
      "Initial State is  [0, 16, 1]\n",
      "episode 4086, reward 784.0, memory_length 2000, epsilon 0.01675522945186236, time 721.0, rides 133\n",
      "Initial State is  [2, 14, 3]\n",
      "episode 4087, reward 876.0, memory_length 2000, epsilon 0.016738474222410496, time 734.0, rides 126\n",
      "Initial State is  [0, 7, 3]\n",
      "episode 4088, reward 1147.0, memory_length 2000, epsilon 0.016721735748188086, time 725.0, rides 132\n",
      "Initial State is  [3, 2, 0]\n",
      "episode 4089, reward 1067.0, memory_length 2000, epsilon 0.016705014012439897, time 729.0, rides 145\n",
      "Initial State is  [4, 10, 3]\n",
      "episode 4090, reward 942.0, memory_length 2000, epsilon 0.016688308998427458, time 722.0, rides 134\n",
      "Initial State is  [0, 15, 6]\n",
      "episode 4091, reward 985.0, memory_length 2000, epsilon 0.01667162068942903, time 732.0, rides 120\n",
      "Initial State is  [0, 23, 0]\n",
      "episode 4092, reward 1021.0, memory_length 2000, epsilon 0.0166549490687396, time 726.0, rides 123\n",
      "Initial State is  [0, 14, 2]\n",
      "episode 4093, reward 770.0, memory_length 2000, epsilon 0.01663829411967086, time 728.0, rides 132\n",
      "Initial State is  [2, 21, 4]\n",
      "episode 4094, reward 702.0, memory_length 2000, epsilon 0.01662165582555119, time 731.0, rides 145\n",
      "Initial State is  [0, 22, 4]\n",
      "episode 4095, reward 649.0, memory_length 2000, epsilon 0.016605034169725637, time 730.0, rides 128\n",
      "Initial State is  [3, 14, 6]\n",
      "episode 4096, reward 871.0, memory_length 2000, epsilon 0.016588429135555912, time 731.0, rides 142\n",
      "Initial State is  [3, 0, 2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 4097, reward 966.0, memory_length 2000, epsilon 0.016571840706420357, time 734.0, rides 129\n",
      "Initial State is  [1, 10, 5]\n",
      "episode 4098, reward 882.0, memory_length 2000, epsilon 0.016555268865713936, time 732.0, rides 119\n",
      "Initial State is  [4, 5, 4]\n",
      "episode 4099, reward 825.0, memory_length 2000, epsilon 0.016538713596848224, time 723.0, rides 142\n",
      "Initial State is  [2, 0, 3]\n",
      "episode 4100, reward 755.0, memory_length 2000, epsilon 0.016522174883251375, time 734.0, rides 142\n",
      "Initial State is  [1, 15, 4]\n",
      "episode 4101, reward 924.0, memory_length 2000, epsilon 0.016505652708368124, time 727.0, rides 133\n",
      "Initial State is  [2, 23, 2]\n",
      "episode 4102, reward 1018.0, memory_length 2000, epsilon 0.016489147055659757, time 724.0, rides 137\n",
      "Initial State is  [0, 13, 5]\n",
      "episode 4103, reward 898.0, memory_length 2000, epsilon 0.016472657908604096, time 734.0, rides 125\n",
      "Initial State is  [4, 21, 0]\n",
      "episode 4104, reward 616.0, memory_length 2000, epsilon 0.016456185250695494, time 731.0, rides 118\n",
      "Initial State is  [0, 1, 3]\n",
      "episode 4105, reward 812.0, memory_length 2000, epsilon 0.0164397290654448, time 739.0, rides 127\n",
      "Initial State is  [3, 0, 2]\n",
      "episode 4106, reward 894.0, memory_length 2000, epsilon 0.016423289336379356, time 733.0, rides 118\n",
      "Initial State is  [4, 0, 1]\n",
      "episode 4107, reward 1023.0, memory_length 2000, epsilon 0.016406866047042976, time 730.0, rides 133\n",
      "Initial State is  [3, 17, 3]\n",
      "episode 4108, reward 906.0, memory_length 2000, epsilon 0.016390459180995933, time 740.0, rides 142\n",
      "Initial State is  [0, 19, 3]\n",
      "episode 4109, reward 776.0, memory_length 2000, epsilon 0.016374068721814936, time 727.0, rides 134\n",
      "Initial State is  [3, 0, 4]\n",
      "episode 4110, reward 930.0, memory_length 2000, epsilon 0.01635769465309312, time 727.0, rides 133\n",
      "Initial State is  [1, 9, 0]\n",
      "episode 4111, reward 830.0, memory_length 2000, epsilon 0.016341336958440027, time 735.0, rides 149\n",
      "Initial State is  [1, 9, 0]\n",
      "episode 4112, reward 809.0, memory_length 2000, epsilon 0.016324995621481587, time 726.0, rides 137\n",
      "Initial State is  [0, 10, 4]\n",
      "episode 4113, reward 757.0, memory_length 2000, epsilon 0.016308670625860104, time 732.0, rides 126\n",
      "Initial State is  [2, 15, 4]\n",
      "episode 4114, reward 797.0, memory_length 2000, epsilon 0.016292361955234244, time 726.0, rides 129\n",
      "Initial State is  [2, 16, 1]\n",
      "episode 4115, reward 958.0, memory_length 2000, epsilon 0.01627606959327901, time 727.0, rides 133\n",
      "Initial State is  [4, 23, 6]\n",
      "episode 4116, reward 560.0, memory_length 2000, epsilon 0.016259793523685732, time 728.0, rides 139\n",
      "Initial State is  [1, 5, 6]\n",
      "episode 4117, reward 860.0, memory_length 2000, epsilon 0.016243533730162046, time 736.0, rides 142\n",
      "Initial State is  [3, 13, 2]\n",
      "episode 4118, reward 1209.0, memory_length 2000, epsilon 0.016227290196431883, time 731.0, rides 123\n",
      "Initial State is  [3, 5, 5]\n",
      "episode 4119, reward 857.0, memory_length 2000, epsilon 0.01621106290623545, time 733.0, rides 150\n",
      "Initial State is  [3, 10, 5]\n",
      "episode 4120, reward 1168.0, memory_length 2000, epsilon 0.016194851843329215, time 723.0, rides 136\n",
      "Initial State is  [2, 2, 6]\n",
      "episode 4121, reward 1051.0, memory_length 2000, epsilon 0.016178656991485886, time 725.0, rides 133\n",
      "Initial State is  [0, 11, 1]\n",
      "episode 4122, reward 1180.0, memory_length 2000, epsilon 0.0161624783344944, time 724.0, rides 143\n",
      "Initial State is  [4, 16, 3]\n",
      "episode 4123, reward 753.0, memory_length 2000, epsilon 0.016146315856159905, time 728.0, rides 128\n",
      "Initial State is  [4, 4, 0]\n",
      "episode 4124, reward 989.0, memory_length 2000, epsilon 0.016130169540303746, time 727.0, rides 142\n",
      "Initial State is  [3, 3, 4]\n",
      "episode 4125, reward 668.0, memory_length 2000, epsilon 0.01611403937076344, time 732.0, rides 128\n",
      "Initial State is  [3, 20, 5]\n",
      "episode 4126, reward 1052.0, memory_length 2000, epsilon 0.016097925331392676, time 724.0, rides 127\n",
      "Initial State is  [4, 23, 1]\n",
      "episode 4127, reward 907.0, memory_length 2000, epsilon 0.016081827406061285, time 728.0, rides 124\n",
      "Initial State is  [0, 11, 1]\n",
      "episode 4128, reward 586.0, memory_length 2000, epsilon 0.016065745578655224, time 730.0, rides 125\n",
      "Initial State is  [3, 18, 2]\n",
      "episode 4129, reward 1076.0, memory_length 2000, epsilon 0.01604967983307657, time 724.0, rides 147\n",
      "Initial State is  [2, 0, 6]\n",
      "episode 4130, reward 926.0, memory_length 2000, epsilon 0.016033630153243494, time 739.0, rides 131\n",
      "Initial State is  [4, 16, 6]\n",
      "episode 4131, reward 894.0, memory_length 2000, epsilon 0.01601759652309025, time 731.0, rides 132\n",
      "Initial State is  [1, 11, 6]\n",
      "episode 4132, reward 922.0, memory_length 2000, epsilon 0.01600157892656716, time 723.0, rides 146\n",
      "Initial State is  [3, 8, 5]\n",
      "episode 4133, reward 1221.0, memory_length 2000, epsilon 0.01598557734764059, time 724.0, rides 141\n",
      "Initial State is  [0, 23, 2]\n",
      "episode 4134, reward 747.0, memory_length 2000, epsilon 0.01596959177029295, time 728.0, rides 143\n",
      "Initial State is  [1, 16, 6]\n",
      "episode 4135, reward 898.0, memory_length 2000, epsilon 0.015953622178522656, time 724.0, rides 132\n",
      "Initial State is  [4, 10, 5]\n",
      "episode 4136, reward 863.0, memory_length 2000, epsilon 0.015937668556344133, time 722.0, rides 147\n",
      "Initial State is  [3, 2, 4]\n",
      "episode 4137, reward 965.0, memory_length 2000, epsilon 0.01592173088778779, time 732.0, rides 131\n",
      "Initial State is  [1, 6, 5]\n",
      "episode 4138, reward 931.0, memory_length 2000, epsilon 0.015905809156900003, time 726.0, rides 140\n",
      "Initial State is  [4, 13, 6]\n",
      "episode 4139, reward 589.0, memory_length 2000, epsilon 0.015889903347743102, time 732.0, rides 124\n",
      "Initial State is  [4, 7, 5]\n",
      "episode 4140, reward 987.0, memory_length 2000, epsilon 0.01587401344439536, time 722.0, rides 119\n",
      "Initial State is  [3, 9, 1]\n",
      "episode 4141, reward 757.0, memory_length 2000, epsilon 0.015858139430950966, time 725.0, rides 132\n",
      "Initial State is  [3, 1, 1]\n",
      "episode 4142, reward 849.0, memory_length 2000, epsilon 0.015842281291520016, time 730.0, rides 131\n",
      "Initial State is  [3, 16, 4]\n",
      "episode 4143, reward 938.0, memory_length 2000, epsilon 0.015826439010228496, time 728.0, rides 143\n",
      "Initial State is  [1, 13, 5]\n",
      "episode 4144, reward 990.0, memory_length 2000, epsilon 0.01581061257121827, time 731.0, rides 128\n",
      "Initial State is  [3, 15, 2]\n",
      "episode 4145, reward 901.0, memory_length 2000, epsilon 0.01579480195864705, time 737.0, rides 134\n",
      "Initial State is  [0, 12, 0]\n",
      "episode 4146, reward 988.0, memory_length 2000, epsilon 0.015779007156688403, time 724.0, rides 124\n",
      "Initial State is  [4, 2, 3]\n",
      "episode 4147, reward 1037.0, memory_length 2000, epsilon 0.015763228149531714, time 727.0, rides 134\n",
      "Initial State is  [1, 23, 1]\n",
      "episode 4148, reward 975.0, memory_length 2000, epsilon 0.01574746492138218, time 730.0, rides 135\n",
      "Initial State is  [0, 10, 1]\n",
      "episode 4149, reward 1012.0, memory_length 2000, epsilon 0.0157317174564608, time 730.0, rides 130\n",
      "Initial State is  [3, 9, 6]\n",
      "episode 4150, reward 762.0, memory_length 2000, epsilon 0.01571598573900434, time 733.0, rides 132\n",
      "Initial State is  [1, 6, 0]\n",
      "episode 4151, reward 827.0, memory_length 2000, epsilon 0.015700269753265335, time 736.0, rides 126\n",
      "Initial State is  [1, 13, 6]\n",
      "episode 4152, reward 804.0, memory_length 2000, epsilon 0.01568456948351207, time 723.0, rides 122\n",
      "Initial State is  [3, 19, 1]\n",
      "episode 4153, reward 857.0, memory_length 2000, epsilon 0.01566888491402856, time 730.0, rides 132\n",
      "Initial State is  [2, 9, 3]\n",
      "episode 4154, reward 860.0, memory_length 2000, epsilon 0.01565321602911453, time 728.0, rides 130\n",
      "Initial State is  [1, 2, 3]\n",
      "episode 4155, reward 844.0, memory_length 2000, epsilon 0.015637562813085416, time 724.0, rides 133\n",
      "Initial State is  [4, 6, 6]\n",
      "episode 4156, reward 670.0, memory_length 2000, epsilon 0.01562192525027233, time 731.0, rides 144\n",
      "Initial State is  [0, 6, 3]\n",
      "episode 4157, reward 977.0, memory_length 2000, epsilon 0.015606303325022058, time 726.0, rides 129\n",
      "Initial State is  [2, 20, 2]\n",
      "episode 4158, reward 788.0, memory_length 2000, epsilon 0.015590697021697035, time 731.0, rides 122\n",
      "Initial State is  [0, 6, 6]\n",
      "episode 4159, reward 907.0, memory_length 2000, epsilon 0.015575106324675339, time 731.0, rides 134\n",
      "Initial State is  [1, 16, 5]\n",
      "episode 4160, reward 969.0, memory_length 2000, epsilon 0.015559531218350664, time 741.0, rides 125\n",
      "Initial State is  [3, 12, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 4161, reward 1081.0, memory_length 2000, epsilon 0.015543971687132313, time 726.0, rides 140\n",
      "Initial State is  [4, 17, 3]\n",
      "episode 4162, reward 988.0, memory_length 2000, epsilon 0.015528427715445181, time 729.0, rides 129\n",
      "Initial State is  [0, 12, 0]\n",
      "episode 4163, reward 1180.0, memory_length 2000, epsilon 0.015512899287729736, time 732.0, rides 141\n",
      "Initial State is  [4, 18, 2]\n",
      "episode 4164, reward 775.0, memory_length 2000, epsilon 0.015497386388442006, time 727.0, rides 134\n",
      "Initial State is  [4, 18, 0]\n",
      "episode 4165, reward 796.0, memory_length 2000, epsilon 0.015481889002053564, time 727.0, rides 135\n",
      "Initial State is  [2, 23, 1]\n",
      "episode 4166, reward 943.0, memory_length 2000, epsilon 0.01546640711305151, time 733.0, rides 140\n",
      "Initial State is  [3, 18, 3]\n",
      "episode 4167, reward 1104.0, memory_length 2000, epsilon 0.015450940705938459, time 731.0, rides 133\n",
      "Initial State is  [3, 4, 3]\n",
      "episode 4168, reward 1042.0, memory_length 2000, epsilon 0.01543548976523252, time 737.0, rides 126\n",
      "Initial State is  [1, 6, 5]\n",
      "episode 4169, reward 938.0, memory_length 2000, epsilon 0.015420054275467287, time 728.0, rides 135\n",
      "Initial State is  [4, 16, 3]\n",
      "episode 4170, reward 1101.0, memory_length 2000, epsilon 0.015404634221191819, time 730.0, rides 141\n",
      "Initial State is  [0, 1, 0]\n",
      "episode 4171, reward 906.0, memory_length 2000, epsilon 0.015389229586970627, time 723.0, rides 138\n",
      "Initial State is  [2, 0, 0]\n",
      "episode 4172, reward 970.0, memory_length 2000, epsilon 0.015373840357383657, time 732.0, rides 132\n",
      "Initial State is  [2, 14, 3]\n",
      "episode 4173, reward 921.0, memory_length 2000, epsilon 0.015358466517026274, time 730.0, rides 131\n",
      "Initial State is  [1, 2, 5]\n",
      "episode 4174, reward 990.0, memory_length 2000, epsilon 0.015343108050509247, time 730.0, rides 139\n",
      "Initial State is  [1, 4, 3]\n",
      "episode 4175, reward 1053.0, memory_length 2000, epsilon 0.015327764942458738, time 736.0, rides 126\n",
      "Initial State is  [4, 12, 2]\n",
      "episode 4176, reward 1099.0, memory_length 2000, epsilon 0.01531243717751628, time 724.0, rides 135\n",
      "Initial State is  [0, 7, 4]\n",
      "episode 4177, reward 797.0, memory_length 2000, epsilon 0.015297124740338763, time 724.0, rides 131\n",
      "Initial State is  [0, 10, 4]\n",
      "episode 4178, reward 643.0, memory_length 2000, epsilon 0.015281827615598424, time 723.0, rides 134\n",
      "Initial State is  [1, 11, 6]\n",
      "episode 4179, reward 1137.0, memory_length 2000, epsilon 0.015266545787982826, time 730.0, rides 141\n",
      "Initial State is  [3, 1, 5]\n",
      "episode 4180, reward 1241.0, memory_length 2000, epsilon 0.015251279242194844, time 732.0, rides 136\n",
      "Initial State is  [0, 11, 3]\n",
      "episode 4181, reward 1022.0, memory_length 2000, epsilon 0.015236027962952649, time 728.0, rides 141\n",
      "Initial State is  [3, 22, 1]\n",
      "episode 4182, reward 967.0, memory_length 2000, epsilon 0.015220791934989696, time 723.0, rides 129\n",
      "Initial State is  [4, 22, 2]\n",
      "episode 4183, reward 1058.0, memory_length 2000, epsilon 0.015205571143054706, time 725.0, rides 132\n",
      "Initial State is  [3, 12, 2]\n",
      "episode 4184, reward 1194.0, memory_length 2000, epsilon 0.01519036557191165, time 731.0, rides 140\n",
      "Initial State is  [1, 11, 2]\n",
      "episode 4185, reward 949.0, memory_length 2000, epsilon 0.01517517520633974, time 729.0, rides 119\n",
      "Initial State is  [0, 4, 6]\n",
      "episode 4186, reward 908.0, memory_length 2000, epsilon 0.0151600000311334, time 730.0, rides 128\n",
      "Initial State is  [2, 14, 3]\n",
      "episode 4187, reward 1046.0, memory_length 2000, epsilon 0.015144840031102266, time 728.0, rides 128\n",
      "Initial State is  [2, 2, 2]\n",
      "episode 4188, reward 1036.0, memory_length 2000, epsilon 0.015129695191071164, time 723.0, rides 134\n",
      "Initial State is  [4, 9, 0]\n",
      "episode 4189, reward 944.0, memory_length 2000, epsilon 0.015114565495880094, time 727.0, rides 135\n",
      "Initial State is  [2, 18, 1]\n",
      "episode 4190, reward 1023.0, memory_length 2000, epsilon 0.015099450930384214, time 727.0, rides 127\n",
      "Initial State is  [1, 6, 1]\n",
      "episode 4191, reward 647.0, memory_length 2000, epsilon 0.01508435147945383, time 725.0, rides 138\n",
      "Initial State is  [1, 12, 6]\n",
      "episode 4192, reward 797.0, memory_length 2000, epsilon 0.015069267127974377, time 725.0, rides 124\n",
      "Initial State is  [4, 12, 3]\n",
      "episode 4193, reward 706.0, memory_length 2000, epsilon 0.015054197860846403, time 727.0, rides 130\n",
      "Initial State is  [4, 6, 2]\n",
      "episode 4194, reward 855.0, memory_length 2000, epsilon 0.015039143662985557, time 728.0, rides 133\n",
      "Initial State is  [2, 11, 1]\n",
      "episode 4195, reward 882.0, memory_length 2000, epsilon 0.015024104519322571, time 729.0, rides 133\n",
      "Initial State is  [3, 5, 4]\n",
      "episode 4196, reward 760.0, memory_length 2000, epsilon 0.015009080414803248, time 729.0, rides 149\n",
      "Initial State is  [4, 17, 4]\n",
      "episode 4197, reward 848.0, memory_length 2000, epsilon 0.014994071334388445, time 738.0, rides 146\n",
      "Initial State is  [0, 21, 3]\n",
      "episode 4198, reward 1098.0, memory_length 2000, epsilon 0.014979077263054056, time 730.0, rides 136\n",
      "Initial State is  [0, 7, 3]\n",
      "episode 4199, reward 1005.0, memory_length 2000, epsilon 0.014964098185791003, time 731.0, rides 126\n",
      "Initial State is  [0, 11, 6]\n",
      "episode 4200, reward 921.0, memory_length 2000, epsilon 0.014949134087605212, time 724.0, rides 144\n",
      "Initial State is  [2, 19, 2]\n",
      "episode 4201, reward 970.0, memory_length 2000, epsilon 0.014934184953517607, time 725.0, rides 139\n",
      "Initial State is  [2, 21, 6]\n",
      "episode 4202, reward 758.0, memory_length 2000, epsilon 0.01491925076856409, time 736.0, rides 129\n",
      "Initial State is  [0, 21, 2]\n",
      "episode 4203, reward 766.0, memory_length 2000, epsilon 0.014904331517795525, time 737.0, rides 125\n",
      "Initial State is  [2, 8, 6]\n",
      "episode 4204, reward 579.0, memory_length 2000, epsilon 0.01488942718627773, time 725.0, rides 115\n",
      "Initial State is  [4, 1, 5]\n",
      "episode 4205, reward 966.0, memory_length 2000, epsilon 0.014874537759091451, time 727.0, rides 134\n",
      "Initial State is  [3, 1, 2]\n",
      "episode 4206, reward 797.0, memory_length 2000, epsilon 0.014859663221332359, time 723.0, rides 128\n",
      "Initial State is  [4, 8, 1]\n",
      "episode 4207, reward 1012.0, memory_length 2000, epsilon 0.014844803558111026, time 731.0, rides 129\n",
      "Initial State is  [4, 21, 3]\n",
      "episode 4208, reward 936.0, memory_length 2000, epsilon 0.014829958754552916, time 729.0, rides 127\n",
      "Initial State is  [3, 11, 4]\n",
      "episode 4209, reward 729.0, memory_length 2000, epsilon 0.014815128795798363, time 735.0, rides 129\n",
      "Initial State is  [0, 7, 5]\n",
      "episode 4210, reward 875.0, memory_length 2000, epsilon 0.014800313667002565, time 729.0, rides 136\n",
      "Initial State is  [2, 10, 2]\n",
      "episode 4211, reward 886.0, memory_length 2000, epsilon 0.014785513353335562, time 727.0, rides 138\n",
      "Initial State is  [4, 2, 3]\n",
      "episode 4212, reward 1140.0, memory_length 2000, epsilon 0.014770727839982227, time 728.0, rides 136\n",
      "Initial State is  [1, 11, 0]\n",
      "episode 4213, reward 1067.0, memory_length 2000, epsilon 0.014755957112142245, time 727.0, rides 132\n",
      "Initial State is  [3, 4, 3]\n",
      "episode 4214, reward 891.0, memory_length 2000, epsilon 0.014741201155030102, time 728.0, rides 133\n",
      "Initial State is  [4, 5, 5]\n",
      "episode 4215, reward 1078.0, memory_length 2000, epsilon 0.014726459953875072, time 725.0, rides 135\n",
      "Initial State is  [4, 23, 0]\n",
      "episode 4216, reward 749.0, memory_length 2000, epsilon 0.014711733493921197, time 732.0, rides 140\n",
      "Initial State is  [1, 18, 2]\n",
      "episode 4217, reward 719.0, memory_length 2000, epsilon 0.014697021760427276, time 727.0, rides 130\n",
      "Initial State is  [4, 14, 4]\n",
      "episode 4218, reward 1002.0, memory_length 2000, epsilon 0.014682324738666848, time 728.0, rides 125\n",
      "Initial State is  [1, 6, 3]\n",
      "episode 4219, reward 876.0, memory_length 2000, epsilon 0.014667642413928181, time 724.0, rides 130\n",
      "Initial State is  [3, 14, 1]\n",
      "episode 4220, reward 1062.0, memory_length 2000, epsilon 0.014652974771514253, time 732.0, rides 127\n",
      "Initial State is  [2, 1, 1]\n",
      "episode 4221, reward 826.0, memory_length 2000, epsilon 0.014638321796742739, time 732.0, rides 137\n",
      "Initial State is  [1, 12, 2]\n",
      "episode 4222, reward 870.0, memory_length 2000, epsilon 0.014623683474945996, time 727.0, rides 146\n",
      "Initial State is  [0, 22, 0]\n",
      "episode 4223, reward 622.0, memory_length 2000, epsilon 0.014609059791471049, time 733.0, rides 119\n",
      "Initial State is  [0, 16, 1]\n",
      "episode 4224, reward 556.0, memory_length 2000, epsilon 0.014594450731679578, time 730.0, rides 119\n",
      "Initial State is  [0, 15, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 4225, reward 1022.0, memory_length 2000, epsilon 0.014579856280947899, time 740.0, rides 145\n",
      "Initial State is  [3, 9, 1]\n",
      "episode 4226, reward 681.0, memory_length 2000, epsilon 0.014565276424666951, time 726.0, rides 139\n",
      "Initial State is  [0, 0, 5]\n",
      "episode 4227, reward 724.0, memory_length 2000, epsilon 0.014550711148242284, time 724.0, rides 131\n",
      "Initial State is  [0, 1, 2]\n",
      "episode 4228, reward 873.0, memory_length 2000, epsilon 0.014536160437094041, time 732.0, rides 126\n",
      "Initial State is  [2, 1, 2]\n",
      "episode 4229, reward 813.0, memory_length 2000, epsilon 0.014521624276656948, time 727.0, rides 137\n",
      "Initial State is  [3, 16, 6]\n",
      "episode 4230, reward 936.0, memory_length 2000, epsilon 0.014507102652380291, time 733.0, rides 146\n",
      "Initial State is  [3, 20, 6]\n",
      "episode 4231, reward 937.0, memory_length 2000, epsilon 0.014492595549727911, time 736.0, rides 129\n",
      "Initial State is  [3, 10, 6]\n",
      "episode 4232, reward 962.0, memory_length 2000, epsilon 0.014478102954178184, time 727.0, rides 131\n",
      "Initial State is  [2, 11, 6]\n",
      "episode 4233, reward 669.0, memory_length 2000, epsilon 0.014463624851224006, time 727.0, rides 137\n",
      "Initial State is  [2, 4, 6]\n",
      "episode 4234, reward 815.0, memory_length 2000, epsilon 0.014449161226372782, time 728.0, rides 127\n",
      "Initial State is  [2, 9, 1]\n",
      "episode 4235, reward 955.0, memory_length 2000, epsilon 0.01443471206514641, time 729.0, rides 135\n",
      "Initial State is  [2, 19, 2]\n",
      "episode 4236, reward 602.0, memory_length 2000, epsilon 0.014420277353081263, time 727.0, rides 138\n",
      "Initial State is  [1, 11, 3]\n",
      "episode 4237, reward 671.0, memory_length 2000, epsilon 0.014405857075728182, time 724.0, rides 130\n",
      "Initial State is  [4, 1, 5]\n",
      "episode 4238, reward 867.0, memory_length 2000, epsilon 0.014391451218652453, time 724.0, rides 147\n",
      "Initial State is  [3, 3, 3]\n",
      "episode 4239, reward 1018.0, memory_length 2000, epsilon 0.014377059767433802, time 725.0, rides 121\n",
      "Initial State is  [1, 4, 1]\n",
      "episode 4240, reward 1140.0, memory_length 2000, epsilon 0.014362682707666368, time 728.0, rides 141\n",
      "Initial State is  [3, 1, 6]\n",
      "episode 4241, reward 713.0, memory_length 2000, epsilon 0.014348320024958701, time 730.0, rides 130\n",
      "Initial State is  [3, 16, 1]\n",
      "episode 4242, reward 883.0, memory_length 2000, epsilon 0.014333971704933742, time 726.0, rides 127\n",
      "Initial State is  [1, 8, 3]\n",
      "episode 4243, reward 744.0, memory_length 2000, epsilon 0.014319637733228809, time 731.0, rides 132\n",
      "Initial State is  [0, 21, 6]\n",
      "episode 4244, reward 791.0, memory_length 2000, epsilon 0.01430531809549558, time 735.0, rides 127\n",
      "Initial State is  [2, 5, 6]\n",
      "episode 4245, reward 1051.0, memory_length 2000, epsilon 0.014291012777400084, time 730.0, rides 137\n",
      "Initial State is  [2, 17, 6]\n",
      "episode 4246, reward 692.0, memory_length 2000, epsilon 0.014276721764622684, time 730.0, rides 135\n",
      "Initial State is  [0, 2, 1]\n",
      "episode 4247, reward 943.0, memory_length 2000, epsilon 0.014262445042858061, time 731.0, rides 145\n",
      "Initial State is  [3, 1, 4]\n",
      "episode 4248, reward 736.0, memory_length 2000, epsilon 0.014248182597815203, time 732.0, rides 137\n",
      "Initial State is  [4, 0, 2]\n",
      "episode 4249, reward 928.0, memory_length 2000, epsilon 0.014233934415217388, time 727.0, rides 135\n",
      "Initial State is  [4, 3, 1]\n",
      "episode 4250, reward 748.0, memory_length 2000, epsilon 0.01421970048080217, time 722.0, rides 136\n",
      "Initial State is  [1, 1, 6]\n",
      "episode 4251, reward 751.0, memory_length 2000, epsilon 0.014205480780321368, time 729.0, rides 138\n",
      "Initial State is  [0, 16, 5]\n",
      "episode 4252, reward 877.0, memory_length 2000, epsilon 0.014191275299541046, time 729.0, rides 143\n",
      "Initial State is  [1, 1, 6]\n",
      "episode 4253, reward 766.0, memory_length 2000, epsilon 0.014177084024241506, time 731.0, rides 143\n",
      "Initial State is  [0, 9, 5]\n",
      "episode 4254, reward 770.0, memory_length 2000, epsilon 0.014162906940217265, time 726.0, rides 136\n",
      "Initial State is  [4, 9, 5]\n",
      "episode 4255, reward 333.0, memory_length 2000, epsilon 0.014148744033277047, time 724.0, rides 135\n",
      "Initial State is  [4, 5, 6]\n",
      "episode 4256, reward 870.0, memory_length 2000, epsilon 0.014134595289243771, time 737.0, rides 133\n",
      "Initial State is  [2, 1, 5]\n",
      "episode 4257, reward 655.0, memory_length 2000, epsilon 0.014120460693954527, time 734.0, rides 134\n",
      "Initial State is  [3, 13, 0]\n",
      "episode 4258, reward 922.0, memory_length 2000, epsilon 0.014106340233260573, time 729.0, rides 119\n",
      "Initial State is  [2, 6, 4]\n",
      "episode 4259, reward 754.0, memory_length 2000, epsilon 0.014092233893027313, time 723.0, rides 139\n",
      "Initial State is  [0, 10, 6]\n",
      "episode 4260, reward 885.0, memory_length 2000, epsilon 0.014078141659134286, time 737.0, rides 130\n",
      "Initial State is  [3, 8, 1]\n",
      "episode 4261, reward 956.0, memory_length 2000, epsilon 0.014064063517475151, time 722.0, rides 146\n",
      "Initial State is  [1, 4, 1]\n",
      "episode 4262, reward 1289.0, memory_length 2000, epsilon 0.014049999453957676, time 722.0, rides 126\n",
      "Initial State is  [3, 23, 5]\n",
      "episode 4263, reward 950.0, memory_length 2000, epsilon 0.014035949454503718, time 726.0, rides 142\n",
      "Initial State is  [0, 11, 0]\n",
      "episode 4264, reward 1010.0, memory_length 2000, epsilon 0.014021913505049215, time 727.0, rides 136\n",
      "Initial State is  [4, 10, 5]\n",
      "episode 4265, reward 1067.0, memory_length 2000, epsilon 0.014007891591544166, time 726.0, rides 133\n",
      "Initial State is  [2, 4, 4]\n",
      "episode 4266, reward 1098.0, memory_length 2000, epsilon 0.013993883699952622, time 727.0, rides 131\n",
      "Initial State is  [3, 7, 2]\n",
      "episode 4267, reward 802.0, memory_length 2000, epsilon 0.01397988981625267, time 733.0, rides 120\n",
      "Initial State is  [2, 4, 6]\n",
      "episode 4268, reward 916.0, memory_length 2000, epsilon 0.013965909926436417, time 722.0, rides 120\n",
      "Initial State is  [3, 12, 5]\n",
      "episode 4269, reward 1041.0, memory_length 2000, epsilon 0.013951944016509982, time 727.0, rides 136\n",
      "Initial State is  [1, 0, 0]\n",
      "episode 4270, reward 883.0, memory_length 2000, epsilon 0.013937992072493471, time 730.0, rides 132\n",
      "Initial State is  [0, 0, 0]\n",
      "episode 4271, reward 935.0, memory_length 2000, epsilon 0.013924054080420977, time 730.0, rides 129\n",
      "Initial State is  [1, 6, 6]\n",
      "episode 4272, reward 797.0, memory_length 2000, epsilon 0.013910130026340556, time 730.0, rides 141\n",
      "Initial State is  [3, 12, 2]\n",
      "episode 4273, reward 1014.0, memory_length 2000, epsilon 0.013896219896314215, time 731.0, rides 126\n",
      "Initial State is  [3, 8, 1]\n",
      "episode 4274, reward 665.0, memory_length 2000, epsilon 0.013882323676417901, time 732.0, rides 129\n",
      "Initial State is  [3, 17, 1]\n",
      "episode 4275, reward 560.0, memory_length 2000, epsilon 0.013868441352741484, time 730.0, rides 128\n",
      "Initial State is  [0, 17, 6]\n",
      "episode 4276, reward 700.0, memory_length 2000, epsilon 0.013854572911388743, time 725.0, rides 143\n",
      "Initial State is  [3, 21, 2]\n",
      "episode 4277, reward 861.0, memory_length 2000, epsilon 0.013840718338477354, time 734.0, rides 126\n",
      "Initial State is  [0, 15, 6]\n",
      "episode 4278, reward 891.0, memory_length 2000, epsilon 0.013826877620138877, time 726.0, rides 151\n",
      "Initial State is  [2, 15, 2]\n",
      "episode 4279, reward 704.0, memory_length 2000, epsilon 0.013813050742518738, time 728.0, rides 134\n",
      "Initial State is  [3, 13, 5]\n",
      "episode 4280, reward 841.0, memory_length 2000, epsilon 0.013799237691776219, time 730.0, rides 118\n",
      "Initial State is  [3, 8, 3]\n",
      "episode 4281, reward 793.0, memory_length 2000, epsilon 0.013785438454084442, time 731.0, rides 126\n",
      "Initial State is  [4, 21, 6]\n",
      "episode 4282, reward 1032.0, memory_length 2000, epsilon 0.013771653015630358, time 727.0, rides 129\n",
      "Initial State is  [2, 18, 6]\n",
      "episode 4283, reward 1153.0, memory_length 2000, epsilon 0.013757881362614728, time 725.0, rides 134\n",
      "Initial State is  [2, 14, 2]\n",
      "episode 4284, reward 711.0, memory_length 2000, epsilon 0.013744123481252113, time 722.0, rides 132\n",
      "Initial State is  [4, 10, 4]\n",
      "episode 4285, reward 902.0, memory_length 2000, epsilon 0.01373037935777086, time 726.0, rides 127\n",
      "Initial State is  [0, 8, 3]\n",
      "episode 4286, reward 938.0, memory_length 2000, epsilon 0.01371664897841309, time 728.0, rides 144\n",
      "Initial State is  [3, 13, 3]\n",
      "episode 4287, reward 610.0, memory_length 2000, epsilon 0.013702932329434677, time 726.0, rides 131\n",
      "Initial State is  [4, 21, 5]\n",
      "episode 4288, reward 910.0, memory_length 2000, epsilon 0.013689229397105242, time 736.0, rides 138\n",
      "Initial State is  [0, 9, 6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 4289, reward 623.0, memory_length 2000, epsilon 0.013675540167708137, time 729.0, rides 127\n",
      "Initial State is  [0, 6, 4]\n",
      "episode 4290, reward 940.0, memory_length 2000, epsilon 0.013661864627540429, time 724.0, rides 144\n",
      "Initial State is  [3, 18, 0]\n",
      "episode 4291, reward 504.0, memory_length 2000, epsilon 0.013648202762912889, time 726.0, rides 133\n",
      "Initial State is  [3, 7, 5]\n",
      "episode 4292, reward 1053.0, memory_length 2000, epsilon 0.013634554560149977, time 732.0, rides 138\n",
      "Initial State is  [1, 16, 2]\n",
      "episode 4293, reward 1078.0, memory_length 2000, epsilon 0.013620920005589827, time 740.0, rides 136\n",
      "Initial State is  [4, 8, 3]\n",
      "episode 4294, reward 967.0, memory_length 2000, epsilon 0.013607299085584237, time 723.0, rides 137\n",
      "Initial State is  [2, 23, 0]\n",
      "episode 4295, reward 701.0, memory_length 2000, epsilon 0.013593691786498652, time 729.0, rides 127\n",
      "Initial State is  [0, 3, 5]\n",
      "episode 4296, reward 964.0, memory_length 2000, epsilon 0.013580098094712154, time 725.0, rides 133\n",
      "Initial State is  [4, 8, 2]\n",
      "episode 4297, reward 1016.0, memory_length 2000, epsilon 0.013566517996617442, time 729.0, rides 136\n",
      "Initial State is  [2, 7, 5]\n",
      "episode 4298, reward 811.0, memory_length 2000, epsilon 0.013552951478620824, time 727.0, rides 133\n",
      "Initial State is  [3, 20, 2]\n",
      "episode 4299, reward 1171.0, memory_length 2000, epsilon 0.013539398527142203, time 730.0, rides 140\n",
      "Initial State is  [4, 0, 5]\n",
      "episode 4300, reward 714.0, memory_length 2000, epsilon 0.01352585912861506, time 731.0, rides 132\n",
      "Initial State is  [4, 13, 4]\n",
      "episode 4301, reward 1087.0, memory_length 2000, epsilon 0.013512333269486445, time 727.0, rides 130\n",
      "Initial State is  [1, 11, 6]\n",
      "episode 4302, reward 670.0, memory_length 2000, epsilon 0.013498820936216958, time 726.0, rides 143\n",
      "Initial State is  [1, 10, 6]\n",
      "episode 4303, reward 798.0, memory_length 2000, epsilon 0.01348532211528074, time 732.0, rides 131\n",
      "Initial State is  [3, 19, 2]\n",
      "episode 4304, reward 906.0, memory_length 2000, epsilon 0.01347183679316546, time 742.0, rides 131\n",
      "Initial State is  [4, 1, 4]\n",
      "episode 4305, reward 809.0, memory_length 2000, epsilon 0.013458364956372294, time 726.0, rides 133\n",
      "Initial State is  [1, 22, 2]\n",
      "episode 4306, reward 735.0, memory_length 2000, epsilon 0.013444906591415923, time 732.0, rides 133\n",
      "Initial State is  [1, 9, 4]\n",
      "episode 4307, reward 1038.0, memory_length 2000, epsilon 0.013431461684824507, time 725.0, rides 139\n",
      "Initial State is  [0, 16, 1]\n",
      "episode 4308, reward 859.0, memory_length 2000, epsilon 0.013418030223139681, time 727.0, rides 133\n",
      "Initial State is  [2, 7, 2]\n",
      "episode 4309, reward 990.0, memory_length 2000, epsilon 0.013404612192916542, time 728.0, rides 144\n",
      "Initial State is  [2, 10, 1]\n",
      "episode 4310, reward 1159.0, memory_length 2000, epsilon 0.013391207580723626, time 724.0, rides 130\n",
      "Initial State is  [3, 2, 5]\n",
      "episode 4311, reward 840.0, memory_length 2000, epsilon 0.013377816373142903, time 729.0, rides 141\n",
      "Initial State is  [3, 21, 4]\n",
      "episode 4312, reward 841.0, memory_length 2000, epsilon 0.01336443855676976, time 730.0, rides 141\n",
      "Initial State is  [0, 5, 2]\n",
      "episode 4313, reward 853.0, memory_length 2000, epsilon 0.01335107411821299, time 728.0, rides 133\n",
      "Initial State is  [1, 8, 4]\n",
      "episode 4314, reward 887.0, memory_length 2000, epsilon 0.013337723044094777, time 727.0, rides 138\n",
      "Initial State is  [4, 14, 6]\n",
      "episode 4315, reward 972.0, memory_length 2000, epsilon 0.013324385321050682, time 725.0, rides 145\n",
      "Initial State is  [2, 8, 3]\n",
      "episode 4316, reward 1039.0, memory_length 2000, epsilon 0.01331106093572963, time 728.0, rides 132\n",
      "Initial State is  [0, 5, 4]\n",
      "episode 4317, reward 1162.0, memory_length 2000, epsilon 0.013297749874793902, time 735.0, rides 129\n",
      "Initial State is  [1, 2, 0]\n",
      "episode 4318, reward 816.0, memory_length 2000, epsilon 0.013284452124919108, time 734.0, rides 134\n",
      "Initial State is  [1, 7, 4]\n",
      "episode 4319, reward 1145.0, memory_length 2000, epsilon 0.013271167672794189, time 732.0, rides 142\n",
      "Initial State is  [1, 17, 1]\n",
      "episode 4320, reward 993.0, memory_length 2000, epsilon 0.013257896505121394, time 725.0, rides 134\n",
      "Initial State is  [1, 13, 1]\n",
      "episode 4321, reward 971.0, memory_length 2000, epsilon 0.013244638608616273, time 730.0, rides 150\n",
      "Initial State is  [3, 18, 4]\n",
      "episode 4322, reward 1109.0, memory_length 2000, epsilon 0.013231393970007657, time 725.0, rides 141\n",
      "Initial State is  [3, 0, 3]\n",
      "episode 4323, reward 1118.0, memory_length 2000, epsilon 0.013218162576037648, time 725.0, rides 137\n",
      "Initial State is  [1, 23, 0]\n",
      "episode 4324, reward 904.0, memory_length 2000, epsilon 0.01320494441346161, time 723.0, rides 150\n",
      "Initial State is  [3, 13, 5]\n",
      "episode 4325, reward 919.0, memory_length 2000, epsilon 0.013191739469048148, time 730.0, rides 125\n",
      "Initial State is  [2, 13, 4]\n",
      "episode 4326, reward 947.0, memory_length 2000, epsilon 0.0131785477295791, time 723.0, rides 150\n",
      "Initial State is  [1, 13, 0]\n",
      "episode 4327, reward 871.0, memory_length 2000, epsilon 0.013165369181849521, time 726.0, rides 128\n",
      "Initial State is  [3, 16, 1]\n",
      "episode 4328, reward 821.0, memory_length 2000, epsilon 0.013152203812667672, time 731.0, rides 141\n",
      "Initial State is  [1, 13, 5]\n",
      "episode 4329, reward 891.0, memory_length 2000, epsilon 0.013139051608855005, time 731.0, rides 141\n",
      "Initial State is  [3, 12, 1]\n",
      "episode 4330, reward 545.0, memory_length 2000, epsilon 0.01312591255724615, time 740.0, rides 142\n",
      "Initial State is  [2, 3, 3]\n",
      "episode 4331, reward 947.0, memory_length 2000, epsilon 0.013112786644688904, time 729.0, rides 140\n",
      "Initial State is  [0, 9, 0]\n",
      "episode 4332, reward 924.0, memory_length 2000, epsilon 0.013099673858044215, time 731.0, rides 150\n",
      "Initial State is  [0, 15, 6]\n",
      "episode 4333, reward 732.0, memory_length 2000, epsilon 0.01308657418418617, time 724.0, rides 142\n",
      "Initial State is  [1, 22, 0]\n",
      "episode 4334, reward 680.0, memory_length 2000, epsilon 0.013073487610001983, time 733.0, rides 138\n",
      "Initial State is  [0, 3, 5]\n",
      "episode 4335, reward 857.0, memory_length 2000, epsilon 0.013060414122391981, time 725.0, rides 128\n",
      "Initial State is  [0, 1, 2]\n",
      "episode 4336, reward 986.0, memory_length 2000, epsilon 0.01304735370826959, time 736.0, rides 134\n",
      "Initial State is  [1, 4, 0]\n",
      "episode 4337, reward 777.0, memory_length 2000, epsilon 0.01303430635456132, time 726.0, rides 147\n",
      "Initial State is  [2, 7, 4]\n",
      "episode 4338, reward 995.0, memory_length 2000, epsilon 0.013021272048206758, time 736.0, rides 136\n",
      "Initial State is  [1, 1, 0]\n",
      "episode 4339, reward 1039.0, memory_length 2000, epsilon 0.013008250776158551, time 721.0, rides 127\n",
      "Initial State is  [1, 21, 1]\n",
      "episode 4340, reward 1042.0, memory_length 2000, epsilon 0.012995242525382393, time 727.0, rides 128\n",
      "Initial State is  [2, 3, 3]\n",
      "episode 4341, reward 844.0, memory_length 2000, epsilon 0.012982247282857011, time 729.0, rides 131\n",
      "Initial State is  [0, 15, 3]\n",
      "episode 4342, reward 977.0, memory_length 2000, epsilon 0.012969265035574154, time 725.0, rides 128\n",
      "Initial State is  [2, 2, 3]\n",
      "episode 4343, reward 834.0, memory_length 2000, epsilon 0.012956295770538579, time 722.0, rides 134\n",
      "Initial State is  [3, 5, 4]\n",
      "episode 4344, reward 849.0, memory_length 2000, epsilon 0.01294333947476804, time 726.0, rides 124\n",
      "Initial State is  [2, 7, 0]\n",
      "episode 4345, reward 1053.0, memory_length 2000, epsilon 0.012930396135293273, time 729.0, rides 132\n",
      "Initial State is  [1, 17, 2]\n",
      "episode 4346, reward 840.0, memory_length 2000, epsilon 0.01291746573915798, time 729.0, rides 123\n",
      "Initial State is  [0, 7, 1]\n",
      "episode 4347, reward 615.0, memory_length 2000, epsilon 0.012904548273418822, time 730.0, rides 127\n",
      "Initial State is  [0, 15, 6]\n",
      "episode 4348, reward 1107.0, memory_length 2000, epsilon 0.012891643725145403, time 725.0, rides 136\n",
      "Initial State is  [4, 0, 1]\n",
      "episode 4349, reward 632.0, memory_length 2000, epsilon 0.012878752081420258, time 734.0, rides 117\n",
      "Initial State is  [3, 19, 1]\n",
      "episode 4350, reward 906.0, memory_length 2000, epsilon 0.012865873329338837, time 721.0, rides 135\n",
      "Initial State is  [2, 1, 4]\n",
      "episode 4351, reward 1042.0, memory_length 2000, epsilon 0.012853007456009499, time 723.0, rides 126\n",
      "Initial State is  [4, 16, 0]\n",
      "episode 4352, reward 1016.0, memory_length 2000, epsilon 0.012840154448553489, time 729.0, rides 124\n",
      "Initial State is  [4, 15, 3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 4353, reward 865.0, memory_length 2000, epsilon 0.012827314294104936, time 728.0, rides 147\n",
      "Initial State is  [4, 22, 0]\n",
      "episode 4354, reward 863.0, memory_length 2000, epsilon 0.012814486979810831, time 733.0, rides 133\n",
      "Initial State is  [0, 20, 5]\n",
      "episode 4355, reward 979.0, memory_length 2000, epsilon 0.01280167249283102, time 739.0, rides 142\n",
      "Initial State is  [3, 5, 4]\n",
      "episode 4356, reward 803.0, memory_length 2000, epsilon 0.01278887082033819, time 733.0, rides 116\n",
      "Initial State is  [4, 15, 5]\n",
      "episode 4357, reward 1057.0, memory_length 2000, epsilon 0.012776081949517853, time 725.0, rides 134\n",
      "Initial State is  [4, 5, 4]\n",
      "episode 4358, reward 919.0, memory_length 2000, epsilon 0.012763305867568336, time 732.0, rides 131\n",
      "Initial State is  [0, 19, 2]\n",
      "episode 4359, reward 901.0, memory_length 2000, epsilon 0.012750542561700768, time 731.0, rides 142\n",
      "Initial State is  [3, 7, 2]\n",
      "episode 4360, reward 988.0, memory_length 2000, epsilon 0.012737792019139067, time 728.0, rides 151\n",
      "Initial State is  [4, 19, 3]\n",
      "episode 4361, reward 789.0, memory_length 2000, epsilon 0.012725054227119928, time 731.0, rides 145\n",
      "Initial State is  [4, 0, 1]\n",
      "episode 4362, reward 982.0, memory_length 2000, epsilon 0.012712329172892809, time 728.0, rides 130\n",
      "Initial State is  [3, 21, 1]\n",
      "episode 4363, reward 1010.0, memory_length 2000, epsilon 0.012699616843719915, time 729.0, rides 128\n",
      "Initial State is  [0, 3, 6]\n",
      "episode 4364, reward 923.0, memory_length 2000, epsilon 0.012686917226876196, time 726.0, rides 130\n",
      "Initial State is  [4, 12, 5]\n",
      "episode 4365, reward 690.0, memory_length 2000, epsilon 0.01267423030964932, time 730.0, rides 130\n",
      "Initial State is  [1, 7, 3]\n",
      "episode 4366, reward 1047.0, memory_length 2000, epsilon 0.012661556079339671, time 726.0, rides 143\n",
      "Initial State is  [2, 20, 6]\n",
      "episode 4367, reward 1113.0, memory_length 2000, epsilon 0.012648894523260332, time 733.0, rides 124\n",
      "Initial State is  [4, 10, 5]\n",
      "episode 4368, reward 1057.0, memory_length 2000, epsilon 0.012636245628737072, time 727.0, rides 130\n",
      "Initial State is  [1, 4, 6]\n",
      "episode 4369, reward 833.0, memory_length 2000, epsilon 0.012623609383108334, time 724.0, rides 136\n",
      "Initial State is  [0, 17, 1]\n",
      "episode 4370, reward 925.0, memory_length 2000, epsilon 0.012610985773725226, time 733.0, rides 135\n",
      "Initial State is  [0, 19, 0]\n",
      "episode 4371, reward 954.0, memory_length 2000, epsilon 0.012598374787951502, time 728.0, rides 143\n",
      "Initial State is  [3, 3, 3]\n",
      "episode 4372, reward 974.0, memory_length 2000, epsilon 0.01258577641316355, time 729.0, rides 134\n",
      "Initial State is  [2, 2, 3]\n",
      "episode 4373, reward 1091.0, memory_length 2000, epsilon 0.012573190636750387, time 726.0, rides 130\n",
      "Initial State is  [0, 12, 4]\n",
      "episode 4374, reward 985.0, memory_length 2000, epsilon 0.012560617446113637, time 727.0, rides 140\n",
      "Initial State is  [0, 23, 3]\n",
      "episode 4375, reward 1290.0, memory_length 2000, epsilon 0.012548056828667524, time 733.0, rides 139\n",
      "Initial State is  [0, 1, 0]\n",
      "episode 4376, reward 789.0, memory_length 2000, epsilon 0.012535508771838856, time 728.0, rides 150\n",
      "Initial State is  [0, 11, 1]\n",
      "episode 4377, reward 1183.0, memory_length 2000, epsilon 0.012522973263067018, time 732.0, rides 144\n",
      "Initial State is  [0, 11, 4]\n",
      "episode 4378, reward 962.0, memory_length 2000, epsilon 0.012510450289803951, time 731.0, rides 137\n",
      "Initial State is  [3, 16, 2]\n",
      "episode 4379, reward 821.0, memory_length 2000, epsilon 0.012497939839514147, time 740.0, rides 131\n",
      "Initial State is  [2, 10, 4]\n",
      "episode 4380, reward 974.0, memory_length 2000, epsilon 0.012485441899674633, time 731.0, rides 151\n",
      "Initial State is  [1, 0, 3]\n",
      "episode 4381, reward 702.0, memory_length 2000, epsilon 0.012472956457774959, time 729.0, rides 130\n",
      "Initial State is  [0, 0, 2]\n",
      "episode 4382, reward 1186.0, memory_length 2000, epsilon 0.012460483501317184, time 732.0, rides 131\n",
      "Initial State is  [3, 10, 4]\n",
      "episode 4383, reward 712.0, memory_length 2000, epsilon 0.012448023017815866, time 730.0, rides 138\n",
      "Initial State is  [1, 23, 1]\n",
      "episode 4384, reward 814.0, memory_length 2000, epsilon 0.01243557499479805, time 728.0, rides 132\n",
      "Initial State is  [4, 1, 5]\n",
      "episode 4385, reward 820.0, memory_length 2000, epsilon 0.012423139419803253, time 732.0, rides 127\n",
      "Initial State is  [3, 13, 2]\n",
      "episode 4386, reward 1040.0, memory_length 2000, epsilon 0.01241071628038345, time 730.0, rides 134\n",
      "Initial State is  [2, 2, 6]\n",
      "episode 4387, reward 941.0, memory_length 2000, epsilon 0.012398305564103066, time 730.0, rides 136\n",
      "Initial State is  [1, 7, 6]\n",
      "episode 4388, reward 1117.0, memory_length 2000, epsilon 0.012385907258538963, time 735.0, rides 138\n",
      "Initial State is  [0, 19, 0]\n",
      "episode 4389, reward 1049.0, memory_length 2000, epsilon 0.012373521351280423, time 727.0, rides 139\n",
      "Initial State is  [3, 12, 4]\n",
      "episode 4390, reward 860.0, memory_length 2000, epsilon 0.012361147829929142, time 730.0, rides 135\n",
      "Initial State is  [2, 2, 4]\n",
      "episode 4391, reward 728.0, memory_length 2000, epsilon 0.012348786682099213, time 729.0, rides 146\n",
      "Initial State is  [0, 23, 3]\n",
      "episode 4392, reward 950.0, memory_length 2000, epsilon 0.012336437895417114, time 728.0, rides 140\n",
      "Initial State is  [3, 2, 6]\n",
      "episode 4393, reward 778.0, memory_length 2000, epsilon 0.012324101457521696, time 724.0, rides 131\n",
      "Initial State is  [0, 6, 6]\n",
      "episode 4394, reward 1104.0, memory_length 2000, epsilon 0.012311777356064174, time 735.0, rides 142\n",
      "Initial State is  [0, 6, 0]\n",
      "episode 4395, reward 1303.0, memory_length 2000, epsilon 0.01229946557870811, time 742.0, rides 133\n",
      "Initial State is  [1, 13, 0]\n",
      "episode 4396, reward 957.0, memory_length 2000, epsilon 0.012287166113129402, time 727.0, rides 136\n",
      "Initial State is  [3, 2, 2]\n",
      "episode 4397, reward 607.0, memory_length 2000, epsilon 0.012274878947016272, time 727.0, rides 132\n",
      "Initial State is  [4, 19, 2]\n",
      "episode 4398, reward 1229.0, memory_length 2000, epsilon 0.012262604068069257, time 733.0, rides 137\n",
      "Initial State is  [2, 9, 3]\n",
      "episode 4399, reward 1212.0, memory_length 2000, epsilon 0.012250341464001188, time 729.0, rides 133\n",
      "Initial State is  [4, 9, 0]\n",
      "episode 4400, reward 1034.0, memory_length 2000, epsilon 0.012238091122537187, time 729.0, rides 149\n",
      "Initial State is  [2, 1, 6]\n",
      "episode 4401, reward 665.0, memory_length 2000, epsilon 0.012225853031414649, time 720.0, rides 149\n",
      "Initial State is  [1, 20, 0]\n",
      "episode 4402, reward 1065.0, memory_length 2000, epsilon 0.012213627178383235, time 725.0, rides 152\n",
      "Initial State is  [4, 20, 5]\n",
      "episode 4403, reward 1089.0, memory_length 2000, epsilon 0.012201413551204852, time 725.0, rides 143\n",
      "Initial State is  [2, 20, 1]\n",
      "episode 4404, reward 975.0, memory_length 2000, epsilon 0.012189212137653647, time 728.0, rides 142\n",
      "Initial State is  [3, 18, 5]\n",
      "episode 4405, reward 1057.0, memory_length 2000, epsilon 0.012177022925515993, time 729.0, rides 141\n",
      "Initial State is  [3, 11, 0]\n",
      "episode 4406, reward 1083.0, memory_length 2000, epsilon 0.012164845902590477, time 739.0, rides 142\n",
      "Initial State is  [4, 9, 3]\n",
      "episode 4407, reward 678.0, memory_length 2000, epsilon 0.012152681056687886, time 728.0, rides 134\n",
      "Initial State is  [4, 14, 1]\n",
      "episode 4408, reward 849.0, memory_length 2000, epsilon 0.012140528375631197, time 726.0, rides 141\n",
      "Initial State is  [3, 0, 5]\n",
      "episode 4409, reward 1173.0, memory_length 2000, epsilon 0.012128387847255567, time 724.0, rides 143\n",
      "Initial State is  [1, 14, 6]\n",
      "episode 4410, reward 893.0, memory_length 2000, epsilon 0.01211625945940831, time 727.0, rides 140\n",
      "Initial State is  [4, 17, 1]\n",
      "episode 4411, reward 1003.0, memory_length 2000, epsilon 0.012104143199948902, time 735.0, rides 125\n",
      "Initial State is  [3, 20, 5]\n",
      "episode 4412, reward 1072.0, memory_length 2000, epsilon 0.012092039056748953, time 736.0, rides 145\n",
      "Initial State is  [2, 6, 1]\n",
      "episode 4413, reward 1142.0, memory_length 2000, epsilon 0.012079947017692204, time 722.0, rides 136\n",
      "Initial State is  [4, 2, 4]\n",
      "episode 4414, reward 1015.0, memory_length 2000, epsilon 0.012067867070674513, time 730.0, rides 132\n",
      "Initial State is  [3, 20, 6]\n",
      "episode 4415, reward 788.0, memory_length 2000, epsilon 0.012055799203603838, time 728.0, rides 128\n",
      "Initial State is  [3, 23, 6]\n",
      "episode 4416, reward 887.0, memory_length 2000, epsilon 0.012043743404400235, time 729.0, rides 135\n",
      "Initial State is  [1, 17, 3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 4417, reward 896.0, memory_length 2000, epsilon 0.012031699660995834, time 726.0, rides 133\n",
      "Initial State is  [1, 6, 6]\n",
      "episode 4418, reward 707.0, memory_length 2000, epsilon 0.012019667961334838, time 731.0, rides 128\n",
      "Initial State is  [4, 6, 4]\n",
      "episode 4419, reward 1107.0, memory_length 2000, epsilon 0.012007648293373504, time 734.0, rides 144\n",
      "Initial State is  [1, 2, 4]\n",
      "episode 4420, reward 1067.0, memory_length 2000, epsilon 0.01199564064508013, time 734.0, rides 147\n",
      "Initial State is  [2, 2, 3]\n",
      "episode 4421, reward 1182.0, memory_length 2000, epsilon 0.01198364500443505, time 729.0, rides 139\n",
      "Initial State is  [0, 11, 2]\n",
      "episode 4422, reward 1035.0, memory_length 2000, epsilon 0.011971661359430615, time 736.0, rides 135\n",
      "Initial State is  [1, 10, 5]\n",
      "episode 4423, reward 590.0, memory_length 2000, epsilon 0.011959689698071185, time 733.0, rides 134\n",
      "Initial State is  [4, 23, 2]\n",
      "episode 4424, reward 910.0, memory_length 2000, epsilon 0.011947730008373113, time 728.0, rides 130\n",
      "Initial State is  [0, 20, 0]\n",
      "episode 4425, reward 1050.0, memory_length 2000, epsilon 0.01193578227836474, time 727.0, rides 142\n",
      "Initial State is  [0, 1, 5]\n",
      "episode 4426, reward 1009.0, memory_length 2000, epsilon 0.011923846496086375, time 735.0, rides 144\n",
      "Initial State is  [2, 12, 5]\n",
      "episode 4427, reward 1091.0, memory_length 2000, epsilon 0.01191192264959029, time 728.0, rides 128\n",
      "Initial State is  [3, 16, 6]\n",
      "episode 4428, reward 901.0, memory_length 2000, epsilon 0.0119000107269407, time 732.0, rides 141\n",
      "Initial State is  [4, 15, 5]\n",
      "episode 4429, reward 835.0, memory_length 2000, epsilon 0.011888110716213759, time 734.0, rides 138\n",
      "Initial State is  [3, 22, 5]\n",
      "episode 4430, reward 920.0, memory_length 2000, epsilon 0.011876222605497545, time 739.0, rides 131\n",
      "Initial State is  [3, 9, 0]\n",
      "episode 4431, reward 1081.0, memory_length 2000, epsilon 0.011864346382892048, time 729.0, rides 133\n",
      "Initial State is  [3, 5, 3]\n",
      "episode 4432, reward 950.0, memory_length 2000, epsilon 0.011852482036509156, time 722.0, rides 146\n",
      "Initial State is  [0, 1, 6]\n",
      "episode 4433, reward 830.0, memory_length 2000, epsilon 0.011840629554472647, time 727.0, rides 129\n",
      "Initial State is  [0, 1, 2]\n",
      "episode 4434, reward 1343.0, memory_length 2000, epsilon 0.011828788924918175, time 727.0, rides 122\n",
      "Initial State is  [1, 10, 1]\n",
      "episode 4435, reward 859.0, memory_length 2000, epsilon 0.011816960135993256, time 728.0, rides 133\n",
      "Initial State is  [1, 3, 6]\n",
      "episode 4436, reward 859.0, memory_length 2000, epsilon 0.011805143175857263, time 726.0, rides 145\n",
      "Initial State is  [1, 15, 3]\n",
      "episode 4437, reward 582.0, memory_length 2000, epsilon 0.011793338032681407, time 728.0, rides 136\n",
      "Initial State is  [1, 5, 4]\n",
      "episode 4438, reward 763.0, memory_length 2000, epsilon 0.011781544694648725, time 728.0, rides 122\n",
      "Initial State is  [2, 21, 4]\n",
      "episode 4439, reward 1193.0, memory_length 2000, epsilon 0.011769763149954076, time 735.0, rides 128\n",
      "Initial State is  [4, 5, 0]\n",
      "episode 4440, reward 811.0, memory_length 2000, epsilon 0.011757993386804121, time 724.0, rides 129\n",
      "Initial State is  [2, 9, 4]\n",
      "episode 4441, reward 868.0, memory_length 2000, epsilon 0.011746235393417317, time 730.0, rides 130\n",
      "Initial State is  [4, 18, 6]\n",
      "episode 4442, reward 690.0, memory_length 2000, epsilon 0.0117344891580239, time 732.0, rides 139\n",
      "Initial State is  [4, 13, 2]\n",
      "episode 4443, reward 1062.0, memory_length 2000, epsilon 0.011722754668865876, time 731.0, rides 147\n",
      "Initial State is  [0, 1, 6]\n",
      "episode 4444, reward 931.0, memory_length 2000, epsilon 0.01171103191419701, time 722.0, rides 144\n",
      "Initial State is  [4, 3, 6]\n",
      "episode 4445, reward 793.0, memory_length 2000, epsilon 0.011699320882282814, time 733.0, rides 137\n",
      "Initial State is  [4, 19, 5]\n",
      "episode 4446, reward 865.0, memory_length 2000, epsilon 0.01168762156140053, time 732.0, rides 137\n",
      "Initial State is  [1, 1, 4]\n",
      "episode 4447, reward 784.0, memory_length 2000, epsilon 0.01167593393983913, time 724.0, rides 132\n",
      "Initial State is  [4, 10, 0]\n",
      "episode 4448, reward 979.0, memory_length 2000, epsilon 0.01166425800589929, time 735.0, rides 125\n",
      "Initial State is  [0, 18, 4]\n",
      "episode 4449, reward 719.0, memory_length 2000, epsilon 0.01165259374789339, time 730.0, rides 147\n",
      "Initial State is  [1, 19, 4]\n",
      "episode 4450, reward 900.0, memory_length 2000, epsilon 0.011640941154145497, time 724.0, rides 138\n",
      "Initial State is  [4, 15, 1]\n",
      "episode 4451, reward 828.0, memory_length 2000, epsilon 0.01162930021299135, time 733.0, rides 148\n",
      "Initial State is  [2, 13, 5]\n",
      "episode 4452, reward 923.0, memory_length 2000, epsilon 0.01161767091277836, time 729.0, rides 119\n",
      "Initial State is  [2, 0, 4]\n",
      "episode 4453, reward 1056.0, memory_length 2000, epsilon 0.011606053241865581, time 727.0, rides 137\n",
      "Initial State is  [1, 2, 1]\n",
      "episode 4454, reward 798.0, memory_length 2000, epsilon 0.011594447188623716, time 730.0, rides 132\n",
      "Initial State is  [1, 4, 6]\n",
      "episode 4455, reward 733.0, memory_length 2000, epsilon 0.011582852741435093, time 730.0, rides 133\n",
      "Initial State is  [2, 12, 1]\n",
      "episode 4456, reward 545.0, memory_length 2000, epsilon 0.011571269888693659, time 728.0, rides 133\n",
      "Initial State is  [0, 13, 6]\n",
      "episode 4457, reward 1005.0, memory_length 2000, epsilon 0.011559698618804966, time 727.0, rides 125\n",
      "Initial State is  [3, 9, 3]\n",
      "episode 4458, reward 735.0, memory_length 2000, epsilon 0.011548138920186161, time 725.0, rides 129\n",
      "Initial State is  [2, 23, 0]\n",
      "episode 4459, reward 992.0, memory_length 2000, epsilon 0.011536590781265975, time 728.0, rides 133\n",
      "Initial State is  [3, 19, 2]\n",
      "episode 4460, reward 1116.0, memory_length 2000, epsilon 0.01152505419048471, time 726.0, rides 131\n",
      "Initial State is  [1, 15, 1]\n",
      "episode 4461, reward 776.0, memory_length 2000, epsilon 0.011513529136294224, time 723.0, rides 139\n",
      "Initial State is  [2, 1, 5]\n",
      "episode 4462, reward 977.0, memory_length 2000, epsilon 0.011502015607157929, time 729.0, rides 140\n",
      "Initial State is  [0, 17, 3]\n",
      "episode 4463, reward 965.0, memory_length 2000, epsilon 0.011490513591550771, time 734.0, rides 144\n",
      "Initial State is  [0, 1, 5]\n",
      "episode 4464, reward 1117.0, memory_length 2000, epsilon 0.01147902307795922, time 726.0, rides 138\n",
      "Initial State is  [3, 0, 6]\n",
      "episode 4465, reward 1009.0, memory_length 2000, epsilon 0.01146754405488126, time 727.0, rides 139\n",
      "Initial State is  [4, 19, 4]\n",
      "episode 4466, reward 1040.0, memory_length 2000, epsilon 0.011456076510826379, time 733.0, rides 133\n",
      "Initial State is  [3, 5, 1]\n",
      "episode 4467, reward 906.0, memory_length 2000, epsilon 0.011444620434315551, time 722.0, rides 134\n",
      "Initial State is  [3, 23, 2]\n",
      "episode 4468, reward 775.0, memory_length 2000, epsilon 0.011433175813881235, time 725.0, rides 132\n",
      "Initial State is  [4, 8, 5]\n",
      "episode 4469, reward 714.0, memory_length 2000, epsilon 0.011421742638067355, time 734.0, rides 137\n",
      "Initial State is  [0, 12, 0]\n",
      "episode 4470, reward 876.0, memory_length 2000, epsilon 0.011410320895429288, time 725.0, rides 134\n",
      "Initial State is  [0, 0, 4]\n",
      "episode 4471, reward 973.0, memory_length 2000, epsilon 0.011398910574533858, time 735.0, rides 137\n",
      "Initial State is  [1, 1, 4]\n",
      "episode 4472, reward 656.0, memory_length 2000, epsilon 0.011387511663959324, time 728.0, rides 139\n",
      "Initial State is  [0, 9, 5]\n",
      "episode 4473, reward 828.0, memory_length 2000, epsilon 0.011376124152295365, time 727.0, rides 141\n",
      "Initial State is  [2, 20, 4]\n",
      "episode 4474, reward 671.0, memory_length 2000, epsilon 0.01136474802814307, time 729.0, rides 141\n",
      "Initial State is  [1, 9, 1]\n",
      "episode 4475, reward 1118.0, memory_length 2000, epsilon 0.011353383280114928, time 725.0, rides 133\n",
      "Initial State is  [1, 19, 4]\n",
      "episode 4476, reward 578.0, memory_length 2000, epsilon 0.011342029896834813, time 728.0, rides 145\n",
      "Initial State is  [0, 20, 4]\n",
      "episode 4477, reward 641.0, memory_length 2000, epsilon 0.011330687866937979, time 725.0, rides 132\n",
      "Initial State is  [0, 19, 5]\n",
      "episode 4478, reward 1107.0, memory_length 2000, epsilon 0.011319357179071041, time 724.0, rides 140\n",
      "Initial State is  [2, 10, 6]\n",
      "episode 4479, reward 838.0, memory_length 2000, epsilon 0.011308037821891971, time 733.0, rides 139\n",
      "Initial State is  [1, 17, 4]\n",
      "episode 4480, reward 931.0, memory_length 2000, epsilon 0.01129672978407008, time 725.0, rides 136\n",
      "Initial State is  [0, 8, 3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 4481, reward 832.0, memory_length 2000, epsilon 0.011285433054286009, time 730.0, rides 131\n",
      "Initial State is  [2, 17, 4]\n",
      "episode 4482, reward 673.0, memory_length 2000, epsilon 0.011274147621231722, time 727.0, rides 137\n",
      "Initial State is  [4, 12, 3]\n",
      "episode 4483, reward 792.0, memory_length 2000, epsilon 0.01126287347361049, time 731.0, rides 129\n",
      "Initial State is  [2, 22, 1]\n",
      "episode 4484, reward 729.0, memory_length 2000, epsilon 0.01125161060013688, time 723.0, rides 135\n",
      "Initial State is  [4, 14, 6]\n",
      "episode 4485, reward 748.0, memory_length 2000, epsilon 0.011240358989536743, time 723.0, rides 150\n",
      "Initial State is  [3, 21, 6]\n",
      "episode 4486, reward 981.0, memory_length 2000, epsilon 0.011229118630547206, time 727.0, rides 143\n",
      "Initial State is  [0, 11, 0]\n",
      "episode 4487, reward 984.0, memory_length 2000, epsilon 0.011217889511916658, time 730.0, rides 133\n",
      "Initial State is  [4, 8, 2]\n",
      "episode 4488, reward 1018.0, memory_length 2000, epsilon 0.011206671622404742, time 730.0, rides 140\n",
      "Initial State is  [0, 11, 0]\n",
      "episode 4489, reward 1244.0, memory_length 2000, epsilon 0.011195464950782337, time 732.0, rides 138\n",
      "Initial State is  [0, 6, 2]\n",
      "episode 4490, reward 716.0, memory_length 2000, epsilon 0.011184269485831554, time 724.0, rides 136\n",
      "Initial State is  [0, 1, 2]\n",
      "episode 4491, reward 891.0, memory_length 2000, epsilon 0.011173085216345722, time 726.0, rides 134\n",
      "Initial State is  [3, 23, 3]\n",
      "episode 4492, reward 1167.0, memory_length 2000, epsilon 0.011161912131129376, time 733.0, rides 148\n",
      "Initial State is  [1, 0, 4]\n",
      "episode 4493, reward 917.0, memory_length 2000, epsilon 0.011150750218998246, time 731.0, rides 142\n",
      "Initial State is  [2, 12, 5]\n",
      "episode 4494, reward 1245.0, memory_length 2000, epsilon 0.011139599468779248, time 731.0, rides 129\n",
      "Initial State is  [1, 23, 4]\n",
      "episode 4495, reward 931.0, memory_length 2000, epsilon 0.01112845986931047, time 735.0, rides 140\n",
      "Initial State is  [0, 23, 6]\n",
      "episode 4496, reward 1004.0, memory_length 2000, epsilon 0.01111733140944116, time 730.0, rides 144\n",
      "Initial State is  [0, 15, 0]\n",
      "episode 4497, reward 1117.0, memory_length 2000, epsilon 0.011106214078031718, time 725.0, rides 134\n",
      "Initial State is  [1, 1, 1]\n",
      "episode 4498, reward 1050.0, memory_length 2000, epsilon 0.011095107863953686, time 730.0, rides 141\n",
      "Initial State is  [3, 7, 1]\n",
      "episode 4499, reward 905.0, memory_length 2000, epsilon 0.011084012756089733, time 732.0, rides 126\n",
      "Initial State is  [2, 18, 1]\n",
      "episode 4500, reward 905.0, memory_length 2000, epsilon 0.011072928743333644, time 728.0, rides 143\n",
      "Initial State is  [4, 14, 2]\n",
      "episode 4501, reward 1052.0, memory_length 2000, epsilon 0.01106185581459031, time 735.0, rides 141\n",
      "Initial State is  [4, 13, 2]\n",
      "episode 4502, reward 845.0, memory_length 2000, epsilon 0.01105079395877572, time 730.0, rides 134\n",
      "Initial State is  [4, 3, 0]\n",
      "episode 4503, reward 802.0, memory_length 2000, epsilon 0.011039743164816944, time 731.0, rides 130\n",
      "Initial State is  [4, 0, 3]\n",
      "episode 4504, reward 755.0, memory_length 2000, epsilon 0.011028703421652127, time 727.0, rides 134\n",
      "Initial State is  [0, 2, 6]\n",
      "episode 4505, reward 1121.0, memory_length 2000, epsilon 0.011017674718230475, time 735.0, rides 146\n",
      "Initial State is  [4, 7, 5]\n",
      "episode 4506, reward 997.0, memory_length 2000, epsilon 0.011006657043512244, time 731.0, rides 136\n",
      "Initial State is  [3, 14, 2]\n",
      "episode 4507, reward 809.0, memory_length 2000, epsilon 0.010995650386468731, time 722.0, rides 129\n",
      "Initial State is  [4, 22, 1]\n",
      "episode 4508, reward 700.0, memory_length 2000, epsilon 0.010984654736082263, time 724.0, rides 143\n",
      "Initial State is  [1, 14, 5]\n",
      "episode 4509, reward 1310.0, memory_length 2000, epsilon 0.010973670081346181, time 726.0, rides 134\n",
      "Initial State is  [4, 17, 5]\n",
      "episode 4510, reward 1207.0, memory_length 2000, epsilon 0.010962696411264836, time 728.0, rides 129\n",
      "Initial State is  [1, 10, 3]\n",
      "episode 4511, reward 815.0, memory_length 2000, epsilon 0.01095173371485357, time 728.0, rides 139\n",
      "Initial State is  [3, 18, 2]\n",
      "episode 4512, reward 546.0, memory_length 2000, epsilon 0.010940781981138717, time 734.0, rides 122\n",
      "Initial State is  [3, 11, 6]\n",
      "episode 4513, reward 809.0, memory_length 2000, epsilon 0.010929841199157578, time 730.0, rides 129\n",
      "Initial State is  [3, 7, 5]\n",
      "episode 4514, reward 793.0, memory_length 2000, epsilon 0.010918911357958421, time 735.0, rides 139\n",
      "Initial State is  [4, 10, 0]\n",
      "episode 4515, reward 1160.0, memory_length 2000, epsilon 0.010907992446600463, time 725.0, rides 140\n",
      "Initial State is  [1, 23, 2]\n",
      "episode 4516, reward 830.0, memory_length 2000, epsilon 0.010897084454153863, time 735.0, rides 138\n",
      "Initial State is  [1, 14, 1]\n",
      "episode 4517, reward 1012.0, memory_length 2000, epsilon 0.01088618736969971, time 729.0, rides 137\n",
      "Initial State is  [0, 13, 2]\n",
      "episode 4518, reward 1049.0, memory_length 2000, epsilon 0.010875301182330011, time 726.0, rides 133\n",
      "Initial State is  [3, 12, 0]\n",
      "episode 4519, reward 846.0, memory_length 2000, epsilon 0.010864425881147681, time 724.0, rides 126\n",
      "Initial State is  [4, 18, 3]\n",
      "episode 4520, reward 944.0, memory_length 2000, epsilon 0.010853561455266534, time 729.0, rides 148\n",
      "Initial State is  [4, 10, 6]\n",
      "episode 4521, reward 1138.0, memory_length 2000, epsilon 0.010842707893811268, time 729.0, rides 142\n",
      "Initial State is  [3, 13, 5]\n",
      "episode 4522, reward 1200.0, memory_length 2000, epsilon 0.010831865185917456, time 724.0, rides 138\n",
      "Initial State is  [3, 3, 1]\n",
      "episode 4523, reward 911.0, memory_length 2000, epsilon 0.01082103332073154, time 728.0, rides 133\n",
      "Initial State is  [3, 17, 0]\n",
      "episode 4524, reward 799.0, memory_length 2000, epsilon 0.010810212287410808, time 727.0, rides 128\n",
      "Initial State is  [2, 18, 5]\n",
      "episode 4525, reward 1055.0, memory_length 2000, epsilon 0.010799402075123397, time 736.0, rides 144\n",
      "Initial State is  [1, 8, 2]\n",
      "episode 4526, reward 1199.0, memory_length 2000, epsilon 0.010788602673048273, time 729.0, rides 138\n",
      "Initial State is  [2, 19, 5]\n",
      "episode 4527, reward 906.0, memory_length 2000, epsilon 0.010777814070375225, time 724.0, rides 129\n",
      "Initial State is  [2, 22, 4]\n",
      "episode 4528, reward 1039.0, memory_length 2000, epsilon 0.01076703625630485, time 728.0, rides 125\n",
      "Initial State is  [3, 11, 2]\n",
      "episode 4529, reward 850.0, memory_length 2000, epsilon 0.010756269220048544, time 731.0, rides 139\n",
      "Initial State is  [3, 3, 0]\n",
      "episode 4530, reward 734.0, memory_length 2000, epsilon 0.010745512950828496, time 728.0, rides 129\n",
      "Initial State is  [1, 2, 3]\n",
      "episode 4531, reward 978.0, memory_length 2000, epsilon 0.010734767437877667, time 730.0, rides 131\n",
      "Initial State is  [2, 2, 6]\n",
      "episode 4532, reward 824.0, memory_length 2000, epsilon 0.01072403267043979, time 725.0, rides 134\n",
      "Initial State is  [4, 19, 4]\n",
      "episode 4533, reward 876.0, memory_length 2000, epsilon 0.01071330863776935, time 734.0, rides 130\n",
      "Initial State is  [2, 10, 4]\n",
      "episode 4534, reward 955.0, memory_length 2000, epsilon 0.010702595329131582, time 726.0, rides 132\n",
      "Initial State is  [0, 13, 0]\n",
      "episode 4535, reward 1314.0, memory_length 2000, epsilon 0.01069189273380245, time 727.0, rides 140\n",
      "Initial State is  [2, 19, 1]\n",
      "episode 4536, reward 1220.0, memory_length 2000, epsilon 0.010681200841068649, time 720.0, rides 128\n",
      "Initial State is  [3, 3, 3]\n",
      "episode 4537, reward 946.0, memory_length 2000, epsilon 0.01067051964022758, time 726.0, rides 142\n",
      "Initial State is  [3, 0, 6]\n",
      "episode 4538, reward 942.0, memory_length 2000, epsilon 0.010659849120587353, time 724.0, rides 144\n",
      "Initial State is  [1, 10, 6]\n",
      "episode 4539, reward 914.0, memory_length 2000, epsilon 0.010649189271466766, time 731.0, rides 133\n",
      "Initial State is  [4, 23, 3]\n",
      "episode 4540, reward 1155.0, memory_length 2000, epsilon 0.0106385400821953, time 725.0, rides 125\n",
      "Initial State is  [3, 8, 1]\n",
      "episode 4541, reward 1010.0, memory_length 2000, epsilon 0.010627901542113105, time 730.0, rides 132\n",
      "Initial State is  [1, 18, 0]\n",
      "episode 4542, reward 732.0, memory_length 2000, epsilon 0.010617273640570992, time 733.0, rides 131\n",
      "Initial State is  [1, 16, 6]\n",
      "episode 4543, reward 834.0, memory_length 2000, epsilon 0.01060665636693042, time 724.0, rides 147\n",
      "Initial State is  [0, 23, 6]\n",
      "episode 4544, reward 915.0, memory_length 2000, epsilon 0.01059604971056349, time 724.0, rides 135\n",
      "Initial State is  [0, 17, 3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 4545, reward 1024.0, memory_length 2000, epsilon 0.010585453660852926, time 727.0, rides 131\n",
      "Initial State is  [1, 18, 1]\n",
      "episode 4546, reward 725.0, memory_length 2000, epsilon 0.010574868207192074, time 734.0, rides 124\n",
      "Initial State is  [0, 1, 0]\n",
      "episode 4547, reward 781.0, memory_length 2000, epsilon 0.01056429333898488, time 729.0, rides 115\n",
      "Initial State is  [2, 1, 5]\n",
      "episode 4548, reward 630.0, memory_length 2000, epsilon 0.010553729045645896, time 729.0, rides 140\n",
      "Initial State is  [4, 18, 1]\n",
      "episode 4549, reward 1022.0, memory_length 2000, epsilon 0.01054317531660025, time 725.0, rides 131\n",
      "Initial State is  [3, 21, 5]\n",
      "episode 4550, reward 990.0, memory_length 2000, epsilon 0.01053263214128365, time 725.0, rides 125\n",
      "Initial State is  [4, 1, 0]\n",
      "episode 4551, reward 1221.0, memory_length 2000, epsilon 0.010522099509142365, time 737.0, rides 131\n",
      "Initial State is  [3, 0, 3]\n",
      "episode 4552, reward 566.0, memory_length 2000, epsilon 0.010511577409633223, time 738.0, rides 133\n",
      "Initial State is  [4, 18, 6]\n",
      "episode 4553, reward 877.0, memory_length 2000, epsilon 0.010501065832223589, time 735.0, rides 131\n",
      "Initial State is  [4, 13, 1]\n",
      "episode 4554, reward 1330.0, memory_length 2000, epsilon 0.010490564766391366, time 724.0, rides 145\n",
      "Initial State is  [1, 18, 3]\n",
      "episode 4555, reward 876.0, memory_length 2000, epsilon 0.010480074201624974, time 724.0, rides 125\n",
      "Initial State is  [0, 8, 1]\n",
      "episode 4556, reward 882.0, memory_length 2000, epsilon 0.01046959412742335, time 729.0, rides 131\n",
      "Initial State is  [1, 13, 3]\n",
      "episode 4557, reward 587.0, memory_length 2000, epsilon 0.010459124533295927, time 726.0, rides 139\n",
      "Initial State is  [1, 9, 0]\n",
      "episode 4558, reward 796.0, memory_length 2000, epsilon 0.01044866540876263, time 729.0, rides 131\n",
      "Initial State is  [1, 5, 1]\n",
      "episode 4559, reward 934.0, memory_length 2000, epsilon 0.010438216743353868, time 732.0, rides 139\n",
      "Initial State is  [4, 12, 1]\n",
      "episode 4560, reward 1089.0, memory_length 2000, epsilon 0.010427778526610514, time 729.0, rides 132\n",
      "Initial State is  [0, 7, 6]\n",
      "episode 4561, reward 969.0, memory_length 2000, epsilon 0.010417350748083904, time 729.0, rides 141\n",
      "Initial State is  [1, 9, 0]\n",
      "episode 4562, reward 935.0, memory_length 2000, epsilon 0.01040693339733582, time 739.0, rides 134\n",
      "Initial State is  [4, 23, 5]\n",
      "episode 4563, reward 774.0, memory_length 2000, epsilon 0.010396526463938485, time 721.0, rides 122\n",
      "Initial State is  [0, 1, 0]\n",
      "episode 4564, reward 1168.0, memory_length 2000, epsilon 0.010386129937474547, time 730.0, rides 140\n",
      "Initial State is  [4, 3, 4]\n",
      "episode 4565, reward 1033.0, memory_length 2000, epsilon 0.010375743807537072, time 722.0, rides 123\n",
      "Initial State is  [2, 19, 3]\n",
      "episode 4566, reward 899.0, memory_length 2000, epsilon 0.010365368063729535, time 733.0, rides 118\n",
      "Initial State is  [4, 11, 5]\n",
      "episode 4567, reward 1301.0, memory_length 2000, epsilon 0.010355002695665807, time 730.0, rides 125\n",
      "Initial State is  [2, 16, 5]\n",
      "episode 4568, reward 1175.0, memory_length 2000, epsilon 0.010344647692970142, time 731.0, rides 131\n",
      "Initial State is  [3, 0, 5]\n",
      "episode 4569, reward 1011.0, memory_length 2000, epsilon 0.010334303045277172, time 728.0, rides 137\n",
      "Initial State is  [1, 21, 6]\n",
      "episode 4570, reward 566.0, memory_length 2000, epsilon 0.010323968742231895, time 727.0, rides 125\n",
      "Initial State is  [2, 8, 1]\n",
      "episode 4571, reward 941.0, memory_length 2000, epsilon 0.010313644773489663, time 731.0, rides 133\n",
      "Initial State is  [4, 3, 1]\n",
      "episode 4572, reward 911.0, memory_length 2000, epsilon 0.010303331128716174, time 730.0, rides 133\n",
      "Initial State is  [3, 0, 0]\n",
      "episode 4573, reward 967.0, memory_length 2000, epsilon 0.010293027797587458, time 728.0, rides 132\n",
      "Initial State is  [2, 3, 2]\n",
      "episode 4574, reward 810.0, memory_length 2000, epsilon 0.01028273476978987, time 728.0, rides 128\n",
      "Initial State is  [3, 7, 5]\n",
      "episode 4575, reward 739.0, memory_length 2000, epsilon 0.01027245203502008, time 727.0, rides 132\n",
      "Initial State is  [0, 9, 5]\n",
      "episode 4576, reward 834.0, memory_length 2000, epsilon 0.01026217958298506, time 728.0, rides 128\n",
      "Initial State is  [1, 17, 0]\n",
      "episode 4577, reward 936.0, memory_length 2000, epsilon 0.010251917403402075, time 723.0, rides 133\n",
      "Initial State is  [4, 6, 0]\n",
      "episode 4578, reward 827.0, memory_length 2000, epsilon 0.010241665485998672, time 729.0, rides 120\n",
      "Initial State is  [1, 19, 2]\n",
      "episode 4579, reward 1249.0, memory_length 2000, epsilon 0.010231423820512673, time 728.0, rides 139\n",
      "Initial State is  [4, 5, 3]\n",
      "episode 4580, reward 693.0, memory_length 2000, epsilon 0.01022119239669216, time 726.0, rides 126\n",
      "Initial State is  [1, 15, 4]\n",
      "episode 4581, reward 859.0, memory_length 2000, epsilon 0.010210971204295469, time 729.0, rides 143\n",
      "Initial State is  [4, 9, 3]\n",
      "episode 4582, reward 998.0, memory_length 2000, epsilon 0.010200760233091173, time 726.0, rides 127\n",
      "Initial State is  [3, 23, 0]\n",
      "episode 4583, reward 972.0, memory_length 2000, epsilon 0.010190559472858081, time 730.0, rides 134\n",
      "Initial State is  [3, 14, 4]\n",
      "episode 4584, reward 1148.0, memory_length 2000, epsilon 0.010180368913385222, time 730.0, rides 141\n",
      "Initial State is  [0, 14, 3]\n",
      "episode 4585, reward 1066.0, memory_length 2000, epsilon 0.010170188544471838, time 731.0, rides 128\n",
      "Initial State is  [2, 1, 6]\n",
      "episode 4586, reward 985.0, memory_length 2000, epsilon 0.010160018355927366, time 728.0, rides 119\n",
      "Initial State is  [2, 14, 5]\n",
      "episode 4587, reward 1097.0, memory_length 2000, epsilon 0.010149858337571439, time 729.0, rides 125\n",
      "Initial State is  [1, 20, 1]\n",
      "episode 4588, reward 954.0, memory_length 2000, epsilon 0.010139708479233867, time 734.0, rides 120\n",
      "Initial State is  [3, 6, 3]\n",
      "episode 4589, reward 892.0, memory_length 2000, epsilon 0.010129568770754632, time 728.0, rides 135\n",
      "Initial State is  [2, 6, 1]\n",
      "episode 4590, reward 985.0, memory_length 2000, epsilon 0.010119439201983878, time 730.0, rides 131\n",
      "Initial State is  [4, 2, 4]\n",
      "episode 4591, reward 1072.0, memory_length 2000, epsilon 0.010109319762781895, time 733.0, rides 130\n",
      "Initial State is  [4, 9, 1]\n",
      "episode 4592, reward 1042.0, memory_length 2000, epsilon 0.010099210443019114, time 729.0, rides 135\n",
      "Initial State is  [4, 22, 4]\n",
      "episode 4593, reward 1064.0, memory_length 2000, epsilon 0.010089111232576095, time 742.0, rides 129\n",
      "Initial State is  [4, 20, 3]\n",
      "episode 4594, reward 741.0, memory_length 2000, epsilon 0.010079022121343519, time 734.0, rides 133\n",
      "Initial State is  [1, 22, 6]\n",
      "episode 4595, reward 723.0, memory_length 2000, epsilon 0.010068943099222174, time 735.0, rides 134\n",
      "Initial State is  [4, 16, 2]\n",
      "episode 4596, reward 1156.0, memory_length 2000, epsilon 0.010058874156122952, time 727.0, rides 133\n",
      "Initial State is  [1, 8, 2]\n",
      "episode 4597, reward 1180.0, memory_length 2000, epsilon 0.01004881528196683, time 730.0, rides 131\n",
      "Initial State is  [0, 3, 3]\n",
      "episode 4598, reward 770.0, memory_length 2000, epsilon 0.010038766466684862, time 728.0, rides 137\n",
      "Initial State is  [2, 6, 0]\n",
      "episode 4599, reward 988.0, memory_length 2000, epsilon 0.010028727700218176, time 729.0, rides 126\n",
      "Initial State is  [0, 16, 0]\n",
      "episode 4600, reward 943.0, memory_length 2000, epsilon 0.010018698972517958, time 732.0, rides 136\n",
      "Initial State is  [2, 3, 0]\n",
      "episode 4601, reward 903.0, memory_length 2000, epsilon 0.01000868027354544, time 733.0, rides 136\n",
      "Initial State is  [0, 3, 6]\n",
      "episode 4602, reward 928.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 128\n",
      "Initial State is  [2, 21, 2]\n",
      "episode 4603, reward 1120.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 143\n",
      "Initial State is  [3, 1, 2]\n",
      "episode 4604, reward 1029.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 134\n",
      "Initial State is  [3, 11, 6]\n",
      "episode 4605, reward 952.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 138\n",
      "Initial State is  [1, 11, 5]\n",
      "episode 4606, reward 1008.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 126\n",
      "Initial State is  [4, 18, 2]\n",
      "episode 4607, reward 942.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 139\n",
      "Initial State is  [2, 3, 2]\n",
      "episode 4608, reward 940.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 136\n",
      "Initial State is  [4, 16, 6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 4609, reward 888.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 144\n",
      "Initial State is  [3, 23, 5]\n",
      "episode 4610, reward 1062.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 132\n",
      "Initial State is  [2, 5, 0]\n",
      "episode 4611, reward 958.0, memory_length 2000, epsilon 0.009998671593271896, time 737.0, rides 126\n",
      "Initial State is  [0, 20, 2]\n",
      "episode 4612, reward 742.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 124\n",
      "Initial State is  [2, 10, 4]\n",
      "episode 4613, reward 954.0, memory_length 2000, epsilon 0.009998671593271896, time 738.0, rides 136\n",
      "Initial State is  [3, 16, 5]\n",
      "episode 4614, reward 773.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 131\n",
      "Initial State is  [4, 0, 2]\n",
      "episode 4615, reward 888.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 136\n",
      "Initial State is  [1, 4, 4]\n",
      "episode 4616, reward 712.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 125\n",
      "Initial State is  [2, 15, 2]\n",
      "episode 4617, reward 1078.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 135\n",
      "Initial State is  [4, 15, 5]\n",
      "episode 4618, reward 808.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 134\n",
      "Initial State is  [3, 21, 1]\n",
      "episode 4619, reward 893.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 136\n",
      "Initial State is  [0, 23, 6]\n",
      "episode 4620, reward 978.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 140\n",
      "Initial State is  [0, 21, 5]\n",
      "episode 4621, reward 847.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 139\n",
      "Initial State is  [3, 4, 5]\n",
      "episode 4622, reward 1169.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 128\n",
      "Initial State is  [0, 22, 6]\n",
      "episode 4623, reward 1430.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 136\n",
      "Initial State is  [4, 12, 1]\n",
      "episode 4624, reward 1012.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 145\n",
      "Initial State is  [2, 0, 6]\n",
      "episode 4625, reward 1069.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 126\n",
      "Initial State is  [0, 9, 3]\n",
      "episode 4626, reward 1053.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 137\n",
      "Initial State is  [0, 5, 3]\n",
      "episode 4627, reward 785.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 131\n",
      "Initial State is  [4, 14, 2]\n",
      "episode 4628, reward 1113.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 142\n",
      "Initial State is  [1, 17, 2]\n",
      "episode 4629, reward 1125.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 120\n",
      "Initial State is  [0, 10, 5]\n",
      "episode 4630, reward 1200.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 140\n",
      "Initial State is  [2, 11, 1]\n",
      "episode 4631, reward 842.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 131\n",
      "Initial State is  [2, 21, 6]\n",
      "episode 4632, reward 1131.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 147\n",
      "Initial State is  [0, 3, 4]\n",
      "episode 4633, reward 832.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 120\n",
      "Initial State is  [1, 21, 0]\n",
      "episode 4634, reward 876.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 124\n",
      "Initial State is  [1, 10, 0]\n",
      "episode 4635, reward 1057.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 136\n",
      "Initial State is  [4, 11, 2]\n",
      "episode 4636, reward 858.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 121\n",
      "Initial State is  [1, 7, 1]\n",
      "episode 4637, reward 1132.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 130\n",
      "Initial State is  [1, 17, 3]\n",
      "episode 4638, reward 726.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 123\n",
      "Initial State is  [1, 6, 0]\n",
      "episode 4639, reward 758.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 127\n",
      "Initial State is  [0, 5, 4]\n",
      "episode 4640, reward 1043.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 126\n",
      "Initial State is  [0, 4, 6]\n",
      "episode 4641, reward 871.0, memory_length 2000, epsilon 0.009998671593271896, time 736.0, rides 136\n",
      "Initial State is  [4, 0, 3]\n",
      "episode 4642, reward 778.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 124\n",
      "Initial State is  [4, 17, 5]\n",
      "episode 4643, reward 828.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 127\n",
      "Initial State is  [4, 6, 3]\n",
      "episode 4644, reward 748.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 129\n",
      "Initial State is  [2, 18, 3]\n",
      "episode 4645, reward 990.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 123\n",
      "Initial State is  [4, 22, 1]\n",
      "episode 4646, reward 650.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 132\n",
      "Initial State is  [3, 6, 4]\n",
      "episode 4647, reward 764.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 138\n",
      "Initial State is  [2, 12, 6]\n",
      "episode 4648, reward 857.0, memory_length 2000, epsilon 0.009998671593271896, time 721.0, rides 124\n",
      "Initial State is  [0, 19, 0]\n",
      "episode 4649, reward 782.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 140\n",
      "Initial State is  [0, 17, 5]\n",
      "episode 4650, reward 789.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 130\n",
      "Initial State is  [2, 8, 1]\n",
      "episode 4651, reward 1062.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 135\n",
      "Initial State is  [0, 16, 0]\n",
      "episode 4652, reward 1098.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 139\n",
      "Initial State is  [3, 10, 4]\n",
      "episode 4653, reward 973.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 128\n",
      "Initial State is  [3, 1, 0]\n",
      "episode 4654, reward 877.0, memory_length 2000, epsilon 0.009998671593271896, time 721.0, rides 132\n",
      "Initial State is  [0, 23, 6]\n",
      "episode 4655, reward 865.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 138\n",
      "Initial State is  [4, 16, 1]\n",
      "episode 4656, reward 688.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 135\n",
      "Initial State is  [1, 8, 3]\n",
      "episode 4657, reward 913.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 125\n",
      "Initial State is  [1, 13, 0]\n",
      "episode 4658, reward 719.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 143\n",
      "Initial State is  [2, 23, 1]\n",
      "episode 4659, reward 840.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 141\n",
      "Initial State is  [1, 18, 4]\n",
      "episode 4660, reward 1011.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 134\n",
      "Initial State is  [2, 2, 0]\n",
      "episode 4661, reward 891.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 142\n",
      "Initial State is  [4, 23, 5]\n",
      "episode 4662, reward 885.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 135\n",
      "Initial State is  [4, 17, 3]\n",
      "episode 4663, reward 676.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 132\n",
      "Initial State is  [1, 22, 4]\n",
      "episode 4664, reward 1181.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 118\n",
      "Initial State is  [0, 15, 5]\n",
      "episode 4665, reward 1005.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 132\n",
      "Initial State is  [1, 17, 4]\n",
      "episode 4666, reward 821.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 134\n",
      "Initial State is  [4, 23, 5]\n",
      "episode 4667, reward 922.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 142\n",
      "Initial State is  [1, 5, 4]\n",
      "episode 4668, reward 874.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 135\n",
      "Initial State is  [0, 5, 0]\n",
      "episode 4669, reward 927.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 137\n",
      "Initial State is  [3, 1, 2]\n",
      "episode 4670, reward 567.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 125\n",
      "Initial State is  [0, 5, 1]\n",
      "episode 4671, reward 907.0, memory_length 2000, epsilon 0.009998671593271896, time 721.0, rides 134\n",
      "Initial State is  [0, 10, 4]\n",
      "episode 4672, reward 1007.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 130\n",
      "Initial State is  [2, 2, 6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 4673, reward 1064.0, memory_length 2000, epsilon 0.009998671593271896, time 737.0, rides 145\n",
      "Initial State is  [2, 23, 0]\n",
      "episode 4674, reward 762.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 139\n",
      "Initial State is  [1, 21, 3]\n",
      "episode 4675, reward 882.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 137\n",
      "Initial State is  [0, 11, 1]\n",
      "episode 4676, reward 857.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 127\n",
      "Initial State is  [3, 5, 6]\n",
      "episode 4677, reward 741.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 127\n",
      "Initial State is  [2, 23, 6]\n",
      "episode 4678, reward 1003.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 145\n",
      "Initial State is  [4, 13, 6]\n",
      "episode 4679, reward 992.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 132\n",
      "Initial State is  [1, 1, 1]\n",
      "episode 4680, reward 667.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 125\n",
      "Initial State is  [4, 17, 2]\n",
      "episode 4681, reward 969.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 141\n",
      "Initial State is  [2, 1, 2]\n",
      "episode 4682, reward 1125.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 127\n",
      "Initial State is  [3, 1, 0]\n",
      "episode 4683, reward 819.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 150\n",
      "Initial State is  [0, 10, 6]\n",
      "episode 4684, reward 930.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 135\n",
      "Initial State is  [4, 18, 1]\n",
      "episode 4685, reward 674.0, memory_length 2000, epsilon 0.009998671593271896, time 721.0, rides 123\n",
      "Initial State is  [1, 15, 0]\n",
      "episode 4686, reward 702.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 142\n",
      "Initial State is  [0, 2, 5]\n",
      "episode 4687, reward 1066.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 150\n",
      "Initial State is  [0, 6, 4]\n",
      "episode 4688, reward 914.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 148\n",
      "Initial State is  [4, 16, 2]\n",
      "episode 4689, reward 822.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 126\n",
      "Initial State is  [1, 10, 5]\n",
      "episode 4690, reward 749.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 127\n",
      "Initial State is  [3, 13, 0]\n",
      "episode 4691, reward 1105.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 133\n",
      "Initial State is  [3, 21, 4]\n",
      "episode 4692, reward 752.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 137\n",
      "Initial State is  [2, 4, 6]\n",
      "episode 4693, reward 866.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 148\n",
      "Initial State is  [3, 14, 0]\n",
      "episode 4694, reward 955.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 137\n",
      "Initial State is  [3, 23, 4]\n",
      "episode 4695, reward 867.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 130\n",
      "Initial State is  [2, 22, 1]\n",
      "episode 4696, reward 1197.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 130\n",
      "Initial State is  [2, 11, 1]\n",
      "episode 4697, reward 1133.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 120\n",
      "Initial State is  [4, 10, 2]\n",
      "episode 4698, reward 1121.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 144\n",
      "Initial State is  [1, 12, 5]\n",
      "episode 4699, reward 939.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 130\n",
      "Initial State is  [2, 19, 2]\n",
      "episode 4700, reward 1041.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 128\n",
      "Initial State is  [0, 5, 0]\n",
      "episode 4701, reward 861.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 139\n",
      "Initial State is  [0, 11, 6]\n",
      "episode 4702, reward 1044.0, memory_length 2000, epsilon 0.009998671593271896, time 737.0, rides 128\n",
      "Initial State is  [1, 18, 1]\n",
      "episode 4703, reward 1030.0, memory_length 2000, epsilon 0.009998671593271896, time 736.0, rides 134\n",
      "Initial State is  [2, 17, 0]\n",
      "episode 4704, reward 809.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 132\n",
      "Initial State is  [2, 13, 3]\n",
      "episode 4705, reward 944.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 147\n",
      "Initial State is  [3, 21, 0]\n",
      "episode 4706, reward 966.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 145\n",
      "Initial State is  [2, 17, 1]\n",
      "episode 4707, reward 999.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 133\n",
      "Initial State is  [2, 9, 3]\n",
      "episode 4708, reward 1093.0, memory_length 2000, epsilon 0.009998671593271896, time 740.0, rides 133\n",
      "Initial State is  [2, 12, 0]\n",
      "episode 4709, reward 829.0, memory_length 2000, epsilon 0.009998671593271896, time 737.0, rides 140\n",
      "Initial State is  [4, 9, 1]\n",
      "episode 4710, reward 1068.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 131\n",
      "Initial State is  [3, 6, 0]\n",
      "episode 4711, reward 1002.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 136\n",
      "Initial State is  [4, 21, 3]\n",
      "episode 4712, reward 1026.0, memory_length 2000, epsilon 0.009998671593271896, time 738.0, rides 147\n",
      "Initial State is  [0, 7, 3]\n",
      "episode 4713, reward 823.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 133\n",
      "Initial State is  [1, 23, 1]\n",
      "episode 4714, reward 920.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 140\n",
      "Initial State is  [2, 18, 3]\n",
      "episode 4715, reward 818.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 128\n",
      "Initial State is  [0, 7, 5]\n",
      "episode 4716, reward 830.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 122\n",
      "Initial State is  [4, 16, 5]\n",
      "episode 4717, reward 537.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 120\n",
      "Initial State is  [0, 3, 4]\n",
      "episode 4718, reward 979.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 128\n",
      "Initial State is  [3, 10, 0]\n",
      "episode 4719, reward 882.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 130\n",
      "Initial State is  [2, 15, 6]\n",
      "episode 4720, reward 650.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 138\n",
      "Initial State is  [1, 8, 6]\n",
      "episode 4721, reward 925.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 138\n",
      "Initial State is  [0, 11, 4]\n",
      "episode 4722, reward 921.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 130\n",
      "Initial State is  [3, 0, 6]\n",
      "episode 4723, reward 974.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 129\n",
      "Initial State is  [1, 16, 3]\n",
      "episode 4724, reward 1048.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 135\n",
      "Initial State is  [0, 17, 3]\n",
      "episode 4725, reward 1067.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 142\n",
      "Initial State is  [0, 13, 1]\n",
      "episode 4726, reward 1004.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 138\n",
      "Initial State is  [1, 10, 1]\n",
      "episode 4727, reward 628.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 126\n",
      "Initial State is  [3, 7, 1]\n",
      "episode 4728, reward 799.0, memory_length 2000, epsilon 0.009998671593271896, time 721.0, rides 132\n",
      "Initial State is  [0, 22, 6]\n",
      "episode 4729, reward 946.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 135\n",
      "Initial State is  [0, 21, 0]\n",
      "episode 4730, reward 691.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 150\n",
      "Initial State is  [0, 9, 2]\n",
      "episode 4731, reward 887.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 140\n",
      "Initial State is  [1, 6, 0]\n",
      "episode 4732, reward 1073.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 133\n",
      "Initial State is  [4, 19, 2]\n",
      "episode 4733, reward 822.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 131\n",
      "Initial State is  [4, 17, 4]\n",
      "episode 4734, reward 1020.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 130\n",
      "Initial State is  [3, 0, 5]\n",
      "episode 4735, reward 883.0, memory_length 2000, epsilon 0.009998671593271896, time 736.0, rides 132\n",
      "Initial State is  [4, 6, 1]\n",
      "episode 4736, reward 1119.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 124\n",
      "Initial State is  [3, 17, 1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 4737, reward 758.0, memory_length 2000, epsilon 0.009998671593271896, time 737.0, rides 120\n",
      "Initial State is  [0, 12, 4]\n",
      "episode 4738, reward 710.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 137\n",
      "Initial State is  [0, 11, 5]\n",
      "episode 4739, reward 1041.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 127\n",
      "Initial State is  [0, 15, 2]\n",
      "episode 4740, reward 947.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 124\n",
      "Initial State is  [3, 8, 3]\n",
      "episode 4741, reward 1156.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 143\n",
      "Initial State is  [3, 7, 5]\n",
      "episode 4742, reward 1003.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 133\n",
      "Initial State is  [1, 18, 1]\n",
      "episode 4743, reward 873.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 133\n",
      "Initial State is  [1, 23, 4]\n",
      "episode 4744, reward 1099.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 132\n",
      "Initial State is  [0, 3, 0]\n",
      "episode 4745, reward 975.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 138\n",
      "Initial State is  [4, 9, 6]\n",
      "episode 4746, reward 1340.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 131\n",
      "Initial State is  [1, 6, 4]\n",
      "episode 4747, reward 991.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 132\n",
      "Initial State is  [3, 2, 3]\n",
      "episode 4748, reward 987.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 133\n",
      "Initial State is  [3, 18, 5]\n",
      "episode 4749, reward 739.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 129\n",
      "Initial State is  [4, 10, 1]\n",
      "episode 4750, reward 1103.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 139\n",
      "Initial State is  [0, 12, 1]\n",
      "episode 4751, reward 728.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 125\n",
      "Initial State is  [2, 20, 5]\n",
      "episode 4752, reward 864.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 124\n",
      "Initial State is  [4, 9, 5]\n",
      "episode 4753, reward 901.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 141\n",
      "Initial State is  [2, 10, 1]\n",
      "episode 4754, reward 869.0, memory_length 2000, epsilon 0.009998671593271896, time 736.0, rides 125\n",
      "Initial State is  [1, 1, 2]\n",
      "episode 4755, reward 1023.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 127\n",
      "Initial State is  [1, 9, 6]\n",
      "episode 4756, reward 901.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 118\n",
      "Initial State is  [4, 18, 3]\n",
      "episode 4757, reward 1225.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 120\n",
      "Initial State is  [0, 9, 6]\n",
      "episode 4758, reward 1111.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 125\n",
      "Initial State is  [1, 20, 5]\n",
      "episode 4759, reward 1009.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 135\n",
      "Initial State is  [4, 16, 2]\n",
      "episode 4760, reward 766.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 135\n",
      "Initial State is  [2, 23, 2]\n",
      "episode 4761, reward 975.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 138\n",
      "Initial State is  [3, 14, 1]\n",
      "episode 4762, reward 1060.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 138\n",
      "Initial State is  [3, 16, 4]\n",
      "episode 4763, reward 939.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 141\n",
      "Initial State is  [1, 20, 6]\n",
      "episode 4764, reward 1055.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 132\n",
      "Initial State is  [1, 8, 5]\n",
      "episode 4765, reward 662.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 129\n",
      "Initial State is  [3, 13, 0]\n",
      "episode 4766, reward 1080.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 135\n",
      "Initial State is  [3, 20, 2]\n",
      "episode 4767, reward 933.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 128\n",
      "Initial State is  [1, 2, 0]\n",
      "episode 4768, reward 922.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 135\n",
      "Initial State is  [0, 10, 3]\n",
      "episode 4769, reward 922.0, memory_length 2000, epsilon 0.009998671593271896, time 736.0, rides 137\n",
      "Initial State is  [0, 5, 6]\n",
      "episode 4770, reward 710.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 124\n",
      "Initial State is  [3, 9, 5]\n",
      "episode 4771, reward 865.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 139\n",
      "Initial State is  [0, 17, 3]\n",
      "episode 4772, reward 1013.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 133\n",
      "Initial State is  [3, 16, 1]\n",
      "episode 4773, reward 832.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 128\n",
      "Initial State is  [2, 17, 1]\n",
      "episode 4774, reward 956.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 140\n",
      "Initial State is  [3, 8, 4]\n",
      "episode 4775, reward 925.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 136\n",
      "Initial State is  [3, 20, 1]\n",
      "episode 4776, reward 1000.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 137\n",
      "Initial State is  [3, 14, 0]\n",
      "episode 4777, reward 1175.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 131\n",
      "Initial State is  [3, 2, 5]\n",
      "episode 4778, reward 1015.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 146\n",
      "Initial State is  [2, 11, 4]\n",
      "episode 4779, reward 1162.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 142\n",
      "Initial State is  [0, 13, 2]\n",
      "episode 4780, reward 1030.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 126\n",
      "Initial State is  [4, 7, 3]\n",
      "episode 4781, reward 1095.0, memory_length 2000, epsilon 0.009998671593271896, time 736.0, rides 137\n",
      "Initial State is  [3, 7, 3]\n",
      "episode 4782, reward 1017.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 134\n",
      "Initial State is  [2, 15, 4]\n",
      "episode 4783, reward 1042.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 141\n",
      "Initial State is  [1, 18, 6]\n",
      "episode 4784, reward 996.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 129\n",
      "Initial State is  [4, 2, 0]\n",
      "episode 4785, reward 1121.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 140\n",
      "Initial State is  [0, 21, 3]\n",
      "episode 4786, reward 1002.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 135\n",
      "Initial State is  [1, 8, 5]\n",
      "episode 4787, reward 1032.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 139\n",
      "Initial State is  [0, 10, 5]\n",
      "episode 4788, reward 840.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 140\n",
      "Initial State is  [2, 8, 3]\n",
      "episode 4789, reward 756.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 142\n",
      "Initial State is  [1, 12, 6]\n",
      "episode 4790, reward 677.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 128\n",
      "Initial State is  [1, 2, 5]\n",
      "episode 4791, reward 1171.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 131\n",
      "Initial State is  [2, 8, 5]\n",
      "episode 4792, reward 915.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 135\n",
      "Initial State is  [2, 17, 4]\n",
      "episode 4793, reward 1040.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 138\n",
      "Initial State is  [3, 21, 4]\n",
      "episode 4794, reward 1143.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 141\n",
      "Initial State is  [0, 20, 5]\n",
      "episode 4795, reward 960.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 134\n",
      "Initial State is  [0, 13, 3]\n",
      "episode 4796, reward 1101.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 148\n",
      "Initial State is  [0, 20, 2]\n",
      "episode 4797, reward 949.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 129\n",
      "Initial State is  [0, 4, 2]\n",
      "episode 4798, reward 974.0, memory_length 2000, epsilon 0.009998671593271896, time 721.0, rides 134\n",
      "Initial State is  [2, 19, 2]\n",
      "episode 4799, reward 862.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 122\n",
      "Initial State is  [4, 15, 6]\n",
      "episode 4800, reward 930.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 122\n",
      "Initial State is  [4, 0, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 4801, reward 946.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 121\n",
      "Initial State is  [0, 23, 4]\n",
      "episode 4802, reward 692.0, memory_length 2000, epsilon 0.009998671593271896, time 738.0, rides 136\n",
      "Initial State is  [1, 16, 1]\n",
      "episode 4803, reward 1327.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 124\n",
      "Initial State is  [0, 11, 3]\n",
      "episode 4804, reward 759.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 137\n",
      "Initial State is  [0, 15, 6]\n",
      "episode 4805, reward 1038.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 142\n",
      "Initial State is  [4, 10, 6]\n",
      "episode 4806, reward 519.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 134\n",
      "Initial State is  [2, 4, 6]\n",
      "episode 4807, reward 799.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 152\n",
      "Initial State is  [3, 19, 5]\n",
      "episode 4808, reward 844.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 140\n",
      "Initial State is  [1, 10, 3]\n",
      "episode 4809, reward 1171.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 142\n",
      "Initial State is  [2, 18, 3]\n",
      "episode 4810, reward 1071.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 136\n",
      "Initial State is  [1, 22, 6]\n",
      "episode 4811, reward 687.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 128\n",
      "Initial State is  [3, 2, 1]\n",
      "episode 4812, reward 956.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 139\n",
      "Initial State is  [0, 18, 5]\n",
      "episode 4813, reward 1212.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 140\n",
      "Initial State is  [1, 1, 2]\n",
      "episode 4814, reward 1009.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 125\n",
      "Initial State is  [4, 4, 2]\n",
      "episode 4815, reward 1111.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 135\n",
      "Initial State is  [4, 17, 0]\n",
      "episode 4816, reward 892.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 124\n",
      "Initial State is  [1, 9, 2]\n",
      "episode 4817, reward 743.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 126\n",
      "Initial State is  [0, 13, 5]\n",
      "episode 4818, reward 856.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 125\n",
      "Initial State is  [2, 20, 6]\n",
      "episode 4819, reward 1264.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 132\n",
      "Initial State is  [1, 17, 2]\n",
      "episode 4820, reward 888.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 121\n",
      "Initial State is  [3, 15, 1]\n",
      "episode 4821, reward 962.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 129\n",
      "Initial State is  [4, 2, 1]\n",
      "episode 4822, reward 1001.0, memory_length 2000, epsilon 0.009998671593271896, time 721.0, rides 142\n",
      "Initial State is  [2, 1, 5]\n",
      "episode 4823, reward 926.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 133\n",
      "Initial State is  [0, 17, 3]\n",
      "episode 4824, reward 1194.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 129\n",
      "Initial State is  [3, 18, 0]\n",
      "episode 4825, reward 1218.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 136\n",
      "Initial State is  [4, 13, 2]\n",
      "episode 4826, reward 873.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 129\n",
      "Initial State is  [3, 20, 1]\n",
      "episode 4827, reward 1142.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 124\n",
      "Initial State is  [1, 6, 6]\n",
      "episode 4828, reward 936.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 137\n",
      "Initial State is  [2, 3, 4]\n",
      "episode 4829, reward 1324.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 134\n",
      "Initial State is  [4, 14, 5]\n",
      "episode 4830, reward 983.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 131\n",
      "Initial State is  [2, 22, 5]\n",
      "episode 4831, reward 916.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 138\n",
      "Initial State is  [1, 2, 5]\n",
      "episode 4832, reward 1119.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 134\n",
      "Initial State is  [4, 18, 3]\n",
      "episode 4833, reward 852.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 136\n",
      "Initial State is  [3, 5, 0]\n",
      "episode 4834, reward 780.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 141\n",
      "Initial State is  [3, 15, 3]\n",
      "episode 4835, reward 502.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 148\n",
      "Initial State is  [4, 4, 1]\n",
      "episode 4836, reward 1094.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 125\n",
      "Initial State is  [2, 10, 5]\n",
      "episode 4837, reward 821.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 136\n",
      "Initial State is  [1, 12, 4]\n",
      "episode 4838, reward 1232.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 137\n",
      "Initial State is  [2, 10, 1]\n",
      "episode 4839, reward 1103.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 147\n",
      "Initial State is  [4, 8, 1]\n",
      "episode 4840, reward 1105.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 131\n",
      "Initial State is  [4, 0, 4]\n",
      "episode 4841, reward 928.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 142\n",
      "Initial State is  [0, 18, 1]\n",
      "episode 4842, reward 1016.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 133\n",
      "Initial State is  [3, 15, 0]\n",
      "episode 4843, reward 1000.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 135\n",
      "Initial State is  [1, 19, 1]\n",
      "episode 4844, reward 956.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 131\n",
      "Initial State is  [0, 22, 1]\n",
      "episode 4845, reward 1031.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 125\n",
      "Initial State is  [4, 10, 3]\n",
      "episode 4846, reward 1062.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 133\n",
      "Initial State is  [4, 18, 0]\n",
      "episode 4847, reward 966.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 141\n",
      "Initial State is  [3, 16, 4]\n",
      "episode 4848, reward 1098.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 139\n",
      "Initial State is  [2, 0, 5]\n",
      "episode 4849, reward 898.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 126\n",
      "Initial State is  [1, 18, 4]\n",
      "episode 4850, reward 739.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 129\n",
      "Initial State is  [1, 18, 0]\n",
      "episode 4851, reward 781.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 128\n",
      "Initial State is  [4, 16, 6]\n",
      "episode 4852, reward 954.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 142\n",
      "Initial State is  [0, 1, 6]\n",
      "episode 4853, reward 1017.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 132\n",
      "Initial State is  [0, 16, 6]\n",
      "episode 4854, reward 1020.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 128\n",
      "Initial State is  [4, 4, 2]\n",
      "episode 4855, reward 977.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 129\n",
      "Initial State is  [0, 11, 2]\n",
      "episode 4856, reward 879.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 129\n",
      "Initial State is  [4, 9, 3]\n",
      "episode 4857, reward 1072.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 140\n",
      "Initial State is  [4, 10, 4]\n",
      "episode 4858, reward 939.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 137\n",
      "Initial State is  [1, 11, 2]\n",
      "episode 4859, reward 551.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 137\n",
      "Initial State is  [0, 9, 5]\n",
      "episode 4860, reward 1025.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 143\n",
      "Initial State is  [4, 2, 0]\n",
      "episode 4861, reward 1113.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 143\n",
      "Initial State is  [4, 19, 5]\n",
      "episode 4862, reward 1002.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 133\n",
      "Initial State is  [1, 11, 0]\n",
      "episode 4863, reward 646.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 123\n",
      "Initial State is  [1, 14, 3]\n",
      "episode 4864, reward 967.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 132\n",
      "Initial State is  [4, 15, 1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 4865, reward 908.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 129\n",
      "Initial State is  [0, 9, 4]\n",
      "episode 4866, reward 1262.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 125\n",
      "Initial State is  [3, 2, 1]\n",
      "episode 4867, reward 1124.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 132\n",
      "Initial State is  [0, 4, 1]\n",
      "episode 4868, reward 1046.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 133\n",
      "Initial State is  [4, 15, 6]\n",
      "episode 4869, reward 1254.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 141\n",
      "Initial State is  [3, 18, 3]\n",
      "episode 4870, reward 1049.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 135\n",
      "Initial State is  [4, 2, 0]\n",
      "episode 4871, reward 1072.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 135\n",
      "Initial State is  [0, 21, 3]\n",
      "episode 4872, reward 932.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 136\n",
      "Initial State is  [0, 13, 3]\n",
      "episode 4873, reward 1113.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 144\n",
      "Initial State is  [0, 7, 0]\n",
      "episode 4874, reward 915.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 131\n",
      "Initial State is  [2, 16, 3]\n",
      "episode 4875, reward 1001.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 133\n",
      "Initial State is  [1, 12, 3]\n",
      "episode 4876, reward 865.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 138\n",
      "Initial State is  [4, 11, 0]\n",
      "episode 4877, reward 875.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 130\n",
      "Initial State is  [3, 8, 3]\n",
      "episode 4878, reward 827.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 138\n",
      "Initial State is  [4, 22, 0]\n",
      "episode 4879, reward 1245.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 127\n",
      "Initial State is  [0, 22, 5]\n",
      "episode 4880, reward 868.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 117\n",
      "Initial State is  [1, 2, 0]\n",
      "episode 4881, reward 911.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 131\n",
      "Initial State is  [0, 20, 4]\n",
      "episode 4882, reward 1158.0, memory_length 2000, epsilon 0.009998671593271896, time 736.0, rides 119\n",
      "Initial State is  [0, 13, 3]\n",
      "episode 4883, reward 928.0, memory_length 2000, epsilon 0.009998671593271896, time 721.0, rides 128\n",
      "Initial State is  [3, 1, 5]\n",
      "episode 4884, reward 987.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 125\n",
      "Initial State is  [4, 3, 4]\n",
      "episode 4885, reward 847.0, memory_length 2000, epsilon 0.009998671593271896, time 738.0, rides 127\n",
      "Initial State is  [0, 19, 1]\n",
      "episode 4886, reward 918.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 140\n",
      "Initial State is  [0, 12, 5]\n",
      "episode 4887, reward 676.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 122\n",
      "Initial State is  [0, 6, 1]\n",
      "episode 4888, reward 1044.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 132\n",
      "Initial State is  [3, 4, 4]\n",
      "episode 4889, reward 942.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 131\n",
      "Initial State is  [1, 9, 6]\n",
      "episode 4890, reward 1068.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 117\n",
      "Initial State is  [4, 22, 6]\n",
      "episode 4891, reward 940.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 138\n",
      "Initial State is  [0, 8, 6]\n",
      "episode 4892, reward 995.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 145\n",
      "Initial State is  [1, 16, 0]\n",
      "episode 4893, reward 725.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 120\n",
      "Initial State is  [3, 5, 3]\n",
      "episode 4894, reward 656.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 125\n",
      "Initial State is  [4, 21, 5]\n",
      "episode 4895, reward 904.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 131\n",
      "Initial State is  [0, 17, 6]\n",
      "episode 4896, reward 808.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 134\n",
      "Initial State is  [3, 15, 0]\n",
      "episode 4897, reward 923.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 132\n",
      "Initial State is  [2, 15, 0]\n",
      "episode 4898, reward 908.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 134\n",
      "Initial State is  [3, 1, 3]\n",
      "episode 4899, reward 1080.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 139\n",
      "Initial State is  [2, 3, 3]\n",
      "episode 4900, reward 1049.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 124\n",
      "Initial State is  [3, 3, 6]\n",
      "episode 4901, reward 1029.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 134\n",
      "Initial State is  [1, 20, 1]\n",
      "episode 4902, reward 870.0, memory_length 2000, epsilon 0.009998671593271896, time 739.0, rides 134\n",
      "Initial State is  [1, 20, 5]\n",
      "episode 4903, reward 846.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 132\n",
      "Initial State is  [0, 5, 1]\n",
      "episode 4904, reward 1091.0, memory_length 2000, epsilon 0.009998671593271896, time 736.0, rides 125\n",
      "Initial State is  [2, 4, 0]\n",
      "episode 4905, reward 763.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 124\n",
      "Initial State is  [1, 9, 6]\n",
      "episode 4906, reward 947.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 124\n",
      "Initial State is  [1, 2, 0]\n",
      "episode 4907, reward 909.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 137\n",
      "Initial State is  [2, 19, 6]\n",
      "episode 4908, reward 841.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 116\n",
      "Initial State is  [0, 7, 3]\n",
      "episode 4909, reward 1105.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 123\n",
      "Initial State is  [4, 20, 0]\n",
      "episode 4910, reward 1086.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 132\n",
      "Initial State is  [1, 18, 3]\n",
      "episode 4911, reward 977.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 127\n",
      "Initial State is  [1, 0, 0]\n",
      "episode 4912, reward 896.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 129\n",
      "Initial State is  [0, 10, 6]\n",
      "episode 4913, reward 905.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 126\n",
      "Initial State is  [3, 6, 0]\n",
      "episode 4914, reward 815.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 133\n",
      "Initial State is  [3, 21, 6]\n",
      "episode 4915, reward 1135.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 151\n",
      "Initial State is  [0, 19, 0]\n",
      "episode 4916, reward 880.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 129\n",
      "Initial State is  [1, 7, 6]\n",
      "episode 4917, reward 660.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 124\n",
      "Initial State is  [4, 4, 2]\n",
      "episode 4918, reward 701.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 126\n",
      "Initial State is  [1, 2, 3]\n",
      "episode 4919, reward 1089.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 148\n",
      "Initial State is  [1, 4, 0]\n",
      "episode 4920, reward 1013.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 129\n",
      "Initial State is  [1, 5, 2]\n",
      "episode 4921, reward 826.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 119\n",
      "Initial State is  [1, 1, 1]\n",
      "episode 4922, reward 827.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 132\n",
      "Initial State is  [0, 10, 1]\n",
      "episode 4923, reward 817.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 137\n",
      "Initial State is  [3, 2, 0]\n",
      "episode 4924, reward 755.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 124\n",
      "Initial State is  [2, 0, 6]\n",
      "episode 4925, reward 1330.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 129\n",
      "Initial State is  [2, 18, 6]\n",
      "episode 4926, reward 1045.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 146\n",
      "Initial State is  [0, 13, 2]\n",
      "episode 4927, reward 1216.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 131\n",
      "Initial State is  [3, 11, 5]\n",
      "episode 4928, reward 928.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 128\n",
      "Initial State is  [1, 16, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 4929, reward 968.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 127\n",
      "Initial State is  [4, 7, 4]\n",
      "episode 4930, reward 691.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 129\n",
      "Initial State is  [1, 1, 1]\n",
      "episode 4931, reward 953.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 124\n",
      "Initial State is  [2, 19, 1]\n",
      "episode 4932, reward 1051.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 120\n",
      "Initial State is  [4, 8, 2]\n",
      "episode 4933, reward 1125.0, memory_length 2000, epsilon 0.009998671593271896, time 738.0, rides 153\n",
      "Initial State is  [3, 11, 6]\n",
      "episode 4934, reward 1059.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 126\n",
      "Initial State is  [1, 19, 3]\n",
      "episode 4935, reward 997.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 131\n",
      "Initial State is  [4, 20, 0]\n",
      "episode 4936, reward 1052.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 126\n",
      "Initial State is  [0, 12, 4]\n",
      "episode 4937, reward 1105.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 127\n",
      "Initial State is  [2, 21, 5]\n",
      "episode 4938, reward 996.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 137\n",
      "Initial State is  [3, 18, 5]\n",
      "episode 4939, reward 817.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 139\n",
      "Initial State is  [4, 17, 1]\n",
      "episode 4940, reward 816.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 139\n",
      "Initial State is  [3, 5, 4]\n",
      "episode 4941, reward 1056.0, memory_length 2000, epsilon 0.009998671593271896, time 736.0, rides 136\n",
      "Initial State is  [4, 6, 4]\n",
      "episode 4942, reward 1000.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 138\n",
      "Initial State is  [2, 6, 1]\n",
      "episode 4943, reward 1070.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 132\n",
      "Initial State is  [4, 6, 3]\n",
      "episode 4944, reward 1009.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 133\n",
      "Initial State is  [3, 16, 4]\n",
      "episode 4945, reward 925.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 140\n",
      "Initial State is  [3, 12, 6]\n",
      "episode 4946, reward 1164.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 131\n",
      "Initial State is  [4, 1, 1]\n",
      "episode 4947, reward 817.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 146\n",
      "Initial State is  [0, 12, 1]\n",
      "episode 4948, reward 1000.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 130\n",
      "Initial State is  [3, 4, 2]\n",
      "episode 4949, reward 981.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 114\n",
      "Initial State is  [4, 19, 1]\n",
      "episode 4950, reward 763.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 121\n",
      "Initial State is  [2, 5, 4]\n",
      "episode 4951, reward 1060.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 137\n",
      "Initial State is  [4, 16, 2]\n",
      "episode 4952, reward 780.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 118\n",
      "Initial State is  [0, 21, 5]\n",
      "episode 4953, reward 1114.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 131\n",
      "Initial State is  [4, 9, 3]\n",
      "episode 4954, reward 901.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 131\n",
      "Initial State is  [2, 6, 3]\n",
      "episode 4955, reward 1132.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 122\n",
      "Initial State is  [2, 19, 6]\n",
      "episode 4956, reward 840.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 136\n",
      "Initial State is  [0, 9, 6]\n",
      "episode 4957, reward 952.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 135\n",
      "Initial State is  [1, 18, 1]\n",
      "episode 4958, reward 950.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 137\n",
      "Initial State is  [1, 5, 2]\n",
      "episode 4959, reward 1054.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 121\n",
      "Initial State is  [4, 22, 2]\n",
      "episode 4960, reward 801.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 125\n",
      "Initial State is  [0, 13, 1]\n",
      "episode 4961, reward 847.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 134\n",
      "Initial State is  [1, 0, 0]\n",
      "episode 4962, reward 866.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 135\n",
      "Initial State is  [2, 3, 3]\n",
      "episode 4963, reward 919.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 124\n",
      "Initial State is  [0, 22, 5]\n",
      "episode 4964, reward 1042.0, memory_length 2000, epsilon 0.009998671593271896, time 736.0, rides 134\n",
      "Initial State is  [4, 11, 5]\n",
      "episode 4965, reward 860.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 135\n",
      "Initial State is  [1, 16, 6]\n",
      "episode 4966, reward 1195.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 122\n",
      "Initial State is  [3, 23, 0]\n",
      "episode 4967, reward 1108.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 134\n",
      "Initial State is  [2, 19, 1]\n",
      "episode 4968, reward 843.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 142\n",
      "Initial State is  [4, 11, 6]\n",
      "episode 4969, reward 752.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 138\n",
      "Initial State is  [4, 4, 2]\n",
      "episode 4970, reward 1138.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 129\n",
      "Initial State is  [1, 4, 4]\n",
      "episode 4971, reward 908.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 133\n",
      "Initial State is  [4, 10, 5]\n",
      "episode 4972, reward 952.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 128\n",
      "Initial State is  [4, 6, 0]\n",
      "episode 4973, reward 1166.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 144\n",
      "Initial State is  [1, 6, 3]\n",
      "episode 4974, reward 753.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 140\n",
      "Initial State is  [1, 6, 4]\n",
      "episode 4975, reward 1007.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 131\n",
      "Initial State is  [1, 14, 1]\n",
      "episode 4976, reward 914.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 131\n",
      "Initial State is  [4, 10, 1]\n",
      "episode 4977, reward 859.0, memory_length 2000, epsilon 0.009998671593271896, time 741.0, rides 130\n",
      "Initial State is  [0, 0, 2]\n",
      "episode 4978, reward 863.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 124\n",
      "Initial State is  [0, 7, 6]\n",
      "episode 4979, reward 1137.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 136\n",
      "Initial State is  [1, 7, 4]\n",
      "episode 4980, reward 746.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 125\n",
      "Initial State is  [3, 20, 3]\n",
      "episode 4981, reward 825.0, memory_length 2000, epsilon 0.009998671593271896, time 721.0, rides 124\n",
      "Initial State is  [4, 9, 0]\n",
      "episode 4982, reward 1028.0, memory_length 2000, epsilon 0.009998671593271896, time 736.0, rides 128\n",
      "Initial State is  [2, 9, 3]\n",
      "episode 4983, reward 863.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 142\n",
      "Initial State is  [4, 12, 2]\n",
      "episode 4984, reward 1093.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 129\n",
      "Initial State is  [3, 10, 0]\n",
      "episode 4985, reward 788.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 140\n",
      "Initial State is  [0, 16, 4]\n",
      "episode 4986, reward 1169.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 123\n",
      "Initial State is  [3, 20, 5]\n",
      "episode 4987, reward 810.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 147\n",
      "Initial State is  [0, 8, 3]\n",
      "episode 4988, reward 1020.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 134\n",
      "Initial State is  [1, 1, 6]\n",
      "episode 4989, reward 973.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 131\n",
      "Initial State is  [0, 10, 6]\n",
      "episode 4990, reward 978.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 143\n",
      "Initial State is  [2, 14, 4]\n",
      "episode 4991, reward 1046.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 137\n",
      "Initial State is  [0, 19, 4]\n",
      "episode 4992, reward 1052.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 132\n",
      "Initial State is  [2, 8, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 4993, reward 727.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 119\n",
      "Initial State is  [3, 17, 6]\n",
      "episode 4994, reward 876.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 135\n",
      "Initial State is  [3, 1, 4]\n",
      "episode 4995, reward 804.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 121\n",
      "Initial State is  [0, 18, 2]\n",
      "episode 4996, reward 992.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 135\n",
      "Initial State is  [2, 12, 6]\n",
      "episode 4997, reward 898.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 140\n",
      "Initial State is  [4, 14, 1]\n",
      "episode 4998, reward 1044.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 136\n",
      "Initial State is  [3, 3, 4]\n",
      "episode 4999, reward 1037.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 138\n",
      "Initial State is  [1, 15, 6]\n",
      "episode 5000, reward 1049.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 135\n",
      "Initial State is  [0, 0, 2]\n",
      "episode 5001, reward 854.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 134\n",
      "Initial State is  [1, 15, 6]\n",
      "episode 5002, reward 1043.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 138\n",
      "Initial State is  [4, 9, 4]\n",
      "episode 5003, reward 646.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 131\n",
      "Initial State is  [3, 11, 5]\n",
      "episode 5004, reward 965.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 126\n",
      "Initial State is  [1, 4, 2]\n",
      "episode 5005, reward 1104.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 127\n",
      "Initial State is  [2, 0, 3]\n",
      "episode 5006, reward 962.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 126\n",
      "Initial State is  [4, 12, 3]\n",
      "episode 5007, reward 1054.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 130\n",
      "Initial State is  [1, 2, 4]\n",
      "episode 5008, reward 1005.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 126\n",
      "Initial State is  [4, 1, 4]\n",
      "episode 5009, reward 726.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 125\n",
      "Initial State is  [3, 1, 5]\n",
      "episode 5010, reward 1003.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 125\n",
      "Initial State is  [2, 20, 4]\n",
      "episode 5011, reward 1230.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 139\n",
      "Initial State is  [1, 8, 4]\n",
      "episode 5012, reward 1113.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 129\n",
      "Initial State is  [2, 1, 1]\n",
      "episode 5013, reward 881.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 122\n",
      "Initial State is  [0, 0, 4]\n",
      "episode 5014, reward 835.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 145\n",
      "Initial State is  [3, 16, 6]\n",
      "episode 5015, reward 1045.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 132\n",
      "Initial State is  [2, 6, 1]\n",
      "episode 5016, reward 963.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 141\n",
      "Initial State is  [0, 22, 5]\n",
      "episode 5017, reward 971.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 125\n",
      "Initial State is  [0, 7, 5]\n",
      "episode 5018, reward 758.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 132\n",
      "Initial State is  [2, 21, 5]\n",
      "episode 5019, reward 1050.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 130\n",
      "Initial State is  [0, 22, 2]\n",
      "episode 5020, reward 1003.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 125\n",
      "Initial State is  [0, 16, 3]\n",
      "episode 5021, reward 862.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 127\n",
      "Initial State is  [3, 9, 5]\n",
      "episode 5022, reward 728.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 134\n",
      "Initial State is  [0, 19, 3]\n",
      "episode 5023, reward 1166.0, memory_length 2000, epsilon 0.009998671593271896, time 739.0, rides 135\n",
      "Initial State is  [0, 3, 1]\n",
      "episode 5024, reward 1101.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 137\n",
      "Initial State is  [4, 20, 1]\n",
      "episode 5025, reward 971.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 127\n",
      "Initial State is  [3, 0, 6]\n",
      "episode 5026, reward 635.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 127\n",
      "Initial State is  [3, 1, 3]\n",
      "episode 5027, reward 1058.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 133\n",
      "Initial State is  [0, 11, 1]\n",
      "episode 5028, reward 1170.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 143\n",
      "Initial State is  [4, 10, 3]\n",
      "episode 5029, reward 847.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 131\n",
      "Initial State is  [1, 14, 6]\n",
      "episode 5030, reward 1028.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 119\n",
      "Initial State is  [0, 9, 1]\n",
      "episode 5031, reward 1171.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 131\n",
      "Initial State is  [3, 20, 4]\n",
      "episode 5032, reward 935.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 129\n",
      "Initial State is  [2, 1, 5]\n",
      "episode 5033, reward 1019.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 131\n",
      "Initial State is  [3, 10, 1]\n",
      "episode 5034, reward 733.0, memory_length 2000, epsilon 0.009998671593271896, time 737.0, rides 136\n",
      "Initial State is  [3, 22, 5]\n",
      "episode 5035, reward 415.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 137\n",
      "Initial State is  [2, 6, 4]\n",
      "episode 5036, reward 879.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 140\n",
      "Initial State is  [3, 4, 5]\n",
      "episode 5037, reward 678.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 134\n",
      "Initial State is  [2, 13, 2]\n",
      "episode 5038, reward 628.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 127\n",
      "Initial State is  [3, 16, 2]\n",
      "episode 5039, reward 1061.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 128\n",
      "Initial State is  [4, 15, 6]\n",
      "episode 5040, reward 628.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 122\n",
      "Initial State is  [0, 23, 1]\n",
      "episode 5041, reward 887.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 123\n",
      "Initial State is  [4, 13, 2]\n",
      "episode 5042, reward 1199.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 128\n",
      "Initial State is  [4, 21, 3]\n",
      "episode 5043, reward 1006.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 128\n",
      "Initial State is  [4, 4, 1]\n",
      "episode 5044, reward 987.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 122\n",
      "Initial State is  [4, 21, 1]\n",
      "episode 5045, reward 852.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 123\n",
      "Initial State is  [0, 3, 3]\n",
      "episode 5046, reward 928.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 138\n",
      "Initial State is  [3, 5, 4]\n",
      "episode 5047, reward 759.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 140\n",
      "Initial State is  [1, 4, 6]\n",
      "episode 5048, reward 545.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 118\n",
      "Initial State is  [0, 5, 0]\n",
      "episode 5049, reward 755.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 122\n",
      "Initial State is  [4, 23, 1]\n",
      "episode 5050, reward 799.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 136\n",
      "Initial State is  [0, 2, 2]\n",
      "episode 5051, reward 1167.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 143\n",
      "Initial State is  [2, 17, 4]\n",
      "episode 5052, reward 883.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 130\n",
      "Initial State is  [1, 5, 5]\n",
      "episode 5053, reward 1033.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 123\n",
      "Initial State is  [3, 15, 0]\n",
      "episode 5054, reward 992.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 139\n",
      "Initial State is  [2, 15, 5]\n",
      "episode 5055, reward 779.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 113\n",
      "Initial State is  [1, 1, 3]\n",
      "episode 5056, reward 1019.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 137\n",
      "Initial State is  [0, 5, 1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 5057, reward 1179.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 127\n",
      "Initial State is  [0, 16, 4]\n",
      "episode 5058, reward 832.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 132\n",
      "Initial State is  [2, 3, 3]\n",
      "episode 5059, reward 703.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 132\n",
      "Initial State is  [3, 14, 6]\n",
      "episode 5060, reward 882.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 142\n",
      "Initial State is  [3, 3, 6]\n",
      "episode 5061, reward 656.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 130\n",
      "Initial State is  [3, 15, 3]\n",
      "episode 5062, reward 921.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 134\n",
      "Initial State is  [2, 4, 3]\n",
      "episode 5063, reward 872.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 134\n",
      "Initial State is  [2, 14, 1]\n",
      "episode 5064, reward 812.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 117\n",
      "Initial State is  [4, 8, 6]\n",
      "episode 5065, reward 895.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 114\n",
      "Initial State is  [4, 23, 1]\n",
      "episode 5066, reward 903.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 122\n",
      "Initial State is  [4, 13, 3]\n",
      "episode 5067, reward 987.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 138\n",
      "Initial State is  [1, 21, 3]\n",
      "episode 5068, reward 1157.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 137\n",
      "Initial State is  [1, 5, 3]\n",
      "episode 5069, reward 1310.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 130\n",
      "Initial State is  [3, 5, 1]\n",
      "episode 5070, reward 849.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 126\n",
      "Initial State is  [3, 2, 5]\n",
      "episode 5071, reward 1255.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 131\n",
      "Initial State is  [2, 16, 1]\n",
      "episode 5072, reward 995.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 136\n",
      "Initial State is  [3, 18, 6]\n",
      "episode 5073, reward 1234.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 126\n",
      "Initial State is  [1, 1, 6]\n",
      "episode 5074, reward 916.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 132\n",
      "Initial State is  [0, 11, 1]\n",
      "episode 5075, reward 1011.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 138\n",
      "Initial State is  [4, 13, 2]\n",
      "episode 5076, reward 1093.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 133\n",
      "Initial State is  [4, 5, 0]\n",
      "episode 5077, reward 1193.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 135\n",
      "Initial State is  [0, 17, 5]\n",
      "episode 5078, reward 969.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 126\n",
      "Initial State is  [3, 13, 1]\n",
      "episode 5079, reward 819.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 133\n",
      "Initial State is  [4, 22, 5]\n",
      "episode 5080, reward 1229.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 125\n",
      "Initial State is  [2, 17, 5]\n",
      "episode 5081, reward 674.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 126\n",
      "Initial State is  [0, 11, 6]\n",
      "episode 5082, reward 1259.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 144\n",
      "Initial State is  [3, 7, 1]\n",
      "episode 5083, reward 645.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 131\n",
      "Initial State is  [4, 19, 0]\n",
      "episode 5084, reward 1021.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 128\n",
      "Initial State is  [0, 7, 0]\n",
      "episode 5085, reward 952.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 134\n",
      "Initial State is  [1, 16, 5]\n",
      "episode 5086, reward 937.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 125\n",
      "Initial State is  [2, 1, 0]\n",
      "episode 5087, reward 756.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 130\n",
      "Initial State is  [0, 3, 4]\n",
      "episode 5088, reward 949.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 115\n",
      "Initial State is  [1, 20, 0]\n",
      "episode 5089, reward 1014.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 132\n",
      "Initial State is  [4, 4, 1]\n",
      "episode 5090, reward 1071.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 134\n",
      "Initial State is  [3, 16, 2]\n",
      "episode 5091, reward 1051.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 129\n",
      "Initial State is  [4, 18, 1]\n",
      "episode 5092, reward 1081.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 132\n",
      "Initial State is  [1, 1, 1]\n",
      "episode 5093, reward 976.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 135\n",
      "Initial State is  [4, 20, 4]\n",
      "episode 5094, reward 889.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 123\n",
      "Initial State is  [3, 14, 6]\n",
      "episode 5095, reward 1224.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 139\n",
      "Initial State is  [4, 17, 0]\n",
      "episode 5096, reward 981.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 126\n",
      "Initial State is  [2, 3, 4]\n",
      "episode 5097, reward 648.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 130\n",
      "Initial State is  [4, 11, 4]\n",
      "episode 5098, reward 774.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 126\n",
      "Initial State is  [0, 22, 0]\n",
      "episode 5099, reward 1148.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 131\n",
      "Initial State is  [3, 19, 3]\n",
      "episode 5100, reward 737.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 134\n",
      "Initial State is  [2, 22, 4]\n",
      "episode 5101, reward 914.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 125\n",
      "Initial State is  [1, 19, 4]\n",
      "episode 5102, reward 931.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 133\n",
      "Initial State is  [4, 4, 5]\n",
      "episode 5103, reward 963.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 120\n",
      "Initial State is  [1, 17, 3]\n",
      "episode 5104, reward 691.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 126\n",
      "Initial State is  [2, 13, 6]\n",
      "episode 5105, reward 975.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 137\n",
      "Initial State is  [4, 18, 3]\n",
      "episode 5106, reward 655.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 119\n",
      "Initial State is  [1, 3, 5]\n",
      "episode 5107, reward 768.0, memory_length 2000, epsilon 0.009998671593271896, time 721.0, rides 121\n",
      "Initial State is  [1, 17, 3]\n",
      "episode 5108, reward 1017.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 131\n",
      "Initial State is  [4, 13, 0]\n",
      "episode 5109, reward 868.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 133\n",
      "Initial State is  [4, 7, 5]\n",
      "episode 5110, reward 1150.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 129\n",
      "Initial State is  [2, 4, 6]\n",
      "episode 5111, reward 1007.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 134\n",
      "Initial State is  [3, 22, 2]\n",
      "episode 5112, reward 1078.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 138\n",
      "Initial State is  [1, 16, 5]\n",
      "episode 5113, reward 765.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 122\n",
      "Initial State is  [2, 15, 6]\n",
      "episode 5114, reward 1000.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 130\n",
      "Initial State is  [2, 18, 0]\n",
      "episode 5115, reward 927.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 125\n",
      "Initial State is  [4, 1, 2]\n",
      "episode 5116, reward 1034.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 139\n",
      "Initial State is  [2, 14, 4]\n",
      "episode 5117, reward 1054.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 133\n",
      "Initial State is  [2, 5, 3]\n",
      "episode 5118, reward 962.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 134\n",
      "Initial State is  [2, 1, 0]\n",
      "episode 5119, reward 1088.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 142\n",
      "Initial State is  [4, 15, 0]\n",
      "episode 5120, reward 692.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 135\n",
      "Initial State is  [3, 20, 3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 5121, reward 872.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 139\n",
      "Initial State is  [2, 11, 1]\n",
      "episode 5122, reward 1133.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 131\n",
      "Initial State is  [2, 8, 5]\n",
      "episode 5123, reward 1088.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 139\n",
      "Initial State is  [3, 3, 1]\n",
      "episode 5124, reward 902.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 123\n",
      "Initial State is  [2, 5, 0]\n",
      "episode 5125, reward 985.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 138\n",
      "Initial State is  [3, 7, 1]\n",
      "episode 5126, reward 951.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 129\n",
      "Initial State is  [4, 13, 1]\n",
      "episode 5127, reward 1055.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 126\n",
      "Initial State is  [0, 1, 4]\n",
      "episode 5128, reward 1002.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 130\n",
      "Initial State is  [3, 5, 0]\n",
      "episode 5129, reward 1042.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 137\n",
      "Initial State is  [0, 17, 5]\n",
      "episode 5130, reward 989.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 122\n",
      "Initial State is  [4, 22, 2]\n",
      "episode 5131, reward 888.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 126\n",
      "Initial State is  [4, 20, 3]\n",
      "episode 5132, reward 746.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 151\n",
      "Initial State is  [0, 5, 1]\n",
      "episode 5133, reward 874.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 141\n",
      "Initial State is  [4, 0, 5]\n",
      "episode 5134, reward 1184.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 129\n",
      "Initial State is  [0, 16, 0]\n",
      "episode 5135, reward 1166.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 130\n",
      "Initial State is  [0, 23, 3]\n",
      "episode 5136, reward 957.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 137\n",
      "Initial State is  [3, 11, 4]\n",
      "episode 5137, reward 738.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 117\n",
      "Initial State is  [2, 3, 5]\n",
      "episode 5138, reward 1215.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 142\n",
      "Initial State is  [2, 23, 2]\n",
      "episode 5139, reward 677.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 124\n",
      "Initial State is  [2, 14, 3]\n",
      "episode 5140, reward 907.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 118\n",
      "Initial State is  [2, 1, 4]\n",
      "episode 5141, reward 1288.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 138\n",
      "Initial State is  [3, 9, 6]\n",
      "episode 5142, reward 1196.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 133\n",
      "Initial State is  [1, 0, 6]\n",
      "episode 5143, reward 1053.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 129\n",
      "Initial State is  [2, 12, 2]\n",
      "episode 5144, reward 929.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 126\n",
      "Initial State is  [3, 22, 0]\n",
      "episode 5145, reward 1048.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 139\n",
      "Initial State is  [1, 16, 4]\n",
      "episode 5146, reward 741.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 124\n",
      "Initial State is  [4, 4, 5]\n",
      "episode 5147, reward 1060.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 129\n",
      "Initial State is  [4, 21, 5]\n",
      "episode 5148, reward 749.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 130\n",
      "Initial State is  [4, 12, 6]\n",
      "episode 5149, reward 1089.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 125\n",
      "Initial State is  [4, 0, 5]\n",
      "episode 5150, reward 1056.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 130\n",
      "Initial State is  [1, 3, 0]\n",
      "episode 5151, reward 1155.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 137\n",
      "Initial State is  [3, 19, 1]\n",
      "episode 5152, reward 1230.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 134\n",
      "Initial State is  [0, 12, 2]\n",
      "episode 5153, reward 810.0, memory_length 2000, epsilon 0.009998671593271896, time 737.0, rides 126\n",
      "Initial State is  [1, 8, 5]\n",
      "episode 5154, reward 811.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 140\n",
      "Initial State is  [1, 12, 0]\n",
      "episode 5155, reward 1055.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 137\n",
      "Initial State is  [4, 13, 0]\n",
      "episode 5156, reward 987.0, memory_length 2000, epsilon 0.009998671593271896, time 721.0, rides 122\n",
      "Initial State is  [3, 12, 5]\n",
      "episode 5157, reward 625.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 126\n",
      "Initial State is  [4, 3, 1]\n",
      "episode 5158, reward 857.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 133\n",
      "Initial State is  [4, 23, 1]\n",
      "episode 5159, reward 907.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 131\n",
      "Initial State is  [2, 0, 5]\n",
      "episode 5160, reward 1003.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 135\n",
      "Initial State is  [2, 9, 4]\n",
      "episode 5161, reward 695.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 137\n",
      "Initial State is  [1, 3, 5]\n",
      "episode 5162, reward 1225.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 138\n",
      "Initial State is  [1, 0, 6]\n",
      "episode 5163, reward 1063.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 133\n",
      "Initial State is  [1, 11, 2]\n",
      "episode 5164, reward 1150.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 134\n",
      "Initial State is  [1, 15, 3]\n",
      "episode 5165, reward 1055.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 133\n",
      "Initial State is  [3, 2, 5]\n",
      "episode 5166, reward 813.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 146\n",
      "Initial State is  [4, 17, 4]\n",
      "episode 5167, reward 1351.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 134\n",
      "Initial State is  [1, 18, 6]\n",
      "episode 5168, reward 1012.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 132\n",
      "Initial State is  [3, 22, 1]\n",
      "episode 5169, reward 918.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 136\n",
      "Initial State is  [1, 11, 2]\n",
      "episode 5170, reward 1016.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 122\n",
      "Initial State is  [0, 21, 2]\n",
      "episode 5171, reward 747.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 132\n",
      "Initial State is  [2, 14, 4]\n",
      "episode 5172, reward 871.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 126\n",
      "Initial State is  [0, 22, 0]\n",
      "episode 5173, reward 1086.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 145\n",
      "Initial State is  [2, 23, 4]\n",
      "episode 5174, reward 996.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 138\n",
      "Initial State is  [0, 20, 6]\n",
      "episode 5175, reward 870.0, memory_length 2000, epsilon 0.009998671593271896, time 738.0, rides 145\n",
      "Initial State is  [1, 7, 5]\n",
      "episode 5176, reward 897.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 122\n",
      "Initial State is  [1, 10, 1]\n",
      "episode 5177, reward 675.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 127\n",
      "Initial State is  [0, 3, 6]\n",
      "episode 5178, reward 905.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 133\n",
      "Initial State is  [3, 3, 2]\n",
      "episode 5179, reward 957.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 133\n",
      "Initial State is  [1, 20, 4]\n",
      "episode 5180, reward 839.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 125\n",
      "Initial State is  [2, 6, 3]\n",
      "episode 5181, reward 551.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 123\n",
      "Initial State is  [0, 18, 0]\n",
      "episode 5182, reward 987.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 127\n",
      "Initial State is  [0, 3, 6]\n",
      "episode 5183, reward 1164.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 139\n",
      "Initial State is  [2, 10, 1]\n",
      "episode 5184, reward 970.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 129\n",
      "Initial State is  [2, 17, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 5185, reward 879.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 128\n",
      "Initial State is  [2, 22, 6]\n",
      "episode 5186, reward 1019.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 134\n",
      "Initial State is  [0, 12, 0]\n",
      "episode 5187, reward 1021.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 137\n",
      "Initial State is  [3, 22, 1]\n",
      "episode 5188, reward 789.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 135\n",
      "Initial State is  [4, 21, 1]\n",
      "episode 5189, reward 988.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 131\n",
      "Initial State is  [0, 19, 2]\n",
      "episode 5190, reward 548.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 115\n",
      "Initial State is  [1, 1, 6]\n",
      "episode 5191, reward 1268.0, memory_length 2000, epsilon 0.009998671593271896, time 721.0, rides 146\n",
      "Initial State is  [1, 15, 5]\n",
      "episode 5192, reward 966.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 134\n",
      "Initial State is  [0, 4, 3]\n",
      "episode 5193, reward 784.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 135\n",
      "Initial State is  [2, 10, 0]\n",
      "episode 5194, reward 1077.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 125\n",
      "Initial State is  [3, 17, 1]\n",
      "episode 5195, reward 810.0, memory_length 2000, epsilon 0.009998671593271896, time 737.0, rides 132\n",
      "Initial State is  [0, 23, 2]\n",
      "episode 5196, reward 1200.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 129\n",
      "Initial State is  [0, 0, 6]\n",
      "episode 5197, reward 1106.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 132\n",
      "Initial State is  [3, 13, 2]\n",
      "episode 5198, reward 833.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 132\n",
      "Initial State is  [3, 20, 4]\n",
      "episode 5199, reward 688.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 135\n",
      "Initial State is  [0, 19, 4]\n",
      "episode 5200, reward 1278.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 128\n",
      "Initial State is  [4, 9, 0]\n",
      "episode 5201, reward 1323.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 138\n",
      "Initial State is  [0, 23, 2]\n",
      "episode 5202, reward 716.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 126\n",
      "Initial State is  [4, 12, 0]\n",
      "episode 5203, reward 833.0, memory_length 2000, epsilon 0.009998671593271896, time 739.0, rides 134\n",
      "Initial State is  [0, 5, 3]\n",
      "episode 5204, reward 922.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 135\n",
      "Initial State is  [3, 10, 0]\n",
      "episode 5205, reward 1103.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 128\n",
      "Initial State is  [0, 21, 2]\n",
      "episode 5206, reward 920.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 134\n",
      "Initial State is  [2, 0, 3]\n",
      "episode 5207, reward 911.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 149\n",
      "Initial State is  [1, 7, 5]\n",
      "episode 5208, reward 634.0, memory_length 2000, epsilon 0.009998671593271896, time 736.0, rides 132\n",
      "Initial State is  [1, 7, 2]\n",
      "episode 5209, reward 917.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 137\n",
      "Initial State is  [4, 13, 5]\n",
      "episode 5210, reward 900.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 130\n",
      "Initial State is  [2, 8, 6]\n",
      "episode 5211, reward 891.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 135\n",
      "Initial State is  [4, 22, 6]\n",
      "episode 5212, reward 902.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 130\n",
      "Initial State is  [4, 2, 3]\n",
      "episode 5213, reward 972.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 139\n",
      "Initial State is  [3, 6, 3]\n",
      "episode 5214, reward 1121.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 136\n",
      "Initial State is  [1, 14, 5]\n",
      "episode 5215, reward 914.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 125\n",
      "Initial State is  [3, 11, 4]\n",
      "episode 5216, reward 802.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 138\n",
      "Initial State is  [0, 3, 2]\n",
      "episode 5217, reward 902.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 134\n",
      "Initial State is  [4, 15, 5]\n",
      "episode 5218, reward 623.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 133\n",
      "Initial State is  [2, 15, 3]\n",
      "episode 5219, reward 1015.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 133\n",
      "Initial State is  [4, 10, 6]\n",
      "episode 5220, reward 805.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 144\n",
      "Initial State is  [2, 19, 2]\n",
      "episode 5221, reward 880.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 126\n",
      "Initial State is  [4, 17, 3]\n",
      "episode 5222, reward 998.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 133\n",
      "Initial State is  [3, 18, 5]\n",
      "episode 5223, reward 1203.0, memory_length 2000, epsilon 0.009998671593271896, time 737.0, rides 133\n",
      "Initial State is  [4, 2, 1]\n",
      "episode 5224, reward 883.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 137\n",
      "Initial State is  [0, 3, 1]\n",
      "episode 5225, reward 848.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 120\n",
      "Initial State is  [0, 16, 3]\n",
      "episode 5226, reward 989.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 122\n",
      "Initial State is  [2, 20, 6]\n",
      "episode 5227, reward 1204.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 139\n",
      "Initial State is  [3, 8, 2]\n",
      "episode 5228, reward 1239.0, memory_length 2000, epsilon 0.009998671593271896, time 736.0, rides 135\n",
      "Initial State is  [0, 11, 4]\n",
      "episode 5229, reward 1118.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 134\n",
      "Initial State is  [0, 0, 0]\n",
      "episode 5230, reward 1070.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 139\n",
      "Initial State is  [3, 14, 2]\n",
      "episode 5231, reward 998.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 129\n",
      "Initial State is  [3, 11, 1]\n",
      "episode 5232, reward 877.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 144\n",
      "Initial State is  [3, 6, 0]\n",
      "episode 5233, reward 984.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 122\n",
      "Initial State is  [4, 14, 5]\n",
      "episode 5234, reward 879.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 138\n",
      "Initial State is  [2, 15, 0]\n",
      "episode 5235, reward 527.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 138\n",
      "Initial State is  [1, 14, 6]\n",
      "episode 5236, reward 1230.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 131\n",
      "Initial State is  [0, 1, 4]\n",
      "episode 5237, reward 835.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 129\n",
      "Initial State is  [3, 6, 3]\n",
      "episode 5238, reward 1137.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 144\n",
      "Initial State is  [3, 14, 3]\n",
      "episode 5239, reward 925.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 128\n",
      "Initial State is  [1, 1, 0]\n",
      "episode 5240, reward 1073.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 138\n",
      "Initial State is  [3, 17, 5]\n",
      "episode 5241, reward 1015.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 136\n",
      "Initial State is  [1, 17, 0]\n",
      "episode 5242, reward 1177.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 142\n",
      "Initial State is  [4, 4, 5]\n",
      "episode 5243, reward 957.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 136\n",
      "Initial State is  [0, 11, 4]\n",
      "episode 5244, reward 1330.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 134\n",
      "Initial State is  [1, 18, 2]\n",
      "episode 5245, reward 1077.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 135\n",
      "Initial State is  [1, 2, 6]\n",
      "episode 5246, reward 1189.0, memory_length 2000, epsilon 0.009998671593271896, time 737.0, rides 141\n",
      "Initial State is  [0, 4, 0]\n",
      "episode 5247, reward 830.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 133\n",
      "Initial State is  [3, 18, 3]\n",
      "episode 5248, reward 835.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 143\n",
      "Initial State is  [3, 14, 4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 5249, reward 998.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 144\n",
      "Initial State is  [2, 5, 2]\n",
      "episode 5250, reward 1029.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 145\n",
      "Initial State is  [1, 3, 0]\n",
      "episode 5251, reward 876.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 132\n",
      "Initial State is  [4, 9, 1]\n",
      "episode 5252, reward 1082.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 139\n",
      "Initial State is  [4, 14, 2]\n",
      "episode 5253, reward 870.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 140\n",
      "Initial State is  [2, 11, 4]\n",
      "episode 5254, reward 1007.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 132\n",
      "Initial State is  [0, 15, 6]\n",
      "episode 5255, reward 931.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 141\n",
      "Initial State is  [2, 2, 5]\n",
      "episode 5256, reward 1094.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 128\n",
      "Initial State is  [2, 13, 3]\n",
      "episode 5257, reward 879.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 141\n",
      "Initial State is  [1, 12, 6]\n",
      "episode 5258, reward 1167.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 128\n",
      "Initial State is  [0, 20, 5]\n",
      "episode 5259, reward 1125.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 133\n",
      "Initial State is  [0, 20, 4]\n",
      "episode 5260, reward 1183.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 134\n",
      "Initial State is  [4, 20, 1]\n",
      "episode 5261, reward 1073.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 123\n",
      "Initial State is  [3, 17, 1]\n",
      "episode 5262, reward 956.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 131\n",
      "Initial State is  [3, 6, 1]\n",
      "episode 5263, reward 759.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 146\n",
      "Initial State is  [3, 2, 4]\n",
      "episode 5264, reward 750.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 133\n",
      "Initial State is  [2, 19, 1]\n",
      "episode 5265, reward 1087.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 135\n",
      "Initial State is  [0, 8, 6]\n",
      "episode 5266, reward 620.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 125\n",
      "Initial State is  [2, 6, 3]\n",
      "episode 5267, reward 1123.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 136\n",
      "Initial State is  [3, 10, 0]\n",
      "episode 5268, reward 918.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 134\n",
      "Initial State is  [3, 17, 5]\n",
      "episode 5269, reward 1151.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 130\n",
      "Initial State is  [1, 16, 1]\n",
      "episode 5270, reward 787.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 135\n",
      "Initial State is  [1, 23, 1]\n",
      "episode 5271, reward 737.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 117\n",
      "Initial State is  [1, 2, 5]\n",
      "episode 5272, reward 970.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 131\n",
      "Initial State is  [2, 15, 3]\n",
      "episode 5273, reward 994.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 137\n",
      "Initial State is  [1, 16, 3]\n",
      "episode 5274, reward 1053.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 145\n",
      "Initial State is  [1, 8, 1]\n",
      "episode 5275, reward 885.0, memory_length 2000, epsilon 0.009998671593271896, time 720.0, rides 118\n",
      "Initial State is  [3, 23, 4]\n",
      "episode 5276, reward 1170.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 135\n",
      "Initial State is  [0, 16, 6]\n",
      "episode 5277, reward 947.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 130\n",
      "Initial State is  [1, 6, 6]\n",
      "episode 5278, reward 874.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 137\n",
      "Initial State is  [4, 19, 4]\n",
      "episode 5279, reward 741.0, memory_length 2000, epsilon 0.009998671593271896, time 721.0, rides 130\n",
      "Initial State is  [0, 23, 6]\n",
      "episode 5280, reward 1111.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 136\n",
      "Initial State is  [3, 21, 5]\n",
      "episode 5281, reward 912.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 129\n",
      "Initial State is  [0, 16, 1]\n",
      "episode 5282, reward 1184.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 136\n",
      "Initial State is  [3, 7, 5]\n",
      "episode 5283, reward 1170.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 135\n",
      "Initial State is  [0, 22, 4]\n",
      "episode 5284, reward 873.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 124\n",
      "Initial State is  [4, 13, 2]\n",
      "episode 5285, reward 987.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 125\n",
      "Initial State is  [0, 7, 5]\n",
      "episode 5286, reward 1022.0, memory_length 2000, epsilon 0.009998671593271896, time 736.0, rides 126\n",
      "Initial State is  [2, 6, 5]\n",
      "episode 5287, reward 1039.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 136\n",
      "Initial State is  [2, 20, 5]\n",
      "episode 5288, reward 839.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 142\n",
      "Initial State is  [3, 23, 4]\n",
      "episode 5289, reward 955.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 129\n",
      "Initial State is  [2, 11, 4]\n",
      "episode 5290, reward 716.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 136\n",
      "Initial State is  [1, 14, 5]\n",
      "episode 5291, reward 1106.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 132\n",
      "Initial State is  [4, 14, 0]\n",
      "episode 5292, reward 1071.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 129\n",
      "Initial State is  [0, 16, 5]\n",
      "episode 5293, reward 1095.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 141\n",
      "Initial State is  [1, 19, 0]\n",
      "episode 5294, reward 968.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 142\n",
      "Initial State is  [4, 7, 2]\n",
      "episode 5295, reward 753.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 132\n",
      "Initial State is  [4, 13, 4]\n",
      "episode 5296, reward 944.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 127\n",
      "Initial State is  [1, 15, 6]\n",
      "episode 5297, reward 833.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 124\n",
      "Initial State is  [2, 8, 0]\n",
      "episode 5298, reward 979.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 126\n",
      "Initial State is  [1, 20, 3]\n",
      "episode 5299, reward 1108.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 127\n",
      "Initial State is  [3, 9, 3]\n",
      "episode 5300, reward 929.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 137\n",
      "Initial State is  [3, 23, 2]\n",
      "episode 5301, reward 732.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 123\n",
      "Initial State is  [2, 1, 1]\n",
      "episode 5302, reward 1018.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 134\n",
      "Initial State is  [4, 4, 3]\n",
      "episode 5303, reward 1021.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 132\n",
      "Initial State is  [1, 10, 5]\n",
      "episode 5304, reward 1062.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 124\n",
      "Initial State is  [2, 21, 1]\n",
      "episode 5305, reward 670.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 129\n",
      "Initial State is  [1, 1, 4]\n",
      "episode 5306, reward 949.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 146\n",
      "Initial State is  [1, 7, 0]\n",
      "episode 5307, reward 965.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 120\n",
      "Initial State is  [4, 23, 4]\n",
      "episode 5308, reward 1054.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 130\n",
      "Initial State is  [2, 3, 1]\n",
      "episode 5309, reward 902.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 132\n",
      "Initial State is  [4, 4, 2]\n",
      "episode 5310, reward 974.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 130\n",
      "Initial State is  [2, 18, 5]\n",
      "episode 5311, reward 869.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 140\n",
      "Initial State is  [3, 18, 1]\n",
      "episode 5312, reward 874.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 125\n",
      "Initial State is  [3, 21, 1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 5313, reward 790.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 137\n",
      "Initial State is  [1, 4, 2]\n",
      "episode 5314, reward 865.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 124\n",
      "Initial State is  [4, 16, 4]\n",
      "episode 5315, reward 1141.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 119\n",
      "Initial State is  [2, 10, 3]\n",
      "episode 5316, reward 1029.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 143\n",
      "Initial State is  [3, 14, 6]\n",
      "episode 5317, reward 1146.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 138\n",
      "Initial State is  [4, 16, 3]\n",
      "episode 5318, reward 853.0, memory_length 2000, epsilon 0.009998671593271896, time 736.0, rides 125\n",
      "Initial State is  [4, 6, 1]\n",
      "episode 5319, reward 1410.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 137\n",
      "Initial State is  [2, 8, 3]\n",
      "episode 5320, reward 979.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 137\n",
      "Initial State is  [4, 11, 0]\n",
      "episode 5321, reward 918.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 136\n",
      "Initial State is  [1, 3, 5]\n",
      "episode 5322, reward 822.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 132\n",
      "Initial State is  [1, 2, 4]\n",
      "episode 5323, reward 985.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 126\n",
      "Initial State is  [0, 17, 5]\n",
      "episode 5324, reward 1075.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 122\n",
      "Initial State is  [0, 14, 6]\n",
      "episode 5325, reward 798.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 133\n",
      "Initial State is  [2, 8, 0]\n",
      "episode 5326, reward 767.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 130\n",
      "Initial State is  [0, 17, 2]\n",
      "episode 5327, reward 1250.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 145\n",
      "Initial State is  [2, 2, 3]\n",
      "episode 5328, reward 583.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 124\n",
      "Initial State is  [1, 21, 5]\n",
      "episode 5329, reward 726.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 136\n",
      "Initial State is  [3, 23, 4]\n",
      "episode 5330, reward 858.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 135\n",
      "Initial State is  [3, 6, 6]\n",
      "episode 5331, reward 1346.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 140\n",
      "Initial State is  [2, 22, 2]\n",
      "episode 5332, reward 632.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 124\n",
      "Initial State is  [3, 6, 6]\n",
      "episode 5333, reward 875.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 128\n",
      "Initial State is  [4, 2, 5]\n",
      "episode 5334, reward 979.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 129\n",
      "Initial State is  [0, 14, 4]\n",
      "episode 5335, reward 772.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 130\n",
      "Initial State is  [0, 13, 6]\n",
      "episode 5336, reward 1049.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 132\n",
      "Initial State is  [4, 12, 0]\n",
      "episode 5337, reward 1087.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 132\n",
      "Initial State is  [0, 0, 3]\n",
      "episode 5338, reward 861.0, memory_length 2000, epsilon 0.009998671593271896, time 737.0, rides 144\n",
      "Initial State is  [4, 23, 0]\n",
      "episode 5339, reward 912.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 134\n",
      "Initial State is  [4, 4, 1]\n",
      "episode 5340, reward 788.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 130\n",
      "Initial State is  [1, 1, 3]\n",
      "episode 5341, reward 895.0, memory_length 2000, epsilon 0.009998671593271896, time 741.0, rides 139\n",
      "Initial State is  [3, 18, 3]\n",
      "episode 5342, reward 464.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 136\n",
      "Initial State is  [4, 10, 4]\n",
      "episode 5343, reward 1099.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 143\n",
      "Initial State is  [1, 3, 4]\n",
      "episode 5344, reward 982.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 135\n",
      "Initial State is  [2, 6, 5]\n",
      "episode 5345, reward 908.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 128\n",
      "Initial State is  [2, 9, 6]\n",
      "episode 5346, reward 954.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 147\n",
      "Initial State is  [3, 0, 6]\n",
      "episode 5347, reward 856.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 124\n",
      "Initial State is  [2, 14, 5]\n",
      "episode 5348, reward 990.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 134\n",
      "Initial State is  [3, 9, 5]\n",
      "episode 5349, reward 702.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 127\n",
      "Initial State is  [0, 2, 0]\n",
      "episode 5350, reward 1183.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 134\n",
      "Initial State is  [2, 0, 6]\n",
      "episode 5351, reward 867.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 133\n",
      "Initial State is  [1, 12, 1]\n",
      "episode 5352, reward 852.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 142\n",
      "Initial State is  [1, 7, 1]\n",
      "episode 5353, reward 1045.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 140\n",
      "Initial State is  [0, 13, 3]\n",
      "episode 5354, reward 1169.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 140\n",
      "Initial State is  [1, 21, 4]\n",
      "episode 5355, reward 1104.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 132\n",
      "Initial State is  [1, 19, 4]\n",
      "episode 5356, reward 1206.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 126\n",
      "Initial State is  [3, 1, 5]\n",
      "episode 5357, reward 1046.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 139\n",
      "Initial State is  [2, 19, 1]\n",
      "episode 5358, reward 941.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 133\n",
      "Initial State is  [2, 21, 6]\n",
      "episode 5359, reward 1016.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 147\n",
      "Initial State is  [3, 4, 5]\n",
      "episode 5360, reward 912.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 120\n",
      "Initial State is  [0, 17, 4]\n",
      "episode 5361, reward 875.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 139\n",
      "Initial State is  [1, 2, 3]\n",
      "episode 5362, reward 1219.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 151\n",
      "Initial State is  [1, 17, 4]\n",
      "episode 5363, reward 919.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 136\n",
      "Initial State is  [0, 12, 4]\n",
      "episode 5364, reward 715.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 138\n",
      "Initial State is  [0, 10, 5]\n",
      "episode 5365, reward 438.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 141\n",
      "Initial State is  [1, 23, 2]\n",
      "episode 5366, reward 1203.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 141\n",
      "Initial State is  [0, 13, 5]\n",
      "episode 5367, reward 998.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 145\n",
      "Initial State is  [3, 17, 0]\n",
      "episode 5368, reward 859.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 132\n",
      "Initial State is  [2, 12, 0]\n",
      "episode 5369, reward 1064.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 117\n",
      "Initial State is  [3, 10, 6]\n",
      "episode 5370, reward 605.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 127\n",
      "Initial State is  [3, 13, 1]\n",
      "episode 5371, reward 1160.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 122\n",
      "Initial State is  [0, 13, 3]\n",
      "episode 5372, reward 951.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 125\n",
      "Initial State is  [2, 21, 4]\n",
      "episode 5373, reward 827.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 133\n",
      "Initial State is  [3, 4, 3]\n",
      "episode 5374, reward 832.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 133\n",
      "Initial State is  [4, 18, 6]\n",
      "episode 5375, reward 857.0, memory_length 2000, epsilon 0.009998671593271896, time 736.0, rides 127\n",
      "Initial State is  [0, 13, 6]\n",
      "episode 5376, reward 1160.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 132\n",
      "Initial State is  [3, 17, 4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 5377, reward 1062.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 142\n",
      "Initial State is  [1, 13, 5]\n",
      "episode 5378, reward 830.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 142\n",
      "Initial State is  [4, 2, 1]\n",
      "episode 5379, reward 1264.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 136\n",
      "Initial State is  [0, 17, 3]\n",
      "episode 5380, reward 1139.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 138\n",
      "Initial State is  [0, 11, 0]\n",
      "episode 5381, reward 675.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 125\n",
      "Initial State is  [3, 8, 2]\n",
      "episode 5382, reward 739.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 142\n",
      "Initial State is  [1, 22, 0]\n",
      "episode 5383, reward 978.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 138\n",
      "Initial State is  [1, 6, 0]\n",
      "episode 5384, reward 1212.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 134\n",
      "Initial State is  [3, 23, 5]\n",
      "episode 5385, reward 1049.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 131\n",
      "Initial State is  [1, 15, 5]\n",
      "episode 5386, reward 933.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 141\n",
      "Initial State is  [2, 15, 0]\n",
      "episode 5387, reward 1012.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 124\n",
      "Initial State is  [2, 13, 5]\n",
      "episode 5388, reward 896.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 143\n",
      "Initial State is  [4, 21, 0]\n",
      "episode 5389, reward 895.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 124\n",
      "Initial State is  [0, 10, 3]\n",
      "episode 5390, reward 1179.0, memory_length 2000, epsilon 0.009998671593271896, time 740.0, rides 127\n",
      "Initial State is  [0, 18, 6]\n",
      "episode 5391, reward 973.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 128\n",
      "Initial State is  [0, 6, 6]\n",
      "episode 5392, reward 1164.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 134\n",
      "Initial State is  [2, 7, 5]\n",
      "episode 5393, reward 955.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 132\n",
      "Initial State is  [4, 11, 3]\n",
      "episode 5394, reward 936.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 142\n",
      "Initial State is  [1, 6, 1]\n",
      "episode 5395, reward 819.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 133\n",
      "Initial State is  [3, 17, 3]\n",
      "episode 5396, reward 891.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 122\n",
      "Initial State is  [2, 7, 2]\n",
      "episode 5397, reward 990.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 125\n",
      "Initial State is  [1, 0, 6]\n",
      "episode 5398, reward 943.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 137\n",
      "Initial State is  [0, 21, 2]\n",
      "episode 5399, reward 841.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 128\n",
      "Initial State is  [0, 12, 6]\n",
      "episode 5400, reward 815.0, memory_length 2000, epsilon 0.009998671593271896, time 721.0, rides 141\n",
      "Initial State is  [0, 8, 2]\n",
      "episode 5401, reward 815.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 129\n",
      "Initial State is  [4, 17, 6]\n",
      "episode 5402, reward 1110.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 134\n",
      "Initial State is  [1, 15, 3]\n",
      "episode 5403, reward 736.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 133\n",
      "Initial State is  [4, 3, 6]\n",
      "episode 5404, reward 1127.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 139\n",
      "Initial State is  [1, 16, 2]\n",
      "episode 5405, reward 973.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 132\n",
      "Initial State is  [4, 22, 6]\n",
      "episode 5406, reward 890.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 141\n",
      "Initial State is  [4, 12, 1]\n",
      "episode 5407, reward 881.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 144\n",
      "Initial State is  [2, 5, 0]\n",
      "episode 5408, reward 1107.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 135\n",
      "Initial State is  [3, 7, 2]\n",
      "episode 5409, reward 1016.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 139\n",
      "Initial State is  [2, 3, 4]\n",
      "episode 5410, reward 794.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 125\n",
      "Initial State is  [0, 9, 5]\n",
      "episode 5411, reward 985.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 131\n",
      "Initial State is  [3, 22, 4]\n",
      "episode 5412, reward 1130.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 159\n",
      "Initial State is  [4, 8, 6]\n",
      "episode 5413, reward 1095.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 139\n",
      "Initial State is  [3, 19, 0]\n",
      "episode 5414, reward 983.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 140\n",
      "Initial State is  [1, 0, 3]\n",
      "episode 5415, reward 896.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 131\n",
      "Initial State is  [3, 9, 4]\n",
      "episode 5416, reward 857.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 137\n",
      "Initial State is  [4, 15, 6]\n",
      "episode 5417, reward 877.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 135\n",
      "Initial State is  [3, 18, 0]\n",
      "episode 5418, reward 796.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 127\n",
      "Initial State is  [2, 16, 0]\n",
      "episode 5419, reward 1135.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 129\n",
      "Initial State is  [2, 13, 1]\n",
      "episode 5420, reward 932.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 123\n",
      "Initial State is  [3, 1, 1]\n",
      "episode 5421, reward 923.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 127\n",
      "Initial State is  [3, 1, 4]\n",
      "episode 5422, reward 1009.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 127\n",
      "Initial State is  [1, 2, 6]\n",
      "episode 5423, reward 578.0, memory_length 2000, epsilon 0.009998671593271896, time 737.0, rides 120\n",
      "Initial State is  [3, 15, 3]\n",
      "episode 5424, reward 947.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 143\n",
      "Initial State is  [4, 1, 0]\n",
      "episode 5425, reward 1113.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 136\n",
      "Initial State is  [0, 1, 3]\n",
      "episode 5426, reward 961.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 127\n",
      "Initial State is  [4, 4, 2]\n",
      "episode 5427, reward 1107.0, memory_length 2000, epsilon 0.009998671593271896, time 739.0, rides 135\n",
      "Initial State is  [2, 15, 3]\n",
      "episode 5428, reward 931.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 135\n",
      "Initial State is  [1, 4, 0]\n",
      "episode 5429, reward 969.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 133\n",
      "Initial State is  [2, 3, 5]\n",
      "episode 5430, reward 1040.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 125\n",
      "Initial State is  [1, 12, 6]\n",
      "episode 5431, reward 1153.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 128\n",
      "Initial State is  [4, 15, 1]\n",
      "episode 5432, reward 734.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 133\n",
      "Initial State is  [0, 23, 4]\n",
      "episode 5433, reward 907.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 149\n",
      "Initial State is  [1, 10, 2]\n",
      "episode 5434, reward 884.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 143\n",
      "Initial State is  [4, 17, 3]\n",
      "episode 5435, reward 588.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 135\n",
      "Initial State is  [3, 9, 5]\n",
      "episode 5436, reward 1053.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 140\n",
      "Initial State is  [2, 5, 1]\n",
      "episode 5437, reward 952.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 144\n",
      "Initial State is  [4, 4, 6]\n",
      "episode 5438, reward 1100.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 129\n",
      "Initial State is  [0, 18, 1]\n",
      "episode 5439, reward 751.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 134\n",
      "Initial State is  [3, 13, 3]\n",
      "episode 5440, reward 1053.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 129\n",
      "Initial State is  [4, 15, 6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 5441, reward 1009.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 152\n",
      "Initial State is  [0, 3, 1]\n",
      "episode 5442, reward 964.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 128\n",
      "Initial State is  [2, 5, 4]\n",
      "episode 5443, reward 645.0, memory_length 2000, epsilon 0.009998671593271896, time 737.0, rides 129\n",
      "Initial State is  [4, 6, 5]\n",
      "episode 5444, reward 1144.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 148\n",
      "Initial State is  [3, 13, 6]\n",
      "episode 5445, reward 838.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 132\n",
      "Initial State is  [1, 0, 0]\n",
      "episode 5446, reward 767.0, memory_length 2000, epsilon 0.009998671593271896, time 721.0, rides 131\n",
      "Initial State is  [0, 7, 5]\n",
      "episode 5447, reward 937.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 143\n",
      "Initial State is  [1, 17, 6]\n",
      "episode 5448, reward 1133.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 135\n",
      "Initial State is  [1, 22, 1]\n",
      "episode 5449, reward 845.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 128\n",
      "Initial State is  [2, 1, 0]\n",
      "episode 5450, reward 1168.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 138\n",
      "Initial State is  [3, 13, 0]\n",
      "episode 5451, reward 767.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 136\n",
      "Initial State is  [4, 19, 4]\n",
      "episode 5452, reward 1080.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 129\n",
      "Initial State is  [0, 17, 0]\n",
      "episode 5453, reward 598.0, memory_length 2000, epsilon 0.009998671593271896, time 739.0, rides 157\n",
      "Initial State is  [0, 5, 5]\n",
      "episode 5454, reward 1132.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 136\n",
      "Initial State is  [2, 21, 4]\n",
      "episode 5455, reward 1144.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 127\n",
      "Initial State is  [1, 13, 1]\n",
      "episode 5456, reward 664.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 128\n",
      "Initial State is  [3, 0, 6]\n",
      "episode 5457, reward 802.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 125\n",
      "Initial State is  [0, 4, 3]\n",
      "episode 5458, reward 710.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 127\n",
      "Initial State is  [0, 12, 5]\n",
      "episode 5459, reward 821.0, memory_length 2000, epsilon 0.009998671593271896, time 740.0, rides 132\n",
      "Initial State is  [3, 1, 1]\n",
      "episode 5460, reward 1307.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 133\n",
      "Initial State is  [4, 11, 2]\n",
      "episode 5461, reward 957.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 141\n",
      "Initial State is  [4, 15, 5]\n",
      "episode 5462, reward 898.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 138\n",
      "Initial State is  [3, 11, 4]\n",
      "episode 5463, reward 950.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 134\n",
      "Initial State is  [4, 20, 2]\n",
      "episode 5464, reward 681.0, memory_length 2000, epsilon 0.009998671593271896, time 736.0, rides 129\n",
      "Initial State is  [2, 1, 2]\n",
      "episode 5465, reward 932.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 126\n",
      "Initial State is  [1, 2, 4]\n",
      "episode 5466, reward 641.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 135\n",
      "Initial State is  [2, 8, 6]\n",
      "episode 5467, reward 998.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 148\n",
      "Initial State is  [2, 9, 6]\n",
      "episode 5468, reward 1226.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 139\n",
      "Initial State is  [3, 12, 4]\n",
      "episode 5469, reward 1120.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 150\n",
      "Initial State is  [3, 19, 4]\n",
      "episode 5470, reward 1123.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 129\n",
      "Initial State is  [4, 5, 4]\n",
      "episode 5471, reward 714.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 129\n",
      "Initial State is  [4, 8, 2]\n",
      "episode 5472, reward 1086.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 124\n",
      "Initial State is  [0, 5, 2]\n",
      "episode 5473, reward 843.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 124\n",
      "Initial State is  [0, 11, 4]\n",
      "episode 5474, reward 1067.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 137\n",
      "Initial State is  [4, 7, 0]\n",
      "episode 5475, reward 945.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 138\n",
      "Initial State is  [2, 10, 3]\n",
      "episode 5476, reward 880.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 133\n",
      "Initial State is  [1, 9, 2]\n",
      "episode 5477, reward 855.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 124\n",
      "Initial State is  [4, 15, 4]\n",
      "episode 5478, reward 1148.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 134\n",
      "Initial State is  [2, 12, 6]\n",
      "episode 5479, reward 949.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 134\n",
      "Initial State is  [3, 19, 6]\n",
      "episode 5480, reward 720.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 136\n",
      "Initial State is  [0, 10, 6]\n",
      "episode 5481, reward 827.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 128\n",
      "Initial State is  [0, 17, 1]\n",
      "episode 5482, reward 673.0, memory_length 2000, epsilon 0.009998671593271896, time 738.0, rides 126\n",
      "Initial State is  [0, 3, 5]\n",
      "episode 5483, reward 1137.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 130\n",
      "Initial State is  [3, 23, 6]\n",
      "episode 5484, reward 906.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 135\n",
      "Initial State is  [3, 7, 3]\n",
      "episode 5485, reward 978.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 130\n",
      "Initial State is  [4, 1, 3]\n",
      "episode 5486, reward 1113.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 135\n",
      "Initial State is  [3, 6, 1]\n",
      "episode 5487, reward 953.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 126\n",
      "Initial State is  [0, 17, 6]\n",
      "episode 5488, reward 1083.0, memory_length 2000, epsilon 0.009998671593271896, time 738.0, rides 135\n",
      "Initial State is  [3, 0, 5]\n",
      "episode 5489, reward 907.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 132\n",
      "Initial State is  [0, 10, 5]\n",
      "episode 5490, reward 706.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 131\n",
      "Initial State is  [1, 16, 2]\n",
      "episode 5491, reward 967.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 132\n",
      "Initial State is  [1, 4, 3]\n",
      "episode 5492, reward 897.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 147\n",
      "Initial State is  [1, 16, 3]\n",
      "episode 5493, reward 663.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 131\n",
      "Initial State is  [0, 9, 0]\n",
      "episode 5494, reward 921.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 133\n",
      "Initial State is  [3, 22, 3]\n",
      "episode 5495, reward 971.0, memory_length 2000, epsilon 0.009998671593271896, time 739.0, rides 136\n",
      "Initial State is  [1, 11, 5]\n",
      "episode 5496, reward 905.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 131\n",
      "Initial State is  [0, 0, 2]\n",
      "episode 5497, reward 1027.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 144\n",
      "Initial State is  [2, 15, 1]\n",
      "episode 5498, reward 1059.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 124\n",
      "Initial State is  [3, 16, 0]\n",
      "episode 5499, reward 1019.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 126\n",
      "Initial State is  [1, 17, 1]\n",
      "episode 5500, reward 890.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 135\n",
      "Initial State is  [0, 9, 6]\n",
      "episode 5501, reward 970.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 126\n",
      "Initial State is  [4, 2, 4]\n",
      "episode 5502, reward 969.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 130\n",
      "Initial State is  [4, 6, 2]\n",
      "episode 5503, reward 991.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 146\n",
      "Initial State is  [3, 21, 3]\n",
      "episode 5504, reward 686.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 130\n",
      "Initial State is  [0, 10, 1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 5505, reward 871.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 134\n",
      "Initial State is  [4, 0, 6]\n",
      "episode 5506, reward 1063.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 131\n",
      "Initial State is  [3, 18, 5]\n",
      "episode 5507, reward 961.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 118\n",
      "Initial State is  [1, 4, 3]\n",
      "episode 5508, reward 1105.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 145\n",
      "Initial State is  [2, 17, 4]\n",
      "episode 5509, reward 667.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 137\n",
      "Initial State is  [3, 13, 6]\n",
      "episode 5510, reward 728.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 146\n",
      "Initial State is  [3, 19, 0]\n",
      "episode 5511, reward 1045.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 128\n",
      "Initial State is  [1, 9, 2]\n",
      "episode 5512, reward 813.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 133\n",
      "Initial State is  [1, 6, 5]\n",
      "episode 5513, reward 689.0, memory_length 2000, epsilon 0.009998671593271896, time 720.0, rides 140\n",
      "Initial State is  [0, 1, 6]\n",
      "episode 5514, reward 885.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 128\n",
      "Initial State is  [2, 12, 0]\n",
      "episode 5515, reward 1123.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 132\n",
      "Initial State is  [4, 11, 5]\n",
      "episode 5516, reward 999.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 142\n",
      "Initial State is  [1, 22, 4]\n",
      "episode 5517, reward 388.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 132\n",
      "Initial State is  [0, 22, 6]\n",
      "episode 5518, reward 966.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 132\n",
      "Initial State is  [4, 2, 2]\n",
      "episode 5519, reward 771.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 118\n",
      "Initial State is  [1, 3, 5]\n",
      "episode 5520, reward 899.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 127\n",
      "Initial State is  [2, 15, 1]\n",
      "episode 5521, reward 592.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 134\n",
      "Initial State is  [1, 14, 4]\n",
      "episode 5522, reward 899.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 131\n",
      "Initial State is  [2, 19, 2]\n",
      "episode 5523, reward 1128.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 152\n",
      "Initial State is  [3, 19, 0]\n",
      "episode 5524, reward 876.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 135\n",
      "Initial State is  [0, 10, 0]\n",
      "episode 5525, reward 737.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 125\n",
      "Initial State is  [4, 14, 3]\n",
      "episode 5526, reward 670.0, memory_length 2000, epsilon 0.009998671593271896, time 740.0, rides 134\n",
      "Initial State is  [2, 8, 1]\n",
      "episode 5527, reward 788.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 128\n",
      "Initial State is  [1, 6, 4]\n",
      "episode 5528, reward 991.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 138\n",
      "Initial State is  [4, 10, 1]\n",
      "episode 5529, reward 907.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 138\n",
      "Initial State is  [3, 5, 1]\n",
      "episode 5530, reward 1044.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 135\n",
      "Initial State is  [1, 17, 2]\n",
      "episode 5531, reward 870.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 124\n",
      "Initial State is  [4, 20, 0]\n",
      "episode 5532, reward 773.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 147\n",
      "Initial State is  [3, 7, 1]\n",
      "episode 5533, reward 1143.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 126\n",
      "Initial State is  [0, 15, 1]\n",
      "episode 5534, reward 1134.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 134\n",
      "Initial State is  [2, 17, 1]\n",
      "episode 5535, reward 796.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 124\n",
      "Initial State is  [0, 18, 5]\n",
      "episode 5536, reward 860.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 131\n",
      "Initial State is  [3, 15, 2]\n",
      "episode 5537, reward 1093.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 127\n",
      "Initial State is  [0, 9, 1]\n",
      "episode 5538, reward 1184.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 144\n",
      "Initial State is  [1, 20, 3]\n",
      "episode 5539, reward 869.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 145\n",
      "Initial State is  [0, 19, 4]\n",
      "episode 5540, reward 1225.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 139\n",
      "Initial State is  [0, 5, 1]\n",
      "episode 5541, reward 1104.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 125\n",
      "Initial State is  [2, 15, 0]\n",
      "episode 5542, reward 986.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 138\n",
      "Initial State is  [0, 1, 1]\n",
      "episode 5543, reward 687.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 129\n",
      "Initial State is  [3, 19, 1]\n",
      "episode 5544, reward 941.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 133\n",
      "Initial State is  [4, 22, 3]\n",
      "episode 5545, reward 1135.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 133\n",
      "Initial State is  [0, 8, 3]\n",
      "episode 5546, reward 1182.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 148\n",
      "Initial State is  [4, 22, 0]\n",
      "episode 5547, reward 697.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 135\n",
      "Initial State is  [2, 23, 5]\n",
      "episode 5548, reward 700.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 130\n",
      "Initial State is  [4, 5, 4]\n",
      "episode 5549, reward 1017.0, memory_length 2000, epsilon 0.009998671593271896, time 721.0, rides 135\n",
      "Initial State is  [2, 6, 3]\n",
      "episode 5550, reward 1070.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 131\n",
      "Initial State is  [1, 1, 0]\n",
      "episode 5551, reward 1019.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 127\n",
      "Initial State is  [4, 5, 5]\n",
      "episode 5552, reward 1250.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 122\n",
      "Initial State is  [2, 20, 6]\n",
      "episode 5553, reward 785.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 135\n",
      "Initial State is  [2, 21, 2]\n",
      "episode 5554, reward 867.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 126\n",
      "Initial State is  [3, 19, 2]\n",
      "episode 5555, reward 1094.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 135\n",
      "Initial State is  [2, 16, 6]\n",
      "episode 5556, reward 1196.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 128\n",
      "Initial State is  [4, 17, 0]\n",
      "episode 5557, reward 1005.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 137\n",
      "Initial State is  [3, 21, 5]\n",
      "episode 5558, reward 1078.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 134\n",
      "Initial State is  [3, 11, 0]\n",
      "episode 5559, reward 965.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 140\n",
      "Initial State is  [1, 22, 4]\n",
      "episode 5560, reward 964.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 141\n",
      "Initial State is  [4, 11, 3]\n",
      "episode 5561, reward 1047.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 133\n",
      "Initial State is  [1, 18, 4]\n",
      "episode 5562, reward 1086.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 133\n",
      "Initial State is  [3, 8, 3]\n",
      "episode 5563, reward 1111.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 158\n",
      "Initial State is  [3, 16, 5]\n",
      "episode 5564, reward 1017.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 136\n",
      "Initial State is  [1, 8, 3]\n",
      "episode 5565, reward 903.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 128\n",
      "Initial State is  [3, 4, 6]\n",
      "episode 5566, reward 1000.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 135\n",
      "Initial State is  [2, 5, 1]\n",
      "episode 5567, reward 1123.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 126\n",
      "Initial State is  [2, 19, 3]\n",
      "episode 5568, reward 718.0, memory_length 2000, epsilon 0.009998671593271896, time 720.0, rides 130\n",
      "Initial State is  [2, 8, 2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 5569, reward 717.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 136\n",
      "Initial State is  [0, 21, 6]\n",
      "episode 5570, reward 1048.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 134\n",
      "Initial State is  [4, 8, 0]\n",
      "episode 5571, reward 758.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 134\n",
      "Initial State is  [2, 13, 5]\n",
      "episode 5572, reward 1270.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 138\n",
      "Initial State is  [0, 23, 4]\n",
      "episode 5573, reward 1176.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 124\n",
      "Initial State is  [3, 6, 0]\n",
      "episode 5574, reward 877.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 132\n",
      "Initial State is  [1, 11, 6]\n",
      "episode 5575, reward 925.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 128\n",
      "Initial State is  [1, 23, 4]\n",
      "episode 5576, reward 1069.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 136\n",
      "Initial State is  [3, 6, 3]\n",
      "episode 5577, reward 808.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 131\n",
      "Initial State is  [1, 4, 6]\n",
      "episode 5578, reward 959.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 130\n",
      "Initial State is  [3, 23, 3]\n",
      "episode 5579, reward 909.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 140\n",
      "Initial State is  [4, 19, 4]\n",
      "episode 5580, reward 883.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 127\n",
      "Initial State is  [0, 4, 6]\n",
      "episode 5581, reward 1026.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 128\n",
      "Initial State is  [1, 10, 5]\n",
      "episode 5582, reward 1153.0, memory_length 2000, epsilon 0.009998671593271896, time 737.0, rides 134\n",
      "Initial State is  [3, 1, 3]\n",
      "episode 5583, reward 983.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 146\n",
      "Initial State is  [2, 20, 6]\n",
      "episode 5584, reward 736.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 140\n",
      "Initial State is  [0, 8, 5]\n",
      "episode 5585, reward 843.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 134\n",
      "Initial State is  [0, 18, 0]\n",
      "episode 5586, reward 1088.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 146\n",
      "Initial State is  [1, 1, 4]\n",
      "episode 5587, reward 653.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 121\n",
      "Initial State is  [2, 5, 4]\n",
      "episode 5588, reward 670.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 137\n",
      "Initial State is  [2, 15, 2]\n",
      "episode 5589, reward 980.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 138\n",
      "Initial State is  [3, 15, 2]\n",
      "episode 5590, reward 848.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 137\n",
      "Initial State is  [4, 1, 4]\n",
      "episode 5591, reward 868.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 151\n",
      "Initial State is  [3, 21, 2]\n",
      "episode 5592, reward 899.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 136\n",
      "Initial State is  [2, 7, 3]\n",
      "episode 5593, reward 1044.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 148\n",
      "Initial State is  [1, 9, 5]\n",
      "episode 5594, reward 966.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 132\n",
      "Initial State is  [0, 11, 4]\n",
      "episode 5595, reward 992.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 142\n",
      "Initial State is  [4, 16, 6]\n",
      "episode 5596, reward 828.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 132\n",
      "Initial State is  [3, 9, 2]\n",
      "episode 5597, reward 698.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 137\n",
      "Initial State is  [2, 10, 5]\n",
      "episode 5598, reward 932.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 143\n",
      "Initial State is  [4, 8, 5]\n",
      "episode 5599, reward 727.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 135\n",
      "Initial State is  [3, 16, 6]\n",
      "episode 5600, reward 794.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 135\n",
      "Initial State is  [2, 1, 6]\n",
      "episode 5601, reward 712.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 136\n",
      "Initial State is  [1, 7, 1]\n",
      "episode 5602, reward 990.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 136\n",
      "Initial State is  [2, 20, 2]\n",
      "episode 5603, reward 866.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 144\n",
      "Initial State is  [2, 15, 3]\n",
      "episode 5604, reward 1099.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 132\n",
      "Initial State is  [3, 21, 3]\n",
      "episode 5605, reward 956.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 139\n",
      "Initial State is  [2, 16, 1]\n",
      "episode 5606, reward 888.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 142\n",
      "Initial State is  [2, 7, 0]\n",
      "episode 5607, reward 823.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 126\n",
      "Initial State is  [3, 13, 4]\n",
      "episode 5608, reward 1053.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 145\n",
      "Initial State is  [2, 12, 3]\n",
      "episode 5609, reward 742.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 154\n",
      "Initial State is  [1, 12, 6]\n",
      "episode 5610, reward 886.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 142\n",
      "Initial State is  [2, 11, 5]\n",
      "episode 5611, reward 1072.0, memory_length 2000, epsilon 0.009998671593271896, time 721.0, rides 129\n",
      "Initial State is  [2, 22, 6]\n",
      "episode 5612, reward 887.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 134\n",
      "Initial State is  [0, 14, 0]\n",
      "episode 5613, reward 676.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 135\n",
      "Initial State is  [0, 22, 0]\n",
      "episode 5614, reward 802.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 128\n",
      "Initial State is  [3, 21, 6]\n",
      "episode 5615, reward 1009.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 139\n",
      "Initial State is  [1, 18, 3]\n",
      "episode 5616, reward 1016.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 131\n",
      "Initial State is  [0, 7, 5]\n",
      "episode 5617, reward 904.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 134\n",
      "Initial State is  [0, 13, 0]\n",
      "episode 5618, reward 1031.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 134\n",
      "Initial State is  [4, 19, 0]\n",
      "episode 5619, reward 1298.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 134\n",
      "Initial State is  [2, 6, 1]\n",
      "episode 5620, reward 422.0, memory_length 2000, epsilon 0.009998671593271896, time 720.0, rides 118\n",
      "Initial State is  [2, 4, 0]\n",
      "episode 5621, reward 865.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 144\n",
      "Initial State is  [2, 14, 2]\n",
      "episode 5622, reward 1224.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 136\n",
      "Initial State is  [4, 0, 2]\n",
      "episode 5623, reward 1154.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 127\n",
      "Initial State is  [4, 15, 5]\n",
      "episode 5624, reward 803.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 123\n",
      "Initial State is  [0, 8, 1]\n",
      "episode 5625, reward 1223.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 131\n",
      "Initial State is  [0, 19, 3]\n",
      "episode 5626, reward 658.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 114\n",
      "Initial State is  [2, 3, 3]\n",
      "episode 5627, reward 1059.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 139\n",
      "Initial State is  [0, 2, 3]\n",
      "episode 5628, reward 964.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 131\n",
      "Initial State is  [1, 17, 4]\n",
      "episode 5629, reward 1010.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 132\n",
      "Initial State is  [0, 17, 0]\n",
      "episode 5630, reward 699.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 124\n",
      "Initial State is  [4, 7, 4]\n",
      "episode 5631, reward 1070.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 133\n",
      "Initial State is  [3, 4, 1]\n",
      "episode 5632, reward 844.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 135\n",
      "Initial State is  [4, 2, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 5633, reward 871.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 125\n",
      "Initial State is  [2, 10, 6]\n",
      "episode 5634, reward 890.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 134\n",
      "Initial State is  [0, 12, 5]\n",
      "episode 5635, reward 759.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 127\n",
      "Initial State is  [1, 23, 2]\n",
      "episode 5636, reward 589.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 145\n",
      "Initial State is  [2, 10, 3]\n",
      "episode 5637, reward 1166.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 143\n",
      "Initial State is  [1, 15, 6]\n",
      "episode 5638, reward 796.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 138\n",
      "Initial State is  [1, 22, 2]\n",
      "episode 5639, reward 756.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 137\n",
      "Initial State is  [0, 8, 2]\n",
      "episode 5640, reward 925.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 137\n",
      "Initial State is  [4, 3, 3]\n",
      "episode 5641, reward 638.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 146\n",
      "Initial State is  [2, 19, 2]\n",
      "episode 5642, reward 695.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 125\n",
      "Initial State is  [4, 13, 4]\n",
      "episode 5643, reward 810.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 126\n",
      "Initial State is  [1, 2, 5]\n",
      "episode 5644, reward 1058.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 134\n",
      "Initial State is  [1, 3, 6]\n",
      "episode 5645, reward 901.0, memory_length 2000, epsilon 0.009998671593271896, time 736.0, rides 123\n",
      "Initial State is  [3, 17, 6]\n",
      "episode 5646, reward 800.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 140\n",
      "Initial State is  [2, 19, 5]\n",
      "episode 5647, reward 1114.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 149\n",
      "Initial State is  [3, 5, 6]\n",
      "episode 5648, reward 713.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 137\n",
      "Initial State is  [2, 0, 0]\n",
      "episode 5649, reward 1172.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 128\n",
      "Initial State is  [3, 6, 2]\n",
      "episode 5650, reward 766.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 127\n",
      "Initial State is  [4, 13, 5]\n",
      "episode 5651, reward 871.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 131\n",
      "Initial State is  [4, 1, 0]\n",
      "episode 5652, reward 1081.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 129\n",
      "Initial State is  [2, 11, 6]\n",
      "episode 5653, reward 642.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 130\n",
      "Initial State is  [3, 11, 6]\n",
      "episode 5654, reward 1009.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 148\n",
      "Initial State is  [3, 17, 0]\n",
      "episode 5655, reward 1078.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 142\n",
      "Initial State is  [2, 21, 2]\n",
      "episode 5656, reward 1220.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 134\n",
      "Initial State is  [4, 21, 1]\n",
      "episode 5657, reward 997.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 130\n",
      "Initial State is  [0, 17, 0]\n",
      "episode 5658, reward 952.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 129\n",
      "Initial State is  [2, 1, 2]\n",
      "episode 5659, reward 704.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 132\n",
      "Initial State is  [1, 23, 5]\n",
      "episode 5660, reward 1064.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 139\n",
      "Initial State is  [2, 0, 4]\n",
      "episode 5661, reward 976.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 134\n",
      "Initial State is  [2, 20, 5]\n",
      "episode 5662, reward 962.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 130\n",
      "Initial State is  [3, 13, 1]\n",
      "episode 5663, reward 1092.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 139\n",
      "Initial State is  [2, 0, 5]\n",
      "episode 5664, reward 801.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 131\n",
      "Initial State is  [4, 8, 1]\n",
      "episode 5665, reward 792.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 133\n",
      "Initial State is  [1, 7, 3]\n",
      "episode 5666, reward 950.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 137\n",
      "Initial State is  [0, 5, 3]\n",
      "episode 5667, reward 1175.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 137\n",
      "Initial State is  [4, 21, 6]\n",
      "episode 5668, reward 1299.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 143\n",
      "Initial State is  [2, 17, 5]\n",
      "episode 5669, reward 910.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 142\n",
      "Initial State is  [0, 15, 4]\n",
      "episode 5670, reward 868.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 130\n",
      "Initial State is  [2, 11, 6]\n",
      "episode 5671, reward 880.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 144\n",
      "Initial State is  [0, 12, 5]\n",
      "episode 5672, reward 999.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 131\n",
      "Initial State is  [2, 3, 0]\n",
      "episode 5673, reward 1069.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 148\n",
      "Initial State is  [4, 8, 6]\n",
      "episode 5674, reward 1231.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 148\n",
      "Initial State is  [1, 3, 2]\n",
      "episode 5675, reward 1234.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 124\n",
      "Initial State is  [0, 22, 1]\n",
      "episode 5676, reward 795.0, memory_length 2000, epsilon 0.009998671593271896, time 721.0, rides 130\n",
      "Initial State is  [3, 13, 1]\n",
      "episode 5677, reward 1155.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 150\n",
      "Initial State is  [1, 22, 3]\n",
      "episode 5678, reward 1186.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 132\n",
      "Initial State is  [2, 10, 2]\n",
      "episode 5679, reward 792.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 124\n",
      "Initial State is  [2, 23, 5]\n",
      "episode 5680, reward 938.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 139\n",
      "Initial State is  [1, 19, 1]\n",
      "episode 5681, reward 1047.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 142\n",
      "Initial State is  [3, 22, 6]\n",
      "episode 5682, reward 736.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 118\n",
      "Initial State is  [1, 9, 6]\n",
      "episode 5683, reward 808.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 141\n",
      "Initial State is  [1, 20, 4]\n",
      "episode 5684, reward 931.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 144\n",
      "Initial State is  [2, 3, 3]\n",
      "episode 5685, reward 1048.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 148\n",
      "Initial State is  [2, 11, 2]\n",
      "episode 5686, reward 918.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 137\n",
      "Initial State is  [4, 4, 0]\n",
      "episode 5687, reward 1095.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 133\n",
      "Initial State is  [4, 6, 0]\n",
      "episode 5688, reward 909.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 132\n",
      "Initial State is  [0, 3, 5]\n",
      "episode 5689, reward 1112.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 125\n",
      "Initial State is  [0, 21, 2]\n",
      "episode 5690, reward 899.0, memory_length 2000, epsilon 0.009998671593271896, time 737.0, rides 139\n",
      "Initial State is  [4, 23, 0]\n",
      "episode 5691, reward 1074.0, memory_length 2000, epsilon 0.009998671593271896, time 721.0, rides 136\n",
      "Initial State is  [0, 4, 5]\n",
      "episode 5692, reward 937.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 117\n",
      "Initial State is  [4, 14, 2]\n",
      "episode 5693, reward 1008.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 134\n",
      "Initial State is  [1, 8, 2]\n",
      "episode 5694, reward 1059.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 137\n",
      "Initial State is  [4, 18, 6]\n",
      "episode 5695, reward 1029.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 128\n",
      "Initial State is  [1, 9, 1]\n",
      "episode 5696, reward 957.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 140\n",
      "Initial State is  [4, 16, 3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 5697, reward 1026.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 136\n",
      "Initial State is  [0, 8, 4]\n",
      "episode 5698, reward 1126.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 140\n",
      "Initial State is  [2, 1, 5]\n",
      "episode 5699, reward 1057.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 130\n",
      "Initial State is  [2, 2, 1]\n",
      "episode 5700, reward 1075.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 132\n",
      "Initial State is  [1, 12, 3]\n",
      "episode 5701, reward 1158.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 135\n",
      "Initial State is  [1, 12, 4]\n",
      "episode 5702, reward 1119.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 136\n",
      "Initial State is  [3, 5, 2]\n",
      "episode 5703, reward 642.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 137\n",
      "Initial State is  [2, 19, 5]\n",
      "episode 5704, reward 1160.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 143\n",
      "Initial State is  [0, 8, 0]\n",
      "episode 5705, reward 1080.0, memory_length 2000, epsilon 0.009998671593271896, time 721.0, rides 139\n",
      "Initial State is  [0, 6, 6]\n",
      "episode 5706, reward 1244.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 125\n",
      "Initial State is  [0, 8, 6]\n",
      "episode 5707, reward 1122.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 131\n",
      "Initial State is  [0, 11, 5]\n",
      "episode 5708, reward 824.0, memory_length 2000, epsilon 0.009998671593271896, time 721.0, rides 134\n",
      "Initial State is  [3, 13, 5]\n",
      "episode 5709, reward 1119.0, memory_length 2000, epsilon 0.009998671593271896, time 736.0, rides 129\n",
      "Initial State is  [1, 16, 3]\n",
      "episode 5710, reward 1021.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 135\n",
      "Initial State is  [4, 20, 2]\n",
      "episode 5711, reward 1276.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 125\n",
      "Initial State is  [1, 15, 3]\n",
      "episode 5712, reward 1165.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 144\n",
      "Initial State is  [1, 0, 5]\n",
      "episode 5713, reward 1014.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 137\n",
      "Initial State is  [3, 8, 5]\n",
      "episode 5714, reward 1079.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 135\n",
      "Initial State is  [4, 15, 1]\n",
      "episode 5715, reward 867.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 130\n",
      "Initial State is  [3, 9, 0]\n",
      "episode 5716, reward 1038.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 141\n",
      "Initial State is  [1, 4, 0]\n",
      "episode 5717, reward 808.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 128\n",
      "Initial State is  [0, 18, 1]\n",
      "episode 5718, reward 902.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 130\n",
      "Initial State is  [2, 4, 2]\n",
      "episode 5719, reward 716.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 135\n",
      "Initial State is  [0, 10, 3]\n",
      "episode 5720, reward 1030.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 136\n",
      "Initial State is  [2, 12, 4]\n",
      "episode 5721, reward 624.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 127\n",
      "Initial State is  [0, 8, 0]\n",
      "episode 5722, reward 942.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 140\n",
      "Initial State is  [1, 5, 0]\n",
      "episode 5723, reward 820.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 130\n",
      "Initial State is  [1, 0, 1]\n",
      "episode 5724, reward 1090.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 133\n",
      "Initial State is  [3, 9, 4]\n",
      "episode 5725, reward 911.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 124\n",
      "Initial State is  [1, 22, 0]\n",
      "episode 5726, reward 967.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 143\n",
      "Initial State is  [3, 0, 0]\n",
      "episode 5727, reward 1193.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 135\n",
      "Initial State is  [1, 1, 4]\n",
      "episode 5728, reward 973.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 131\n",
      "Initial State is  [3, 6, 1]\n",
      "episode 5729, reward 822.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 137\n",
      "Initial State is  [4, 14, 1]\n",
      "episode 5730, reward 722.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 124\n",
      "Initial State is  [1, 11, 3]\n",
      "episode 5731, reward 908.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 135\n",
      "Initial State is  [4, 20, 1]\n",
      "episode 5732, reward 1334.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 128\n",
      "Initial State is  [3, 15, 2]\n",
      "episode 5733, reward 1029.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 127\n",
      "Initial State is  [2, 5, 2]\n",
      "episode 5734, reward 866.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 143\n",
      "Initial State is  [0, 16, 4]\n",
      "episode 5735, reward 920.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 133\n",
      "Initial State is  [0, 4, 6]\n",
      "episode 5736, reward 843.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 131\n",
      "Initial State is  [4, 9, 1]\n",
      "episode 5737, reward 816.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 147\n",
      "Initial State is  [4, 0, 0]\n",
      "episode 5738, reward 956.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 139\n",
      "Initial State is  [3, 2, 5]\n",
      "episode 5739, reward 973.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 131\n",
      "Initial State is  [0, 11, 4]\n",
      "episode 5740, reward 751.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 140\n",
      "Initial State is  [0, 14, 3]\n",
      "episode 5741, reward 983.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 139\n",
      "Initial State is  [0, 19, 3]\n",
      "episode 5742, reward 943.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 137\n",
      "Initial State is  [2, 23, 2]\n",
      "episode 5743, reward 765.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 140\n",
      "Initial State is  [4, 15, 1]\n",
      "episode 5744, reward 1085.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 135\n",
      "Initial State is  [0, 7, 2]\n",
      "episode 5745, reward 694.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 109\n",
      "Initial State is  [0, 13, 1]\n",
      "episode 5746, reward 922.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 132\n",
      "Initial State is  [2, 20, 0]\n",
      "episode 5747, reward 902.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 131\n",
      "Initial State is  [0, 14, 3]\n",
      "episode 5748, reward 938.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 129\n",
      "Initial State is  [2, 5, 6]\n",
      "episode 5749, reward 761.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 127\n",
      "Initial State is  [0, 12, 2]\n",
      "episode 5750, reward 940.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 130\n",
      "Initial State is  [0, 22, 1]\n",
      "episode 5751, reward 751.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 124\n",
      "Initial State is  [0, 5, 1]\n",
      "episode 5752, reward 711.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 128\n",
      "Initial State is  [2, 23, 4]\n",
      "episode 5753, reward 869.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 135\n",
      "Initial State is  [1, 4, 3]\n",
      "episode 5754, reward 701.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 130\n",
      "Initial State is  [2, 23, 2]\n",
      "episode 5755, reward 688.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 125\n",
      "Initial State is  [4, 20, 1]\n",
      "episode 5756, reward 1025.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 136\n",
      "Initial State is  [3, 9, 5]\n",
      "episode 5757, reward 775.0, memory_length 2000, epsilon 0.009998671593271896, time 736.0, rides 134\n",
      "Initial State is  [1, 7, 1]\n",
      "episode 5758, reward 643.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 123\n",
      "Initial State is  [2, 13, 0]\n",
      "episode 5759, reward 1072.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 132\n",
      "Initial State is  [0, 13, 0]\n",
      "episode 5760, reward 1252.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 144\n",
      "Initial State is  [0, 16, 4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 5761, reward 961.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 139\n",
      "Initial State is  [4, 0, 1]\n",
      "episode 5762, reward 1036.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 141\n",
      "Initial State is  [3, 9, 2]\n",
      "episode 5763, reward 723.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 140\n",
      "Initial State is  [2, 1, 3]\n",
      "episode 5764, reward 788.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 133\n",
      "Initial State is  [0, 22, 0]\n",
      "episode 5765, reward 1148.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 136\n",
      "Initial State is  [1, 15, 6]\n",
      "episode 5766, reward 810.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 131\n",
      "Initial State is  [0, 15, 6]\n",
      "episode 5767, reward 1014.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 143\n",
      "Initial State is  [3, 18, 2]\n",
      "episode 5768, reward 899.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 125\n",
      "Initial State is  [0, 21, 4]\n",
      "episode 5769, reward 997.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 125\n",
      "Initial State is  [3, 1, 2]\n",
      "episode 5770, reward 910.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 127\n",
      "Initial State is  [4, 4, 4]\n",
      "episode 5771, reward 1049.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 138\n",
      "Initial State is  [4, 9, 2]\n",
      "episode 5772, reward 812.0, memory_length 2000, epsilon 0.009998671593271896, time 741.0, rides 128\n",
      "Initial State is  [3, 21, 3]\n",
      "episode 5773, reward 1258.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 142\n",
      "Initial State is  [2, 19, 4]\n",
      "episode 5774, reward 1098.0, memory_length 2000, epsilon 0.009998671593271896, time 738.0, rides 130\n",
      "Initial State is  [1, 5, 0]\n",
      "episode 5775, reward 887.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 126\n",
      "Initial State is  [1, 13, 5]\n",
      "episode 5776, reward 817.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 125\n",
      "Initial State is  [1, 9, 1]\n",
      "episode 5777, reward 796.0, memory_length 2000, epsilon 0.009998671593271896, time 738.0, rides 124\n",
      "Initial State is  [0, 4, 4]\n",
      "episode 5778, reward 838.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 133\n",
      "Initial State is  [1, 10, 4]\n",
      "episode 5779, reward 1018.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 125\n",
      "Initial State is  [2, 0, 2]\n",
      "episode 5780, reward 1031.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 124\n",
      "Initial State is  [1, 3, 0]\n",
      "episode 5781, reward 1287.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 130\n",
      "Initial State is  [1, 19, 5]\n",
      "episode 5782, reward 698.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 128\n",
      "Initial State is  [0, 3, 3]\n",
      "episode 5783, reward 879.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 129\n",
      "Initial State is  [1, 1, 2]\n",
      "episode 5784, reward 832.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 140\n",
      "Initial State is  [3, 10, 3]\n",
      "episode 5785, reward 674.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 127\n",
      "Initial State is  [0, 14, 0]\n",
      "episode 5786, reward 1270.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 146\n",
      "Initial State is  [0, 4, 2]\n",
      "episode 5787, reward 1088.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 137\n",
      "Initial State is  [4, 9, 0]\n",
      "episode 5788, reward 1051.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 116\n",
      "Initial State is  [1, 22, 4]\n",
      "episode 5789, reward 944.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 132\n",
      "Initial State is  [3, 7, 2]\n",
      "episode 5790, reward 1226.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 134\n",
      "Initial State is  [0, 22, 4]\n",
      "episode 5791, reward 1211.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 146\n",
      "Initial State is  [4, 9, 0]\n",
      "episode 5792, reward 808.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 136\n",
      "Initial State is  [0, 14, 1]\n",
      "episode 5793, reward 1086.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 136\n",
      "Initial State is  [0, 1, 2]\n",
      "episode 5794, reward 780.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 143\n",
      "Initial State is  [0, 3, 1]\n",
      "episode 5795, reward 987.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 135\n",
      "Initial State is  [4, 6, 4]\n",
      "episode 5796, reward 883.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 135\n",
      "Initial State is  [0, 11, 3]\n",
      "episode 5797, reward 1091.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 129\n",
      "Initial State is  [1, 6, 5]\n",
      "episode 5798, reward 1250.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 132\n",
      "Initial State is  [0, 10, 2]\n",
      "episode 5799, reward 1370.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 132\n",
      "Initial State is  [2, 15, 3]\n",
      "episode 5800, reward 1193.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 137\n",
      "Initial State is  [4, 19, 0]\n",
      "episode 5801, reward 1099.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 136\n",
      "Initial State is  [4, 16, 3]\n",
      "episode 5802, reward 862.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 136\n",
      "Initial State is  [3, 6, 6]\n",
      "episode 5803, reward 776.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 122\n",
      "Initial State is  [1, 8, 2]\n",
      "episode 5804, reward 1187.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 142\n",
      "Initial State is  [2, 8, 4]\n",
      "episode 5805, reward 962.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 142\n",
      "Initial State is  [1, 20, 4]\n",
      "episode 5806, reward 695.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 127\n",
      "Initial State is  [3, 18, 4]\n",
      "episode 5807, reward 984.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 128\n",
      "Initial State is  [3, 13, 2]\n",
      "episode 5808, reward 813.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 145\n",
      "Initial State is  [2, 14, 4]\n",
      "episode 5809, reward 904.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 137\n",
      "Initial State is  [3, 0, 6]\n",
      "episode 5810, reward 928.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 143\n",
      "Initial State is  [0, 2, 1]\n",
      "episode 5811, reward 821.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 139\n",
      "Initial State is  [1, 0, 3]\n",
      "episode 5812, reward 969.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 149\n",
      "Initial State is  [1, 10, 0]\n",
      "episode 5813, reward 1066.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 140\n",
      "Initial State is  [1, 23, 6]\n",
      "episode 5814, reward 984.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 130\n",
      "Initial State is  [2, 6, 5]\n",
      "episode 5815, reward 1034.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 128\n",
      "Initial State is  [4, 12, 4]\n",
      "episode 5816, reward 951.0, memory_length 2000, epsilon 0.009998671593271896, time 721.0, rides 130\n",
      "Initial State is  [1, 11, 2]\n",
      "episode 5817, reward 1223.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 148\n",
      "Initial State is  [0, 9, 4]\n",
      "episode 5818, reward 920.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 138\n",
      "Initial State is  [0, 7, 4]\n",
      "episode 5819, reward 1182.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 133\n",
      "Initial State is  [4, 19, 0]\n",
      "episode 5820, reward 1347.0, memory_length 2000, epsilon 0.009998671593271896, time 721.0, rides 132\n",
      "Initial State is  [1, 12, 1]\n",
      "episode 5821, reward 1026.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 133\n",
      "Initial State is  [4, 3, 2]\n",
      "episode 5822, reward 960.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 133\n",
      "Initial State is  [1, 10, 2]\n",
      "episode 5823, reward 975.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 133\n",
      "Initial State is  [4, 22, 3]\n",
      "episode 5824, reward 1007.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 135\n",
      "Initial State is  [2, 6, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 5825, reward 899.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 135\n",
      "Initial State is  [3, 6, 5]\n",
      "episode 5826, reward 1089.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 131\n",
      "Initial State is  [0, 20, 3]\n",
      "episode 5827, reward 980.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 131\n",
      "Initial State is  [3, 6, 5]\n",
      "episode 5828, reward 1172.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 137\n",
      "Initial State is  [2, 7, 3]\n",
      "episode 5829, reward 738.0, memory_length 2000, epsilon 0.009998671593271896, time 737.0, rides 151\n",
      "Initial State is  [3, 11, 5]\n",
      "episode 5830, reward 946.0, memory_length 2000, epsilon 0.009998671593271896, time 738.0, rides 137\n",
      "Initial State is  [4, 7, 5]\n",
      "episode 5831, reward 1033.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 141\n",
      "Initial State is  [2, 22, 4]\n",
      "episode 5832, reward 1027.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 133\n",
      "Initial State is  [4, 20, 5]\n",
      "episode 5833, reward 1112.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 123\n",
      "Initial State is  [4, 14, 0]\n",
      "episode 5834, reward 985.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 131\n",
      "Initial State is  [1, 3, 0]\n",
      "episode 5835, reward 980.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 142\n",
      "Initial State is  [2, 20, 6]\n",
      "episode 5836, reward 1007.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 133\n",
      "Initial State is  [0, 11, 2]\n",
      "episode 5837, reward 823.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 144\n",
      "Initial State is  [1, 21, 5]\n",
      "episode 5838, reward 995.0, memory_length 2000, epsilon 0.009998671593271896, time 737.0, rides 144\n",
      "Initial State is  [2, 9, 4]\n",
      "episode 5839, reward 1031.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 127\n",
      "Initial State is  [1, 1, 1]\n",
      "episode 5840, reward 854.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 131\n",
      "Initial State is  [4, 19, 0]\n",
      "episode 5841, reward 1053.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 140\n",
      "Initial State is  [4, 8, 5]\n",
      "episode 5842, reward 1218.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 137\n",
      "Initial State is  [0, 14, 1]\n",
      "episode 5843, reward 992.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 141\n",
      "Initial State is  [0, 8, 6]\n",
      "episode 5844, reward 1090.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 144\n",
      "Initial State is  [4, 19, 4]\n",
      "episode 5845, reward 834.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 131\n",
      "Initial State is  [1, 14, 2]\n",
      "episode 5846, reward 771.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 141\n",
      "Initial State is  [0, 2, 4]\n",
      "episode 5847, reward 1064.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 146\n",
      "Initial State is  [2, 10, 2]\n",
      "episode 5848, reward 957.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 136\n",
      "Initial State is  [4, 17, 0]\n",
      "episode 5849, reward 1099.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 127\n",
      "Initial State is  [2, 21, 1]\n",
      "episode 5850, reward 997.0, memory_length 2000, epsilon 0.009998671593271896, time 721.0, rides 154\n",
      "Initial State is  [0, 0, 0]\n",
      "episode 5851, reward 655.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 133\n",
      "Initial State is  [4, 8, 2]\n",
      "episode 5852, reward 735.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 139\n",
      "Initial State is  [3, 6, 6]\n",
      "episode 5853, reward 1151.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 140\n",
      "Initial State is  [3, 7, 5]\n",
      "episode 5854, reward 982.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 131\n",
      "Initial State is  [3, 18, 6]\n",
      "episode 5855, reward 887.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 152\n",
      "Initial State is  [3, 7, 3]\n",
      "episode 5856, reward 1131.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 128\n",
      "Initial State is  [1, 13, 2]\n",
      "episode 5857, reward 886.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 138\n",
      "Initial State is  [0, 17, 6]\n",
      "episode 5858, reward 964.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 136\n",
      "Initial State is  [3, 11, 3]\n",
      "episode 5859, reward 751.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 130\n",
      "Initial State is  [1, 3, 5]\n",
      "episode 5860, reward 707.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 137\n",
      "Initial State is  [3, 21, 0]\n",
      "episode 5861, reward 1038.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 122\n",
      "Initial State is  [3, 21, 0]\n",
      "episode 5862, reward 1052.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 128\n",
      "Initial State is  [0, 23, 6]\n",
      "episode 5863, reward 964.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 141\n",
      "Initial State is  [3, 20, 5]\n",
      "episode 5864, reward 800.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 129\n",
      "Initial State is  [2, 14, 2]\n",
      "episode 5865, reward 957.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 127\n",
      "Initial State is  [3, 23, 6]\n",
      "episode 5866, reward 1009.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 134\n",
      "Initial State is  [3, 22, 4]\n",
      "episode 5867, reward 1047.0, memory_length 2000, epsilon 0.009998671593271896, time 736.0, rides 125\n",
      "Initial State is  [4, 7, 5]\n",
      "episode 5868, reward 909.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 129\n",
      "Initial State is  [4, 13, 3]\n",
      "episode 5869, reward 952.0, memory_length 2000, epsilon 0.009998671593271896, time 721.0, rides 141\n",
      "Initial State is  [3, 21, 1]\n",
      "episode 5870, reward 921.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 135\n",
      "Initial State is  [4, 13, 6]\n",
      "episode 5871, reward 955.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 139\n",
      "Initial State is  [3, 1, 0]\n",
      "episode 5872, reward 1099.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 124\n",
      "Initial State is  [3, 6, 5]\n",
      "episode 5873, reward 995.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 140\n",
      "Initial State is  [3, 9, 3]\n",
      "episode 5874, reward 982.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 145\n",
      "Initial State is  [0, 23, 1]\n",
      "episode 5875, reward 1065.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 131\n",
      "Initial State is  [3, 23, 4]\n",
      "episode 5876, reward 810.0, memory_length 2000, epsilon 0.009998671593271896, time 740.0, rides 131\n",
      "Initial State is  [4, 13, 6]\n",
      "episode 5877, reward 820.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 140\n",
      "Initial State is  [3, 7, 2]\n",
      "episode 5878, reward 798.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 139\n",
      "Initial State is  [2, 1, 1]\n",
      "episode 5879, reward 1074.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 130\n",
      "Initial State is  [3, 9, 0]\n",
      "episode 5880, reward 919.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 139\n",
      "Initial State is  [4, 5, 1]\n",
      "episode 5881, reward 1092.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 129\n",
      "Initial State is  [2, 15, 5]\n",
      "episode 5882, reward 1014.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 142\n",
      "Initial State is  [0, 13, 3]\n",
      "episode 5883, reward 1237.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 131\n",
      "Initial State is  [3, 13, 4]\n",
      "episode 5884, reward 657.0, memory_length 2000, epsilon 0.009998671593271896, time 738.0, rides 137\n",
      "Initial State is  [2, 22, 6]\n",
      "episode 5885, reward 1100.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 149\n",
      "Initial State is  [1, 16, 3]\n",
      "episode 5886, reward 1113.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 142\n",
      "Initial State is  [1, 16, 6]\n",
      "episode 5887, reward 1242.0, memory_length 2000, epsilon 0.009998671593271896, time 736.0, rides 133\n",
      "Initial State is  [1, 22, 3]\n",
      "episode 5888, reward 1009.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 135\n",
      "Initial State is  [3, 4, 3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 5889, reward 1127.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 138\n",
      "Initial State is  [0, 4, 4]\n",
      "episode 5890, reward 1170.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 139\n",
      "Initial State is  [1, 5, 0]\n",
      "episode 5891, reward 1128.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 131\n",
      "Initial State is  [1, 4, 0]\n",
      "episode 5892, reward 1303.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 133\n",
      "Initial State is  [2, 5, 2]\n",
      "episode 5893, reward 817.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 142\n",
      "Initial State is  [2, 12, 5]\n",
      "episode 5894, reward 662.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 129\n",
      "Initial State is  [1, 18, 5]\n",
      "episode 5895, reward 1044.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 132\n",
      "Initial State is  [2, 10, 2]\n",
      "episode 5896, reward 914.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 140\n",
      "Initial State is  [2, 9, 3]\n",
      "episode 5897, reward 967.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 138\n",
      "Initial State is  [1, 13, 4]\n",
      "episode 5898, reward 858.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 125\n",
      "Initial State is  [4, 2, 4]\n",
      "episode 5899, reward 781.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 142\n",
      "Initial State is  [3, 3, 3]\n",
      "episode 5900, reward 894.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 130\n",
      "Initial State is  [3, 0, 6]\n",
      "episode 5901, reward 776.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 131\n",
      "Initial State is  [2, 13, 6]\n",
      "episode 5902, reward 1073.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 147\n",
      "Initial State is  [1, 23, 2]\n",
      "episode 5903, reward 1059.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 132\n",
      "Initial State is  [1, 3, 5]\n",
      "episode 5904, reward 1348.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 133\n",
      "Initial State is  [3, 0, 6]\n",
      "episode 5905, reward 793.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 137\n",
      "Initial State is  [1, 4, 3]\n",
      "episode 5906, reward 964.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 139\n",
      "Initial State is  [0, 13, 6]\n",
      "episode 5907, reward 1073.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 139\n",
      "Initial State is  [0, 10, 3]\n",
      "episode 5908, reward 949.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 141\n",
      "Initial State is  [0, 2, 6]\n",
      "episode 5909, reward 807.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 147\n",
      "Initial State is  [0, 10, 5]\n",
      "episode 5910, reward 925.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 138\n",
      "Initial State is  [2, 22, 0]\n",
      "episode 5911, reward 875.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 149\n",
      "Initial State is  [4, 0, 6]\n",
      "episode 5912, reward 872.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 120\n",
      "Initial State is  [3, 4, 6]\n",
      "episode 5913, reward 1009.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 148\n",
      "Initial State is  [4, 14, 2]\n",
      "episode 5914, reward 945.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 143\n",
      "Initial State is  [3, 14, 0]\n",
      "episode 5915, reward 783.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 122\n",
      "Initial State is  [4, 15, 1]\n",
      "episode 5916, reward 697.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 136\n",
      "Initial State is  [2, 4, 1]\n",
      "episode 5917, reward 1051.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 131\n",
      "Initial State is  [2, 21, 6]\n",
      "episode 5918, reward 980.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 131\n",
      "Initial State is  [3, 21, 3]\n",
      "episode 5919, reward 824.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 130\n",
      "Initial State is  [4, 4, 5]\n",
      "episode 5920, reward 859.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 146\n",
      "Initial State is  [3, 12, 2]\n",
      "episode 5921, reward 1017.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 122\n",
      "Initial State is  [0, 12, 1]\n",
      "episode 5922, reward 1095.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 135\n",
      "Initial State is  [4, 14, 5]\n",
      "episode 5923, reward 1075.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 130\n",
      "Initial State is  [4, 10, 6]\n",
      "episode 5924, reward 816.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 134\n",
      "Initial State is  [0, 19, 3]\n",
      "episode 5925, reward 1233.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 138\n",
      "Initial State is  [3, 12, 1]\n",
      "episode 5926, reward 1189.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 129\n",
      "Initial State is  [3, 2, 3]\n",
      "episode 5927, reward 907.0, memory_length 2000, epsilon 0.009998671593271896, time 741.0, rides 138\n",
      "Initial State is  [0, 12, 4]\n",
      "episode 5928, reward 600.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 137\n",
      "Initial State is  [3, 14, 6]\n",
      "episode 5929, reward 1005.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 124\n",
      "Initial State is  [0, 1, 3]\n",
      "episode 5930, reward 1244.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 139\n",
      "Initial State is  [4, 20, 3]\n",
      "episode 5931, reward 1150.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 139\n",
      "Initial State is  [1, 16, 3]\n",
      "episode 5932, reward 899.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 142\n",
      "Initial State is  [1, 6, 6]\n",
      "episode 5933, reward 486.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 127\n",
      "Initial State is  [3, 23, 0]\n",
      "episode 5934, reward 1144.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 136\n",
      "Initial State is  [4, 0, 3]\n",
      "episode 5935, reward 610.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 139\n",
      "Initial State is  [2, 4, 2]\n",
      "episode 5936, reward 875.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 131\n",
      "Initial State is  [3, 23, 3]\n",
      "episode 5937, reward 1113.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 133\n",
      "Initial State is  [3, 17, 5]\n",
      "episode 5938, reward 671.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 122\n",
      "Initial State is  [3, 10, 3]\n",
      "episode 5939, reward 812.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 136\n",
      "Initial State is  [0, 18, 5]\n",
      "episode 5940, reward 970.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 140\n",
      "Initial State is  [3, 4, 0]\n",
      "episode 5941, reward 724.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 119\n",
      "Initial State is  [0, 22, 2]\n",
      "episode 5942, reward 875.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 133\n",
      "Initial State is  [2, 21, 4]\n",
      "episode 5943, reward 656.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 128\n",
      "Initial State is  [2, 15, 1]\n",
      "episode 5944, reward 864.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 134\n",
      "Initial State is  [1, 7, 3]\n",
      "episode 5945, reward 1027.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 131\n",
      "Initial State is  [3, 3, 2]\n",
      "episode 5946, reward 721.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 128\n",
      "Initial State is  [1, 9, 0]\n",
      "episode 5947, reward 1028.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 121\n",
      "Initial State is  [4, 13, 2]\n",
      "episode 5948, reward 913.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 132\n",
      "Initial State is  [4, 12, 1]\n",
      "episode 5949, reward 1082.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 118\n",
      "Initial State is  [2, 22, 4]\n",
      "episode 5950, reward 926.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 123\n",
      "Initial State is  [3, 18, 2]\n",
      "episode 5951, reward 988.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 127\n",
      "Initial State is  [0, 23, 0]\n",
      "episode 5952, reward 1017.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 123\n",
      "Initial State is  [1, 12, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 5953, reward 578.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 123\n",
      "Initial State is  [2, 6, 0]\n",
      "episode 5954, reward 785.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 131\n",
      "Initial State is  [2, 17, 2]\n",
      "episode 5955, reward 1047.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 129\n",
      "Initial State is  [1, 11, 2]\n",
      "episode 5956, reward 1075.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 119\n",
      "Initial State is  [3, 15, 6]\n",
      "episode 5957, reward 978.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 137\n",
      "Initial State is  [4, 14, 3]\n",
      "episode 5958, reward 998.0, memory_length 2000, epsilon 0.009998671593271896, time 721.0, rides 139\n",
      "Initial State is  [3, 17, 4]\n",
      "episode 5959, reward 1015.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 151\n",
      "Initial State is  [0, 7, 0]\n",
      "episode 5960, reward 1332.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 123\n",
      "Initial State is  [4, 18, 5]\n",
      "episode 5961, reward 806.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 140\n",
      "Initial State is  [0, 6, 0]\n",
      "episode 5962, reward 978.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 138\n",
      "Initial State is  [0, 18, 2]\n",
      "episode 5963, reward 799.0, memory_length 2000, epsilon 0.009998671593271896, time 736.0, rides 148\n",
      "Initial State is  [4, 21, 3]\n",
      "episode 5964, reward 830.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 140\n",
      "Initial State is  [1, 0, 5]\n",
      "episode 5965, reward 988.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 131\n",
      "Initial State is  [4, 19, 0]\n",
      "episode 5966, reward 1078.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 119\n",
      "Initial State is  [0, 16, 1]\n",
      "episode 5967, reward 1218.0, memory_length 2000, epsilon 0.009998671593271896, time 739.0, rides 133\n",
      "Initial State is  [3, 23, 4]\n",
      "episode 5968, reward 1052.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 136\n",
      "Initial State is  [0, 20, 3]\n",
      "episode 5969, reward 971.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 133\n",
      "Initial State is  [0, 14, 0]\n",
      "episode 5970, reward 670.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 133\n",
      "Initial State is  [2, 2, 4]\n",
      "episode 5971, reward 1064.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 129\n",
      "Initial State is  [4, 6, 6]\n",
      "episode 5972, reward 1023.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 142\n",
      "Initial State is  [0, 10, 3]\n",
      "episode 5973, reward 1155.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 130\n",
      "Initial State is  [0, 16, 0]\n",
      "episode 5974, reward 1320.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 138\n",
      "Initial State is  [2, 13, 2]\n",
      "episode 5975, reward 1026.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 125\n",
      "Initial State is  [3, 0, 5]\n",
      "episode 5976, reward 526.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 133\n",
      "Initial State is  [3, 7, 1]\n",
      "episode 5977, reward 1405.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 146\n",
      "Initial State is  [4, 15, 5]\n",
      "episode 5978, reward 825.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 130\n",
      "Initial State is  [3, 21, 0]\n",
      "episode 5979, reward 585.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 133\n",
      "Initial State is  [2, 18, 2]\n",
      "episode 5980, reward 1036.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 131\n",
      "Initial State is  [4, 15, 1]\n",
      "episode 5981, reward 884.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 123\n",
      "Initial State is  [0, 18, 3]\n",
      "episode 5982, reward 1018.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 140\n",
      "Initial State is  [3, 10, 5]\n",
      "episode 5983, reward 648.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 138\n",
      "Initial State is  [3, 10, 4]\n",
      "episode 5984, reward 1084.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 133\n",
      "Initial State is  [1, 20, 4]\n",
      "episode 5985, reward 787.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 126\n",
      "Initial State is  [0, 17, 4]\n",
      "episode 5986, reward 909.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 136\n",
      "Initial State is  [1, 18, 0]\n",
      "episode 5987, reward 918.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 128\n",
      "Initial State is  [3, 10, 5]\n",
      "episode 5988, reward 967.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 134\n",
      "Initial State is  [3, 8, 3]\n",
      "episode 5989, reward 813.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 144\n",
      "Initial State is  [3, 11, 6]\n",
      "episode 5990, reward 1022.0, memory_length 2000, epsilon 0.009998671593271896, time 721.0, rides 138\n",
      "Initial State is  [3, 1, 3]\n",
      "episode 5991, reward 853.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 135\n",
      "Initial State is  [2, 7, 6]\n",
      "episode 5992, reward 948.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 124\n",
      "Initial State is  [0, 7, 4]\n",
      "episode 5993, reward 954.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 141\n",
      "Initial State is  [2, 2, 3]\n",
      "episode 5994, reward 1144.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 145\n",
      "Initial State is  [2, 1, 2]\n",
      "episode 5995, reward 781.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 129\n",
      "Initial State is  [1, 3, 2]\n",
      "episode 5996, reward 839.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 120\n",
      "Initial State is  [1, 3, 5]\n",
      "episode 5997, reward 838.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 136\n",
      "Initial State is  [4, 3, 4]\n",
      "episode 5998, reward 716.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 134\n",
      "Initial State is  [0, 7, 0]\n",
      "episode 5999, reward 1081.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 130\n",
      "Initial State is  [2, 10, 1]\n",
      "episode 6000, reward 1080.0, memory_length 2000, epsilon 0.009998671593271896, time 741.0, rides 133\n",
      "Initial State is  [4, 10, 0]\n",
      "episode 6001, reward 659.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 144\n",
      "Initial State is  [2, 11, 1]\n",
      "episode 6002, reward 925.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 131\n",
      "Initial State is  [4, 16, 0]\n",
      "episode 6003, reward 1065.0, memory_length 2000, epsilon 0.009998671593271896, time 737.0, rides 137\n",
      "Initial State is  [4, 4, 3]\n",
      "episode 6004, reward 638.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 133\n",
      "Initial State is  [1, 9, 2]\n",
      "episode 6005, reward 1272.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 136\n",
      "Initial State is  [0, 16, 0]\n",
      "episode 6006, reward 634.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 126\n",
      "Initial State is  [3, 13, 4]\n",
      "episode 6007, reward 951.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 137\n",
      "Initial State is  [3, 14, 6]\n",
      "episode 6008, reward 1080.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 149\n",
      "Initial State is  [0, 10, 1]\n",
      "episode 6009, reward 870.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 125\n",
      "Initial State is  [0, 21, 3]\n",
      "episode 6010, reward 975.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 131\n",
      "Initial State is  [3, 17, 3]\n",
      "episode 6011, reward 1071.0, memory_length 2000, epsilon 0.009998671593271896, time 738.0, rides 136\n",
      "Initial State is  [1, 23, 4]\n",
      "episode 6012, reward 690.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 129\n",
      "Initial State is  [0, 17, 3]\n",
      "episode 6013, reward 802.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 129\n",
      "Initial State is  [0, 11, 3]\n",
      "episode 6014, reward 824.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 132\n",
      "Initial State is  [0, 21, 0]\n",
      "episode 6015, reward 927.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 134\n",
      "Initial State is  [1, 21, 3]\n",
      "episode 6016, reward 934.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 132\n",
      "Initial State is  [0, 14, 3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 6017, reward 1017.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 134\n",
      "Initial State is  [1, 12, 5]\n",
      "episode 6018, reward 1063.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 133\n",
      "Initial State is  [2, 0, 4]\n",
      "episode 6019, reward 978.0, memory_length 2000, epsilon 0.009998671593271896, time 742.0, rides 138\n",
      "Initial State is  [2, 7, 1]\n",
      "episode 6020, reward 994.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 130\n",
      "Initial State is  [2, 19, 3]\n",
      "episode 6021, reward 1025.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 130\n",
      "Initial State is  [1, 17, 5]\n",
      "episode 6022, reward 799.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 137\n",
      "Initial State is  [1, 9, 3]\n",
      "episode 6023, reward 944.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 131\n",
      "Initial State is  [1, 10, 1]\n",
      "episode 6024, reward 1118.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 119\n",
      "Initial State is  [4, 18, 2]\n",
      "episode 6025, reward 607.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 130\n",
      "Initial State is  [3, 23, 6]\n",
      "episode 6026, reward 1205.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 129\n",
      "Initial State is  [1, 14, 5]\n",
      "episode 6027, reward 822.0, memory_length 2000, epsilon 0.009998671593271896, time 738.0, rides 138\n",
      "Initial State is  [3, 23, 3]\n",
      "episode 6028, reward 1243.0, memory_length 2000, epsilon 0.009998671593271896, time 738.0, rides 128\n",
      "Initial State is  [1, 2, 4]\n",
      "episode 6029, reward 783.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 130\n",
      "Initial State is  [2, 2, 1]\n",
      "episode 6030, reward 856.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 128\n",
      "Initial State is  [2, 0, 6]\n",
      "episode 6031, reward 826.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 136\n",
      "Initial State is  [4, 23, 1]\n",
      "episode 6032, reward 959.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 140\n",
      "Initial State is  [3, 7, 4]\n",
      "episode 6033, reward 1110.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 134\n",
      "Initial State is  [0, 3, 5]\n",
      "episode 6034, reward 980.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 144\n",
      "Initial State is  [1, 11, 1]\n",
      "episode 6035, reward 976.0, memory_length 2000, epsilon 0.009998671593271896, time 736.0, rides 135\n",
      "Initial State is  [3, 15, 3]\n",
      "episode 6036, reward 1160.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 143\n",
      "Initial State is  [0, 19, 3]\n",
      "episode 6037, reward 982.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 144\n",
      "Initial State is  [3, 19, 6]\n",
      "episode 6038, reward 1019.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 147\n",
      "Initial State is  [4, 11, 5]\n",
      "episode 6039, reward 1282.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 126\n",
      "Initial State is  [1, 9, 5]\n",
      "episode 6040, reward 646.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 128\n",
      "Initial State is  [3, 6, 5]\n",
      "episode 6041, reward 736.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 127\n",
      "Initial State is  [3, 11, 5]\n",
      "episode 6042, reward 984.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 143\n",
      "Initial State is  [4, 23, 5]\n",
      "episode 6043, reward 939.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 125\n",
      "Initial State is  [2, 2, 3]\n",
      "episode 6044, reward 1060.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 137\n",
      "Initial State is  [4, 16, 5]\n",
      "episode 6045, reward 1060.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 125\n",
      "Initial State is  [2, 12, 5]\n",
      "episode 6046, reward 983.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 135\n",
      "Initial State is  [4, 13, 4]\n",
      "episode 6047, reward 783.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 130\n",
      "Initial State is  [2, 13, 2]\n",
      "episode 6048, reward 851.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 129\n",
      "Initial State is  [4, 17, 1]\n",
      "episode 6049, reward 868.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 127\n",
      "Initial State is  [2, 16, 6]\n",
      "episode 6050, reward 743.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 129\n",
      "Initial State is  [1, 0, 1]\n",
      "episode 6051, reward 1012.0, memory_length 2000, epsilon 0.009998671593271896, time 737.0, rides 128\n",
      "Initial State is  [0, 2, 1]\n",
      "episode 6052, reward 1138.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 130\n",
      "Initial State is  [2, 10, 5]\n",
      "episode 6053, reward 1196.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 133\n",
      "Initial State is  [3, 16, 0]\n",
      "episode 6054, reward 1053.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 120\n",
      "Initial State is  [2, 17, 3]\n",
      "episode 6055, reward 691.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 130\n",
      "Initial State is  [2, 16, 6]\n",
      "episode 6056, reward 844.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 120\n",
      "Initial State is  [4, 9, 1]\n",
      "episode 6057, reward 847.0, memory_length 2000, epsilon 0.009998671593271896, time 721.0, rides 135\n",
      "Initial State is  [0, 19, 0]\n",
      "episode 6058, reward 970.0, memory_length 2000, epsilon 0.009998671593271896, time 720.0, rides 123\n",
      "Initial State is  [3, 8, 3]\n",
      "episode 6059, reward 894.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 141\n",
      "Initial State is  [0, 23, 3]\n",
      "episode 6060, reward 986.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 133\n",
      "Initial State is  [3, 6, 3]\n",
      "episode 6061, reward 1280.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 129\n",
      "Initial State is  [0, 18, 3]\n",
      "episode 6062, reward 860.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 129\n",
      "Initial State is  [4, 0, 0]\n",
      "episode 6063, reward 1123.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 134\n",
      "Initial State is  [2, 9, 6]\n",
      "episode 6064, reward 962.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 130\n",
      "Initial State is  [0, 10, 1]\n",
      "episode 6065, reward 896.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 137\n",
      "Initial State is  [1, 22, 5]\n",
      "episode 6066, reward 1090.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 127\n",
      "Initial State is  [0, 14, 3]\n",
      "episode 6067, reward 850.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 127\n",
      "Initial State is  [2, 10, 0]\n",
      "episode 6068, reward 1106.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 132\n",
      "Initial State is  [3, 21, 2]\n",
      "episode 6069, reward 870.0, memory_length 2000, epsilon 0.009998671593271896, time 737.0, rides 132\n",
      "Initial State is  [0, 22, 2]\n",
      "episode 6070, reward 839.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 131\n",
      "Initial State is  [1, 5, 5]\n",
      "episode 6071, reward 511.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 124\n",
      "Initial State is  [4, 15, 3]\n",
      "episode 6072, reward 628.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 127\n",
      "Initial State is  [0, 22, 2]\n",
      "episode 6073, reward 1004.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 136\n",
      "Initial State is  [0, 12, 2]\n",
      "episode 6074, reward 1152.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 137\n",
      "Initial State is  [4, 2, 1]\n",
      "episode 6075, reward 1104.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 144\n",
      "Initial State is  [1, 11, 1]\n",
      "episode 6076, reward 802.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 128\n",
      "Initial State is  [3, 0, 3]\n",
      "episode 6077, reward 1060.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 138\n",
      "Initial State is  [4, 18, 5]\n",
      "episode 6078, reward 736.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 130\n",
      "Initial State is  [4, 18, 4]\n",
      "episode 6079, reward 935.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 132\n",
      "Initial State is  [1, 23, 3]\n",
      "episode 6080, reward 778.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 133\n",
      "Initial State is  [4, 20, 4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 6081, reward 904.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 148\n",
      "Initial State is  [2, 14, 0]\n",
      "episode 6082, reward 814.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 126\n",
      "Initial State is  [2, 12, 3]\n",
      "episode 6083, reward 762.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 145\n",
      "Initial State is  [1, 15, 0]\n",
      "episode 6084, reward 840.0, memory_length 2000, epsilon 0.009998671593271896, time 721.0, rides 127\n",
      "Initial State is  [1, 11, 1]\n",
      "episode 6085, reward 870.0, memory_length 2000, epsilon 0.009998671593271896, time 738.0, rides 127\n",
      "Initial State is  [3, 1, 3]\n",
      "episode 6086, reward 978.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 136\n",
      "Initial State is  [4, 0, 3]\n",
      "episode 6087, reward 802.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 137\n",
      "Initial State is  [3, 2, 5]\n",
      "episode 6088, reward 1068.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 138\n",
      "Initial State is  [2, 7, 1]\n",
      "episode 6089, reward 605.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 139\n",
      "Initial State is  [2, 12, 4]\n",
      "episode 6090, reward 840.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 134\n",
      "Initial State is  [4, 5, 2]\n",
      "episode 6091, reward 1080.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 142\n",
      "Initial State is  [4, 9, 2]\n",
      "episode 6092, reward 1003.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 133\n",
      "Initial State is  [1, 11, 0]\n",
      "episode 6093, reward 988.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 129\n",
      "Initial State is  [0, 15, 5]\n",
      "episode 6094, reward 1157.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 128\n",
      "Initial State is  [2, 17, 1]\n",
      "episode 6095, reward 882.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 122\n",
      "Initial State is  [3, 10, 0]\n",
      "episode 6096, reward 992.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 138\n",
      "Initial State is  [0, 19, 3]\n",
      "episode 6097, reward 859.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 134\n",
      "Initial State is  [4, 22, 4]\n",
      "episode 6098, reward 843.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 129\n",
      "Initial State is  [1, 12, 4]\n",
      "episode 6099, reward 1016.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 131\n",
      "Initial State is  [4, 5, 3]\n",
      "episode 6100, reward 762.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 141\n",
      "Initial State is  [0, 21, 4]\n",
      "episode 6101, reward 866.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 137\n",
      "Initial State is  [3, 15, 4]\n",
      "episode 6102, reward 1223.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 135\n",
      "Initial State is  [3, 4, 1]\n",
      "episode 6103, reward 883.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 134\n",
      "Initial State is  [3, 9, 1]\n",
      "episode 6104, reward 868.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 140\n",
      "Initial State is  [2, 23, 1]\n",
      "episode 6105, reward 1099.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 139\n",
      "Initial State is  [0, 8, 6]\n",
      "episode 6106, reward 787.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 130\n",
      "Initial State is  [1, 21, 2]\n",
      "episode 6107, reward 927.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 141\n",
      "Initial State is  [4, 21, 4]\n",
      "episode 6108, reward 1045.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 131\n",
      "Initial State is  [2, 16, 0]\n",
      "episode 6109, reward 977.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 126\n",
      "Initial State is  [1, 21, 5]\n",
      "episode 6110, reward 1067.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 139\n",
      "Initial State is  [4, 18, 4]\n",
      "episode 6111, reward 709.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 127\n",
      "Initial State is  [2, 17, 2]\n",
      "episode 6112, reward 1016.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 135\n",
      "Initial State is  [1, 13, 5]\n",
      "episode 6113, reward 880.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 136\n",
      "Initial State is  [1, 12, 3]\n",
      "episode 6114, reward 981.0, memory_length 2000, epsilon 0.009998671593271896, time 740.0, rides 136\n",
      "Initial State is  [2, 15, 1]\n",
      "episode 6115, reward 1074.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 130\n",
      "Initial State is  [0, 10, 6]\n",
      "episode 6116, reward 941.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 126\n",
      "Initial State is  [3, 5, 1]\n",
      "episode 6117, reward 1021.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 133\n",
      "Initial State is  [4, 18, 1]\n",
      "episode 6118, reward 1193.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 126\n",
      "Initial State is  [3, 19, 0]\n",
      "episode 6119, reward 984.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 121\n",
      "Initial State is  [4, 23, 2]\n",
      "episode 6120, reward 999.0, memory_length 2000, epsilon 0.009998671593271896, time 720.0, rides 133\n",
      "Initial State is  [3, 0, 4]\n",
      "episode 6121, reward 824.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 123\n",
      "Initial State is  [3, 0, 2]\n",
      "episode 6122, reward 1001.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 123\n",
      "Initial State is  [0, 2, 2]\n",
      "episode 6123, reward 1153.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 125\n",
      "Initial State is  [4, 11, 0]\n",
      "episode 6124, reward 939.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 142\n",
      "Initial State is  [1, 15, 6]\n",
      "episode 6125, reward 1239.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 141\n",
      "Initial State is  [0, 14, 3]\n",
      "episode 6126, reward 1068.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 137\n",
      "Initial State is  [4, 6, 0]\n",
      "episode 6127, reward 1209.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 143\n",
      "Initial State is  [1, 20, 5]\n",
      "episode 6128, reward 786.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 130\n",
      "Initial State is  [1, 15, 4]\n",
      "episode 6129, reward 921.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 122\n",
      "Initial State is  [0, 16, 3]\n",
      "episode 6130, reward 1110.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 138\n",
      "Initial State is  [2, 20, 6]\n",
      "episode 6131, reward 826.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 131\n",
      "Initial State is  [2, 8, 0]\n",
      "episode 6132, reward 701.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 133\n",
      "Initial State is  [4, 7, 5]\n",
      "episode 6133, reward 839.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 137\n",
      "Initial State is  [0, 0, 1]\n",
      "episode 6134, reward 760.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 127\n",
      "Initial State is  [0, 9, 6]\n",
      "episode 6135, reward 868.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 129\n",
      "Initial State is  [1, 6, 6]\n",
      "episode 6136, reward 946.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 136\n",
      "Initial State is  [3, 12, 0]\n",
      "episode 6137, reward 1194.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 134\n",
      "Initial State is  [4, 8, 6]\n",
      "episode 6138, reward 1017.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 129\n",
      "Initial State is  [2, 7, 6]\n",
      "episode 6139, reward 953.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 132\n",
      "Initial State is  [3, 5, 2]\n",
      "episode 6140, reward 753.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 134\n",
      "Initial State is  [1, 13, 4]\n",
      "episode 6141, reward 1038.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 125\n",
      "Initial State is  [4, 6, 3]\n",
      "episode 6142, reward 699.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 132\n",
      "Initial State is  [3, 23, 2]\n",
      "episode 6143, reward 1043.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 121\n",
      "Initial State is  [4, 2, 5]\n",
      "episode 6144, reward 1146.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 129\n",
      "Initial State is  [2, 17, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 6145, reward 908.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 133\n",
      "Initial State is  [2, 22, 4]\n",
      "episode 6146, reward 1043.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 117\n",
      "Initial State is  [0, 5, 0]\n",
      "episode 6147, reward 931.0, memory_length 2000, epsilon 0.009998671593271896, time 720.0, rides 134\n",
      "Initial State is  [3, 0, 4]\n",
      "episode 6148, reward 886.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 132\n",
      "Initial State is  [1, 22, 5]\n",
      "episode 6149, reward 794.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 147\n",
      "Initial State is  [4, 19, 1]\n",
      "episode 6150, reward 516.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 134\n",
      "Initial State is  [1, 19, 0]\n",
      "episode 6151, reward 1306.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 122\n",
      "Initial State is  [0, 9, 6]\n",
      "episode 6152, reward 1029.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 125\n",
      "Initial State is  [0, 4, 4]\n",
      "episode 6153, reward 1102.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 124\n",
      "Initial State is  [0, 19, 0]\n",
      "episode 6154, reward 861.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 129\n",
      "Initial State is  [0, 6, 4]\n",
      "episode 6155, reward 746.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 133\n",
      "Initial State is  [1, 19, 6]\n",
      "episode 6156, reward 928.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 119\n",
      "Initial State is  [1, 17, 2]\n",
      "episode 6157, reward 843.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 150\n",
      "Initial State is  [3, 16, 3]\n",
      "episode 6158, reward 900.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 125\n",
      "Initial State is  [1, 8, 2]\n",
      "episode 6159, reward 682.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 139\n",
      "Initial State is  [2, 14, 0]\n",
      "episode 6160, reward 879.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 129\n",
      "Initial State is  [3, 6, 2]\n",
      "episode 6161, reward 969.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 133\n",
      "Initial State is  [0, 1, 1]\n",
      "episode 6162, reward 601.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 136\n",
      "Initial State is  [0, 15, 2]\n",
      "episode 6163, reward 713.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 130\n",
      "Initial State is  [1, 6, 2]\n",
      "episode 6164, reward 655.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 133\n",
      "Initial State is  [2, 3, 6]\n",
      "episode 6165, reward 737.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 135\n",
      "Initial State is  [2, 10, 6]\n",
      "episode 6166, reward 1030.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 129\n",
      "Initial State is  [1, 4, 5]\n",
      "episode 6167, reward 918.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 129\n",
      "Initial State is  [4, 15, 4]\n",
      "episode 6168, reward 1103.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 126\n",
      "Initial State is  [3, 19, 5]\n",
      "episode 6169, reward 1123.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 123\n",
      "Initial State is  [1, 14, 4]\n",
      "episode 6170, reward 919.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 133\n",
      "Initial State is  [0, 9, 1]\n",
      "episode 6171, reward 972.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 129\n",
      "Initial State is  [3, 16, 3]\n",
      "episode 6172, reward 762.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 125\n",
      "Initial State is  [2, 11, 6]\n",
      "episode 6173, reward 564.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 131\n",
      "Initial State is  [4, 1, 4]\n",
      "episode 6174, reward 779.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 127\n",
      "Initial State is  [0, 1, 3]\n",
      "episode 6175, reward 1141.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 131\n",
      "Initial State is  [0, 18, 2]\n",
      "episode 6176, reward 1024.0, memory_length 2000, epsilon 0.009998671593271896, time 737.0, rides 129\n",
      "Initial State is  [3, 3, 4]\n",
      "episode 6177, reward 1251.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 136\n",
      "Initial State is  [1, 1, 2]\n",
      "episode 6178, reward 731.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 134\n",
      "Initial State is  [0, 6, 2]\n",
      "episode 6179, reward 1099.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 139\n",
      "Initial State is  [3, 2, 5]\n",
      "episode 6180, reward 1084.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 142\n",
      "Initial State is  [4, 7, 5]\n",
      "episode 6181, reward 921.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 123\n",
      "Initial State is  [2, 18, 3]\n",
      "episode 6182, reward 978.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 152\n",
      "Initial State is  [0, 22, 5]\n",
      "episode 6183, reward 1171.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 137\n",
      "Initial State is  [1, 8, 4]\n",
      "episode 6184, reward 929.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 132\n",
      "Initial State is  [0, 13, 5]\n",
      "episode 6185, reward 1007.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 133\n",
      "Initial State is  [2, 15, 0]\n",
      "episode 6186, reward 795.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 141\n",
      "Initial State is  [4, 23, 0]\n",
      "episode 6187, reward 1066.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 132\n",
      "Initial State is  [1, 16, 2]\n",
      "episode 6188, reward 1249.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 135\n",
      "Initial State is  [4, 2, 6]\n",
      "episode 6189, reward 563.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 130\n",
      "Initial State is  [1, 7, 6]\n",
      "episode 6190, reward 705.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 115\n",
      "Initial State is  [4, 19, 1]\n",
      "episode 6191, reward 876.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 145\n",
      "Initial State is  [4, 7, 6]\n",
      "episode 6192, reward 922.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 136\n",
      "Initial State is  [0, 14, 2]\n",
      "episode 6193, reward 917.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 125\n",
      "Initial State is  [3, 21, 1]\n",
      "episode 6194, reward 762.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 143\n",
      "Initial State is  [3, 10, 5]\n",
      "episode 6195, reward 757.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 133\n",
      "Initial State is  [3, 1, 2]\n",
      "episode 6196, reward 914.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 119\n",
      "Initial State is  [2, 21, 5]\n",
      "episode 6197, reward 849.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 127\n",
      "Initial State is  [3, 23, 6]\n",
      "episode 6198, reward 1052.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 135\n",
      "Initial State is  [1, 1, 2]\n",
      "episode 6199, reward 1166.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 139\n",
      "Initial State is  [4, 5, 2]\n",
      "episode 6200, reward 838.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 135\n",
      "Initial State is  [1, 13, 5]\n",
      "episode 6201, reward 992.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 138\n",
      "Initial State is  [2, 0, 1]\n",
      "episode 6202, reward 828.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 141\n",
      "Initial State is  [1, 8, 3]\n",
      "episode 6203, reward 1114.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 129\n",
      "Initial State is  [0, 2, 1]\n",
      "episode 6204, reward 954.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 138\n",
      "Initial State is  [3, 19, 5]\n",
      "episode 6205, reward 764.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 127\n",
      "Initial State is  [1, 15, 3]\n",
      "episode 6206, reward 941.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 125\n",
      "Initial State is  [3, 23, 0]\n",
      "episode 6207, reward 1049.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 126\n",
      "Initial State is  [0, 6, 4]\n",
      "episode 6208, reward 940.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 124\n",
      "Initial State is  [1, 21, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 6209, reward 878.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 122\n",
      "Initial State is  [0, 17, 3]\n",
      "episode 6210, reward 1061.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 128\n",
      "Initial State is  [0, 23, 5]\n",
      "episode 6211, reward 565.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 143\n",
      "Initial State is  [4, 6, 3]\n",
      "episode 6212, reward 1071.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 143\n",
      "Initial State is  [1, 20, 2]\n",
      "episode 6213, reward 972.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 128\n",
      "Initial State is  [2, 21, 0]\n",
      "episode 6214, reward 1237.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 128\n",
      "Initial State is  [2, 20, 5]\n",
      "episode 6215, reward 375.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 132\n",
      "Initial State is  [3, 5, 0]\n",
      "episode 6216, reward 714.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 118\n",
      "Initial State is  [0, 6, 5]\n",
      "episode 6217, reward 1065.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 125\n",
      "Initial State is  [0, 20, 4]\n",
      "episode 6218, reward 1016.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 135\n",
      "Initial State is  [3, 10, 0]\n",
      "episode 6219, reward 1017.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 147\n",
      "Initial State is  [3, 7, 1]\n",
      "episode 6220, reward 1063.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 127\n",
      "Initial State is  [4, 6, 6]\n",
      "episode 6221, reward 974.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 140\n",
      "Initial State is  [0, 10, 3]\n",
      "episode 6222, reward 1137.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 131\n",
      "Initial State is  [0, 10, 6]\n",
      "episode 6223, reward 1200.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 133\n",
      "Initial State is  [0, 13, 0]\n",
      "episode 6224, reward 1016.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 123\n",
      "Initial State is  [3, 11, 2]\n",
      "episode 6225, reward 954.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 128\n",
      "Initial State is  [4, 3, 2]\n",
      "episode 6226, reward 748.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 128\n",
      "Initial State is  [0, 13, 0]\n",
      "episode 6227, reward 877.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 125\n",
      "Initial State is  [1, 0, 4]\n",
      "episode 6228, reward 861.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 130\n",
      "Initial State is  [4, 20, 6]\n",
      "episode 6229, reward 766.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 136\n",
      "Initial State is  [3, 16, 2]\n",
      "episode 6230, reward 986.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 135\n",
      "Initial State is  [2, 4, 3]\n",
      "episode 6231, reward 864.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 132\n",
      "Initial State is  [3, 15, 6]\n",
      "episode 6232, reward 1002.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 129\n",
      "Initial State is  [0, 4, 0]\n",
      "episode 6233, reward 922.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 138\n",
      "Initial State is  [0, 3, 4]\n",
      "episode 6234, reward 788.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 117\n",
      "Initial State is  [2, 16, 1]\n",
      "episode 6235, reward 723.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 130\n",
      "Initial State is  [3, 1, 0]\n",
      "episode 6236, reward 1078.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 142\n",
      "Initial State is  [3, 8, 5]\n",
      "episode 6237, reward 618.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 135\n",
      "Initial State is  [1, 1, 2]\n",
      "episode 6238, reward 1125.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 121\n",
      "Initial State is  [1, 22, 0]\n",
      "episode 6239, reward 950.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 120\n",
      "Initial State is  [3, 17, 1]\n",
      "episode 6240, reward 1100.0, memory_length 2000, epsilon 0.009998671593271896, time 739.0, rides 126\n",
      "Initial State is  [2, 2, 1]\n",
      "episode 6241, reward 748.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 124\n",
      "Initial State is  [1, 0, 2]\n",
      "episode 6242, reward 948.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 124\n",
      "Initial State is  [1, 19, 0]\n",
      "episode 6243, reward 1055.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 146\n",
      "Initial State is  [1, 8, 5]\n",
      "episode 6244, reward 804.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 127\n",
      "Initial State is  [0, 0, 0]\n",
      "episode 6245, reward 833.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 126\n",
      "Initial State is  [4, 21, 6]\n",
      "episode 6246, reward 1243.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 133\n",
      "Initial State is  [4, 12, 2]\n",
      "episode 6247, reward 823.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 132\n",
      "Initial State is  [4, 14, 5]\n",
      "episode 6248, reward 873.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 132\n",
      "Initial State is  [3, 10, 4]\n",
      "episode 6249, reward 1025.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 139\n",
      "Initial State is  [1, 21, 4]\n",
      "episode 6250, reward 681.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 122\n",
      "Initial State is  [0, 21, 6]\n",
      "episode 6251, reward 995.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 134\n",
      "Initial State is  [4, 18, 5]\n",
      "episode 6252, reward 912.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 134\n",
      "Initial State is  [4, 5, 1]\n",
      "episode 6253, reward 896.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 135\n",
      "Initial State is  [3, 20, 1]\n",
      "episode 6254, reward 663.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 135\n",
      "Initial State is  [3, 18, 2]\n",
      "episode 6255, reward 913.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 140\n",
      "Initial State is  [1, 15, 4]\n",
      "episode 6256, reward 912.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 144\n",
      "Initial State is  [2, 22, 6]\n",
      "episode 6257, reward 847.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 142\n",
      "Initial State is  [1, 20, 3]\n",
      "episode 6258, reward 673.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 135\n",
      "Initial State is  [1, 21, 0]\n",
      "episode 6259, reward 1177.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 129\n",
      "Initial State is  [2, 2, 6]\n",
      "episode 6260, reward 1048.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 132\n",
      "Initial State is  [0, 2, 5]\n",
      "episode 6261, reward 840.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 124\n",
      "Initial State is  [0, 8, 1]\n",
      "episode 6262, reward 713.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 140\n",
      "Initial State is  [3, 19, 3]\n",
      "episode 6263, reward 979.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 123\n",
      "Initial State is  [4, 12, 3]\n",
      "episode 6264, reward 1020.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 136\n",
      "Initial State is  [2, 19, 3]\n",
      "episode 6265, reward 1153.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 131\n",
      "Initial State is  [4, 4, 1]\n",
      "episode 6266, reward 984.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 133\n",
      "Initial State is  [1, 14, 1]\n",
      "episode 6267, reward 885.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 132\n",
      "Initial State is  [4, 21, 5]\n",
      "episode 6268, reward 1138.0, memory_length 2000, epsilon 0.009998671593271896, time 721.0, rides 147\n",
      "Initial State is  [3, 13, 5]\n",
      "episode 6269, reward 1259.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 128\n",
      "Initial State is  [0, 8, 5]\n",
      "episode 6270, reward 880.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 130\n",
      "Initial State is  [4, 6, 5]\n",
      "episode 6271, reward 880.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 129\n",
      "Initial State is  [1, 2, 1]\n",
      "episode 6272, reward 1134.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 123\n",
      "Initial State is  [2, 9, 1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 6273, reward 882.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 129\n",
      "Initial State is  [2, 16, 3]\n",
      "episode 6274, reward 878.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 125\n",
      "Initial State is  [3, 10, 0]\n",
      "episode 6275, reward 721.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 132\n",
      "Initial State is  [0, 17, 4]\n",
      "episode 6276, reward 733.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 127\n",
      "Initial State is  [2, 21, 2]\n",
      "episode 6277, reward 1158.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 139\n",
      "Initial State is  [1, 15, 5]\n",
      "episode 6278, reward 987.0, memory_length 2000, epsilon 0.009998671593271896, time 736.0, rides 123\n",
      "Initial State is  [0, 16, 3]\n",
      "episode 6279, reward 922.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 123\n",
      "Initial State is  [3, 3, 6]\n",
      "episode 6280, reward 924.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 138\n",
      "Initial State is  [4, 9, 0]\n",
      "episode 6281, reward 909.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 131\n",
      "Initial State is  [0, 2, 2]\n",
      "episode 6282, reward 782.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 125\n",
      "Initial State is  [0, 5, 2]\n",
      "episode 6283, reward 915.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 133\n",
      "Initial State is  [2, 21, 3]\n",
      "episode 6284, reward 1107.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 137\n",
      "Initial State is  [4, 5, 1]\n",
      "episode 6285, reward 1104.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 142\n",
      "Initial State is  [2, 4, 0]\n",
      "episode 6286, reward 920.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 133\n",
      "Initial State is  [2, 11, 2]\n",
      "episode 6287, reward 678.0, memory_length 2000, epsilon 0.009998671593271896, time 736.0, rides 135\n",
      "Initial State is  [4, 2, 4]\n",
      "episode 6288, reward 835.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 130\n",
      "Initial State is  [4, 10, 0]\n",
      "episode 6289, reward 398.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 135\n",
      "Initial State is  [4, 2, 4]\n",
      "episode 6290, reward 1032.0, memory_length 2000, epsilon 0.009998671593271896, time 736.0, rides 148\n",
      "Initial State is  [2, 9, 2]\n",
      "episode 6291, reward 1113.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 135\n",
      "Initial State is  [3, 22, 6]\n",
      "episode 6292, reward 1065.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 128\n",
      "Initial State is  [2, 14, 5]\n",
      "episode 6293, reward 599.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 135\n",
      "Initial State is  [2, 14, 1]\n",
      "episode 6294, reward 824.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 141\n",
      "Initial State is  [1, 19, 3]\n",
      "episode 6295, reward 879.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 136\n",
      "Initial State is  [1, 11, 3]\n",
      "episode 6296, reward 874.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 144\n",
      "Initial State is  [2, 17, 0]\n",
      "episode 6297, reward 927.0, memory_length 2000, epsilon 0.009998671593271896, time 721.0, rides 142\n",
      "Initial State is  [2, 2, 0]\n",
      "episode 6298, reward 959.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 118\n",
      "Initial State is  [2, 17, 1]\n",
      "episode 6299, reward 939.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 144\n",
      "Initial State is  [1, 3, 3]\n",
      "episode 6300, reward 646.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 135\n",
      "Initial State is  [3, 20, 4]\n",
      "episode 6301, reward 1050.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 132\n",
      "Initial State is  [4, 6, 5]\n",
      "episode 6302, reward 1046.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 140\n",
      "Initial State is  [2, 20, 2]\n",
      "episode 6303, reward 842.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 125\n",
      "Initial State is  [4, 1, 5]\n",
      "episode 6304, reward 1015.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 130\n",
      "Initial State is  [1, 17, 6]\n",
      "episode 6305, reward 1136.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 132\n",
      "Initial State is  [1, 3, 6]\n",
      "episode 6306, reward 1022.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 139\n",
      "Initial State is  [0, 12, 6]\n",
      "episode 6307, reward 825.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 138\n",
      "Initial State is  [3, 0, 4]\n",
      "episode 6308, reward 889.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 116\n",
      "Initial State is  [0, 21, 4]\n",
      "episode 6309, reward 1034.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 123\n",
      "Initial State is  [4, 4, 4]\n",
      "episode 6310, reward 1181.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 138\n",
      "Initial State is  [2, 10, 2]\n",
      "episode 6311, reward 667.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 123\n",
      "Initial State is  [2, 10, 3]\n",
      "episode 6312, reward 845.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 134\n",
      "Initial State is  [3, 17, 5]\n",
      "episode 6313, reward 734.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 142\n",
      "Initial State is  [0, 7, 6]\n",
      "episode 6314, reward 793.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 134\n",
      "Initial State is  [4, 18, 3]\n",
      "episode 6315, reward 896.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 120\n",
      "Initial State is  [1, 8, 6]\n",
      "episode 6316, reward 1031.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 134\n",
      "Initial State is  [3, 15, 5]\n",
      "episode 6317, reward 1387.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 140\n",
      "Initial State is  [3, 11, 6]\n",
      "episode 6318, reward 1008.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 135\n",
      "Initial State is  [3, 11, 6]\n",
      "episode 6319, reward 967.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 130\n",
      "Initial State is  [4, 20, 6]\n",
      "episode 6320, reward 892.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 127\n",
      "Initial State is  [4, 22, 0]\n",
      "episode 6321, reward 833.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 136\n",
      "Initial State is  [2, 7, 0]\n",
      "episode 6322, reward 792.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 132\n",
      "Initial State is  [0, 19, 2]\n",
      "episode 6323, reward 837.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 133\n",
      "Initial State is  [4, 10, 2]\n",
      "episode 6324, reward 698.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 126\n",
      "Initial State is  [1, 0, 1]\n",
      "episode 6325, reward 1076.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 121\n",
      "Initial State is  [4, 18, 2]\n",
      "episode 6326, reward 707.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 132\n",
      "Initial State is  [2, 21, 6]\n",
      "episode 6327, reward 792.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 134\n",
      "Initial State is  [0, 0, 4]\n",
      "episode 6328, reward 931.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 134\n",
      "Initial State is  [1, 19, 1]\n",
      "episode 6329, reward 940.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 128\n",
      "Initial State is  [1, 11, 2]\n",
      "episode 6330, reward 1245.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 131\n",
      "Initial State is  [4, 12, 3]\n",
      "episode 6331, reward 949.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 146\n",
      "Initial State is  [3, 13, 0]\n",
      "episode 6332, reward 833.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 127\n",
      "Initial State is  [2, 10, 5]\n",
      "episode 6333, reward 947.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 144\n",
      "Initial State is  [0, 21, 2]\n",
      "episode 6334, reward 973.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 133\n",
      "Initial State is  [3, 15, 6]\n",
      "episode 6335, reward 878.0, memory_length 2000, epsilon 0.009998671593271896, time 737.0, rides 121\n",
      "Initial State is  [3, 20, 0]\n",
      "episode 6336, reward 992.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 135\n",
      "Initial State is  [0, 23, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 6337, reward 958.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 141\n",
      "Initial State is  [3, 9, 2]\n",
      "episode 6338, reward 801.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 126\n",
      "Initial State is  [0, 3, 3]\n",
      "episode 6339, reward 845.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 125\n",
      "Initial State is  [3, 21, 2]\n",
      "episode 6340, reward 650.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 130\n",
      "Initial State is  [1, 14, 3]\n",
      "episode 6341, reward 813.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 121\n",
      "Initial State is  [0, 16, 5]\n",
      "episode 6342, reward 930.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 132\n",
      "Initial State is  [4, 10, 2]\n",
      "episode 6343, reward 1251.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 138\n",
      "Initial State is  [0, 3, 2]\n",
      "episode 6344, reward 948.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 127\n",
      "Initial State is  [0, 1, 3]\n",
      "episode 6345, reward 1021.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 141\n",
      "Initial State is  [2, 11, 0]\n",
      "episode 6346, reward 993.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 133\n",
      "Initial State is  [0, 9, 0]\n",
      "episode 6347, reward 953.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 126\n",
      "Initial State is  [3, 10, 4]\n",
      "episode 6348, reward 909.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 123\n",
      "Initial State is  [2, 6, 3]\n",
      "episode 6349, reward 1077.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 137\n",
      "Initial State is  [3, 21, 6]\n",
      "episode 6350, reward 1094.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 134\n",
      "Initial State is  [2, 5, 0]\n",
      "episode 6351, reward 854.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 118\n",
      "Initial State is  [0, 6, 6]\n",
      "episode 6352, reward 818.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 146\n",
      "Initial State is  [3, 2, 5]\n",
      "episode 6353, reward 1056.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 128\n",
      "Initial State is  [1, 17, 3]\n",
      "episode 6354, reward 1126.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 137\n",
      "Initial State is  [3, 14, 5]\n",
      "episode 6355, reward 1089.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 132\n",
      "Initial State is  [4, 13, 4]\n",
      "episode 6356, reward 658.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 137\n",
      "Initial State is  [1, 5, 4]\n",
      "episode 6357, reward 920.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 142\n",
      "Initial State is  [4, 14, 2]\n",
      "episode 6358, reward 1052.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 134\n",
      "Initial State is  [3, 17, 4]\n",
      "episode 6359, reward 957.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 127\n",
      "Initial State is  [4, 13, 0]\n",
      "episode 6360, reward 773.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 140\n",
      "Initial State is  [4, 10, 0]\n",
      "episode 6361, reward 714.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 124\n",
      "Initial State is  [3, 17, 0]\n",
      "episode 6362, reward 660.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 120\n",
      "Initial State is  [4, 2, 6]\n",
      "episode 6363, reward 1198.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 136\n",
      "Initial State is  [3, 4, 6]\n",
      "episode 6364, reward 871.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 139\n",
      "Initial State is  [4, 1, 0]\n",
      "episode 6365, reward 616.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 142\n",
      "Initial State is  [4, 12, 4]\n",
      "episode 6366, reward 1081.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 144\n",
      "Initial State is  [0, 9, 6]\n",
      "episode 6367, reward 1046.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 138\n",
      "Initial State is  [1, 16, 6]\n",
      "episode 6368, reward 797.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 145\n",
      "Initial State is  [2, 17, 0]\n",
      "episode 6369, reward 992.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 143\n",
      "Initial State is  [3, 10, 2]\n",
      "episode 6370, reward 939.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 147\n",
      "Initial State is  [3, 15, 4]\n",
      "episode 6371, reward 901.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 144\n",
      "Initial State is  [4, 21, 1]\n",
      "episode 6372, reward 782.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 144\n",
      "Initial State is  [4, 1, 0]\n",
      "episode 6373, reward 1141.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 139\n",
      "Initial State is  [0, 13, 6]\n",
      "episode 6374, reward 952.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 131\n",
      "Initial State is  [4, 16, 6]\n",
      "episode 6375, reward 1045.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 122\n",
      "Initial State is  [2, 0, 1]\n",
      "episode 6376, reward 835.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 130\n",
      "Initial State is  [1, 15, 5]\n",
      "episode 6377, reward 979.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 132\n",
      "Initial State is  [3, 10, 3]\n",
      "episode 6378, reward 866.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 137\n",
      "Initial State is  [2, 18, 4]\n",
      "episode 6379, reward 795.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 136\n",
      "Initial State is  [1, 12, 2]\n",
      "episode 6380, reward 1055.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 146\n",
      "Initial State is  [2, 11, 4]\n",
      "episode 6381, reward 1004.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 123\n",
      "Initial State is  [0, 5, 1]\n",
      "episode 6382, reward 917.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 129\n",
      "Initial State is  [0, 16, 1]\n",
      "episode 6383, reward 800.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 139\n",
      "Initial State is  [2, 15, 2]\n",
      "episode 6384, reward 981.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 133\n",
      "Initial State is  [4, 2, 5]\n",
      "episode 6385, reward 1023.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 136\n",
      "Initial State is  [4, 17, 3]\n",
      "episode 6386, reward 745.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 127\n",
      "Initial State is  [3, 23, 6]\n",
      "episode 6387, reward 557.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 135\n",
      "Initial State is  [2, 21, 3]\n",
      "episode 6388, reward 1208.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 136\n",
      "Initial State is  [2, 18, 2]\n",
      "episode 6389, reward 1013.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 133\n",
      "Initial State is  [1, 16, 3]\n",
      "episode 6390, reward 1111.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 127\n",
      "Initial State is  [2, 5, 0]\n",
      "episode 6391, reward 545.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 138\n",
      "Initial State is  [2, 18, 6]\n",
      "episode 6392, reward 965.0, memory_length 2000, epsilon 0.009998671593271896, time 736.0, rides 132\n",
      "Initial State is  [2, 9, 5]\n",
      "episode 6393, reward 603.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 130\n",
      "Initial State is  [0, 10, 5]\n",
      "episode 6394, reward 929.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 125\n",
      "Initial State is  [0, 16, 6]\n",
      "episode 6395, reward 819.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 131\n",
      "Initial State is  [3, 8, 3]\n",
      "episode 6396, reward 980.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 131\n",
      "Initial State is  [3, 4, 1]\n",
      "episode 6397, reward 871.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 141\n",
      "Initial State is  [2, 18, 1]\n",
      "episode 6398, reward 744.0, memory_length 2000, epsilon 0.009998671593271896, time 721.0, rides 140\n",
      "Initial State is  [0, 22, 5]\n",
      "episode 6399, reward 960.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 141\n",
      "Initial State is  [2, 19, 2]\n",
      "episode 6400, reward 837.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 138\n",
      "Initial State is  [1, 8, 3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 6401, reward 1049.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 123\n",
      "Initial State is  [1, 5, 6]\n",
      "episode 6402, reward 1238.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 137\n",
      "Initial State is  [2, 20, 2]\n",
      "episode 6403, reward 1200.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 137\n",
      "Initial State is  [1, 11, 4]\n",
      "episode 6404, reward 887.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 150\n",
      "Initial State is  [1, 19, 4]\n",
      "episode 6405, reward 1088.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 123\n",
      "Initial State is  [4, 13, 3]\n",
      "episode 6406, reward 1019.0, memory_length 2000, epsilon 0.009998671593271896, time 736.0, rides 135\n",
      "Initial State is  [0, 6, 6]\n",
      "episode 6407, reward 892.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 130\n",
      "Initial State is  [3, 8, 0]\n",
      "episode 6408, reward 962.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 127\n",
      "Initial State is  [3, 7, 1]\n",
      "episode 6409, reward 926.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 135\n",
      "Initial State is  [1, 2, 2]\n",
      "episode 6410, reward 1086.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 139\n",
      "Initial State is  [4, 2, 6]\n",
      "episode 6411, reward 899.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 136\n",
      "Initial State is  [2, 7, 2]\n",
      "episode 6412, reward 686.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 125\n",
      "Initial State is  [2, 4, 4]\n",
      "episode 6413, reward 1042.0, memory_length 2000, epsilon 0.009998671593271896, time 747.0, rides 130\n",
      "Initial State is  [2, 20, 0]\n",
      "episode 6414, reward 933.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 125\n",
      "Initial State is  [1, 0, 2]\n",
      "episode 6415, reward 944.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 149\n",
      "Initial State is  [4, 3, 4]\n",
      "episode 6416, reward 962.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 140\n",
      "Initial State is  [0, 8, 3]\n",
      "episode 6417, reward 1298.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 137\n",
      "Initial State is  [3, 20, 1]\n",
      "episode 6418, reward 1180.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 133\n",
      "Initial State is  [1, 9, 5]\n",
      "episode 6419, reward 922.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 129\n",
      "Initial State is  [1, 21, 6]\n",
      "episode 6420, reward 1009.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 132\n",
      "Initial State is  [4, 15, 5]\n",
      "episode 6421, reward 579.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 142\n",
      "Initial State is  [4, 2, 4]\n",
      "episode 6422, reward 886.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 138\n",
      "Initial State is  [3, 2, 2]\n",
      "episode 6423, reward 861.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 117\n",
      "Initial State is  [4, 9, 6]\n",
      "episode 6424, reward 798.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 126\n",
      "Initial State is  [0, 2, 4]\n",
      "episode 6425, reward 610.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 130\n",
      "Initial State is  [3, 3, 5]\n",
      "episode 6426, reward 1123.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 139\n",
      "Initial State is  [4, 11, 1]\n",
      "episode 6427, reward 814.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 136\n",
      "Initial State is  [4, 15, 1]\n",
      "episode 6428, reward 722.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 141\n",
      "Initial State is  [3, 18, 5]\n",
      "episode 6429, reward 1068.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 125\n",
      "Initial State is  [0, 21, 4]\n",
      "episode 6430, reward 1176.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 134\n",
      "Initial State is  [3, 9, 4]\n",
      "episode 6431, reward 1025.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 136\n",
      "Initial State is  [3, 20, 6]\n",
      "episode 6432, reward 1074.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 131\n",
      "Initial State is  [2, 1, 1]\n",
      "episode 6433, reward 1072.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 129\n",
      "Initial State is  [1, 14, 2]\n",
      "episode 6434, reward 831.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 123\n",
      "Initial State is  [0, 7, 0]\n",
      "episode 6435, reward 993.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 120\n",
      "Initial State is  [3, 11, 5]\n",
      "episode 6436, reward 733.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 146\n",
      "Initial State is  [2, 3, 3]\n",
      "episode 6437, reward 1046.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 133\n",
      "Initial State is  [0, 13, 0]\n",
      "episode 6438, reward 850.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 145\n",
      "Initial State is  [3, 11, 3]\n",
      "episode 6439, reward 1014.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 125\n",
      "Initial State is  [2, 15, 1]\n",
      "episode 6440, reward 1046.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 133\n",
      "Initial State is  [2, 9, 5]\n",
      "episode 6441, reward 1028.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 143\n",
      "Initial State is  [4, 2, 4]\n",
      "episode 6442, reward 982.0, memory_length 2000, epsilon 0.009998671593271896, time 721.0, rides 147\n",
      "Initial State is  [1, 1, 4]\n",
      "episode 6443, reward 753.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 144\n",
      "Initial State is  [3, 23, 3]\n",
      "episode 6444, reward 981.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 136\n",
      "Initial State is  [1, 3, 1]\n",
      "episode 6445, reward 1197.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 149\n",
      "Initial State is  [2, 6, 0]\n",
      "episode 6446, reward 907.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 132\n",
      "Initial State is  [0, 9, 2]\n",
      "episode 6447, reward 759.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 131\n",
      "Initial State is  [2, 9, 1]\n",
      "episode 6448, reward 768.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 135\n",
      "Initial State is  [0, 15, 6]\n",
      "episode 6449, reward 1058.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 131\n",
      "Initial State is  [2, 21, 1]\n",
      "episode 6450, reward 764.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 136\n",
      "Initial State is  [1, 9, 5]\n",
      "episode 6451, reward 1022.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 135\n",
      "Initial State is  [2, 5, 6]\n",
      "episode 6452, reward 998.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 130\n",
      "Initial State is  [0, 9, 6]\n",
      "episode 6453, reward 849.0, memory_length 2000, epsilon 0.009998671593271896, time 736.0, rides 127\n",
      "Initial State is  [0, 5, 6]\n",
      "episode 6454, reward 938.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 130\n",
      "Initial State is  [1, 9, 4]\n",
      "episode 6455, reward 809.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 127\n",
      "Initial State is  [1, 18, 0]\n",
      "episode 6456, reward 732.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 124\n",
      "Initial State is  [3, 10, 6]\n",
      "episode 6457, reward 666.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 128\n",
      "Initial State is  [0, 22, 3]\n",
      "episode 6458, reward 1184.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 128\n",
      "Initial State is  [4, 6, 2]\n",
      "episode 6459, reward 801.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 129\n",
      "Initial State is  [4, 22, 3]\n",
      "episode 6460, reward 618.0, memory_length 2000, epsilon 0.009998671593271896, time 736.0, rides 125\n",
      "Initial State is  [1, 16, 6]\n",
      "episode 6461, reward 864.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 136\n",
      "Initial State is  [4, 19, 2]\n",
      "episode 6462, reward 822.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 134\n",
      "Initial State is  [4, 5, 0]\n",
      "episode 6463, reward 619.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 128\n",
      "Initial State is  [0, 3, 1]\n",
      "episode 6464, reward 767.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 140\n",
      "Initial State is  [0, 2, 2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 6465, reward 737.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 135\n",
      "Initial State is  [4, 8, 5]\n",
      "episode 6466, reward 804.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 127\n",
      "Initial State is  [2, 6, 6]\n",
      "episode 6467, reward 1155.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 145\n",
      "Initial State is  [1, 8, 2]\n",
      "episode 6468, reward 843.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 131\n",
      "Initial State is  [2, 14, 6]\n",
      "episode 6469, reward 1210.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 131\n",
      "Initial State is  [0, 19, 2]\n",
      "episode 6470, reward 1023.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 136\n",
      "Initial State is  [4, 15, 1]\n",
      "episode 6471, reward 696.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 131\n",
      "Initial State is  [0, 5, 0]\n",
      "episode 6472, reward 864.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 144\n",
      "Initial State is  [1, 2, 4]\n",
      "episode 6473, reward 640.0, memory_length 2000, epsilon 0.009998671593271896, time 721.0, rides 135\n",
      "Initial State is  [2, 0, 0]\n",
      "episode 6474, reward 873.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 144\n",
      "Initial State is  [2, 23, 6]\n",
      "episode 6475, reward 939.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 127\n",
      "Initial State is  [4, 20, 0]\n",
      "episode 6476, reward 1419.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 133\n",
      "Initial State is  [0, 2, 2]\n",
      "episode 6477, reward 875.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 135\n",
      "Initial State is  [3, 1, 0]\n",
      "episode 6478, reward 1185.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 142\n",
      "Initial State is  [4, 18, 4]\n",
      "episode 6479, reward 764.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 136\n",
      "Initial State is  [3, 12, 4]\n",
      "episode 6480, reward 1040.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 122\n",
      "Initial State is  [1, 8, 5]\n",
      "episode 6481, reward 1059.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 139\n",
      "Initial State is  [3, 22, 6]\n",
      "episode 6482, reward 992.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 129\n",
      "Initial State is  [1, 19, 1]\n",
      "episode 6483, reward 602.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 124\n",
      "Initial State is  [0, 23, 0]\n",
      "episode 6484, reward 989.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 129\n",
      "Initial State is  [2, 13, 3]\n",
      "episode 6485, reward 621.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 141\n",
      "Initial State is  [4, 12, 6]\n",
      "episode 6486, reward 1186.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 134\n",
      "Initial State is  [3, 13, 5]\n",
      "episode 6487, reward 1233.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 140\n",
      "Initial State is  [1, 1, 2]\n",
      "episode 6488, reward 915.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 124\n",
      "Initial State is  [1, 13, 0]\n",
      "episode 6489, reward 1171.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 136\n",
      "Initial State is  [3, 14, 5]\n",
      "episode 6490, reward 993.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 117\n",
      "Initial State is  [2, 8, 4]\n",
      "episode 6491, reward 856.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 121\n",
      "Initial State is  [1, 12, 0]\n",
      "episode 6492, reward 886.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 131\n",
      "Initial State is  [1, 0, 3]\n",
      "episode 6493, reward 581.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 143\n",
      "Initial State is  [2, 9, 6]\n",
      "episode 6494, reward 982.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 124\n",
      "Initial State is  [3, 18, 3]\n",
      "episode 6495, reward 1007.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 126\n",
      "Initial State is  [4, 17, 6]\n",
      "episode 6496, reward 589.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 124\n",
      "Initial State is  [3, 1, 5]\n",
      "episode 6497, reward 1021.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 136\n",
      "Initial State is  [3, 19, 3]\n",
      "episode 6498, reward 842.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 146\n",
      "Initial State is  [4, 11, 4]\n",
      "episode 6499, reward 637.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 138\n",
      "Initial State is  [4, 7, 5]\n",
      "episode 6500, reward 846.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 133\n",
      "Initial State is  [2, 17, 5]\n",
      "episode 6501, reward 1032.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 121\n",
      "Initial State is  [4, 21, 3]\n",
      "episode 6502, reward 815.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 126\n",
      "Initial State is  [0, 9, 3]\n",
      "episode 6503, reward 836.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 128\n",
      "Initial State is  [4, 7, 4]\n",
      "episode 6504, reward 990.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 140\n",
      "Initial State is  [0, 5, 3]\n",
      "episode 6505, reward 733.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 130\n",
      "Initial State is  [1, 13, 6]\n",
      "episode 6506, reward 1170.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 133\n",
      "Initial State is  [4, 2, 1]\n",
      "episode 6507, reward 788.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 137\n",
      "Initial State is  [4, 0, 4]\n",
      "episode 6508, reward 803.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 127\n",
      "Initial State is  [1, 7, 4]\n",
      "episode 6509, reward 1233.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 150\n",
      "Initial State is  [4, 4, 0]\n",
      "episode 6510, reward 990.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 132\n",
      "Initial State is  [0, 12, 4]\n",
      "episode 6511, reward 968.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 142\n",
      "Initial State is  [1, 19, 6]\n",
      "episode 6512, reward 955.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 124\n",
      "Initial State is  [0, 23, 0]\n",
      "episode 6513, reward 753.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 136\n",
      "Initial State is  [3, 0, 1]\n",
      "episode 6514, reward 916.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 129\n",
      "Initial State is  [4, 0, 4]\n",
      "episode 6515, reward 1100.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 139\n",
      "Initial State is  [1, 23, 0]\n",
      "episode 6516, reward 1172.0, memory_length 2000, epsilon 0.009998671593271896, time 743.0, rides 133\n",
      "Initial State is  [2, 21, 1]\n",
      "episode 6517, reward 741.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 144\n",
      "Initial State is  [2, 16, 4]\n",
      "episode 6518, reward 1019.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 134\n",
      "Initial State is  [0, 1, 0]\n",
      "episode 6519, reward 1311.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 138\n",
      "Initial State is  [1, 0, 2]\n",
      "episode 6520, reward 656.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 128\n",
      "Initial State is  [4, 2, 0]\n",
      "episode 6521, reward 968.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 120\n",
      "Initial State is  [4, 6, 6]\n",
      "episode 6522, reward 923.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 127\n",
      "Initial State is  [0, 13, 2]\n",
      "episode 6523, reward 1052.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 128\n",
      "Initial State is  [3, 15, 1]\n",
      "episode 6524, reward 565.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 133\n",
      "Initial State is  [3, 6, 6]\n",
      "episode 6525, reward 1068.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 125\n",
      "Initial State is  [1, 9, 3]\n",
      "episode 6526, reward 758.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 136\n",
      "Initial State is  [3, 23, 5]\n",
      "episode 6527, reward 781.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 128\n",
      "Initial State is  [0, 11, 3]\n",
      "episode 6528, reward 932.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 124\n",
      "Initial State is  [3, 22, 1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 6529, reward 767.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 121\n",
      "Initial State is  [4, 18, 4]\n",
      "episode 6530, reward 1124.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 125\n",
      "Initial State is  [0, 0, 3]\n",
      "episode 6531, reward 762.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 138\n",
      "Initial State is  [3, 14, 4]\n",
      "episode 6532, reward 1044.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 147\n",
      "Initial State is  [2, 21, 0]\n",
      "episode 6533, reward 845.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 131\n",
      "Initial State is  [3, 22, 1]\n",
      "episode 6534, reward 746.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 128\n",
      "Initial State is  [4, 21, 2]\n",
      "episode 6535, reward 871.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 129\n",
      "Initial State is  [1, 10, 0]\n",
      "episode 6536, reward 871.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 139\n",
      "Initial State is  [4, 20, 4]\n",
      "episode 6537, reward 851.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 141\n",
      "Initial State is  [2, 12, 1]\n",
      "episode 6538, reward 907.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 132\n",
      "Initial State is  [3, 22, 2]\n",
      "episode 6539, reward 972.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 137\n",
      "Initial State is  [4, 5, 3]\n",
      "episode 6540, reward 772.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 125\n",
      "Initial State is  [3, 7, 1]\n",
      "episode 6541, reward 945.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 136\n",
      "Initial State is  [4, 13, 4]\n",
      "episode 6542, reward 1012.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 136\n",
      "Initial State is  [2, 1, 1]\n",
      "episode 6543, reward 711.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 136\n",
      "Initial State is  [3, 15, 1]\n",
      "episode 6544, reward 733.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 128\n",
      "Initial State is  [4, 6, 3]\n",
      "episode 6545, reward 1217.0, memory_length 2000, epsilon 0.009998671593271896, time 740.0, rides 135\n",
      "Initial State is  [0, 23, 1]\n",
      "episode 6546, reward 924.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 135\n",
      "Initial State is  [4, 19, 0]\n",
      "episode 6547, reward 668.0, memory_length 2000, epsilon 0.009998671593271896, time 736.0, rides 132\n",
      "Initial State is  [2, 8, 6]\n",
      "episode 6548, reward 1062.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 137\n",
      "Initial State is  [4, 22, 4]\n",
      "episode 6549, reward 775.0, memory_length 2000, epsilon 0.009998671593271896, time 739.0, rides 125\n",
      "Initial State is  [3, 2, 1]\n",
      "episode 6550, reward 689.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 140\n",
      "Initial State is  [4, 9, 5]\n",
      "episode 6551, reward 515.0, memory_length 2000, epsilon 0.009998671593271896, time 737.0, rides 131\n",
      "Initial State is  [3, 1, 1]\n",
      "episode 6552, reward 960.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 135\n",
      "Initial State is  [3, 12, 5]\n",
      "episode 6553, reward 1229.0, memory_length 2000, epsilon 0.009998671593271896, time 721.0, rides 149\n",
      "Initial State is  [4, 8, 4]\n",
      "episode 6554, reward 1100.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 128\n",
      "Initial State is  [3, 16, 4]\n",
      "episode 6555, reward 641.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 122\n",
      "Initial State is  [0, 23, 1]\n",
      "episode 6556, reward 728.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 142\n",
      "Initial State is  [0, 16, 2]\n",
      "episode 6557, reward 617.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 119\n",
      "Initial State is  [0, 13, 0]\n",
      "episode 6558, reward 850.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 128\n",
      "Initial State is  [1, 5, 4]\n",
      "episode 6559, reward 1080.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 133\n",
      "Initial State is  [0, 23, 3]\n",
      "episode 6560, reward 830.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 143\n",
      "Initial State is  [0, 7, 0]\n",
      "episode 6561, reward 1057.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 130\n",
      "Initial State is  [3, 22, 3]\n",
      "episode 6562, reward 944.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 146\n",
      "Initial State is  [3, 2, 5]\n",
      "episode 6563, reward 1085.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 138\n",
      "Initial State is  [2, 3, 4]\n",
      "episode 6564, reward 709.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 141\n",
      "Initial State is  [1, 4, 2]\n",
      "episode 6565, reward 962.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 128\n",
      "Initial State is  [0, 21, 6]\n",
      "episode 6566, reward 861.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 152\n",
      "Initial State is  [0, 3, 5]\n",
      "episode 6567, reward 814.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 133\n",
      "Initial State is  [0, 6, 4]\n",
      "episode 6568, reward 670.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 141\n",
      "Initial State is  [3, 17, 0]\n",
      "episode 6569, reward 772.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 122\n",
      "Initial State is  [2, 2, 2]\n",
      "episode 6570, reward 613.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 128\n",
      "Initial State is  [0, 6, 4]\n",
      "episode 6571, reward 955.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 130\n",
      "Initial State is  [1, 9, 4]\n",
      "episode 6572, reward 848.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 148\n",
      "Initial State is  [2, 14, 6]\n",
      "episode 6573, reward 1157.0, memory_length 2000, epsilon 0.009998671593271896, time 721.0, rides 135\n",
      "Initial State is  [0, 21, 4]\n",
      "episode 6574, reward 1091.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 129\n",
      "Initial State is  [1, 14, 6]\n",
      "episode 6575, reward 1099.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 134\n",
      "Initial State is  [4, 15, 5]\n",
      "episode 6576, reward 842.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 124\n",
      "Initial State is  [2, 15, 6]\n",
      "episode 6577, reward 1048.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 137\n",
      "Initial State is  [0, 4, 2]\n",
      "episode 6578, reward 530.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 125\n",
      "Initial State is  [0, 14, 5]\n",
      "episode 6579, reward 851.0, memory_length 2000, epsilon 0.009998671593271896, time 740.0, rides 116\n",
      "Initial State is  [2, 19, 6]\n",
      "episode 6580, reward 767.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 134\n",
      "Initial State is  [3, 10, 2]\n",
      "episode 6581, reward 1020.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 134\n",
      "Initial State is  [2, 22, 1]\n",
      "episode 6582, reward 883.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 134\n",
      "Initial State is  [0, 15, 3]\n",
      "episode 6583, reward 969.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 135\n",
      "Initial State is  [1, 16, 5]\n",
      "episode 6584, reward 786.0, memory_length 2000, epsilon 0.009998671593271896, time 743.0, rides 128\n",
      "Initial State is  [3, 12, 1]\n",
      "episode 6585, reward 960.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 144\n",
      "Initial State is  [0, 5, 4]\n",
      "episode 6586, reward 811.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 136\n",
      "Initial State is  [3, 16, 1]\n",
      "episode 6587, reward 813.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 127\n",
      "Initial State is  [1, 2, 1]\n",
      "episode 6588, reward 1134.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 134\n",
      "Initial State is  [2, 17, 1]\n",
      "episode 6589, reward 1176.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 129\n",
      "Initial State is  [4, 0, 2]\n",
      "episode 6590, reward 864.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 129\n",
      "Initial State is  [1, 16, 0]\n",
      "episode 6591, reward 944.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 119\n",
      "Initial State is  [4, 11, 5]\n",
      "episode 6592, reward 827.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 145\n",
      "Initial State is  [3, 10, 5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 6593, reward 706.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 141\n",
      "Initial State is  [3, 3, 6]\n",
      "episode 6594, reward 909.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 140\n",
      "Initial State is  [0, 16, 0]\n",
      "episode 6595, reward 857.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 130\n",
      "Initial State is  [0, 4, 1]\n",
      "episode 6596, reward 837.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 133\n",
      "Initial State is  [4, 15, 1]\n",
      "episode 6597, reward 785.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 129\n",
      "Initial State is  [3, 2, 0]\n",
      "episode 6598, reward 1108.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 122\n",
      "Initial State is  [4, 21, 3]\n",
      "episode 6599, reward 1033.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 127\n",
      "Initial State is  [1, 13, 4]\n",
      "episode 6600, reward 1081.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 131\n",
      "Initial State is  [4, 23, 6]\n",
      "episode 6601, reward 779.0, memory_length 2000, epsilon 0.009998671593271896, time 739.0, rides 144\n",
      "Initial State is  [2, 10, 6]\n",
      "episode 6602, reward 953.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 131\n",
      "Initial State is  [3, 16, 5]\n",
      "episode 6603, reward 844.0, memory_length 2000, epsilon 0.009998671593271896, time 737.0, rides 122\n",
      "Initial State is  [3, 13, 4]\n",
      "episode 6604, reward 895.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 120\n",
      "Initial State is  [4, 17, 5]\n",
      "episode 6605, reward 707.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 134\n",
      "Initial State is  [0, 9, 2]\n",
      "episode 6606, reward 734.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 119\n",
      "Initial State is  [2, 21, 3]\n",
      "episode 6607, reward 1103.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 146\n",
      "Initial State is  [2, 11, 4]\n",
      "episode 6608, reward 988.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 138\n",
      "Initial State is  [3, 14, 4]\n",
      "episode 6609, reward 1038.0, memory_length 2000, epsilon 0.009998671593271896, time 738.0, rides 127\n",
      "Initial State is  [0, 13, 1]\n",
      "episode 6610, reward 1097.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 135\n",
      "Initial State is  [2, 9, 4]\n",
      "episode 6611, reward 766.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 141\n",
      "Initial State is  [4, 1, 0]\n",
      "episode 6612, reward 626.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 151\n",
      "Initial State is  [3, 7, 1]\n",
      "episode 6613, reward 774.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 131\n",
      "Initial State is  [2, 7, 4]\n",
      "episode 6614, reward 1178.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 125\n",
      "Initial State is  [0, 17, 1]\n",
      "episode 6615, reward 954.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 135\n",
      "Initial State is  [0, 0, 2]\n",
      "episode 6616, reward 702.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 139\n",
      "Initial State is  [3, 14, 6]\n",
      "episode 6617, reward 794.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 119\n",
      "Initial State is  [2, 16, 3]\n",
      "episode 6618, reward 1076.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 138\n",
      "Initial State is  [2, 11, 4]\n",
      "episode 6619, reward 1144.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 135\n",
      "Initial State is  [0, 19, 1]\n",
      "episode 6620, reward 967.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 146\n",
      "Initial State is  [2, 6, 1]\n",
      "episode 6621, reward 1078.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 127\n",
      "Initial State is  [1, 20, 4]\n",
      "episode 6622, reward 812.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 138\n",
      "Initial State is  [4, 9, 3]\n",
      "episode 6623, reward 1124.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 152\n",
      "Initial State is  [3, 13, 4]\n",
      "episode 6624, reward 1347.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 136\n",
      "Initial State is  [1, 22, 3]\n",
      "episode 6625, reward 688.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 142\n",
      "Initial State is  [1, 1, 0]\n",
      "episode 6626, reward 783.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 134\n",
      "Initial State is  [0, 21, 1]\n",
      "episode 6627, reward 1182.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 134\n",
      "Initial State is  [0, 2, 4]\n",
      "episode 6628, reward 667.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 149\n",
      "Initial State is  [0, 12, 0]\n",
      "episode 6629, reward 1091.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 140\n",
      "Initial State is  [3, 7, 0]\n",
      "episode 6630, reward 832.0, memory_length 2000, epsilon 0.009998671593271896, time 720.0, rides 141\n",
      "Initial State is  [4, 13, 2]\n",
      "episode 6631, reward 653.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 131\n",
      "Initial State is  [1, 18, 5]\n",
      "episode 6632, reward 723.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 137\n",
      "Initial State is  [4, 19, 1]\n",
      "episode 6633, reward 919.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 126\n",
      "Initial State is  [3, 21, 6]\n",
      "episode 6634, reward 983.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 137\n",
      "Initial State is  [2, 2, 1]\n",
      "episode 6635, reward 1180.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 152\n",
      "Initial State is  [0, 11, 0]\n",
      "episode 6636, reward 682.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 134\n",
      "Initial State is  [0, 9, 4]\n",
      "episode 6637, reward 742.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 134\n",
      "Initial State is  [2, 12, 0]\n",
      "episode 6638, reward 1008.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 131\n",
      "Initial State is  [0, 8, 1]\n",
      "episode 6639, reward 971.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 135\n",
      "Initial State is  [2, 6, 2]\n",
      "episode 6640, reward 864.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 141\n",
      "Initial State is  [4, 14, 5]\n",
      "episode 6641, reward 1113.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 133\n",
      "Initial State is  [3, 8, 6]\n",
      "episode 6642, reward 829.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 142\n",
      "Initial State is  [3, 17, 6]\n",
      "episode 6643, reward 920.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 141\n",
      "Initial State is  [1, 7, 1]\n",
      "episode 6644, reward 810.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 142\n",
      "Initial State is  [3, 16, 6]\n",
      "episode 6645, reward 933.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 149\n",
      "Initial State is  [1, 15, 6]\n",
      "episode 6646, reward 869.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 126\n",
      "Initial State is  [4, 12, 4]\n",
      "episode 6647, reward 908.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 127\n",
      "Initial State is  [3, 0, 4]\n",
      "episode 6648, reward 825.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 133\n",
      "Initial State is  [4, 10, 0]\n",
      "episode 6649, reward 630.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 128\n",
      "Initial State is  [3, 14, 0]\n",
      "episode 6650, reward 1054.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 140\n",
      "Initial State is  [0, 5, 1]\n",
      "episode 6651, reward 927.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 126\n",
      "Initial State is  [4, 3, 3]\n",
      "episode 6652, reward 994.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 141\n",
      "Initial State is  [3, 22, 1]\n",
      "episode 6653, reward 1053.0, memory_length 2000, epsilon 0.009998671593271896, time 736.0, rides 137\n",
      "Initial State is  [0, 3, 3]\n",
      "episode 6654, reward 875.0, memory_length 2000, epsilon 0.009998671593271896, time 736.0, rides 139\n",
      "Initial State is  [3, 3, 5]\n",
      "episode 6655, reward 1229.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 132\n",
      "Initial State is  [1, 4, 3]\n",
      "episode 6656, reward 875.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 148\n",
      "Initial State is  [0, 1, 4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 6657, reward 836.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 128\n",
      "Initial State is  [4, 2, 6]\n",
      "episode 6658, reward 928.0, memory_length 2000, epsilon 0.009998671593271896, time 721.0, rides 136\n",
      "Initial State is  [0, 9, 6]\n",
      "episode 6659, reward 867.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 134\n",
      "Initial State is  [1, 21, 0]\n",
      "episode 6660, reward 835.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 129\n",
      "Initial State is  [0, 0, 6]\n",
      "episode 6661, reward 1064.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 129\n",
      "Initial State is  [2, 11, 4]\n",
      "episode 6662, reward 1239.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 141\n",
      "Initial State is  [3, 18, 2]\n",
      "episode 6663, reward 637.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 137\n",
      "Initial State is  [3, 5, 2]\n",
      "episode 6664, reward 838.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 143\n",
      "Initial State is  [0, 21, 3]\n",
      "episode 6665, reward 1035.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 134\n",
      "Initial State is  [0, 21, 4]\n",
      "episode 6666, reward 1101.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 128\n",
      "Initial State is  [3, 10, 3]\n",
      "episode 6667, reward 915.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 131\n",
      "Initial State is  [3, 3, 1]\n",
      "episode 6668, reward 1030.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 136\n",
      "Initial State is  [1, 5, 2]\n",
      "episode 6669, reward 1304.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 134\n",
      "Initial State is  [4, 14, 1]\n",
      "episode 6670, reward 1114.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 130\n",
      "Initial State is  [0, 0, 2]\n",
      "episode 6671, reward 1317.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 143\n",
      "Initial State is  [0, 0, 5]\n",
      "episode 6672, reward 930.0, memory_length 2000, epsilon 0.009998671593271896, time 737.0, rides 127\n",
      "Initial State is  [1, 19, 2]\n",
      "episode 6673, reward 1069.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 121\n",
      "Initial State is  [0, 12, 6]\n",
      "episode 6674, reward 1094.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 135\n",
      "Initial State is  [2, 12, 3]\n",
      "episode 6675, reward 955.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 136\n",
      "Initial State is  [1, 13, 5]\n",
      "episode 6676, reward 1070.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 142\n",
      "Initial State is  [1, 2, 5]\n",
      "episode 6677, reward 1005.0, memory_length 2000, epsilon 0.009998671593271896, time 739.0, rides 136\n",
      "Initial State is  [2, 14, 5]\n",
      "episode 6678, reward 1032.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 135\n",
      "Initial State is  [0, 4, 6]\n",
      "episode 6679, reward 571.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 136\n",
      "Initial State is  [2, 16, 3]\n",
      "episode 6680, reward 732.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 142\n",
      "Initial State is  [2, 9, 5]\n",
      "episode 6681, reward 1018.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 138\n",
      "Initial State is  [0, 12, 0]\n",
      "episode 6682, reward 923.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 132\n",
      "Initial State is  [1, 5, 6]\n",
      "episode 6683, reward 908.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 137\n",
      "Initial State is  [0, 10, 0]\n",
      "episode 6684, reward 1089.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 125\n",
      "Initial State is  [4, 20, 6]\n",
      "episode 6685, reward 1121.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 130\n",
      "Initial State is  [2, 1, 2]\n",
      "episode 6686, reward 695.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 135\n",
      "Initial State is  [1, 2, 5]\n",
      "episode 6687, reward 843.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 141\n",
      "Initial State is  [4, 14, 2]\n",
      "episode 6688, reward 1083.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 133\n",
      "Initial State is  [3, 20, 3]\n",
      "episode 6689, reward 1005.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 130\n",
      "Initial State is  [0, 22, 3]\n",
      "episode 6690, reward 1028.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 143\n",
      "Initial State is  [0, 3, 5]\n",
      "episode 6691, reward 1045.0, memory_length 2000, epsilon 0.009998671593271896, time 741.0, rides 146\n",
      "Initial State is  [4, 9, 5]\n",
      "episode 6692, reward 995.0, memory_length 2000, epsilon 0.009998671593271896, time 737.0, rides 125\n",
      "Initial State is  [2, 2, 6]\n",
      "episode 6693, reward 985.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 123\n",
      "Initial State is  [0, 2, 5]\n",
      "episode 6694, reward 899.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 127\n",
      "Initial State is  [1, 8, 4]\n",
      "episode 6695, reward 1173.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 139\n",
      "Initial State is  [0, 1, 6]\n",
      "episode 6696, reward 1075.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 129\n",
      "Initial State is  [0, 22, 5]\n",
      "episode 6697, reward 961.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 143\n",
      "Initial State is  [0, 23, 1]\n",
      "episode 6698, reward 883.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 129\n",
      "Initial State is  [2, 12, 5]\n",
      "episode 6699, reward 912.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 138\n",
      "Initial State is  [1, 7, 3]\n",
      "episode 6700, reward 846.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 148\n",
      "Initial State is  [3, 19, 3]\n",
      "episode 6701, reward 818.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 141\n",
      "Initial State is  [2, 1, 6]\n",
      "episode 6702, reward 1033.0, memory_length 2000, epsilon 0.009998671593271896, time 720.0, rides 144\n",
      "Initial State is  [4, 16, 2]\n",
      "episode 6703, reward 887.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 134\n",
      "Initial State is  [2, 2, 2]\n",
      "episode 6704, reward 750.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 128\n",
      "Initial State is  [2, 5, 5]\n",
      "episode 6705, reward 1014.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 138\n",
      "Initial State is  [2, 21, 2]\n",
      "episode 6706, reward 887.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 130\n",
      "Initial State is  [3, 8, 6]\n",
      "episode 6707, reward 792.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 131\n",
      "Initial State is  [0, 19, 5]\n",
      "episode 6708, reward 924.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 129\n",
      "Initial State is  [4, 6, 6]\n",
      "episode 6709, reward 681.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 130\n",
      "Initial State is  [0, 14, 3]\n",
      "episode 6710, reward 995.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 143\n",
      "Initial State is  [2, 5, 5]\n",
      "episode 6711, reward 835.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 128\n",
      "Initial State is  [1, 7, 0]\n",
      "episode 6712, reward 944.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 143\n",
      "Initial State is  [3, 1, 4]\n",
      "episode 6713, reward 566.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 134\n",
      "Initial State is  [0, 9, 1]\n",
      "episode 6714, reward 922.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 133\n",
      "Initial State is  [2, 10, 5]\n",
      "episode 6715, reward 741.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 130\n",
      "Initial State is  [1, 10, 4]\n",
      "episode 6716, reward 801.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 132\n",
      "Initial State is  [2, 12, 3]\n",
      "episode 6717, reward 887.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 133\n",
      "Initial State is  [0, 14, 0]\n",
      "episode 6718, reward 1031.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 127\n",
      "Initial State is  [4, 6, 6]\n",
      "episode 6719, reward 764.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 122\n",
      "Initial State is  [4, 10, 4]\n",
      "episode 6720, reward 1017.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 123\n",
      "Initial State is  [4, 5, 4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 6721, reward 1057.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 144\n",
      "Initial State is  [4, 7, 5]\n",
      "episode 6722, reward 964.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 130\n",
      "Initial State is  [1, 23, 0]\n",
      "episode 6723, reward 993.0, memory_length 2000, epsilon 0.009998671593271896, time 736.0, rides 129\n",
      "Initial State is  [0, 18, 5]\n",
      "episode 6724, reward 911.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 138\n",
      "Initial State is  [4, 17, 1]\n",
      "episode 6725, reward 974.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 141\n",
      "Initial State is  [4, 5, 0]\n",
      "episode 6726, reward 1147.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 137\n",
      "Initial State is  [2, 8, 4]\n",
      "episode 6727, reward 615.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 128\n",
      "Initial State is  [4, 18, 1]\n",
      "episode 6728, reward 732.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 120\n",
      "Initial State is  [1, 3, 1]\n",
      "episode 6729, reward 824.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 143\n",
      "Initial State is  [3, 23, 0]\n",
      "episode 6730, reward 901.0, memory_length 2000, epsilon 0.009998671593271896, time 736.0, rides 129\n",
      "Initial State is  [4, 1, 6]\n",
      "episode 6731, reward 965.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 127\n",
      "Initial State is  [3, 21, 0]\n",
      "episode 6732, reward 852.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 128\n",
      "Initial State is  [2, 17, 6]\n",
      "episode 6733, reward 677.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 135\n",
      "Initial State is  [0, 3, 0]\n",
      "episode 6734, reward 640.0, memory_length 2000, epsilon 0.009998671593271896, time 720.0, rides 123\n",
      "Initial State is  [4, 10, 2]\n",
      "episode 6735, reward 783.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 125\n",
      "Initial State is  [3, 0, 4]\n",
      "episode 6736, reward 1041.0, memory_length 2000, epsilon 0.009998671593271896, time 720.0, rides 140\n",
      "Initial State is  [4, 6, 4]\n",
      "episode 6737, reward 918.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 144\n",
      "Initial State is  [4, 17, 6]\n",
      "episode 6738, reward 996.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 133\n",
      "Initial State is  [2, 4, 6]\n",
      "episode 6739, reward 758.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 150\n",
      "Initial State is  [4, 15, 2]\n",
      "episode 6740, reward 968.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 139\n",
      "Initial State is  [1, 22, 3]\n",
      "episode 6741, reward 841.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 127\n",
      "Initial State is  [1, 22, 2]\n",
      "episode 6742, reward 993.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 136\n",
      "Initial State is  [4, 9, 6]\n",
      "episode 6743, reward 712.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 119\n",
      "Initial State is  [1, 17, 0]\n",
      "episode 6744, reward 1008.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 123\n",
      "Initial State is  [1, 22, 5]\n",
      "episode 6745, reward 788.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 134\n",
      "Initial State is  [3, 12, 0]\n",
      "episode 6746, reward 980.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 132\n",
      "Initial State is  [2, 17, 2]\n",
      "episode 6747, reward 742.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 133\n",
      "Initial State is  [0, 3, 2]\n",
      "episode 6748, reward 963.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 130\n",
      "Initial State is  [3, 16, 6]\n",
      "episode 6749, reward 963.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 133\n",
      "Initial State is  [1, 12, 3]\n",
      "episode 6750, reward 939.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 128\n",
      "Initial State is  [0, 14, 0]\n",
      "episode 6751, reward 738.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 137\n",
      "Initial State is  [4, 21, 1]\n",
      "episode 6752, reward 796.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 135\n",
      "Initial State is  [0, 1, 6]\n",
      "episode 6753, reward 804.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 128\n",
      "Initial State is  [4, 16, 3]\n",
      "episode 6754, reward 932.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 132\n",
      "Initial State is  [4, 10, 0]\n",
      "episode 6755, reward 839.0, memory_length 2000, epsilon 0.009998671593271896, time 741.0, rides 161\n",
      "Initial State is  [4, 6, 1]\n",
      "episode 6756, reward 982.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 125\n",
      "Initial State is  [0, 3, 1]\n",
      "episode 6757, reward 1107.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 147\n",
      "Initial State is  [1, 23, 2]\n",
      "episode 6758, reward 974.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 130\n",
      "Initial State is  [0, 17, 4]\n",
      "episode 6759, reward 969.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 129\n",
      "Initial State is  [3, 14, 3]\n",
      "episode 6760, reward 1030.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 125\n",
      "Initial State is  [0, 18, 2]\n",
      "episode 6761, reward 1140.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 136\n",
      "Initial State is  [4, 3, 0]\n",
      "episode 6762, reward 1019.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 140\n",
      "Initial State is  [2, 20, 4]\n",
      "episode 6763, reward 746.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 130\n",
      "Initial State is  [2, 13, 0]\n",
      "episode 6764, reward 610.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 129\n",
      "Initial State is  [3, 1, 3]\n",
      "episode 6765, reward 889.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 116\n",
      "Initial State is  [1, 16, 2]\n",
      "episode 6766, reward 742.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 133\n",
      "Initial State is  [0, 1, 1]\n",
      "episode 6767, reward 781.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 117\n",
      "Initial State is  [4, 13, 0]\n",
      "episode 6768, reward 832.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 130\n",
      "Initial State is  [1, 13, 1]\n",
      "episode 6769, reward 707.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 121\n",
      "Initial State is  [2, 8, 6]\n",
      "episode 6770, reward 734.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 148\n",
      "Initial State is  [2, 6, 4]\n",
      "episode 6771, reward 815.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 123\n",
      "Initial State is  [0, 20, 6]\n",
      "episode 6772, reward 1336.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 120\n",
      "Initial State is  [1, 14, 1]\n",
      "episode 6773, reward 1163.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 122\n",
      "Initial State is  [4, 8, 3]\n",
      "episode 6774, reward 748.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 136\n",
      "Initial State is  [2, 10, 5]\n",
      "episode 6775, reward 937.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 129\n",
      "Initial State is  [4, 4, 1]\n",
      "episode 6776, reward 936.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 131\n",
      "Initial State is  [2, 6, 5]\n",
      "episode 6777, reward 1072.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 142\n",
      "Initial State is  [2, 23, 2]\n",
      "episode 6778, reward 927.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 142\n",
      "Initial State is  [1, 4, 6]\n",
      "episode 6779, reward 756.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 135\n",
      "Initial State is  [3, 12, 1]\n",
      "episode 6780, reward 1096.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 131\n",
      "Initial State is  [0, 20, 3]\n",
      "episode 6781, reward 854.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 126\n",
      "Initial State is  [4, 3, 6]\n",
      "episode 6782, reward 945.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 132\n",
      "Initial State is  [0, 23, 5]\n",
      "episode 6783, reward 1255.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 133\n",
      "Initial State is  [4, 23, 4]\n",
      "episode 6784, reward 840.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 145\n",
      "Initial State is  [2, 15, 4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 6785, reward 1038.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 137\n",
      "Initial State is  [3, 21, 3]\n",
      "episode 6786, reward 750.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 134\n",
      "Initial State is  [3, 18, 0]\n",
      "episode 6787, reward 1075.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 143\n",
      "Initial State is  [3, 22, 5]\n",
      "episode 6788, reward 840.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 136\n",
      "Initial State is  [3, 5, 6]\n",
      "episode 6789, reward 1007.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 142\n",
      "Initial State is  [2, 15, 2]\n",
      "episode 6790, reward 959.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 131\n",
      "Initial State is  [4, 11, 5]\n",
      "episode 6791, reward 1168.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 125\n",
      "Initial State is  [2, 14, 0]\n",
      "episode 6792, reward 799.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 136\n",
      "Initial State is  [4, 21, 2]\n",
      "episode 6793, reward 935.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 131\n",
      "Initial State is  [3, 22, 2]\n",
      "episode 6794, reward 1117.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 126\n",
      "Initial State is  [3, 18, 4]\n",
      "episode 6795, reward 980.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 128\n",
      "Initial State is  [0, 18, 0]\n",
      "episode 6796, reward 1013.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 137\n",
      "Initial State is  [1, 19, 6]\n",
      "episode 6797, reward 723.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 129\n",
      "Initial State is  [4, 23, 4]\n",
      "episode 6798, reward 858.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 134\n",
      "Initial State is  [1, 13, 4]\n",
      "episode 6799, reward 759.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 138\n",
      "Initial State is  [0, 16, 0]\n",
      "episode 6800, reward 888.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 144\n",
      "Initial State is  [0, 16, 5]\n",
      "episode 6801, reward 830.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 132\n",
      "Initial State is  [3, 5, 4]\n",
      "episode 6802, reward 997.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 133\n",
      "Initial State is  [1, 6, 5]\n",
      "episode 6803, reward 845.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 138\n",
      "Initial State is  [1, 18, 2]\n",
      "episode 6804, reward 1120.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 141\n",
      "Initial State is  [3, 20, 6]\n",
      "episode 6805, reward 887.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 131\n",
      "Initial State is  [1, 15, 0]\n",
      "episode 6806, reward 969.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 138\n",
      "Initial State is  [0, 22, 0]\n",
      "episode 6807, reward 894.0, memory_length 2000, epsilon 0.009998671593271896, time 739.0, rides 135\n",
      "Initial State is  [1, 11, 6]\n",
      "episode 6808, reward 753.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 139\n",
      "Initial State is  [3, 19, 6]\n",
      "episode 6809, reward 980.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 121\n",
      "Initial State is  [1, 20, 6]\n",
      "episode 6810, reward 1037.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 150\n",
      "Initial State is  [0, 9, 4]\n",
      "episode 6811, reward 1024.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 130\n",
      "Initial State is  [1, 12, 1]\n",
      "episode 6812, reward 783.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 133\n",
      "Initial State is  [4, 8, 6]\n",
      "episode 6813, reward 897.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 133\n",
      "Initial State is  [3, 0, 2]\n",
      "episode 6814, reward 709.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 131\n",
      "Initial State is  [0, 2, 3]\n",
      "episode 6815, reward 1305.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 132\n",
      "Initial State is  [1, 9, 6]\n",
      "episode 6816, reward 942.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 131\n",
      "Initial State is  [4, 23, 5]\n",
      "episode 6817, reward 1030.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 132\n",
      "Initial State is  [2, 11, 0]\n",
      "episode 6818, reward 867.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 125\n",
      "Initial State is  [3, 22, 0]\n",
      "episode 6819, reward 1019.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 125\n",
      "Initial State is  [4, 5, 2]\n",
      "episode 6820, reward 857.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 125\n",
      "Initial State is  [1, 11, 5]\n",
      "episode 6821, reward 812.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 122\n",
      "Initial State is  [0, 19, 6]\n",
      "episode 6822, reward 958.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 142\n",
      "Initial State is  [1, 5, 1]\n",
      "episode 6823, reward 1030.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 136\n",
      "Initial State is  [1, 17, 1]\n",
      "episode 6824, reward 1143.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 130\n",
      "Initial State is  [0, 15, 2]\n",
      "episode 6825, reward 704.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 128\n",
      "Initial State is  [1, 7, 3]\n",
      "episode 6826, reward 1189.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 141\n",
      "Initial State is  [4, 20, 0]\n",
      "episode 6827, reward 1027.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 126\n",
      "Initial State is  [0, 10, 4]\n",
      "episode 6828, reward 1080.0, memory_length 2000, epsilon 0.009998671593271896, time 720.0, rides 142\n",
      "Initial State is  [1, 18, 2]\n",
      "episode 6829, reward 778.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 127\n",
      "Initial State is  [2, 2, 5]\n",
      "episode 6830, reward 1094.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 141\n",
      "Initial State is  [4, 20, 0]\n",
      "episode 6831, reward 1042.0, memory_length 2000, epsilon 0.009998671593271896, time 737.0, rides 134\n",
      "Initial State is  [1, 6, 5]\n",
      "episode 6832, reward 768.0, memory_length 2000, epsilon 0.009998671593271896, time 742.0, rides 124\n",
      "Initial State is  [2, 18, 2]\n",
      "episode 6833, reward 821.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 129\n",
      "Initial State is  [1, 10, 0]\n",
      "episode 6834, reward 911.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 134\n",
      "Initial State is  [2, 2, 6]\n",
      "episode 6835, reward 877.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 140\n",
      "Initial State is  [4, 18, 1]\n",
      "episode 6836, reward 1093.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 146\n",
      "Initial State is  [4, 10, 4]\n",
      "episode 6837, reward 919.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 130\n",
      "Initial State is  [3, 14, 5]\n",
      "episode 6838, reward 808.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 136\n",
      "Initial State is  [3, 2, 3]\n",
      "episode 6839, reward 766.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 132\n",
      "Initial State is  [0, 14, 0]\n",
      "episode 6840, reward 827.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 136\n",
      "Initial State is  [2, 10, 2]\n",
      "episode 6841, reward 922.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 131\n",
      "Initial State is  [0, 20, 0]\n",
      "episode 6842, reward 803.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 137\n",
      "Initial State is  [4, 10, 5]\n",
      "episode 6843, reward 994.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 133\n",
      "Initial State is  [0, 15, 2]\n",
      "episode 6844, reward 1041.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 140\n",
      "Initial State is  [4, 20, 4]\n",
      "episode 6845, reward 945.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 144\n",
      "Initial State is  [2, 9, 6]\n",
      "episode 6846, reward 921.0, memory_length 2000, epsilon 0.009998671593271896, time 736.0, rides 142\n",
      "Initial State is  [2, 8, 4]\n",
      "episode 6847, reward 1044.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 144\n",
      "Initial State is  [4, 9, 2]\n",
      "episode 6848, reward 876.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 128\n",
      "Initial State is  [4, 11, 5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 6849, reward 1151.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 134\n",
      "Initial State is  [1, 15, 2]\n",
      "episode 6850, reward 1154.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 146\n",
      "Initial State is  [2, 6, 2]\n",
      "episode 6851, reward 1221.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 125\n",
      "Initial State is  [2, 16, 1]\n",
      "episode 6852, reward 1043.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 139\n",
      "Initial State is  [4, 23, 0]\n",
      "episode 6853, reward 1121.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 116\n",
      "Initial State is  [1, 12, 6]\n",
      "episode 6854, reward 1120.0, memory_length 2000, epsilon 0.009998671593271896, time 736.0, rides 133\n",
      "Initial State is  [3, 10, 0]\n",
      "episode 6855, reward 825.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 141\n",
      "Initial State is  [0, 20, 3]\n",
      "episode 6856, reward 551.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 134\n",
      "Initial State is  [4, 1, 4]\n",
      "episode 6857, reward 960.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 140\n",
      "Initial State is  [4, 11, 3]\n",
      "episode 6858, reward 1257.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 131\n",
      "Initial State is  [3, 16, 0]\n",
      "episode 6859, reward 525.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 131\n",
      "Initial State is  [4, 6, 1]\n",
      "episode 6860, reward 1026.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 131\n",
      "Initial State is  [4, 14, 5]\n",
      "episode 6861, reward 1050.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 131\n",
      "Initial State is  [0, 6, 2]\n",
      "episode 6862, reward 1097.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 143\n",
      "Initial State is  [0, 12, 2]\n",
      "episode 6863, reward 1324.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 132\n",
      "Initial State is  [3, 18, 6]\n",
      "episode 6864, reward 861.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 124\n",
      "Initial State is  [2, 15, 5]\n",
      "episode 6865, reward 1063.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 134\n",
      "Initial State is  [4, 15, 3]\n",
      "episode 6866, reward 675.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 131\n",
      "Initial State is  [2, 14, 1]\n",
      "episode 6867, reward 949.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 130\n",
      "Initial State is  [3, 19, 5]\n",
      "episode 6868, reward 924.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 134\n",
      "Initial State is  [3, 22, 5]\n",
      "episode 6869, reward 940.0, memory_length 2000, epsilon 0.009998671593271896, time 737.0, rides 141\n",
      "Initial State is  [3, 1, 3]\n",
      "episode 6870, reward 1026.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 135\n",
      "Initial State is  [1, 16, 5]\n",
      "episode 6871, reward 928.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 144\n",
      "Initial State is  [0, 2, 2]\n",
      "episode 6872, reward 1250.0, memory_length 2000, epsilon 0.009998671593271896, time 737.0, rides 144\n",
      "Initial State is  [0, 15, 0]\n",
      "episode 6873, reward 875.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 126\n",
      "Initial State is  [1, 6, 3]\n",
      "episode 6874, reward 862.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 138\n",
      "Initial State is  [3, 19, 1]\n",
      "episode 6875, reward 1036.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 133\n",
      "Initial State is  [0, 1, 5]\n",
      "episode 6876, reward 1247.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 130\n",
      "Initial State is  [4, 15, 1]\n",
      "episode 6877, reward 910.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 138\n",
      "Initial State is  [4, 12, 3]\n",
      "episode 6878, reward 1056.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 133\n",
      "Initial State is  [0, 1, 6]\n",
      "episode 6879, reward 844.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 133\n",
      "Initial State is  [2, 19, 0]\n",
      "episode 6880, reward 812.0, memory_length 2000, epsilon 0.009998671593271896, time 737.0, rides 138\n",
      "Initial State is  [0, 2, 0]\n",
      "episode 6881, reward 814.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 135\n",
      "Initial State is  [1, 21, 0]\n",
      "episode 6882, reward 1060.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 137\n",
      "Initial State is  [1, 22, 5]\n",
      "episode 6883, reward 837.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 123\n",
      "Initial State is  [3, 20, 1]\n",
      "episode 6884, reward 877.0, memory_length 2000, epsilon 0.009998671593271896, time 736.0, rides 132\n",
      "Initial State is  [1, 2, 2]\n",
      "episode 6885, reward 900.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 141\n",
      "Initial State is  [2, 19, 5]\n",
      "episode 6886, reward 902.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 134\n",
      "Initial State is  [2, 19, 0]\n",
      "episode 6887, reward 840.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 149\n",
      "Initial State is  [1, 18, 4]\n",
      "episode 6888, reward 957.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 122\n",
      "Initial State is  [0, 6, 4]\n",
      "episode 6889, reward 745.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 134\n",
      "Initial State is  [4, 22, 6]\n",
      "episode 6890, reward 649.0, memory_length 2000, epsilon 0.009998671593271896, time 736.0, rides 130\n",
      "Initial State is  [1, 13, 5]\n",
      "episode 6891, reward 768.0, memory_length 2000, epsilon 0.009998671593271896, time 721.0, rides 144\n",
      "Initial State is  [4, 22, 0]\n",
      "episode 6892, reward 1038.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 143\n",
      "Initial State is  [4, 11, 3]\n",
      "episode 6893, reward 1054.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 139\n",
      "Initial State is  [0, 7, 3]\n",
      "episode 6894, reward 1150.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 142\n",
      "Initial State is  [2, 7, 5]\n",
      "episode 6895, reward 930.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 146\n",
      "Initial State is  [0, 7, 3]\n",
      "episode 6896, reward 913.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 132\n",
      "Initial State is  [2, 2, 3]\n",
      "episode 6897, reward 742.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 141\n",
      "Initial State is  [2, 8, 4]\n",
      "episode 6898, reward 982.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 139\n",
      "Initial State is  [1, 14, 6]\n",
      "episode 6899, reward 1164.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 135\n",
      "Initial State is  [1, 4, 1]\n",
      "episode 6900, reward 1039.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 148\n",
      "Initial State is  [3, 8, 6]\n",
      "episode 6901, reward 668.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 136\n",
      "Initial State is  [4, 1, 3]\n",
      "episode 6902, reward 682.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 133\n",
      "Initial State is  [1, 11, 5]\n",
      "episode 6903, reward 781.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 140\n",
      "Initial State is  [3, 22, 5]\n",
      "episode 6904, reward 1054.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 144\n",
      "Initial State is  [1, 16, 0]\n",
      "episode 6905, reward 1199.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 147\n",
      "Initial State is  [0, 20, 5]\n",
      "episode 6906, reward 729.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 141\n",
      "Initial State is  [1, 7, 3]\n",
      "episode 6907, reward 881.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 140\n",
      "Initial State is  [4, 1, 3]\n",
      "episode 6908, reward 1046.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 133\n",
      "Initial State is  [3, 6, 2]\n",
      "episode 6909, reward 716.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 138\n",
      "Initial State is  [4, 18, 5]\n",
      "episode 6910, reward 829.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 129\n",
      "Initial State is  [2, 22, 2]\n",
      "episode 6911, reward 735.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 120\n",
      "Initial State is  [2, 0, 1]\n",
      "episode 6912, reward 997.0, memory_length 2000, epsilon 0.009998671593271896, time 740.0, rides 137\n",
      "Initial State is  [3, 20, 4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 6913, reward 738.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 125\n",
      "Initial State is  [1, 1, 3]\n",
      "episode 6914, reward 933.0, memory_length 2000, epsilon 0.009998671593271896, time 720.0, rides 123\n",
      "Initial State is  [0, 20, 0]\n",
      "episode 6915, reward 1173.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 144\n",
      "Initial State is  [4, 9, 4]\n",
      "episode 6916, reward 892.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 142\n",
      "Initial State is  [1, 3, 3]\n",
      "episode 6917, reward 1006.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 133\n",
      "Initial State is  [1, 10, 0]\n",
      "episode 6918, reward 1039.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 121\n",
      "Initial State is  [1, 4, 5]\n",
      "episode 6919, reward 973.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 129\n",
      "Initial State is  [2, 4, 4]\n",
      "episode 6920, reward 935.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 147\n",
      "Initial State is  [1, 2, 2]\n",
      "episode 6921, reward 828.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 135\n",
      "Initial State is  [3, 20, 3]\n",
      "episode 6922, reward 1007.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 125\n",
      "Initial State is  [1, 20, 1]\n",
      "episode 6923, reward 1027.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 129\n",
      "Initial State is  [2, 18, 1]\n",
      "episode 6924, reward 1031.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 134\n",
      "Initial State is  [3, 10, 2]\n",
      "episode 6925, reward 1311.0, memory_length 2000, epsilon 0.009998671593271896, time 737.0, rides 135\n",
      "Initial State is  [0, 23, 3]\n",
      "episode 6926, reward 608.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 136\n",
      "Initial State is  [2, 7, 3]\n",
      "episode 6927, reward 1082.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 151\n",
      "Initial State is  [0, 12, 5]\n",
      "episode 6928, reward 1036.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 152\n",
      "Initial State is  [2, 17, 3]\n",
      "episode 6929, reward 1016.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 138\n",
      "Initial State is  [3, 10, 5]\n",
      "episode 6930, reward 1079.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 130\n",
      "Initial State is  [3, 13, 3]\n",
      "episode 6931, reward 947.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 122\n",
      "Initial State is  [3, 15, 2]\n",
      "episode 6932, reward 820.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 131\n",
      "Initial State is  [2, 10, 5]\n",
      "episode 6933, reward 986.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 131\n",
      "Initial State is  [2, 17, 1]\n",
      "episode 6934, reward 904.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 126\n",
      "Initial State is  [0, 4, 6]\n",
      "episode 6935, reward 919.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 130\n",
      "Initial State is  [2, 7, 4]\n",
      "episode 6936, reward 818.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 133\n",
      "Initial State is  [3, 18, 5]\n",
      "episode 6937, reward 1128.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 124\n",
      "Initial State is  [2, 10, 1]\n",
      "episode 6938, reward 1025.0, memory_length 2000, epsilon 0.009998671593271896, time 738.0, rides 133\n",
      "Initial State is  [2, 17, 5]\n",
      "episode 6939, reward 985.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 134\n",
      "Initial State is  [1, 5, 2]\n",
      "episode 6940, reward 967.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 132\n",
      "Initial State is  [0, 10, 0]\n",
      "episode 6941, reward 671.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 133\n",
      "Initial State is  [2, 0, 0]\n",
      "episode 6942, reward 1239.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 147\n",
      "Initial State is  [2, 1, 0]\n",
      "episode 6943, reward 846.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 128\n",
      "Initial State is  [0, 22, 0]\n",
      "episode 6944, reward 931.0, memory_length 2000, epsilon 0.009998671593271896, time 737.0, rides 137\n",
      "Initial State is  [4, 10, 4]\n",
      "episode 6945, reward 1075.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 129\n",
      "Initial State is  [2, 7, 4]\n",
      "episode 6946, reward 698.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 122\n",
      "Initial State is  [4, 15, 5]\n",
      "episode 6947, reward 949.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 148\n",
      "Initial State is  [4, 10, 0]\n",
      "episode 6948, reward 1245.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 131\n",
      "Initial State is  [3, 1, 2]\n",
      "episode 6949, reward 995.0, memory_length 2000, epsilon 0.009998671593271896, time 741.0, rides 138\n",
      "Initial State is  [2, 5, 3]\n",
      "episode 6950, reward 1011.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 128\n",
      "Initial State is  [2, 20, 2]\n",
      "episode 6951, reward 990.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 140\n",
      "Initial State is  [2, 8, 5]\n",
      "episode 6952, reward 975.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 140\n",
      "Initial State is  [0, 15, 2]\n",
      "episode 6953, reward 1095.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 134\n",
      "Initial State is  [0, 4, 2]\n",
      "episode 6954, reward 1122.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 132\n",
      "Initial State is  [0, 3, 5]\n",
      "episode 6955, reward 856.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 136\n",
      "Initial State is  [0, 15, 6]\n",
      "episode 6956, reward 818.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 137\n",
      "Initial State is  [0, 19, 0]\n",
      "episode 6957, reward 930.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 137\n",
      "Initial State is  [1, 16, 6]\n",
      "episode 6958, reward 1046.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 139\n",
      "Initial State is  [2, 2, 6]\n",
      "episode 6959, reward 1001.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 138\n",
      "Initial State is  [1, 0, 5]\n",
      "episode 6960, reward 777.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 118\n",
      "Initial State is  [0, 3, 6]\n",
      "episode 6961, reward 785.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 135\n",
      "Initial State is  [1, 19, 0]\n",
      "episode 6962, reward 1035.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 122\n",
      "Initial State is  [3, 3, 4]\n",
      "episode 6963, reward 1178.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 133\n",
      "Initial State is  [4, 18, 5]\n",
      "episode 6964, reward 1000.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 134\n",
      "Initial State is  [2, 6, 1]\n",
      "episode 6965, reward 998.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 132\n",
      "Initial State is  [3, 21, 3]\n",
      "episode 6966, reward 1038.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 140\n",
      "Initial State is  [2, 6, 2]\n",
      "episode 6967, reward 967.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 151\n",
      "Initial State is  [3, 23, 4]\n",
      "episode 6968, reward 985.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 144\n",
      "Initial State is  [4, 10, 2]\n",
      "episode 6969, reward 1197.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 138\n",
      "Initial State is  [4, 9, 2]\n",
      "episode 6970, reward 735.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 145\n",
      "Initial State is  [1, 6, 3]\n",
      "episode 6971, reward 804.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 130\n",
      "Initial State is  [3, 0, 5]\n",
      "episode 6972, reward 762.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 142\n",
      "Initial State is  [2, 23, 1]\n",
      "episode 6973, reward 874.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 133\n",
      "Initial State is  [2, 15, 1]\n",
      "episode 6974, reward 742.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 137\n",
      "Initial State is  [0, 5, 1]\n",
      "episode 6975, reward 652.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 133\n",
      "Initial State is  [0, 5, 1]\n",
      "episode 6976, reward 877.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 131\n",
      "Initial State is  [2, 15, 3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 6977, reward 481.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 145\n",
      "Initial State is  [2, 20, 0]\n",
      "episode 6978, reward 969.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 136\n",
      "Initial State is  [1, 22, 6]\n",
      "episode 6979, reward 1086.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 142\n",
      "Initial State is  [3, 0, 0]\n",
      "episode 6980, reward 905.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 140\n",
      "Initial State is  [1, 1, 0]\n",
      "episode 6981, reward 1046.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 142\n",
      "Initial State is  [4, 12, 5]\n",
      "episode 6982, reward 711.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 129\n",
      "Initial State is  [2, 8, 1]\n",
      "episode 6983, reward 777.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 135\n",
      "Initial State is  [1, 5, 0]\n",
      "episode 6984, reward 1043.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 140\n",
      "Initial State is  [4, 22, 6]\n",
      "episode 6985, reward 975.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 148\n",
      "Initial State is  [4, 0, 3]\n",
      "episode 6986, reward 807.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 140\n",
      "Initial State is  [2, 17, 2]\n",
      "episode 6987, reward 675.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 131\n",
      "Initial State is  [0, 5, 4]\n",
      "episode 6988, reward 930.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 139\n",
      "Initial State is  [0, 16, 6]\n",
      "episode 6989, reward 931.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 133\n",
      "Initial State is  [4, 2, 6]\n",
      "episode 6990, reward 695.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 145\n",
      "Initial State is  [2, 1, 2]\n",
      "episode 6991, reward 847.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 131\n",
      "Initial State is  [3, 6, 0]\n",
      "episode 6992, reward 789.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 131\n",
      "Initial State is  [0, 14, 1]\n",
      "episode 6993, reward 1106.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 134\n",
      "Initial State is  [1, 6, 6]\n",
      "episode 6994, reward 852.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 131\n",
      "Initial State is  [4, 2, 1]\n",
      "episode 6995, reward 863.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 126\n",
      "Initial State is  [1, 21, 2]\n",
      "episode 6996, reward 822.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 132\n",
      "Initial State is  [2, 1, 1]\n",
      "episode 6997, reward 806.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 131\n",
      "Initial State is  [1, 3, 4]\n",
      "episode 6998, reward 972.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 127\n",
      "Initial State is  [2, 23, 2]\n",
      "episode 6999, reward 860.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 141\n",
      "Initial State is  [4, 11, 0]\n",
      "episode 7000, reward 892.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 125\n",
      "Initial State is  [1, 8, 1]\n",
      "episode 7001, reward 1018.0, memory_length 2000, epsilon 0.009998671593271896, time 736.0, rides 131\n",
      "Initial State is  [1, 20, 2]\n",
      "episode 7002, reward 1158.0, memory_length 2000, epsilon 0.009998671593271896, time 736.0, rides 126\n",
      "Initial State is  [0, 7, 0]\n",
      "episode 7003, reward 648.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 141\n",
      "Initial State is  [1, 1, 3]\n",
      "episode 7004, reward 1074.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 129\n",
      "Initial State is  [0, 16, 5]\n",
      "episode 7005, reward 702.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 133\n",
      "Initial State is  [2, 6, 4]\n",
      "episode 7006, reward 1114.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 134\n",
      "Initial State is  [4, 7, 1]\n",
      "episode 7007, reward 902.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 131\n",
      "Initial State is  [1, 5, 5]\n",
      "episode 7008, reward 1089.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 132\n",
      "Initial State is  [0, 6, 5]\n",
      "episode 7009, reward 911.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 138\n",
      "Initial State is  [0, 19, 5]\n",
      "episode 7010, reward 1132.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 135\n",
      "Initial State is  [1, 19, 3]\n",
      "episode 7011, reward 649.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 124\n",
      "Initial State is  [2, 2, 4]\n",
      "episode 7012, reward 809.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 130\n",
      "Initial State is  [2, 23, 4]\n",
      "episode 7013, reward 853.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 146\n",
      "Initial State is  [2, 23, 4]\n",
      "episode 7014, reward 970.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 144\n",
      "Initial State is  [2, 6, 2]\n",
      "episode 7015, reward 837.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 139\n",
      "Initial State is  [3, 9, 2]\n",
      "episode 7016, reward 936.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 138\n",
      "Initial State is  [0, 22, 5]\n",
      "episode 7017, reward 1039.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 131\n",
      "Initial State is  [0, 1, 5]\n",
      "episode 7018, reward 992.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 135\n",
      "Initial State is  [4, 9, 3]\n",
      "episode 7019, reward 1016.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 137\n",
      "Initial State is  [4, 11, 4]\n",
      "episode 7020, reward 1087.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 132\n",
      "Initial State is  [4, 18, 4]\n",
      "episode 7021, reward 1281.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 125\n",
      "Initial State is  [1, 1, 4]\n",
      "episode 7022, reward 1164.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 126\n",
      "Initial State is  [0, 16, 2]\n",
      "episode 7023, reward 1135.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 134\n",
      "Initial State is  [1, 18, 2]\n",
      "episode 7024, reward 741.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 129\n",
      "Initial State is  [0, 20, 0]\n",
      "episode 7025, reward 1102.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 130\n",
      "Initial State is  [3, 9, 4]\n",
      "episode 7026, reward 1191.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 140\n",
      "Initial State is  [3, 19, 4]\n",
      "episode 7027, reward 1081.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 125\n",
      "Initial State is  [1, 1, 3]\n",
      "episode 7028, reward 1390.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 136\n",
      "Initial State is  [4, 11, 6]\n",
      "episode 7029, reward 1065.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 137\n",
      "Initial State is  [2, 10, 2]\n",
      "episode 7030, reward 1109.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 128\n",
      "Initial State is  [0, 17, 6]\n",
      "episode 7031, reward 932.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 131\n",
      "Initial State is  [0, 18, 2]\n",
      "episode 7032, reward 864.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 122\n",
      "Initial State is  [3, 2, 0]\n",
      "episode 7033, reward 726.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 141\n",
      "Initial State is  [0, 11, 5]\n",
      "episode 7034, reward 1030.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 134\n",
      "Initial State is  [3, 13, 3]\n",
      "episode 7035, reward 1431.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 125\n",
      "Initial State is  [0, 5, 4]\n",
      "episode 7036, reward 622.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 133\n",
      "Initial State is  [0, 1, 0]\n",
      "episode 7037, reward 767.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 131\n",
      "Initial State is  [4, 2, 5]\n",
      "episode 7038, reward 855.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 147\n",
      "Initial State is  [1, 3, 2]\n",
      "episode 7039, reward 1002.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 134\n",
      "Initial State is  [1, 1, 5]\n",
      "episode 7040, reward 733.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 135\n",
      "Initial State is  [1, 20, 2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 7041, reward 720.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 123\n",
      "Initial State is  [0, 18, 3]\n",
      "episode 7042, reward 863.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 128\n",
      "Initial State is  [0, 16, 6]\n",
      "episode 7043, reward 984.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 134\n",
      "Initial State is  [2, 2, 1]\n",
      "episode 7044, reward 1087.0, memory_length 2000, epsilon 0.009998671593271896, time 720.0, rides 130\n",
      "Initial State is  [4, 19, 3]\n",
      "episode 7045, reward 1169.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 137\n",
      "Initial State is  [2, 1, 4]\n",
      "episode 7046, reward 500.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 144\n",
      "Initial State is  [0, 11, 0]\n",
      "episode 7047, reward 788.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 139\n",
      "Initial State is  [1, 10, 5]\n",
      "episode 7048, reward 712.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 131\n",
      "Initial State is  [1, 8, 2]\n",
      "episode 7049, reward 1242.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 135\n",
      "Initial State is  [1, 4, 0]\n",
      "episode 7050, reward 1136.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 135\n",
      "Initial State is  [2, 18, 4]\n",
      "episode 7051, reward 1091.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 136\n",
      "Initial State is  [4, 20, 3]\n",
      "episode 7052, reward 976.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 136\n",
      "Initial State is  [2, 2, 1]\n",
      "episode 7053, reward 862.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 141\n",
      "Initial State is  [4, 13, 2]\n",
      "episode 7054, reward 709.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 130\n",
      "Initial State is  [0, 21, 3]\n",
      "episode 7055, reward 1188.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 146\n",
      "Initial State is  [0, 21, 3]\n",
      "episode 7056, reward 1136.0, memory_length 2000, epsilon 0.009998671593271896, time 721.0, rides 143\n",
      "Initial State is  [3, 17, 4]\n",
      "episode 7057, reward 847.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 149\n",
      "Initial State is  [4, 23, 2]\n",
      "episode 7058, reward 767.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 136\n",
      "Initial State is  [2, 17, 6]\n",
      "episode 7059, reward 570.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 127\n",
      "Initial State is  [2, 11, 1]\n",
      "episode 7060, reward 743.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 138\n",
      "Initial State is  [2, 13, 4]\n",
      "episode 7061, reward 930.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 135\n",
      "Initial State is  [1, 20, 2]\n",
      "episode 7062, reward 780.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 141\n",
      "Initial State is  [4, 19, 1]\n",
      "episode 7063, reward 925.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 129\n",
      "Initial State is  [1, 3, 1]\n",
      "episode 7064, reward 1013.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 135\n",
      "Initial State is  [0, 21, 5]\n",
      "episode 7065, reward 1160.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 137\n",
      "Initial State is  [1, 0, 1]\n",
      "episode 7066, reward 958.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 147\n",
      "Initial State is  [3, 16, 1]\n",
      "episode 7067, reward 899.0, memory_length 2000, epsilon 0.009998671593271896, time 741.0, rides 141\n",
      "Initial State is  [0, 14, 6]\n",
      "episode 7068, reward 1236.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 137\n",
      "Initial State is  [0, 9, 3]\n",
      "episode 7069, reward 994.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 139\n",
      "Initial State is  [1, 23, 4]\n",
      "episode 7070, reward 874.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 130\n",
      "Initial State is  [1, 22, 1]\n",
      "episode 7071, reward 931.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 138\n",
      "Initial State is  [4, 20, 4]\n",
      "episode 7072, reward 811.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 144\n",
      "Initial State is  [0, 0, 0]\n",
      "episode 7073, reward 932.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 134\n",
      "Initial State is  [0, 6, 3]\n",
      "episode 7074, reward 980.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 126\n",
      "Initial State is  [2, 2, 1]\n",
      "episode 7075, reward 1034.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 133\n",
      "Initial State is  [2, 2, 6]\n",
      "episode 7076, reward 989.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 136\n",
      "Initial State is  [4, 9, 2]\n",
      "episode 7077, reward 1079.0, memory_length 2000, epsilon 0.009998671593271896, time 738.0, rides 147\n",
      "Initial State is  [3, 0, 1]\n",
      "episode 7078, reward 654.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 152\n",
      "Initial State is  [2, 8, 1]\n",
      "episode 7079, reward 929.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 121\n",
      "Initial State is  [1, 22, 1]\n",
      "episode 7080, reward 775.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 127\n",
      "Initial State is  [0, 2, 2]\n",
      "episode 7081, reward 873.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 130\n",
      "Initial State is  [3, 11, 0]\n",
      "episode 7082, reward 920.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 133\n",
      "Initial State is  [1, 7, 6]\n",
      "episode 7083, reward 826.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 140\n",
      "Initial State is  [4, 16, 3]\n",
      "episode 7084, reward 870.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 129\n",
      "Initial State is  [1, 6, 5]\n",
      "episode 7085, reward 1132.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 138\n",
      "Initial State is  [2, 13, 3]\n",
      "episode 7086, reward 774.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 154\n",
      "Initial State is  [3, 19, 2]\n",
      "episode 7087, reward 991.0, memory_length 2000, epsilon 0.009998671593271896, time 737.0, rides 133\n",
      "Initial State is  [4, 20, 6]\n",
      "episode 7088, reward 1014.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 133\n",
      "Initial State is  [3, 13, 5]\n",
      "episode 7089, reward 966.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 133\n",
      "Initial State is  [4, 5, 2]\n",
      "episode 7090, reward 979.0, memory_length 2000, epsilon 0.009998671593271896, time 738.0, rides 135\n",
      "Initial State is  [4, 5, 2]\n",
      "episode 7091, reward 803.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 120\n",
      "Initial State is  [4, 7, 5]\n",
      "episode 7092, reward 717.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 141\n",
      "Initial State is  [1, 0, 3]\n",
      "episode 7093, reward 1139.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 140\n",
      "Initial State is  [4, 18, 3]\n",
      "episode 7094, reward 981.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 135\n",
      "Initial State is  [0, 6, 3]\n",
      "episode 7095, reward 1288.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 142\n",
      "Initial State is  [2, 10, 0]\n",
      "episode 7096, reward 858.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 137\n",
      "Initial State is  [3, 1, 4]\n",
      "episode 7097, reward 763.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 134\n",
      "Initial State is  [1, 10, 4]\n",
      "episode 7098, reward 850.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 122\n",
      "Initial State is  [2, 10, 6]\n",
      "episode 7099, reward 1018.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 132\n",
      "Initial State is  [1, 0, 5]\n",
      "episode 7100, reward 654.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 129\n",
      "Initial State is  [2, 13, 4]\n",
      "episode 7101, reward 911.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 139\n",
      "Initial State is  [1, 22, 5]\n",
      "episode 7102, reward 947.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 135\n",
      "Initial State is  [0, 23, 3]\n",
      "episode 7103, reward 905.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 147\n",
      "Initial State is  [1, 9, 4]\n",
      "episode 7104, reward 1157.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 135\n",
      "Initial State is  [3, 8, 6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 7105, reward 834.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 137\n",
      "Initial State is  [3, 20, 2]\n",
      "episode 7106, reward 1131.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 136\n",
      "Initial State is  [4, 19, 0]\n",
      "episode 7107, reward 848.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 137\n",
      "Initial State is  [2, 2, 6]\n",
      "episode 7108, reward 854.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 137\n",
      "Initial State is  [1, 12, 5]\n",
      "episode 7109, reward 1001.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 127\n",
      "Initial State is  [1, 2, 6]\n",
      "episode 7110, reward 827.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 129\n",
      "Initial State is  [1, 23, 6]\n",
      "episode 7111, reward 856.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 125\n",
      "Initial State is  [1, 9, 3]\n",
      "episode 7112, reward 827.0, memory_length 2000, epsilon 0.009998671593271896, time 739.0, rides 137\n",
      "Initial State is  [3, 8, 1]\n",
      "episode 7113, reward 815.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 126\n",
      "Initial State is  [0, 0, 4]\n",
      "episode 7114, reward 926.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 130\n",
      "Initial State is  [2, 2, 2]\n",
      "episode 7115, reward 706.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 139\n",
      "Initial State is  [4, 15, 6]\n",
      "episode 7116, reward 1051.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 128\n",
      "Initial State is  [2, 16, 0]\n",
      "episode 7117, reward 934.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 141\n",
      "Initial State is  [4, 5, 0]\n",
      "episode 7118, reward 1031.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 136\n",
      "Initial State is  [4, 4, 5]\n",
      "episode 7119, reward 937.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 128\n",
      "Initial State is  [3, 11, 6]\n",
      "episode 7120, reward 666.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 134\n",
      "Initial State is  [3, 4, 1]\n",
      "episode 7121, reward 1170.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 130\n",
      "Initial State is  [2, 0, 4]\n",
      "episode 7122, reward 845.0, memory_length 2000, epsilon 0.009998671593271896, time 737.0, rides 144\n",
      "Initial State is  [0, 14, 6]\n",
      "episode 7123, reward 1023.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 132\n",
      "Initial State is  [1, 18, 3]\n",
      "episode 7124, reward 896.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 127\n",
      "Initial State is  [3, 18, 5]\n",
      "episode 7125, reward 987.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 134\n",
      "Initial State is  [3, 4, 2]\n",
      "episode 7126, reward 1065.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 123\n",
      "Initial State is  [1, 23, 1]\n",
      "episode 7127, reward 897.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 126\n",
      "Initial State is  [0, 14, 3]\n",
      "episode 7128, reward 930.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 132\n",
      "Initial State is  [1, 22, 3]\n",
      "episode 7129, reward 1275.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 134\n",
      "Initial State is  [0, 15, 1]\n",
      "episode 7130, reward 881.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 125\n",
      "Initial State is  [0, 21, 1]\n",
      "episode 7131, reward 725.0, memory_length 2000, epsilon 0.009998671593271896, time 740.0, rides 131\n",
      "Initial State is  [1, 3, 2]\n",
      "episode 7132, reward 666.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 127\n",
      "Initial State is  [1, 3, 2]\n",
      "episode 7133, reward 827.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 128\n",
      "Initial State is  [4, 16, 3]\n",
      "episode 7134, reward 932.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 138\n",
      "Initial State is  [0, 7, 1]\n",
      "episode 7135, reward 1093.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 128\n",
      "Initial State is  [3, 6, 1]\n",
      "episode 7136, reward 663.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 131\n",
      "Initial State is  [4, 1, 6]\n",
      "episode 7137, reward 898.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 113\n",
      "Initial State is  [2, 9, 1]\n",
      "episode 7138, reward 832.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 124\n",
      "Initial State is  [1, 14, 1]\n",
      "episode 7139, reward 1121.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 132\n",
      "Initial State is  [2, 6, 2]\n",
      "episode 7140, reward 833.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 135\n",
      "Initial State is  [2, 17, 5]\n",
      "episode 7141, reward 796.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 143\n",
      "Initial State is  [3, 14, 4]\n",
      "episode 7142, reward 1027.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 132\n",
      "Initial State is  [2, 18, 0]\n",
      "episode 7143, reward 1169.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 140\n",
      "Initial State is  [4, 15, 2]\n",
      "episode 7144, reward 958.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 141\n",
      "Initial State is  [2, 20, 5]\n",
      "episode 7145, reward 789.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 131\n",
      "Initial State is  [0, 4, 2]\n",
      "episode 7146, reward 844.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 136\n",
      "Initial State is  [3, 22, 6]\n",
      "episode 7147, reward 1121.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 126\n",
      "Initial State is  [4, 9, 6]\n",
      "episode 7148, reward 964.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 139\n",
      "Initial State is  [2, 3, 2]\n",
      "episode 7149, reward 1041.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 148\n",
      "Initial State is  [3, 7, 5]\n",
      "episode 7150, reward 1043.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 134\n",
      "Initial State is  [1, 19, 5]\n",
      "episode 7151, reward 1092.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 135\n",
      "Initial State is  [3, 2, 2]\n",
      "episode 7152, reward 1002.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 126\n",
      "Initial State is  [3, 16, 1]\n",
      "episode 7153, reward 1047.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 134\n",
      "Initial State is  [3, 18, 2]\n",
      "episode 7154, reward 979.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 135\n",
      "Initial State is  [4, 6, 0]\n",
      "episode 7155, reward 810.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 139\n",
      "Initial State is  [3, 17, 4]\n",
      "episode 7156, reward 774.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 134\n",
      "Initial State is  [0, 22, 4]\n",
      "episode 7157, reward 974.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 133\n",
      "Initial State is  [1, 5, 2]\n",
      "episode 7158, reward 740.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 127\n",
      "Initial State is  [2, 1, 1]\n",
      "episode 7159, reward 803.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 135\n",
      "Initial State is  [3, 13, 3]\n",
      "episode 7160, reward 1027.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 135\n",
      "Initial State is  [0, 10, 5]\n",
      "episode 7161, reward 750.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 126\n",
      "Initial State is  [2, 18, 1]\n",
      "episode 7162, reward 1120.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 136\n",
      "Initial State is  [0, 9, 1]\n",
      "episode 7163, reward 961.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 141\n",
      "Initial State is  [0, 7, 6]\n",
      "episode 7164, reward 1023.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 146\n",
      "Initial State is  [0, 2, 6]\n",
      "episode 7165, reward 837.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 123\n",
      "Initial State is  [4, 2, 1]\n",
      "episode 7166, reward 743.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 129\n",
      "Initial State is  [4, 14, 5]\n",
      "episode 7167, reward 781.0, memory_length 2000, epsilon 0.009998671593271896, time 736.0, rides 134\n",
      "Initial State is  [2, 7, 2]\n",
      "episode 7168, reward 911.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 132\n",
      "Initial State is  [1, 19, 2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 7169, reward 1127.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 130\n",
      "Initial State is  [0, 10, 3]\n",
      "episode 7170, reward 1004.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 138\n",
      "Initial State is  [0, 20, 2]\n",
      "episode 7171, reward 416.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 121\n",
      "Initial State is  [0, 23, 4]\n",
      "episode 7172, reward 1066.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 140\n",
      "Initial State is  [2, 20, 5]\n",
      "episode 7173, reward 1035.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 137\n",
      "Initial State is  [3, 19, 5]\n",
      "episode 7174, reward 1104.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 122\n",
      "Initial State is  [0, 7, 6]\n",
      "episode 7175, reward 1210.0, memory_length 2000, epsilon 0.009998671593271896, time 720.0, rides 150\n",
      "Initial State is  [4, 13, 4]\n",
      "episode 7176, reward 1131.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 133\n",
      "Initial State is  [0, 16, 4]\n",
      "episode 7177, reward 1120.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 136\n",
      "Initial State is  [3, 15, 1]\n",
      "episode 7178, reward 664.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 121\n",
      "Initial State is  [4, 14, 2]\n",
      "episode 7179, reward 1098.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 119\n",
      "Initial State is  [2, 14, 4]\n",
      "episode 7180, reward 856.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 139\n",
      "Initial State is  [0, 19, 4]\n",
      "episode 7181, reward 963.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 139\n",
      "Initial State is  [0, 17, 1]\n",
      "episode 7182, reward 789.0, memory_length 2000, epsilon 0.009998671593271896, time 738.0, rides 123\n",
      "Initial State is  [0, 19, 2]\n",
      "episode 7183, reward 1077.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 127\n",
      "Initial State is  [4, 23, 5]\n",
      "episode 7184, reward 721.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 127\n",
      "Initial State is  [4, 17, 5]\n",
      "episode 7185, reward 1047.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 130\n",
      "Initial State is  [3, 6, 4]\n",
      "episode 7186, reward 974.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 120\n",
      "Initial State is  [2, 3, 4]\n",
      "episode 7187, reward 851.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 125\n",
      "Initial State is  [2, 0, 5]\n",
      "episode 7188, reward 1085.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 140\n",
      "Initial State is  [0, 16, 2]\n",
      "episode 7189, reward 1076.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 139\n",
      "Initial State is  [1, 20, 6]\n",
      "episode 7190, reward 1005.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 127\n",
      "Initial State is  [0, 19, 2]\n",
      "episode 7191, reward 921.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 128\n",
      "Initial State is  [1, 18, 3]\n",
      "episode 7192, reward 853.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 136\n",
      "Initial State is  [4, 2, 0]\n",
      "episode 7193, reward 989.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 141\n",
      "Initial State is  [0, 12, 5]\n",
      "episode 7194, reward 848.0, memory_length 2000, epsilon 0.009998671593271896, time 738.0, rides 141\n",
      "Initial State is  [2, 8, 6]\n",
      "episode 7195, reward 896.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 135\n",
      "Initial State is  [3, 5, 6]\n",
      "episode 7196, reward 985.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 146\n",
      "Initial State is  [0, 20, 5]\n",
      "episode 7197, reward 1148.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 129\n",
      "Initial State is  [2, 20, 3]\n",
      "episode 7198, reward 1125.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 126\n",
      "Initial State is  [3, 6, 1]\n",
      "episode 7199, reward 1062.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 139\n",
      "Initial State is  [3, 5, 4]\n",
      "episode 7200, reward 818.0, memory_length 2000, epsilon 0.009998671593271896, time 721.0, rides 137\n",
      "Initial State is  [4, 1, 0]\n",
      "episode 7201, reward 1181.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 145\n",
      "Initial State is  [0, 15, 4]\n",
      "episode 7202, reward 1282.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 140\n",
      "Initial State is  [3, 18, 4]\n",
      "episode 7203, reward 1028.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 149\n",
      "Initial State is  [0, 17, 2]\n",
      "episode 7204, reward 1221.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 134\n",
      "Initial State is  [4, 8, 6]\n",
      "episode 7205, reward 879.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 136\n",
      "Initial State is  [1, 3, 0]\n",
      "episode 7206, reward 629.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 127\n",
      "Initial State is  [3, 21, 4]\n",
      "episode 7207, reward 797.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 134\n",
      "Initial State is  [1, 0, 3]\n",
      "episode 7208, reward 1054.0, memory_length 2000, epsilon 0.009998671593271896, time 736.0, rides 144\n",
      "Initial State is  [1, 13, 0]\n",
      "episode 7209, reward 922.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 130\n",
      "Initial State is  [4, 0, 1]\n",
      "episode 7210, reward 827.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 137\n",
      "Initial State is  [2, 9, 6]\n",
      "episode 7211, reward 1028.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 119\n",
      "Initial State is  [2, 3, 1]\n",
      "episode 7212, reward 739.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 121\n",
      "Initial State is  [0, 16, 0]\n",
      "episode 7213, reward 1067.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 145\n",
      "Initial State is  [0, 20, 4]\n",
      "episode 7214, reward 957.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 135\n",
      "Initial State is  [0, 4, 5]\n",
      "episode 7215, reward 797.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 122\n",
      "Initial State is  [4, 17, 6]\n",
      "episode 7216, reward 932.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 144\n",
      "Initial State is  [0, 10, 6]\n",
      "episode 7217, reward 1010.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 131\n",
      "Initial State is  [4, 12, 4]\n",
      "episode 7218, reward 938.0, memory_length 2000, epsilon 0.009998671593271896, time 737.0, rides 145\n",
      "Initial State is  [3, 9, 3]\n",
      "episode 7219, reward 1130.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 142\n",
      "Initial State is  [3, 23, 2]\n",
      "episode 7220, reward 1157.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 133\n",
      "Initial State is  [0, 10, 5]\n",
      "episode 7221, reward 930.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 136\n",
      "Initial State is  [2, 18, 2]\n",
      "episode 7222, reward 981.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 143\n",
      "Initial State is  [2, 14, 4]\n",
      "episode 7223, reward 795.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 130\n",
      "Initial State is  [0, 17, 6]\n",
      "episode 7224, reward 1086.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 131\n",
      "Initial State is  [2, 4, 2]\n",
      "episode 7225, reward 1217.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 136\n",
      "Initial State is  [0, 0, 3]\n",
      "episode 7226, reward 1070.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 144\n",
      "Initial State is  [3, 17, 5]\n",
      "episode 7227, reward 1004.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 139\n",
      "Initial State is  [4, 19, 2]\n",
      "episode 7228, reward 998.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 142\n",
      "Initial State is  [1, 3, 2]\n",
      "episode 7229, reward 1114.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 141\n",
      "Initial State is  [4, 8, 0]\n",
      "episode 7230, reward 687.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 130\n",
      "Initial State is  [1, 23, 3]\n",
      "episode 7231, reward 933.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 139\n",
      "Initial State is  [3, 19, 3]\n",
      "episode 7232, reward 1079.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 133\n",
      "Initial State is  [0, 1, 6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 7233, reward 1105.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 137\n",
      "Initial State is  [0, 3, 3]\n",
      "episode 7234, reward 796.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 124\n",
      "Initial State is  [4, 13, 0]\n",
      "episode 7235, reward 949.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 122\n",
      "Initial State is  [2, 1, 4]\n",
      "episode 7236, reward 949.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 124\n",
      "Initial State is  [4, 8, 0]\n",
      "episode 7237, reward 858.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 134\n",
      "Initial State is  [4, 9, 0]\n",
      "episode 7238, reward 980.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 133\n",
      "Initial State is  [2, 14, 2]\n",
      "episode 7239, reward 821.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 143\n",
      "Initial State is  [2, 23, 0]\n",
      "episode 7240, reward 899.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 145\n",
      "Initial State is  [2, 17, 6]\n",
      "episode 7241, reward 816.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 134\n",
      "Initial State is  [1, 8, 4]\n",
      "episode 7242, reward 994.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 142\n",
      "Initial State is  [2, 22, 2]\n",
      "episode 7243, reward 1040.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 129\n",
      "Initial State is  [3, 6, 4]\n",
      "episode 7244, reward 941.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 137\n",
      "Initial State is  [0, 9, 6]\n",
      "episode 7245, reward 886.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 140\n",
      "Initial State is  [3, 21, 4]\n",
      "episode 7246, reward 805.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 145\n",
      "Initial State is  [1, 13, 6]\n",
      "episode 7247, reward 848.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 130\n",
      "Initial State is  [0, 9, 4]\n",
      "episode 7248, reward 916.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 136\n",
      "Initial State is  [0, 5, 3]\n",
      "episode 7249, reward 892.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 133\n",
      "Initial State is  [2, 19, 5]\n",
      "episode 7250, reward 811.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 132\n",
      "Initial State is  [2, 4, 2]\n",
      "episode 7251, reward 1050.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 146\n",
      "Initial State is  [4, 11, 4]\n",
      "episode 7252, reward 1343.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 143\n",
      "Initial State is  [3, 18, 4]\n",
      "episode 7253, reward 962.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 135\n",
      "Initial State is  [1, 9, 5]\n",
      "episode 7254, reward 948.0, memory_length 2000, epsilon 0.009998671593271896, time 736.0, rides 137\n",
      "Initial State is  [0, 1, 1]\n",
      "episode 7255, reward 1146.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 139\n",
      "Initial State is  [4, 11, 1]\n",
      "episode 7256, reward 627.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 142\n",
      "Initial State is  [0, 7, 6]\n",
      "episode 7257, reward 1133.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 138\n",
      "Initial State is  [2, 12, 5]\n",
      "episode 7258, reward 1038.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 125\n",
      "Initial State is  [0, 3, 0]\n",
      "episode 7259, reward 967.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 127\n",
      "Initial State is  [2, 0, 5]\n",
      "episode 7260, reward 1021.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 135\n",
      "Initial State is  [1, 5, 3]\n",
      "episode 7261, reward 967.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 135\n",
      "Initial State is  [0, 0, 3]\n",
      "episode 7262, reward 1021.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 154\n",
      "Initial State is  [2, 8, 3]\n",
      "episode 7263, reward 1031.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 130\n",
      "Initial State is  [0, 4, 4]\n",
      "episode 7264, reward 960.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 131\n",
      "Initial State is  [0, 10, 4]\n",
      "episode 7265, reward 1251.0, memory_length 2000, epsilon 0.009998671593271896, time 737.0, rides 148\n",
      "Initial State is  [0, 19, 0]\n",
      "episode 7266, reward 1019.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 126\n",
      "Initial State is  [2, 17, 0]\n",
      "episode 7267, reward 1031.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 144\n",
      "Initial State is  [3, 18, 1]\n",
      "episode 7268, reward 1010.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 135\n",
      "Initial State is  [1, 10, 5]\n",
      "episode 7269, reward 945.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 122\n",
      "Initial State is  [4, 8, 4]\n",
      "episode 7270, reward 1058.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 134\n",
      "Initial State is  [2, 16, 3]\n",
      "episode 7271, reward 932.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 143\n",
      "Initial State is  [4, 4, 1]\n",
      "episode 7272, reward 834.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 138\n",
      "Initial State is  [2, 5, 1]\n",
      "episode 7273, reward 1230.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 126\n",
      "Initial State is  [3, 5, 1]\n",
      "episode 7274, reward 845.0, memory_length 2000, epsilon 0.009998671593271896, time 721.0, rides 131\n",
      "Initial State is  [1, 18, 5]\n",
      "episode 7275, reward 1015.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 147\n",
      "Initial State is  [4, 16, 6]\n",
      "episode 7276, reward 1061.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 137\n",
      "Initial State is  [0, 6, 2]\n",
      "episode 7277, reward 1050.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 145\n",
      "Initial State is  [3, 20, 5]\n",
      "episode 7278, reward 1177.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 139\n",
      "Initial State is  [3, 23, 4]\n",
      "episode 7279, reward 709.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 127\n",
      "Initial State is  [4, 2, 1]\n",
      "episode 7280, reward 780.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 139\n",
      "Initial State is  [0, 16, 4]\n",
      "episode 7281, reward 1130.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 140\n",
      "Initial State is  [1, 12, 2]\n",
      "episode 7282, reward 904.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 132\n",
      "Initial State is  [2, 5, 2]\n",
      "episode 7283, reward 781.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 142\n",
      "Initial State is  [2, 23, 3]\n",
      "episode 7284, reward 1094.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 134\n",
      "Initial State is  [0, 5, 4]\n",
      "episode 7285, reward 1135.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 139\n",
      "Initial State is  [4, 2, 2]\n",
      "episode 7286, reward 1053.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 132\n",
      "Initial State is  [0, 13, 5]\n",
      "episode 7287, reward 913.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 122\n",
      "Initial State is  [4, 11, 0]\n",
      "episode 7288, reward 769.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 125\n",
      "Initial State is  [0, 4, 3]\n",
      "episode 7289, reward 1016.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 128\n",
      "Initial State is  [2, 9, 1]\n",
      "episode 7290, reward 962.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 135\n",
      "Initial State is  [0, 9, 2]\n",
      "episode 7291, reward 1098.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 135\n",
      "Initial State is  [1, 4, 5]\n",
      "episode 7292, reward 1153.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 132\n",
      "Initial State is  [4, 15, 1]\n",
      "episode 7293, reward 954.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 125\n",
      "Initial State is  [4, 14, 3]\n",
      "episode 7294, reward 741.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 135\n",
      "Initial State is  [1, 13, 3]\n",
      "episode 7295, reward 790.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 125\n",
      "Initial State is  [2, 6, 3]\n",
      "episode 7296, reward 885.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 129\n",
      "Initial State is  [1, 5, 2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 7297, reward 948.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 130\n",
      "Initial State is  [1, 19, 2]\n",
      "episode 7298, reward 971.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 138\n",
      "Initial State is  [1, 0, 2]\n",
      "episode 7299, reward 894.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 140\n",
      "Initial State is  [2, 21, 5]\n",
      "episode 7300, reward 825.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 125\n",
      "Initial State is  [3, 6, 2]\n",
      "episode 7301, reward 1071.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 143\n",
      "Initial State is  [4, 23, 6]\n",
      "episode 7302, reward 909.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 124\n",
      "Initial State is  [4, 8, 3]\n",
      "episode 7303, reward 721.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 131\n",
      "Initial State is  [0, 14, 5]\n",
      "episode 7304, reward 1012.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 126\n",
      "Initial State is  [3, 4, 4]\n",
      "episode 7305, reward 884.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 145\n",
      "Initial State is  [2, 17, 3]\n",
      "episode 7306, reward 1078.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 141\n",
      "Initial State is  [3, 7, 0]\n",
      "episode 7307, reward 1012.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 139\n",
      "Initial State is  [0, 1, 6]\n",
      "episode 7308, reward 1292.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 145\n",
      "Initial State is  [3, 0, 5]\n",
      "episode 7309, reward 899.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 118\n",
      "Initial State is  [0, 19, 0]\n",
      "episode 7310, reward 892.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 143\n",
      "Initial State is  [2, 20, 6]\n",
      "episode 7311, reward 900.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 136\n",
      "Initial State is  [4, 0, 1]\n",
      "episode 7312, reward 1037.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 146\n",
      "Initial State is  [3, 7, 0]\n",
      "episode 7313, reward 1206.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 140\n",
      "Initial State is  [3, 0, 1]\n",
      "episode 7314, reward 1265.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 149\n",
      "Initial State is  [4, 15, 4]\n",
      "episode 7315, reward 756.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 140\n",
      "Initial State is  [2, 17, 5]\n",
      "episode 7316, reward 941.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 139\n",
      "Initial State is  [4, 4, 0]\n",
      "episode 7317, reward 841.0, memory_length 2000, epsilon 0.009998671593271896, time 740.0, rides 135\n",
      "Initial State is  [4, 16, 1]\n",
      "episode 7318, reward 997.0, memory_length 2000, epsilon 0.009998671593271896, time 736.0, rides 140\n",
      "Initial State is  [1, 0, 6]\n",
      "episode 7319, reward 755.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 142\n",
      "Initial State is  [2, 19, 3]\n",
      "episode 7320, reward 769.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 153\n",
      "Initial State is  [4, 4, 6]\n",
      "episode 7321, reward 1135.0, memory_length 2000, epsilon 0.009998671593271896, time 739.0, rides 154\n",
      "Initial State is  [0, 14, 3]\n",
      "episode 7322, reward 1033.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 140\n",
      "Initial State is  [2, 18, 2]\n",
      "episode 7323, reward 703.0, memory_length 2000, epsilon 0.009998671593271896, time 721.0, rides 128\n",
      "Initial State is  [4, 1, 1]\n",
      "episode 7324, reward 841.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 137\n",
      "Initial State is  [4, 6, 1]\n",
      "episode 7325, reward 1075.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 153\n",
      "Initial State is  [3, 23, 1]\n",
      "episode 7326, reward 1046.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 132\n",
      "Initial State is  [2, 14, 0]\n",
      "episode 7327, reward 935.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 133\n",
      "Initial State is  [0, 19, 2]\n",
      "episode 7328, reward 1145.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 132\n",
      "Initial State is  [2, 8, 0]\n",
      "episode 7329, reward 951.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 137\n",
      "Initial State is  [0, 13, 3]\n",
      "episode 7330, reward 615.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 117\n",
      "Initial State is  [1, 16, 0]\n",
      "episode 7331, reward 808.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 136\n",
      "Initial State is  [2, 1, 0]\n",
      "episode 7332, reward 1028.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 132\n",
      "Initial State is  [4, 0, 4]\n",
      "episode 7333, reward 1083.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 133\n",
      "Initial State is  [0, 5, 1]\n",
      "episode 7334, reward 778.0, memory_length 2000, epsilon 0.009998671593271896, time 744.0, rides 141\n",
      "Initial State is  [0, 6, 6]\n",
      "episode 7335, reward 1046.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 134\n",
      "Initial State is  [0, 11, 5]\n",
      "episode 7336, reward 744.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 151\n",
      "Initial State is  [1, 4, 4]\n",
      "episode 7337, reward 990.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 134\n",
      "Initial State is  [4, 19, 0]\n",
      "episode 7338, reward 1257.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 132\n",
      "Initial State is  [4, 11, 0]\n",
      "episode 7339, reward 655.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 135\n",
      "Initial State is  [2, 6, 5]\n",
      "episode 7340, reward 855.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 126\n",
      "Initial State is  [2, 10, 3]\n",
      "episode 7341, reward 1048.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 130\n",
      "Initial State is  [2, 23, 4]\n",
      "episode 7342, reward 756.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 139\n",
      "Initial State is  [0, 1, 2]\n",
      "episode 7343, reward 1064.0, memory_length 2000, epsilon 0.009998671593271896, time 736.0, rides 140\n",
      "Initial State is  [2, 3, 3]\n",
      "episode 7344, reward 1218.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 140\n",
      "Initial State is  [4, 8, 3]\n",
      "episode 7345, reward 902.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 140\n",
      "Initial State is  [3, 5, 0]\n",
      "episode 7346, reward 749.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 118\n",
      "Initial State is  [3, 14, 6]\n",
      "episode 7347, reward 1205.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 133\n",
      "Initial State is  [0, 3, 3]\n",
      "episode 7348, reward 771.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 145\n",
      "Initial State is  [3, 17, 0]\n",
      "episode 7349, reward 872.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 122\n",
      "Initial State is  [3, 23, 4]\n",
      "episode 7350, reward 1158.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 128\n",
      "Initial State is  [0, 10, 3]\n",
      "episode 7351, reward 805.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 144\n",
      "Initial State is  [1, 22, 0]\n",
      "episode 7352, reward 967.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 133\n",
      "Initial State is  [3, 9, 4]\n",
      "episode 7353, reward 1012.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 124\n",
      "Initial State is  [3, 8, 2]\n",
      "episode 7354, reward 834.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 145\n",
      "Initial State is  [2, 23, 6]\n",
      "episode 7355, reward 659.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 143\n",
      "Initial State is  [3, 14, 5]\n",
      "episode 7356, reward 761.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 131\n",
      "Initial State is  [2, 14, 0]\n",
      "episode 7357, reward 988.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 135\n",
      "Initial State is  [1, 14, 3]\n",
      "episode 7358, reward 831.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 128\n",
      "Initial State is  [4, 14, 0]\n",
      "episode 7359, reward 772.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 129\n",
      "Initial State is  [3, 16, 3]\n",
      "episode 7360, reward 837.0, memory_length 2000, epsilon 0.009998671593271896, time 739.0, rides 126\n",
      "Initial State is  [2, 16, 2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 7361, reward 901.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 118\n",
      "Initial State is  [4, 21, 5]\n",
      "episode 7362, reward 838.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 138\n",
      "Initial State is  [3, 19, 2]\n",
      "episode 7363, reward 1007.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 144\n",
      "Initial State is  [1, 17, 3]\n",
      "episode 7364, reward 992.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 140\n",
      "Initial State is  [4, 16, 6]\n",
      "episode 7365, reward 951.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 117\n",
      "Initial State is  [0, 3, 4]\n",
      "episode 7366, reward 1121.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 136\n",
      "Initial State is  [2, 21, 5]\n",
      "episode 7367, reward 1021.0, memory_length 2000, epsilon 0.009998671593271896, time 744.0, rides 129\n",
      "Initial State is  [1, 6, 1]\n",
      "episode 7368, reward 958.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 124\n",
      "Initial State is  [2, 10, 3]\n",
      "episode 7369, reward 1243.0, memory_length 2000, epsilon 0.009998671593271896, time 721.0, rides 139\n",
      "Initial State is  [3, 14, 3]\n",
      "episode 7370, reward 942.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 133\n",
      "Initial State is  [1, 5, 3]\n",
      "episode 7371, reward 674.0, memory_length 2000, epsilon 0.009998671593271896, time 736.0, rides 127\n",
      "Initial State is  [3, 21, 0]\n",
      "episode 7372, reward 1054.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 132\n",
      "Initial State is  [1, 12, 1]\n",
      "episode 7373, reward 1010.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 128\n",
      "Initial State is  [4, 15, 0]\n",
      "episode 7374, reward 1219.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 131\n",
      "Initial State is  [4, 14, 4]\n",
      "episode 7375, reward 777.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 130\n",
      "Initial State is  [1, 1, 0]\n",
      "episode 7376, reward 879.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 128\n",
      "Initial State is  [1, 8, 1]\n",
      "episode 7377, reward 1125.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 122\n",
      "Initial State is  [1, 18, 3]\n",
      "episode 7378, reward 955.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 127\n",
      "Initial State is  [3, 0, 4]\n",
      "episode 7379, reward 963.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 140\n",
      "Initial State is  [0, 17, 3]\n",
      "episode 7380, reward 809.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 140\n",
      "Initial State is  [3, 18, 0]\n",
      "episode 7381, reward 773.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 138\n",
      "Initial State is  [2, 18, 0]\n",
      "episode 7382, reward 902.0, memory_length 2000, epsilon 0.009998671593271896, time 721.0, rides 135\n",
      "Initial State is  [0, 17, 0]\n",
      "episode 7383, reward 799.0, memory_length 2000, epsilon 0.009998671593271896, time 738.0, rides 126\n",
      "Initial State is  [4, 11, 4]\n",
      "episode 7384, reward 987.0, memory_length 2000, epsilon 0.009998671593271896, time 737.0, rides 141\n",
      "Initial State is  [4, 2, 6]\n",
      "episode 7385, reward 794.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 136\n",
      "Initial State is  [4, 2, 3]\n",
      "episode 7386, reward 927.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 129\n",
      "Initial State is  [2, 4, 2]\n",
      "episode 7387, reward 980.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 135\n",
      "Initial State is  [0, 4, 5]\n",
      "episode 7388, reward 515.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 141\n",
      "Initial State is  [1, 15, 5]\n",
      "episode 7389, reward 889.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 131\n",
      "Initial State is  [0, 12, 5]\n",
      "episode 7390, reward 958.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 144\n",
      "Initial State is  [2, 15, 4]\n",
      "episode 7391, reward 815.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 143\n",
      "Initial State is  [0, 3, 2]\n",
      "episode 7392, reward 614.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 131\n",
      "Initial State is  [3, 8, 3]\n",
      "episode 7393, reward 915.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 130\n",
      "Initial State is  [0, 15, 5]\n",
      "episode 7394, reward 1031.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 144\n",
      "Initial State is  [1, 20, 4]\n",
      "episode 7395, reward 737.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 132\n",
      "Initial State is  [3, 15, 5]\n",
      "episode 7396, reward 726.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 154\n",
      "Initial State is  [0, 1, 0]\n",
      "episode 7397, reward 1024.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 148\n",
      "Initial State is  [2, 18, 6]\n",
      "episode 7398, reward 1011.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 153\n",
      "Initial State is  [1, 3, 1]\n",
      "episode 7399, reward 895.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 129\n",
      "Initial State is  [0, 11, 5]\n",
      "episode 7400, reward 1056.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 143\n",
      "Initial State is  [1, 12, 1]\n",
      "episode 7401, reward 1088.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 139\n",
      "Initial State is  [2, 1, 3]\n",
      "episode 7402, reward 865.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 129\n",
      "Initial State is  [1, 11, 6]\n",
      "episode 7403, reward 818.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 147\n",
      "Initial State is  [0, 4, 3]\n",
      "episode 7404, reward 689.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 131\n",
      "Initial State is  [3, 12, 6]\n",
      "episode 7405, reward 779.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 129\n",
      "Initial State is  [0, 13, 0]\n",
      "episode 7406, reward 1005.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 124\n",
      "Initial State is  [4, 5, 4]\n",
      "episode 7407, reward 1037.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 136\n",
      "Initial State is  [0, 18, 0]\n",
      "episode 7408, reward 1200.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 138\n",
      "Initial State is  [4, 13, 5]\n",
      "episode 7409, reward 1179.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 129\n",
      "Initial State is  [1, 22, 6]\n",
      "episode 7410, reward 670.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 128\n",
      "Initial State is  [1, 11, 0]\n",
      "episode 7411, reward 861.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 133\n",
      "Initial State is  [3, 22, 3]\n",
      "episode 7412, reward 1203.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 128\n",
      "Initial State is  [0, 2, 2]\n",
      "episode 7413, reward 899.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 136\n",
      "Initial State is  [0, 14, 1]\n",
      "episode 7414, reward 1052.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 124\n",
      "Initial State is  [3, 14, 2]\n",
      "episode 7415, reward 951.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 137\n",
      "Initial State is  [2, 11, 3]\n",
      "episode 7416, reward 1182.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 145\n",
      "Initial State is  [2, 18, 5]\n",
      "episode 7417, reward 910.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 131\n",
      "Initial State is  [4, 19, 1]\n",
      "episode 7418, reward 1218.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 145\n",
      "Initial State is  [1, 4, 2]\n",
      "episode 7419, reward 918.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 132\n",
      "Initial State is  [2, 10, 0]\n",
      "episode 7420, reward 1045.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 132\n",
      "Initial State is  [2, 1, 1]\n",
      "episode 7421, reward 1235.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 139\n",
      "Initial State is  [4, 16, 3]\n",
      "episode 7422, reward 1009.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 140\n",
      "Initial State is  [2, 20, 5]\n",
      "episode 7423, reward 741.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 133\n",
      "Initial State is  [0, 18, 6]\n",
      "episode 7424, reward 1067.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 142\n",
      "Initial State is  [1, 22, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 7425, reward 1290.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 127\n",
      "Initial State is  [4, 22, 0]\n",
      "episode 7426, reward 829.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 139\n",
      "Initial State is  [3, 7, 2]\n",
      "episode 7427, reward 1154.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 127\n",
      "Initial State is  [0, 0, 0]\n",
      "episode 7428, reward 1172.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 135\n",
      "Initial State is  [1, 3, 4]\n",
      "episode 7429, reward 741.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 130\n",
      "Initial State is  [2, 2, 6]\n",
      "episode 7430, reward 765.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 115\n",
      "Initial State is  [3, 18, 3]\n",
      "episode 7431, reward 1067.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 128\n",
      "Initial State is  [2, 5, 6]\n",
      "episode 7432, reward 870.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 142\n",
      "Initial State is  [4, 13, 2]\n",
      "episode 7433, reward 993.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 131\n",
      "Initial State is  [3, 2, 4]\n",
      "episode 7434, reward 868.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 130\n",
      "Initial State is  [1, 15, 0]\n",
      "episode 7435, reward 1179.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 142\n",
      "Initial State is  [1, 10, 0]\n",
      "episode 7436, reward 969.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 142\n",
      "Initial State is  [4, 17, 0]\n",
      "episode 7437, reward 1022.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 135\n",
      "Initial State is  [3, 11, 5]\n",
      "episode 7438, reward 774.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 139\n",
      "Initial State is  [0, 9, 0]\n",
      "episode 7439, reward 955.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 134\n",
      "Initial State is  [3, 19, 3]\n",
      "episode 7440, reward 718.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 141\n",
      "Initial State is  [1, 13, 1]\n",
      "episode 7441, reward 1139.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 131\n",
      "Initial State is  [2, 19, 5]\n",
      "episode 7442, reward 662.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 137\n",
      "Initial State is  [2, 12, 0]\n",
      "episode 7443, reward 680.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 137\n",
      "Initial State is  [0, 13, 5]\n",
      "episode 7444, reward 998.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 131\n",
      "Initial State is  [4, 9, 2]\n",
      "episode 7445, reward 969.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 135\n",
      "Initial State is  [3, 20, 2]\n",
      "episode 7446, reward 879.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 136\n",
      "Initial State is  [4, 15, 2]\n",
      "episode 7447, reward 1086.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 131\n",
      "Initial State is  [0, 13, 0]\n",
      "episode 7448, reward 1340.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 125\n",
      "Initial State is  [3, 2, 1]\n",
      "episode 7449, reward 1044.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 117\n",
      "Initial State is  [1, 20, 6]\n",
      "episode 7450, reward 940.0, memory_length 2000, epsilon 0.009998671593271896, time 721.0, rides 118\n",
      "Initial State is  [4, 2, 6]\n",
      "episode 7451, reward 941.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 146\n",
      "Initial State is  [0, 6, 3]\n",
      "episode 7452, reward 834.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 124\n",
      "Initial State is  [0, 9, 5]\n",
      "episode 7453, reward 986.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 154\n",
      "Initial State is  [3, 8, 0]\n",
      "episode 7454, reward 881.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 140\n",
      "Initial State is  [2, 2, 6]\n",
      "episode 7455, reward 696.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 120\n",
      "Initial State is  [4, 8, 3]\n",
      "episode 7456, reward 827.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 133\n",
      "Initial State is  [2, 21, 1]\n",
      "episode 7457, reward 1092.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 132\n",
      "Initial State is  [1, 19, 4]\n",
      "episode 7458, reward 1148.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 152\n",
      "Initial State is  [1, 13, 2]\n",
      "episode 7459, reward 765.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 135\n",
      "Initial State is  [4, 17, 4]\n",
      "episode 7460, reward 993.0, memory_length 2000, epsilon 0.009998671593271896, time 737.0, rides 136\n",
      "Initial State is  [2, 12, 5]\n",
      "episode 7461, reward 449.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 129\n",
      "Initial State is  [0, 14, 0]\n",
      "episode 7462, reward 610.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 129\n",
      "Initial State is  [3, 12, 2]\n",
      "episode 7463, reward 847.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 132\n",
      "Initial State is  [3, 5, 6]\n",
      "episode 7464, reward 885.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 130\n",
      "Initial State is  [0, 22, 1]\n",
      "episode 7465, reward 1206.0, memory_length 2000, epsilon 0.009998671593271896, time 738.0, rides 137\n",
      "Initial State is  [3, 1, 6]\n",
      "episode 7466, reward 906.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 130\n",
      "Initial State is  [0, 20, 0]\n",
      "episode 7467, reward 875.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 143\n",
      "Initial State is  [0, 14, 4]\n",
      "episode 7468, reward 877.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 138\n",
      "Initial State is  [0, 10, 0]\n",
      "episode 7469, reward 1127.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 138\n",
      "Initial State is  [0, 6, 3]\n",
      "episode 7470, reward 1009.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 125\n",
      "Initial State is  [4, 21, 1]\n",
      "episode 7471, reward 978.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 137\n",
      "Initial State is  [3, 0, 5]\n",
      "episode 7472, reward 981.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 135\n",
      "Initial State is  [0, 15, 4]\n",
      "episode 7473, reward 733.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 138\n",
      "Initial State is  [3, 4, 1]\n",
      "episode 7474, reward 1292.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 130\n",
      "Initial State is  [0, 3, 2]\n",
      "episode 7475, reward 786.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 128\n",
      "Initial State is  [3, 14, 0]\n",
      "episode 7476, reward 999.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 132\n",
      "Initial State is  [3, 1, 5]\n",
      "episode 7477, reward 809.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 130\n",
      "Initial State is  [3, 19, 3]\n",
      "episode 7478, reward 465.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 119\n",
      "Initial State is  [3, 20, 0]\n",
      "episode 7479, reward 821.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 137\n",
      "Initial State is  [0, 19, 1]\n",
      "episode 7480, reward 951.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 129\n",
      "Initial State is  [4, 13, 2]\n",
      "episode 7481, reward 731.0, memory_length 2000, epsilon 0.009998671593271896, time 742.0, rides 132\n",
      "Initial State is  [0, 13, 4]\n",
      "episode 7482, reward 655.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 128\n",
      "Initial State is  [1, 0, 0]\n",
      "episode 7483, reward 740.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 138\n",
      "Initial State is  [0, 8, 5]\n",
      "episode 7484, reward 760.0, memory_length 2000, epsilon 0.009998671593271896, time 736.0, rides 133\n",
      "Initial State is  [4, 10, 5]\n",
      "episode 7485, reward 828.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 143\n",
      "Initial State is  [0, 18, 5]\n",
      "episode 7486, reward 1115.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 148\n",
      "Initial State is  [4, 17, 6]\n",
      "episode 7487, reward 1056.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 157\n",
      "Initial State is  [2, 9, 5]\n",
      "episode 7488, reward 971.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 138\n",
      "Initial State is  [0, 17, 5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 7489, reward 837.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 147\n",
      "Initial State is  [2, 15, 4]\n",
      "episode 7490, reward 1376.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 155\n",
      "Initial State is  [1, 3, 4]\n",
      "episode 7491, reward 996.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 134\n",
      "Initial State is  [3, 16, 1]\n",
      "episode 7492, reward 1000.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 154\n",
      "Initial State is  [4, 21, 3]\n",
      "episode 7493, reward 657.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 144\n",
      "Initial State is  [4, 10, 6]\n",
      "episode 7494, reward 1036.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 146\n",
      "Initial State is  [3, 21, 2]\n",
      "episode 7495, reward 980.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 135\n",
      "Initial State is  [3, 14, 3]\n",
      "episode 7496, reward 1092.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 146\n",
      "Initial State is  [0, 18, 2]\n",
      "episode 7497, reward 880.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 129\n",
      "Initial State is  [0, 15, 1]\n",
      "episode 7498, reward 780.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 143\n",
      "Initial State is  [1, 15, 4]\n",
      "episode 7499, reward 1165.0, memory_length 2000, epsilon 0.009998671593271896, time 720.0, rides 148\n",
      "Initial State is  [0, 16, 6]\n",
      "episode 7500, reward 1119.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 151\n",
      "Initial State is  [4, 16, 4]\n",
      "episode 7501, reward 1262.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 131\n",
      "Initial State is  [3, 13, 2]\n",
      "episode 7502, reward 786.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 135\n",
      "Initial State is  [2, 2, 5]\n",
      "episode 7503, reward 876.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 128\n",
      "Initial State is  [1, 5, 6]\n",
      "episode 7504, reward 930.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 134\n",
      "Initial State is  [2, 21, 5]\n",
      "episode 7505, reward 1166.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 144\n",
      "Initial State is  [0, 15, 5]\n",
      "episode 7506, reward 909.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 145\n",
      "Initial State is  [0, 7, 5]\n",
      "episode 7507, reward 924.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 125\n",
      "Initial State is  [3, 15, 1]\n",
      "episode 7508, reward 997.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 147\n",
      "Initial State is  [0, 5, 5]\n",
      "episode 7509, reward 819.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 137\n",
      "Initial State is  [0, 21, 3]\n",
      "episode 7510, reward 1171.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 141\n",
      "Initial State is  [2, 23, 3]\n",
      "episode 7511, reward 996.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 139\n",
      "Initial State is  [0, 12, 2]\n",
      "episode 7512, reward 966.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 125\n",
      "Initial State is  [1, 5, 6]\n",
      "episode 7513, reward 637.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 142\n",
      "Initial State is  [1, 0, 5]\n",
      "episode 7514, reward 681.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 138\n",
      "Initial State is  [4, 23, 5]\n",
      "episode 7515, reward 900.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 138\n",
      "Initial State is  [0, 17, 2]\n",
      "episode 7516, reward 1021.0, memory_length 2000, epsilon 0.009998671593271896, time 737.0, rides 133\n",
      "Initial State is  [1, 21, 6]\n",
      "episode 7517, reward 1002.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 139\n",
      "Initial State is  [0, 23, 6]\n",
      "episode 7518, reward 1013.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 143\n",
      "Initial State is  [4, 5, 5]\n",
      "episode 7519, reward 1023.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 126\n",
      "Initial State is  [4, 11, 3]\n",
      "episode 7520, reward 987.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 140\n",
      "Initial State is  [3, 7, 1]\n",
      "episode 7521, reward 807.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 131\n",
      "Initial State is  [2, 4, 1]\n",
      "episode 7522, reward 892.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 128\n",
      "Initial State is  [0, 8, 0]\n",
      "episode 7523, reward 795.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 135\n",
      "Initial State is  [1, 14, 0]\n",
      "episode 7524, reward 902.0, memory_length 2000, epsilon 0.009998671593271896, time 741.0, rides 139\n",
      "Initial State is  [4, 17, 1]\n",
      "episode 7525, reward 1157.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 136\n",
      "Initial State is  [0, 1, 4]\n",
      "episode 7526, reward 820.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 149\n",
      "Initial State is  [4, 13, 3]\n",
      "episode 7527, reward 777.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 130\n",
      "Initial State is  [1, 20, 1]\n",
      "episode 7528, reward 887.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 133\n",
      "Initial State is  [3, 15, 5]\n",
      "episode 7529, reward 968.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 133\n",
      "Initial State is  [3, 13, 0]\n",
      "episode 7530, reward 999.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 141\n",
      "Initial State is  [2, 10, 4]\n",
      "episode 7531, reward 1185.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 138\n",
      "Initial State is  [4, 9, 1]\n",
      "episode 7532, reward 756.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 135\n",
      "Initial State is  [3, 12, 6]\n",
      "episode 7533, reward 811.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 138\n",
      "Initial State is  [2, 4, 1]\n",
      "episode 7534, reward 1159.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 141\n",
      "Initial State is  [0, 23, 5]\n",
      "episode 7535, reward 968.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 133\n",
      "Initial State is  [3, 12, 6]\n",
      "episode 7536, reward 573.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 133\n",
      "Initial State is  [3, 0, 1]\n",
      "episode 7537, reward 1025.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 128\n",
      "Initial State is  [3, 8, 3]\n",
      "episode 7538, reward 947.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 146\n",
      "Initial State is  [3, 19, 0]\n",
      "episode 7539, reward 732.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 132\n",
      "Initial State is  [1, 20, 2]\n",
      "episode 7540, reward 905.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 120\n",
      "Initial State is  [1, 1, 1]\n",
      "episode 7541, reward 762.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 132\n",
      "Initial State is  [3, 16, 2]\n",
      "episode 7542, reward 1045.0, memory_length 2000, epsilon 0.009998671593271896, time 740.0, rides 140\n",
      "Initial State is  [4, 19, 5]\n",
      "episode 7543, reward 1096.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 129\n",
      "Initial State is  [3, 22, 2]\n",
      "episode 7544, reward 724.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 137\n",
      "Initial State is  [4, 23, 5]\n",
      "episode 7545, reward 1008.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 149\n",
      "Initial State is  [3, 7, 6]\n",
      "episode 7546, reward 756.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 142\n",
      "Initial State is  [4, 21, 6]\n",
      "episode 7547, reward 866.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 127\n",
      "Initial State is  [2, 11, 3]\n",
      "episode 7548, reward 976.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 135\n",
      "Initial State is  [4, 6, 0]\n",
      "episode 7549, reward 700.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 132\n",
      "Initial State is  [0, 8, 1]\n",
      "episode 7550, reward 1054.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 125\n",
      "Initial State is  [1, 8, 0]\n",
      "episode 7551, reward 927.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 128\n",
      "Initial State is  [4, 19, 6]\n",
      "episode 7552, reward 990.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 133\n",
      "Initial State is  [3, 12, 1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 7553, reward 758.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 150\n",
      "Initial State is  [1, 9, 6]\n",
      "episode 7554, reward 1031.0, memory_length 2000, epsilon 0.009998671593271896, time 737.0, rides 133\n",
      "Initial State is  [3, 18, 4]\n",
      "episode 7555, reward 903.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 125\n",
      "Initial State is  [2, 17, 6]\n",
      "episode 7556, reward 921.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 131\n",
      "Initial State is  [4, 11, 2]\n",
      "episode 7557, reward 885.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 157\n",
      "Initial State is  [0, 23, 5]\n",
      "episode 7558, reward 994.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 127\n",
      "Initial State is  [2, 18, 0]\n",
      "episode 7559, reward 1062.0, memory_length 2000, epsilon 0.009998671593271896, time 736.0, rides 148\n",
      "Initial State is  [4, 2, 0]\n",
      "episode 7560, reward 1004.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 141\n",
      "Initial State is  [4, 2, 3]\n",
      "episode 7561, reward 968.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 136\n",
      "Initial State is  [4, 17, 3]\n",
      "episode 7562, reward 818.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 135\n",
      "Initial State is  [0, 8, 2]\n",
      "episode 7563, reward 699.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 132\n",
      "Initial State is  [4, 5, 1]\n",
      "episode 7564, reward 1069.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 134\n",
      "Initial State is  [3, 15, 3]\n",
      "episode 7565, reward 918.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 141\n",
      "Initial State is  [0, 10, 2]\n",
      "episode 7566, reward 550.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 134\n",
      "Initial State is  [1, 15, 3]\n",
      "episode 7567, reward 1139.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 136\n",
      "Initial State is  [4, 12, 5]\n",
      "episode 7568, reward 890.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 125\n",
      "Initial State is  [4, 2, 3]\n",
      "episode 7569, reward 993.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 122\n",
      "Initial State is  [3, 18, 0]\n",
      "episode 7570, reward 641.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 128\n",
      "Initial State is  [3, 22, 2]\n",
      "episode 7571, reward 705.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 133\n",
      "Initial State is  [4, 16, 0]\n",
      "episode 7572, reward 1049.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 131\n",
      "Initial State is  [4, 13, 0]\n",
      "episode 7573, reward 657.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 137\n",
      "Initial State is  [4, 5, 1]\n",
      "episode 7574, reward 1008.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 131\n",
      "Initial State is  [4, 8, 4]\n",
      "episode 7575, reward 646.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 139\n",
      "Initial State is  [4, 5, 4]\n",
      "episode 7576, reward 1086.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 123\n",
      "Initial State is  [1, 20, 6]\n",
      "episode 7577, reward 994.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 136\n",
      "Initial State is  [4, 2, 3]\n",
      "episode 7578, reward 1057.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 143\n",
      "Initial State is  [2, 2, 2]\n",
      "episode 7579, reward 998.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 140\n",
      "Initial State is  [2, 3, 4]\n",
      "episode 7580, reward 739.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 129\n",
      "Initial State is  [2, 19, 5]\n",
      "episode 7581, reward 947.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 128\n",
      "Initial State is  [0, 5, 2]\n",
      "episode 7582, reward 725.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 129\n",
      "Initial State is  [0, 13, 5]\n",
      "episode 7583, reward 759.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 136\n",
      "Initial State is  [2, 7, 4]\n",
      "episode 7584, reward 901.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 137\n",
      "Initial State is  [0, 16, 0]\n",
      "episode 7585, reward 1030.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 138\n",
      "Initial State is  [1, 2, 5]\n",
      "episode 7586, reward 787.0, memory_length 2000, epsilon 0.009998671593271896, time 737.0, rides 149\n",
      "Initial State is  [1, 23, 2]\n",
      "episode 7587, reward 746.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 134\n",
      "Initial State is  [2, 1, 5]\n",
      "episode 7588, reward 585.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 134\n",
      "Initial State is  [1, 15, 2]\n",
      "episode 7589, reward 1149.0, memory_length 2000, epsilon 0.009998671593271896, time 736.0, rides 132\n",
      "Initial State is  [4, 12, 3]\n",
      "episode 7590, reward 1130.0, memory_length 2000, epsilon 0.009998671593271896, time 737.0, rides 142\n",
      "Initial State is  [2, 2, 3]\n",
      "episode 7591, reward 656.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 140\n",
      "Initial State is  [3, 18, 3]\n",
      "episode 7592, reward 1187.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 132\n",
      "Initial State is  [3, 7, 3]\n",
      "episode 7593, reward 864.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 141\n",
      "Initial State is  [1, 11, 1]\n",
      "episode 7594, reward 669.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 140\n",
      "Initial State is  [4, 3, 4]\n",
      "episode 7595, reward 1299.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 137\n",
      "Initial State is  [1, 22, 4]\n",
      "episode 7596, reward 930.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 143\n",
      "Initial State is  [4, 11, 5]\n",
      "episode 7597, reward 683.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 125\n",
      "Initial State is  [0, 0, 2]\n",
      "episode 7598, reward 949.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 133\n",
      "Initial State is  [4, 12, 5]\n",
      "episode 7599, reward 778.0, memory_length 2000, epsilon 0.009998671593271896, time 739.0, rides 138\n",
      "Initial State is  [2, 18, 6]\n",
      "episode 7600, reward 1214.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 128\n",
      "Initial State is  [4, 8, 5]\n",
      "episode 7601, reward 569.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 133\n",
      "Initial State is  [2, 18, 5]\n",
      "episode 7602, reward 1167.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 135\n",
      "Initial State is  [1, 21, 4]\n",
      "episode 7603, reward 835.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 145\n",
      "Initial State is  [1, 1, 1]\n",
      "episode 7604, reward 770.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 133\n",
      "Initial State is  [0, 5, 3]\n",
      "episode 7605, reward 728.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 119\n",
      "Initial State is  [3, 18, 3]\n",
      "episode 7606, reward 1150.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 143\n",
      "Initial State is  [4, 19, 3]\n",
      "episode 7607, reward 1252.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 127\n",
      "Initial State is  [4, 2, 5]\n",
      "episode 7608, reward 793.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 140\n",
      "Initial State is  [3, 0, 0]\n",
      "episode 7609, reward 1212.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 127\n",
      "Initial State is  [1, 16, 1]\n",
      "episode 7610, reward 1326.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 143\n",
      "Initial State is  [3, 22, 3]\n",
      "episode 7611, reward 808.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 133\n",
      "Initial State is  [4, 8, 6]\n",
      "episode 7612, reward 827.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 140\n",
      "Initial State is  [3, 5, 6]\n",
      "episode 7613, reward 1266.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 126\n",
      "Initial State is  [2, 6, 6]\n",
      "episode 7614, reward 1066.0, memory_length 2000, epsilon 0.009998671593271896, time 736.0, rides 129\n",
      "Initial State is  [3, 19, 6]\n",
      "episode 7615, reward 1097.0, memory_length 2000, epsilon 0.009998671593271896, time 721.0, rides 127\n",
      "Initial State is  [3, 14, 0]\n",
      "episode 7616, reward 722.0, memory_length 2000, epsilon 0.009998671593271896, time 738.0, rides 138\n",
      "Initial State is  [4, 20, 5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 7617, reward 1235.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 135\n",
      "Initial State is  [2, 20, 5]\n",
      "episode 7618, reward 1083.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 138\n",
      "Initial State is  [2, 10, 6]\n",
      "episode 7619, reward 783.0, memory_length 2000, epsilon 0.009998671593271896, time 736.0, rides 142\n",
      "Initial State is  [4, 1, 2]\n",
      "episode 7620, reward 1015.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 125\n",
      "Initial State is  [3, 21, 6]\n",
      "episode 7621, reward 651.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 126\n",
      "Initial State is  [1, 2, 5]\n",
      "episode 7622, reward 1087.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 140\n",
      "Initial State is  [4, 23, 0]\n",
      "episode 7623, reward 1057.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 140\n",
      "Initial State is  [2, 21, 4]\n",
      "episode 7624, reward 684.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 140\n",
      "Initial State is  [0, 5, 2]\n",
      "episode 7625, reward 1001.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 134\n",
      "Initial State is  [4, 0, 5]\n",
      "episode 7626, reward 922.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 136\n",
      "Initial State is  [0, 6, 0]\n",
      "episode 7627, reward 588.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 139\n",
      "Initial State is  [4, 6, 3]\n",
      "episode 7628, reward 721.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 135\n",
      "Initial State is  [4, 2, 5]\n",
      "episode 7629, reward 1059.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 131\n",
      "Initial State is  [3, 3, 5]\n",
      "episode 7630, reward 1169.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 132\n",
      "Initial State is  [4, 1, 4]\n",
      "episode 7631, reward 823.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 132\n",
      "Initial State is  [1, 21, 0]\n",
      "episode 7632, reward 1091.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 149\n",
      "Initial State is  [1, 8, 2]\n",
      "episode 7633, reward 983.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 124\n",
      "Initial State is  [1, 13, 1]\n",
      "episode 7634, reward 1053.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 136\n",
      "Initial State is  [1, 18, 2]\n",
      "episode 7635, reward 938.0, memory_length 2000, epsilon 0.009998671593271896, time 721.0, rides 145\n",
      "Initial State is  [0, 15, 2]\n",
      "episode 7636, reward 788.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 136\n",
      "Initial State is  [1, 5, 5]\n",
      "episode 7637, reward 883.0, memory_length 2000, epsilon 0.009998671593271896, time 720.0, rides 128\n",
      "Initial State is  [4, 1, 3]\n",
      "episode 7638, reward 1055.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 135\n",
      "Initial State is  [2, 0, 1]\n",
      "episode 7639, reward 1098.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 125\n",
      "Initial State is  [2, 4, 6]\n",
      "episode 7640, reward 1093.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 136\n",
      "Initial State is  [2, 13, 6]\n",
      "episode 7641, reward 1257.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 139\n",
      "Initial State is  [2, 4, 1]\n",
      "episode 7642, reward 807.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 123\n",
      "Initial State is  [4, 10, 2]\n",
      "episode 7643, reward 819.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 123\n",
      "Initial State is  [4, 0, 1]\n",
      "episode 7644, reward 909.0, memory_length 2000, epsilon 0.009998671593271896, time 740.0, rides 135\n",
      "Initial State is  [0, 15, 2]\n",
      "episode 7645, reward 1355.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 138\n",
      "Initial State is  [0, 14, 4]\n",
      "episode 7646, reward 1087.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 132\n",
      "Initial State is  [2, 8, 2]\n",
      "episode 7647, reward 960.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 132\n",
      "Initial State is  [2, 17, 2]\n",
      "episode 7648, reward 1013.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 125\n",
      "Initial State is  [4, 9, 2]\n",
      "episode 7649, reward 997.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 127\n",
      "Initial State is  [2, 2, 6]\n",
      "episode 7650, reward 886.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 121\n",
      "Initial State is  [1, 7, 4]\n",
      "episode 7651, reward 702.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 129\n",
      "Initial State is  [1, 7, 3]\n",
      "episode 7652, reward 866.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 140\n",
      "Initial State is  [3, 23, 5]\n",
      "episode 7653, reward 941.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 155\n",
      "Initial State is  [1, 16, 2]\n",
      "episode 7654, reward 912.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 124\n",
      "Initial State is  [0, 14, 0]\n",
      "episode 7655, reward 939.0, memory_length 2000, epsilon 0.009998671593271896, time 737.0, rides 137\n",
      "Initial State is  [2, 1, 5]\n",
      "episode 7656, reward 1062.0, memory_length 2000, epsilon 0.009998671593271896, time 738.0, rides 149\n",
      "Initial State is  [2, 5, 3]\n",
      "episode 7657, reward 989.0, memory_length 2000, epsilon 0.009998671593271896, time 738.0, rides 130\n",
      "Initial State is  [3, 6, 6]\n",
      "episode 7658, reward 980.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 120\n",
      "Initial State is  [1, 19, 1]\n",
      "episode 7659, reward 881.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 135\n",
      "Initial State is  [2, 15, 0]\n",
      "episode 7660, reward 883.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 142\n",
      "Initial State is  [3, 9, 4]\n",
      "episode 7661, reward 1105.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 136\n",
      "Initial State is  [4, 20, 2]\n",
      "episode 7662, reward 1040.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 134\n",
      "Initial State is  [0, 22, 4]\n",
      "episode 7663, reward 1238.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 134\n",
      "Initial State is  [2, 20, 3]\n",
      "episode 7664, reward 783.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 139\n",
      "Initial State is  [3, 8, 4]\n",
      "episode 7665, reward 1450.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 139\n",
      "Initial State is  [3, 11, 3]\n",
      "episode 7666, reward 726.0, memory_length 2000, epsilon 0.009998671593271896, time 737.0, rides 128\n",
      "Initial State is  [4, 12, 6]\n",
      "episode 7667, reward 893.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 129\n",
      "Initial State is  [4, 0, 3]\n",
      "episode 7668, reward 750.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 125\n",
      "Initial State is  [2, 22, 0]\n",
      "episode 7669, reward 1009.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 123\n",
      "Initial State is  [2, 14, 6]\n",
      "episode 7670, reward 984.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 129\n",
      "Initial State is  [2, 11, 3]\n",
      "episode 7671, reward 712.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 138\n",
      "Initial State is  [2, 10, 0]\n",
      "episode 7672, reward 902.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 134\n",
      "Initial State is  [4, 9, 3]\n",
      "episode 7673, reward 1147.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 138\n",
      "Initial State is  [1, 11, 2]\n",
      "episode 7674, reward 754.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 132\n",
      "Initial State is  [1, 20, 2]\n",
      "episode 7675, reward 938.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 136\n",
      "Initial State is  [3, 12, 1]\n",
      "episode 7676, reward 631.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 139\n",
      "Initial State is  [0, 22, 0]\n",
      "episode 7677, reward 868.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 129\n",
      "Initial State is  [3, 23, 2]\n",
      "episode 7678, reward 934.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 138\n",
      "Initial State is  [0, 17, 4]\n",
      "episode 7679, reward 1227.0, memory_length 2000, epsilon 0.009998671593271896, time 745.0, rides 128\n",
      "Initial State is  [2, 20, 3]\n",
      "episode 7680, reward 597.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 129\n",
      "Initial State is  [4, 6, 6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 7681, reward 886.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 135\n",
      "Initial State is  [2, 19, 6]\n",
      "episode 7682, reward 1122.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 134\n",
      "Initial State is  [0, 15, 6]\n",
      "episode 7683, reward 1122.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 128\n",
      "Initial State is  [4, 14, 6]\n",
      "episode 7684, reward 682.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 132\n",
      "Initial State is  [2, 4, 2]\n",
      "episode 7685, reward 820.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 126\n",
      "Initial State is  [3, 2, 6]\n",
      "episode 7686, reward 1180.0, memory_length 2000, epsilon 0.009998671593271896, time 736.0, rides 130\n",
      "Initial State is  [0, 4, 0]\n",
      "episode 7687, reward 811.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 138\n",
      "Initial State is  [4, 9, 1]\n",
      "episode 7688, reward 1119.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 130\n",
      "Initial State is  [4, 22, 2]\n",
      "episode 7689, reward 878.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 134\n",
      "Initial State is  [2, 23, 0]\n",
      "episode 7690, reward 806.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 124\n",
      "Initial State is  [4, 12, 3]\n",
      "episode 7691, reward 872.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 134\n",
      "Initial State is  [0, 3, 3]\n",
      "episode 7692, reward 958.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 124\n",
      "Initial State is  [0, 18, 1]\n",
      "episode 7693, reward 871.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 135\n",
      "Initial State is  [2, 13, 0]\n",
      "episode 7694, reward 1090.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 130\n",
      "Initial State is  [3, 9, 2]\n",
      "episode 7695, reward 918.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 131\n",
      "Initial State is  [4, 20, 2]\n",
      "episode 7696, reward 1018.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 144\n",
      "Initial State is  [1, 15, 3]\n",
      "episode 7697, reward 1034.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 129\n",
      "Initial State is  [1, 0, 5]\n",
      "episode 7698, reward 1167.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 142\n",
      "Initial State is  [4, 11, 5]\n",
      "episode 7699, reward 1132.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 142\n",
      "Initial State is  [1, 6, 2]\n",
      "episode 7700, reward 1073.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 134\n",
      "Initial State is  [0, 14, 1]\n",
      "episode 7701, reward 1101.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 139\n",
      "Initial State is  [0, 16, 6]\n",
      "episode 7702, reward 1026.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 132\n",
      "Initial State is  [1, 23, 6]\n",
      "episode 7703, reward 1089.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 155\n",
      "Initial State is  [0, 23, 2]\n",
      "episode 7704, reward 686.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 129\n",
      "Initial State is  [4, 0, 0]\n",
      "episode 7705, reward 1014.0, memory_length 2000, epsilon 0.009998671593271896, time 721.0, rides 126\n",
      "Initial State is  [0, 7, 4]\n",
      "episode 7706, reward 1060.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 145\n",
      "Initial State is  [2, 19, 5]\n",
      "episode 7707, reward 967.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 131\n",
      "Initial State is  [3, 19, 2]\n",
      "episode 7708, reward 1180.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 126\n",
      "Initial State is  [3, 3, 5]\n",
      "episode 7709, reward 975.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 123\n",
      "Initial State is  [0, 23, 2]\n",
      "episode 7710, reward 1008.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 136\n",
      "Initial State is  [4, 18, 1]\n",
      "episode 7711, reward 1109.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 134\n",
      "Initial State is  [0, 17, 2]\n",
      "episode 7712, reward 1006.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 142\n",
      "Initial State is  [4, 12, 6]\n",
      "episode 7713, reward 856.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 144\n",
      "Initial State is  [2, 23, 3]\n",
      "episode 7714, reward 1077.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 138\n",
      "Initial State is  [2, 17, 0]\n",
      "episode 7715, reward 976.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 133\n",
      "Initial State is  [3, 12, 4]\n",
      "episode 7716, reward 1023.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 141\n",
      "Initial State is  [2, 20, 3]\n",
      "episode 7717, reward 856.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 140\n",
      "Initial State is  [4, 18, 4]\n",
      "episode 7718, reward 1142.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 136\n",
      "Initial State is  [2, 20, 5]\n",
      "episode 7719, reward 1060.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 126\n",
      "Initial State is  [1, 1, 6]\n",
      "episode 7720, reward 927.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 137\n",
      "Initial State is  [3, 21, 3]\n",
      "episode 7721, reward 671.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 140\n",
      "Initial State is  [3, 18, 4]\n",
      "episode 7722, reward 844.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 124\n",
      "Initial State is  [2, 19, 0]\n",
      "episode 7723, reward 962.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 148\n",
      "Initial State is  [4, 16, 1]\n",
      "episode 7724, reward 971.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 146\n",
      "Initial State is  [2, 8, 3]\n",
      "episode 7725, reward 936.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 124\n",
      "Initial State is  [3, 20, 3]\n",
      "episode 7726, reward 777.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 139\n",
      "Initial State is  [1, 13, 1]\n",
      "episode 7727, reward 852.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 133\n",
      "Initial State is  [3, 6, 3]\n",
      "episode 7728, reward 552.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 129\n",
      "Initial State is  [4, 8, 4]\n",
      "episode 7729, reward 1333.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 134\n",
      "Initial State is  [2, 1, 0]\n",
      "episode 7730, reward 1222.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 138\n",
      "Initial State is  [3, 21, 3]\n",
      "episode 7731, reward 1013.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 139\n",
      "Initial State is  [2, 5, 3]\n",
      "episode 7732, reward 775.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 125\n",
      "Initial State is  [1, 23, 6]\n",
      "episode 7733, reward 949.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 123\n",
      "Initial State is  [2, 3, 2]\n",
      "episode 7734, reward 1007.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 129\n",
      "Initial State is  [3, 19, 2]\n",
      "episode 7735, reward 850.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 135\n",
      "Initial State is  [3, 11, 4]\n",
      "episode 7736, reward 1116.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 141\n",
      "Initial State is  [4, 23, 5]\n",
      "episode 7737, reward 1029.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 135\n",
      "Initial State is  [3, 1, 0]\n",
      "episode 7738, reward 937.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 127\n",
      "Initial State is  [1, 13, 2]\n",
      "episode 7739, reward 863.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 137\n",
      "Initial State is  [2, 17, 3]\n",
      "episode 7740, reward 665.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 123\n",
      "Initial State is  [0, 16, 0]\n",
      "episode 7741, reward 840.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 125\n",
      "Initial State is  [3, 16, 1]\n",
      "episode 7742, reward 935.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 121\n",
      "Initial State is  [4, 4, 0]\n",
      "episode 7743, reward 931.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 133\n",
      "Initial State is  [4, 14, 6]\n",
      "episode 7744, reward 982.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 141\n",
      "Initial State is  [3, 16, 3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 7745, reward 954.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 137\n",
      "Initial State is  [4, 18, 0]\n",
      "episode 7746, reward 774.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 135\n",
      "Initial State is  [3, 19, 6]\n",
      "episode 7747, reward 1039.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 136\n",
      "Initial State is  [3, 13, 1]\n",
      "episode 7748, reward 808.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 131\n",
      "Initial State is  [1, 12, 0]\n",
      "episode 7749, reward 992.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 139\n",
      "Initial State is  [2, 22, 0]\n",
      "episode 7750, reward 1077.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 128\n",
      "Initial State is  [1, 6, 4]\n",
      "episode 7751, reward 1191.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 139\n",
      "Initial State is  [3, 21, 6]\n",
      "episode 7752, reward 1010.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 140\n",
      "Initial State is  [1, 10, 1]\n",
      "episode 7753, reward 1143.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 149\n",
      "Initial State is  [1, 7, 0]\n",
      "episode 7754, reward 976.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 130\n",
      "Initial State is  [0, 7, 0]\n",
      "episode 7755, reward 1188.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 129\n",
      "Initial State is  [1, 2, 0]\n",
      "episode 7756, reward 1065.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 133\n",
      "Initial State is  [0, 1, 6]\n",
      "episode 7757, reward 1045.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 135\n",
      "Initial State is  [0, 14, 5]\n",
      "episode 7758, reward 884.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 145\n",
      "Initial State is  [4, 3, 2]\n",
      "episode 7759, reward 1032.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 137\n",
      "Initial State is  [0, 17, 2]\n",
      "episode 7760, reward 1262.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 135\n",
      "Initial State is  [2, 0, 3]\n",
      "episode 7761, reward 780.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 137\n",
      "Initial State is  [2, 12, 5]\n",
      "episode 7762, reward 974.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 143\n",
      "Initial State is  [1, 21, 1]\n",
      "episode 7763, reward 1096.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 131\n",
      "Initial State is  [2, 9, 5]\n",
      "episode 7764, reward 902.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 137\n",
      "Initial State is  [0, 7, 4]\n",
      "episode 7765, reward 816.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 139\n",
      "Initial State is  [2, 2, 6]\n",
      "episode 7766, reward 928.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 133\n",
      "Initial State is  [1, 18, 0]\n",
      "episode 7767, reward 946.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 133\n",
      "Initial State is  [3, 2, 0]\n",
      "episode 7768, reward 1210.0, memory_length 2000, epsilon 0.009998671593271896, time 741.0, rides 122\n",
      "Initial State is  [3, 9, 2]\n",
      "episode 7769, reward 1279.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 138\n",
      "Initial State is  [0, 15, 0]\n",
      "episode 7770, reward 856.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 127\n",
      "Initial State is  [1, 18, 4]\n",
      "episode 7771, reward 878.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 135\n",
      "Initial State is  [1, 15, 1]\n",
      "episode 7772, reward 832.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 123\n",
      "Initial State is  [0, 22, 6]\n",
      "episode 7773, reward 1082.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 124\n",
      "Initial State is  [1, 8, 0]\n",
      "episode 7774, reward 766.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 127\n",
      "Initial State is  [4, 7, 3]\n",
      "episode 7775, reward 882.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 139\n",
      "Initial State is  [0, 4, 5]\n",
      "episode 7776, reward 940.0, memory_length 2000, epsilon 0.009998671593271896, time 742.0, rides 131\n",
      "Initial State is  [4, 5, 6]\n",
      "episode 7777, reward 672.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 130\n",
      "Initial State is  [4, 23, 1]\n",
      "episode 7778, reward 775.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 125\n",
      "Initial State is  [2, 11, 2]\n",
      "episode 7779, reward 1121.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 148\n",
      "Initial State is  [3, 22, 6]\n",
      "episode 7780, reward 866.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 130\n",
      "Initial State is  [4, 13, 3]\n",
      "episode 7781, reward 704.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 133\n",
      "Initial State is  [2, 3, 6]\n",
      "episode 7782, reward 718.0, memory_length 2000, epsilon 0.009998671593271896, time 743.0, rides 129\n",
      "Initial State is  [3, 23, 2]\n",
      "episode 7783, reward 923.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 134\n",
      "Initial State is  [2, 23, 1]\n",
      "episode 7784, reward 1007.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 140\n",
      "Initial State is  [0, 1, 3]\n",
      "episode 7785, reward 804.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 133\n",
      "Initial State is  [4, 0, 0]\n",
      "episode 7786, reward 1179.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 129\n",
      "Initial State is  [4, 19, 5]\n",
      "episode 7787, reward 659.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 122\n",
      "Initial State is  [3, 14, 5]\n",
      "episode 7788, reward 1020.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 128\n",
      "Initial State is  [1, 19, 0]\n",
      "episode 7789, reward 936.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 141\n",
      "Initial State is  [2, 13, 4]\n",
      "episode 7790, reward 774.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 130\n",
      "Initial State is  [2, 7, 2]\n",
      "episode 7791, reward 777.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 135\n",
      "Initial State is  [0, 2, 1]\n",
      "episode 7792, reward 1104.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 131\n",
      "Initial State is  [4, 17, 6]\n",
      "episode 7793, reward 764.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 122\n",
      "Initial State is  [2, 17, 4]\n",
      "episode 7794, reward 1120.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 134\n",
      "Initial State is  [3, 8, 6]\n",
      "episode 7795, reward 710.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 123\n",
      "Initial State is  [2, 13, 0]\n",
      "episode 7796, reward 909.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 126\n",
      "Initial State is  [1, 22, 1]\n",
      "episode 7797, reward 900.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 139\n",
      "Initial State is  [4, 4, 5]\n",
      "episode 7798, reward 1081.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 139\n",
      "Initial State is  [3, 15, 1]\n",
      "episode 7799, reward 840.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 139\n",
      "Initial State is  [4, 0, 5]\n",
      "episode 7800, reward 1029.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 141\n",
      "Initial State is  [1, 20, 5]\n",
      "episode 7801, reward 1105.0, memory_length 2000, epsilon 0.009998671593271896, time 721.0, rides 136\n",
      "Initial State is  [1, 14, 0]\n",
      "episode 7802, reward 726.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 138\n",
      "Initial State is  [2, 12, 1]\n",
      "episode 7803, reward 859.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 134\n",
      "Initial State is  [3, 0, 1]\n",
      "episode 7804, reward 1041.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 138\n",
      "Initial State is  [2, 17, 0]\n",
      "episode 7805, reward 965.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 128\n",
      "Initial State is  [4, 17, 2]\n",
      "episode 7806, reward 913.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 139\n",
      "Initial State is  [2, 3, 6]\n",
      "episode 7807, reward 1242.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 144\n",
      "Initial State is  [0, 21, 0]\n",
      "episode 7808, reward 921.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 144\n",
      "Initial State is  [4, 0, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 7809, reward 1002.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 122\n",
      "Initial State is  [3, 13, 4]\n",
      "episode 7810, reward 976.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 129\n",
      "Initial State is  [4, 13, 3]\n",
      "episode 7811, reward 747.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 130\n",
      "Initial State is  [3, 17, 4]\n",
      "episode 7812, reward 724.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 136\n",
      "Initial State is  [4, 12, 6]\n",
      "episode 7813, reward 1176.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 126\n",
      "Initial State is  [1, 8, 5]\n",
      "episode 7814, reward 1211.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 137\n",
      "Initial State is  [4, 3, 2]\n",
      "episode 7815, reward 993.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 137\n",
      "Initial State is  [4, 4, 6]\n",
      "episode 7816, reward 842.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 130\n",
      "Initial State is  [1, 6, 3]\n",
      "episode 7817, reward 745.0, memory_length 2000, epsilon 0.009998671593271896, time 736.0, rides 143\n",
      "Initial State is  [4, 3, 2]\n",
      "episode 7818, reward 900.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 135\n",
      "Initial State is  [4, 6, 4]\n",
      "episode 7819, reward 916.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 130\n",
      "Initial State is  [1, 4, 3]\n",
      "episode 7820, reward 1207.0, memory_length 2000, epsilon 0.009998671593271896, time 737.0, rides 130\n",
      "Initial State is  [4, 23, 6]\n",
      "episode 7821, reward 723.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 129\n",
      "Initial State is  [1, 7, 6]\n",
      "episode 7822, reward 934.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 141\n",
      "Initial State is  [4, 9, 2]\n",
      "episode 7823, reward 913.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 128\n",
      "Initial State is  [1, 12, 3]\n",
      "episode 7824, reward 1012.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 128\n",
      "Initial State is  [4, 5, 4]\n",
      "episode 7825, reward 1060.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 131\n",
      "Initial State is  [0, 2, 0]\n",
      "episode 7826, reward 1103.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 134\n",
      "Initial State is  [4, 0, 6]\n",
      "episode 7827, reward 835.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 133\n",
      "Initial State is  [2, 22, 6]\n",
      "episode 7828, reward 717.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 114\n",
      "Initial State is  [2, 14, 5]\n",
      "episode 7829, reward 1262.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 130\n",
      "Initial State is  [4, 13, 2]\n",
      "episode 7830, reward 587.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 124\n",
      "Initial State is  [4, 19, 1]\n",
      "episode 7831, reward 841.0, memory_length 2000, epsilon 0.009998671593271896, time 720.0, rides 136\n",
      "Initial State is  [1, 18, 4]\n",
      "episode 7832, reward 944.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 132\n",
      "Initial State is  [4, 2, 0]\n",
      "episode 7833, reward 1110.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 127\n",
      "Initial State is  [4, 10, 5]\n",
      "episode 7834, reward 1390.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 141\n",
      "Initial State is  [4, 23, 0]\n",
      "episode 7835, reward 1107.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 137\n",
      "Initial State is  [3, 3, 2]\n",
      "episode 7836, reward 1037.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 130\n",
      "Initial State is  [0, 1, 3]\n",
      "episode 7837, reward 1123.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 129\n",
      "Initial State is  [1, 10, 1]\n",
      "episode 7838, reward 909.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 137\n",
      "Initial State is  [2, 1, 3]\n",
      "episode 7839, reward 817.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 127\n",
      "Initial State is  [2, 9, 4]\n",
      "episode 7840, reward 929.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 126\n",
      "Initial State is  [2, 20, 3]\n",
      "episode 7841, reward 1045.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 120\n",
      "Initial State is  [0, 17, 5]\n",
      "episode 7842, reward 1113.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 144\n",
      "Initial State is  [0, 20, 6]\n",
      "episode 7843, reward 1014.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 119\n",
      "Initial State is  [4, 20, 2]\n",
      "episode 7844, reward 1180.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 127\n",
      "Initial State is  [0, 8, 6]\n",
      "episode 7845, reward 1018.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 142\n",
      "Initial State is  [2, 17, 1]\n",
      "episode 7846, reward 993.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 133\n",
      "Initial State is  [0, 4, 6]\n",
      "episode 7847, reward 853.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 142\n",
      "Initial State is  [3, 1, 6]\n",
      "episode 7848, reward 841.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 134\n",
      "Initial State is  [3, 0, 2]\n",
      "episode 7849, reward 762.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 131\n",
      "Initial State is  [4, 9, 3]\n",
      "episode 7850, reward 1057.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 144\n",
      "Initial State is  [1, 7, 6]\n",
      "episode 7851, reward 1182.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 129\n",
      "Initial State is  [0, 5, 4]\n",
      "episode 7852, reward 1045.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 133\n",
      "Initial State is  [2, 17, 1]\n",
      "episode 7853, reward 789.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 119\n",
      "Initial State is  [4, 3, 1]\n",
      "episode 7854, reward 963.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 141\n",
      "Initial State is  [0, 21, 1]\n",
      "episode 7855, reward 1204.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 123\n",
      "Initial State is  [0, 3, 2]\n",
      "episode 7856, reward 855.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 128\n",
      "Initial State is  [1, 11, 0]\n",
      "episode 7857, reward 778.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 122\n",
      "Initial State is  [3, 11, 6]\n",
      "episode 7858, reward 1079.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 125\n",
      "Initial State is  [3, 12, 1]\n",
      "episode 7859, reward 1033.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 143\n",
      "Initial State is  [0, 18, 5]\n",
      "episode 7860, reward 917.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 137\n",
      "Initial State is  [3, 10, 1]\n",
      "episode 7861, reward 1013.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 127\n",
      "Initial State is  [4, 8, 2]\n",
      "episode 7862, reward 1150.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 129\n",
      "Initial State is  [0, 7, 1]\n",
      "episode 7863, reward 863.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 135\n",
      "Initial State is  [1, 16, 2]\n",
      "episode 7864, reward 1143.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 134\n",
      "Initial State is  [0, 5, 5]\n",
      "episode 7865, reward 993.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 133\n",
      "Initial State is  [3, 16, 6]\n",
      "episode 7866, reward 973.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 145\n",
      "Initial State is  [4, 5, 3]\n",
      "episode 7867, reward 923.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 145\n",
      "Initial State is  [2, 10, 6]\n",
      "episode 7868, reward 865.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 140\n",
      "Initial State is  [1, 19, 4]\n",
      "episode 7869, reward 875.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 139\n",
      "Initial State is  [3, 10, 2]\n",
      "episode 7870, reward 999.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 134\n",
      "Initial State is  [0, 21, 3]\n",
      "episode 7871, reward 878.0, memory_length 2000, epsilon 0.009998671593271896, time 720.0, rides 131\n",
      "Initial State is  [2, 10, 5]\n",
      "episode 7872, reward 656.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 138\n",
      "Initial State is  [1, 10, 1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 7873, reward 946.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 140\n",
      "Initial State is  [4, 9, 3]\n",
      "episode 7874, reward 896.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 112\n",
      "Initial State is  [3, 8, 2]\n",
      "episode 7875, reward 709.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 131\n",
      "Initial State is  [0, 7, 3]\n",
      "episode 7876, reward 976.0, memory_length 2000, epsilon 0.009998671593271896, time 741.0, rides 142\n",
      "Initial State is  [3, 9, 2]\n",
      "episode 7877, reward 992.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 137\n",
      "Initial State is  [3, 19, 5]\n",
      "episode 7878, reward 876.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 124\n",
      "Initial State is  [0, 1, 4]\n",
      "episode 7879, reward 1079.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 145\n",
      "Initial State is  [0, 21, 1]\n",
      "episode 7880, reward 849.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 137\n",
      "Initial State is  [3, 3, 6]\n",
      "episode 7881, reward 665.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 123\n",
      "Initial State is  [2, 12, 1]\n",
      "episode 7882, reward 826.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 127\n",
      "Initial State is  [2, 7, 1]\n",
      "episode 7883, reward 945.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 135\n",
      "Initial State is  [2, 6, 1]\n",
      "episode 7884, reward 832.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 123\n",
      "Initial State is  [1, 21, 5]\n",
      "episode 7885, reward 891.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 134\n",
      "Initial State is  [0, 17, 0]\n",
      "episode 7886, reward 1114.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 131\n",
      "Initial State is  [3, 0, 3]\n",
      "episode 7887, reward 748.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 140\n",
      "Initial State is  [1, 15, 4]\n",
      "episode 7888, reward 741.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 126\n",
      "Initial State is  [3, 15, 1]\n",
      "episode 7889, reward 1176.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 138\n",
      "Initial State is  [3, 2, 2]\n",
      "episode 7890, reward 1144.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 141\n",
      "Initial State is  [4, 2, 6]\n",
      "episode 7891, reward 971.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 139\n",
      "Initial State is  [3, 8, 5]\n",
      "episode 7892, reward 836.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 132\n",
      "Initial State is  [1, 12, 5]\n",
      "episode 7893, reward 808.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 133\n",
      "Initial State is  [0, 11, 4]\n",
      "episode 7894, reward 779.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 135\n",
      "Initial State is  [0, 20, 1]\n",
      "episode 7895, reward 1092.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 124\n",
      "Initial State is  [0, 17, 1]\n",
      "episode 7896, reward 650.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 125\n",
      "Initial State is  [2, 2, 3]\n",
      "episode 7897, reward 690.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 141\n",
      "Initial State is  [3, 17, 5]\n",
      "episode 7898, reward 1152.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 147\n",
      "Initial State is  [1, 13, 6]\n",
      "episode 7899, reward 1230.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 141\n",
      "Initial State is  [3, 23, 2]\n",
      "episode 7900, reward 876.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 124\n",
      "Initial State is  [4, 1, 0]\n",
      "episode 7901, reward 1100.0, memory_length 2000, epsilon 0.009998671593271896, time 721.0, rides 130\n",
      "Initial State is  [4, 3, 5]\n",
      "episode 7902, reward 643.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 135\n",
      "Initial State is  [2, 7, 4]\n",
      "episode 7903, reward 950.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 142\n",
      "Initial State is  [2, 22, 6]\n",
      "episode 7904, reward 949.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 124\n",
      "Initial State is  [2, 18, 3]\n",
      "episode 7905, reward 891.0, memory_length 2000, epsilon 0.009998671593271896, time 736.0, rides 139\n",
      "Initial State is  [1, 1, 5]\n",
      "episode 7906, reward 1019.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 136\n",
      "Initial State is  [3, 5, 5]\n",
      "episode 7907, reward 1027.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 129\n",
      "Initial State is  [3, 21, 1]\n",
      "episode 7908, reward 762.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 130\n",
      "Initial State is  [3, 16, 0]\n",
      "episode 7909, reward 933.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 127\n",
      "Initial State is  [3, 14, 6]\n",
      "episode 7910, reward 936.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 142\n",
      "Initial State is  [1, 4, 2]\n",
      "episode 7911, reward 1253.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 137\n",
      "Initial State is  [4, 8, 2]\n",
      "episode 7912, reward 593.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 123\n",
      "Initial State is  [0, 14, 3]\n",
      "episode 7913, reward 698.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 140\n",
      "Initial State is  [3, 0, 0]\n",
      "episode 7914, reward 864.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 125\n",
      "Initial State is  [2, 0, 3]\n",
      "episode 7915, reward 1010.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 129\n",
      "Initial State is  [2, 13, 2]\n",
      "episode 7916, reward 878.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 139\n",
      "Initial State is  [1, 0, 2]\n",
      "episode 7917, reward 925.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 131\n",
      "Initial State is  [3, 16, 4]\n",
      "episode 7918, reward 1076.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 138\n",
      "Initial State is  [3, 0, 6]\n",
      "episode 7919, reward 988.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 131\n",
      "Initial State is  [3, 22, 2]\n",
      "episode 7920, reward 1141.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 140\n",
      "Initial State is  [1, 13, 2]\n",
      "episode 7921, reward 742.0, memory_length 2000, epsilon 0.009998671593271896, time 737.0, rides 132\n",
      "Initial State is  [2, 22, 2]\n",
      "episode 7922, reward 659.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 126\n",
      "Initial State is  [0, 13, 0]\n",
      "episode 7923, reward 733.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 131\n",
      "Initial State is  [4, 22, 2]\n",
      "episode 7924, reward 719.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 135\n",
      "Initial State is  [0, 16, 4]\n",
      "episode 7925, reward 931.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 124\n",
      "Initial State is  [0, 3, 6]\n",
      "episode 7926, reward 979.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 128\n",
      "Initial State is  [3, 0, 0]\n",
      "episode 7927, reward 804.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 128\n",
      "Initial State is  [1, 16, 4]\n",
      "episode 7928, reward 816.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 115\n",
      "Initial State is  [3, 12, 5]\n",
      "episode 7929, reward 874.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 135\n",
      "Initial State is  [3, 7, 4]\n",
      "episode 7930, reward 935.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 148\n",
      "Initial State is  [4, 15, 3]\n",
      "episode 7931, reward 606.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 122\n",
      "Initial State is  [0, 0, 5]\n",
      "episode 7932, reward 1011.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 125\n",
      "Initial State is  [0, 1, 3]\n",
      "episode 7933, reward 715.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 133\n",
      "Initial State is  [1, 19, 0]\n",
      "episode 7934, reward 1222.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 136\n",
      "Initial State is  [3, 13, 1]\n",
      "episode 7935, reward 984.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 127\n",
      "Initial State is  [1, 16, 6]\n",
      "episode 7936, reward 969.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 121\n",
      "Initial State is  [3, 6, 5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 7937, reward 914.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 129\n",
      "Initial State is  [3, 21, 3]\n",
      "episode 7938, reward 785.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 135\n",
      "Initial State is  [3, 9, 0]\n",
      "episode 7939, reward 1146.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 136\n",
      "Initial State is  [3, 22, 5]\n",
      "episode 7940, reward 786.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 119\n",
      "Initial State is  [0, 7, 6]\n",
      "episode 7941, reward 1107.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 142\n",
      "Initial State is  [1, 11, 3]\n",
      "episode 7942, reward 996.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 144\n",
      "Initial State is  [0, 7, 0]\n",
      "episode 7943, reward 902.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 128\n",
      "Initial State is  [0, 9, 3]\n",
      "episode 7944, reward 594.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 129\n",
      "Initial State is  [3, 5, 2]\n",
      "episode 7945, reward 874.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 134\n",
      "Initial State is  [3, 22, 4]\n",
      "episode 7946, reward 864.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 130\n",
      "Initial State is  [2, 17, 4]\n",
      "episode 7947, reward 1044.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 133\n",
      "Initial State is  [4, 19, 6]\n",
      "episode 7948, reward 912.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 135\n",
      "Initial State is  [2, 18, 6]\n",
      "episode 7949, reward 689.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 142\n",
      "Initial State is  [0, 9, 1]\n",
      "episode 7950, reward 966.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 121\n",
      "Initial State is  [0, 0, 2]\n",
      "episode 7951, reward 848.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 121\n",
      "Initial State is  [0, 0, 0]\n",
      "episode 7952, reward 1000.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 126\n",
      "Initial State is  [2, 5, 5]\n",
      "episode 7953, reward 933.0, memory_length 2000, epsilon 0.009998671593271896, time 738.0, rides 145\n",
      "Initial State is  [0, 9, 1]\n",
      "episode 7954, reward 981.0, memory_length 2000, epsilon 0.009998671593271896, time 721.0, rides 127\n",
      "Initial State is  [2, 20, 6]\n",
      "episode 7955, reward 625.0, memory_length 2000, epsilon 0.009998671593271896, time 737.0, rides 141\n",
      "Initial State is  [2, 11, 0]\n",
      "episode 7956, reward 1110.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 138\n",
      "Initial State is  [2, 5, 1]\n",
      "episode 7957, reward 1244.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 127\n",
      "Initial State is  [4, 13, 6]\n",
      "episode 7958, reward 958.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 126\n",
      "Initial State is  [1, 22, 2]\n",
      "episode 7959, reward 998.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 118\n",
      "Initial State is  [1, 8, 6]\n",
      "episode 7960, reward 793.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 131\n",
      "Initial State is  [1, 16, 0]\n",
      "episode 7961, reward 1002.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 127\n",
      "Initial State is  [4, 11, 6]\n",
      "episode 7962, reward 860.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 128\n",
      "Initial State is  [1, 16, 0]\n",
      "episode 7963, reward 1038.0, memory_length 2000, epsilon 0.009998671593271896, time 737.0, rides 129\n",
      "Initial State is  [1, 6, 3]\n",
      "episode 7964, reward 630.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 136\n",
      "Initial State is  [3, 12, 6]\n",
      "episode 7965, reward 1046.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 135\n",
      "Initial State is  [3, 3, 0]\n",
      "episode 7966, reward 870.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 124\n",
      "Initial State is  [3, 19, 3]\n",
      "episode 7967, reward 1061.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 140\n",
      "Initial State is  [1, 3, 2]\n",
      "episode 7968, reward 934.0, memory_length 2000, epsilon 0.009998671593271896, time 720.0, rides 149\n",
      "Initial State is  [3, 9, 3]\n",
      "episode 7969, reward 974.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 125\n",
      "Initial State is  [2, 21, 2]\n",
      "episode 7970, reward 800.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 137\n",
      "Initial State is  [4, 22, 5]\n",
      "episode 7971, reward 773.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 142\n",
      "Initial State is  [1, 23, 1]\n",
      "episode 7972, reward 661.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 134\n",
      "Initial State is  [3, 23, 5]\n",
      "episode 7973, reward 1046.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 142\n",
      "Initial State is  [0, 19, 0]\n",
      "episode 7974, reward 907.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 126\n",
      "Initial State is  [4, 16, 4]\n",
      "episode 7975, reward 709.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 126\n",
      "Initial State is  [1, 8, 1]\n",
      "episode 7976, reward 641.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 126\n",
      "Initial State is  [1, 9, 2]\n",
      "episode 7977, reward 948.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 121\n",
      "Initial State is  [2, 14, 2]\n",
      "episode 7978, reward 1223.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 143\n",
      "Initial State is  [3, 1, 6]\n",
      "episode 7979, reward 930.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 122\n",
      "Initial State is  [1, 6, 1]\n",
      "episode 7980, reward 917.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 130\n",
      "Initial State is  [2, 14, 6]\n",
      "episode 7981, reward 1046.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 129\n",
      "Initial State is  [3, 8, 5]\n",
      "episode 7982, reward 1077.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 138\n",
      "Initial State is  [3, 20, 3]\n",
      "episode 7983, reward 949.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 124\n",
      "Initial State is  [3, 5, 6]\n",
      "episode 7984, reward 786.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 121\n",
      "Initial State is  [0, 22, 0]\n",
      "episode 7985, reward 1130.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 139\n",
      "Initial State is  [4, 19, 4]\n",
      "episode 7986, reward 1368.0, memory_length 2000, epsilon 0.009998671593271896, time 736.0, rides 143\n",
      "Initial State is  [3, 23, 2]\n",
      "episode 7987, reward 998.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 133\n",
      "Initial State is  [4, 7, 6]\n",
      "episode 7988, reward 756.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 133\n",
      "Initial State is  [0, 11, 5]\n",
      "episode 7989, reward 1260.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 139\n",
      "Initial State is  [4, 22, 2]\n",
      "episode 7990, reward 791.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 139\n",
      "Initial State is  [0, 22, 5]\n",
      "episode 7991, reward 903.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 128\n",
      "Initial State is  [1, 7, 2]\n",
      "episode 7992, reward 1100.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 129\n",
      "Initial State is  [4, 11, 0]\n",
      "episode 7993, reward 778.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 136\n",
      "Initial State is  [2, 0, 5]\n",
      "episode 7994, reward 895.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 137\n",
      "Initial State is  [4, 1, 3]\n",
      "episode 7995, reward 975.0, memory_length 2000, epsilon 0.009998671593271896, time 737.0, rides 147\n",
      "Initial State is  [3, 7, 5]\n",
      "episode 7996, reward 841.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 137\n",
      "Initial State is  [4, 23, 2]\n",
      "episode 7997, reward 838.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 120\n",
      "Initial State is  [2, 9, 2]\n",
      "episode 7998, reward 889.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 139\n",
      "Initial State is  [2, 17, 1]\n",
      "episode 7999, reward 896.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 130\n",
      "Initial State is  [0, 12, 4]\n",
      "episode 8000, reward 802.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 122\n",
      "Initial State is  [2, 19, 5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 8001, reward 786.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 134\n",
      "Initial State is  [3, 8, 3]\n",
      "episode 8002, reward 831.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 131\n",
      "Initial State is  [0, 5, 4]\n",
      "episode 8003, reward 926.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 134\n",
      "Initial State is  [1, 19, 5]\n",
      "episode 8004, reward 981.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 134\n",
      "Initial State is  [4, 22, 1]\n",
      "episode 8005, reward 608.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 139\n",
      "Initial State is  [3, 23, 3]\n",
      "episode 8006, reward 802.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 125\n",
      "Initial State is  [3, 11, 2]\n",
      "episode 8007, reward 894.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 131\n",
      "Initial State is  [1, 22, 2]\n",
      "episode 8008, reward 1053.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 128\n",
      "Initial State is  [1, 15, 1]\n",
      "episode 8009, reward 954.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 122\n",
      "Initial State is  [0, 11, 4]\n",
      "episode 8010, reward 914.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 139\n",
      "Initial State is  [1, 4, 0]\n",
      "episode 8011, reward 894.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 133\n",
      "Initial State is  [4, 4, 4]\n",
      "episode 8012, reward 1031.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 132\n",
      "Initial State is  [0, 10, 5]\n",
      "episode 8013, reward 849.0, memory_length 2000, epsilon 0.009998671593271896, time 721.0, rides 149\n",
      "Initial State is  [2, 8, 2]\n",
      "episode 8014, reward 860.0, memory_length 2000, epsilon 0.009998671593271896, time 721.0, rides 128\n",
      "Initial State is  [2, 12, 6]\n",
      "episode 8015, reward 1143.0, memory_length 2000, epsilon 0.009998671593271896, time 737.0, rides 144\n",
      "Initial State is  [4, 6, 1]\n",
      "episode 8016, reward 992.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 139\n",
      "Initial State is  [2, 19, 2]\n",
      "episode 8017, reward 915.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 123\n",
      "Initial State is  [0, 12, 4]\n",
      "episode 8018, reward 1014.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 129\n",
      "Initial State is  [0, 14, 1]\n",
      "episode 8019, reward 858.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 130\n",
      "Initial State is  [1, 13, 0]\n",
      "episode 8020, reward 879.0, memory_length 2000, epsilon 0.009998671593271896, time 740.0, rides 132\n",
      "Initial State is  [1, 8, 2]\n",
      "episode 8021, reward 1003.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 131\n",
      "Initial State is  [2, 12, 4]\n",
      "episode 8022, reward 808.0, memory_length 2000, epsilon 0.009998671593271896, time 721.0, rides 150\n",
      "Initial State is  [0, 0, 3]\n",
      "episode 8023, reward 655.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 129\n",
      "Initial State is  [4, 13, 1]\n",
      "episode 8024, reward 1086.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 127\n",
      "Initial State is  [2, 22, 4]\n",
      "episode 8025, reward 1009.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 137\n",
      "Initial State is  [1, 19, 2]\n",
      "episode 8026, reward 924.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 132\n",
      "Initial State is  [4, 6, 1]\n",
      "episode 8027, reward 1057.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 144\n",
      "Initial State is  [0, 20, 6]\n",
      "episode 8028, reward 1013.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 137\n",
      "Initial State is  [4, 17, 5]\n",
      "episode 8029, reward 1114.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 130\n",
      "Initial State is  [4, 3, 4]\n",
      "episode 8030, reward 845.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 133\n",
      "Initial State is  [1, 10, 4]\n",
      "episode 8031, reward 1016.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 127\n",
      "Initial State is  [3, 15, 3]\n",
      "episode 8032, reward 992.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 142\n",
      "Initial State is  [0, 13, 4]\n",
      "episode 8033, reward 866.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 121\n",
      "Initial State is  [4, 10, 2]\n",
      "episode 8034, reward 854.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 153\n",
      "Initial State is  [4, 16, 6]\n",
      "episode 8035, reward 1034.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 142\n",
      "Initial State is  [0, 17, 4]\n",
      "episode 8036, reward 862.0, memory_length 2000, epsilon 0.009998671593271896, time 736.0, rides 142\n",
      "Initial State is  [1, 6, 4]\n",
      "episode 8037, reward 464.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 124\n",
      "Initial State is  [0, 13, 5]\n",
      "episode 8038, reward 1009.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 136\n",
      "Initial State is  [4, 20, 3]\n",
      "episode 8039, reward 718.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 116\n",
      "Initial State is  [4, 21, 5]\n",
      "episode 8040, reward 1068.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 127\n",
      "Initial State is  [3, 22, 6]\n",
      "episode 8041, reward 813.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 136\n",
      "Initial State is  [2, 8, 4]\n",
      "episode 8042, reward 955.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 137\n",
      "Initial State is  [2, 7, 1]\n",
      "episode 8043, reward 724.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 131\n",
      "Initial State is  [1, 20, 2]\n",
      "episode 8044, reward 937.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 136\n",
      "Initial State is  [1, 11, 1]\n",
      "episode 8045, reward 602.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 125\n",
      "Initial State is  [3, 8, 5]\n",
      "episode 8046, reward 796.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 134\n",
      "Initial State is  [4, 20, 5]\n",
      "episode 8047, reward 1066.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 147\n",
      "Initial State is  [0, 15, 2]\n",
      "episode 8048, reward 1014.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 134\n",
      "Initial State is  [4, 4, 3]\n",
      "episode 8049, reward 1146.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 146\n",
      "Initial State is  [3, 1, 5]\n",
      "episode 8050, reward 890.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 127\n",
      "Initial State is  [1, 4, 3]\n",
      "episode 8051, reward 1021.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 129\n",
      "Initial State is  [3, 2, 6]\n",
      "episode 8052, reward 1054.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 132\n",
      "Initial State is  [0, 10, 1]\n",
      "episode 8053, reward 982.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 151\n",
      "Initial State is  [1, 18, 3]\n",
      "episode 8054, reward 873.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 139\n",
      "Initial State is  [0, 5, 3]\n",
      "episode 8055, reward 785.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 125\n",
      "Initial State is  [2, 8, 0]\n",
      "episode 8056, reward 1056.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 126\n",
      "Initial State is  [0, 16, 4]\n",
      "episode 8057, reward 838.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 130\n",
      "Initial State is  [0, 0, 2]\n",
      "episode 8058, reward 1263.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 149\n",
      "Initial State is  [1, 19, 6]\n",
      "episode 8059, reward 325.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 130\n",
      "Initial State is  [0, 6, 6]\n",
      "episode 8060, reward 1054.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 135\n",
      "Initial State is  [3, 18, 0]\n",
      "episode 8061, reward 950.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 132\n",
      "Initial State is  [1, 21, 3]\n",
      "episode 8062, reward 1045.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 134\n",
      "Initial State is  [4, 19, 1]\n",
      "episode 8063, reward 1015.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 128\n",
      "Initial State is  [3, 8, 4]\n",
      "episode 8064, reward 1229.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 131\n",
      "Initial State is  [3, 4, 1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 8065, reward 1066.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 132\n",
      "Initial State is  [2, 12, 1]\n",
      "episode 8066, reward 961.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 144\n",
      "Initial State is  [2, 17, 1]\n",
      "episode 8067, reward 811.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 142\n",
      "Initial State is  [2, 18, 3]\n",
      "episode 8068, reward 815.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 138\n",
      "Initial State is  [1, 4, 5]\n",
      "episode 8069, reward 1173.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 139\n",
      "Initial State is  [2, 21, 2]\n",
      "episode 8070, reward 1145.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 136\n",
      "Initial State is  [1, 3, 3]\n",
      "episode 8071, reward 837.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 133\n",
      "Initial State is  [0, 18, 6]\n",
      "episode 8072, reward 428.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 130\n",
      "Initial State is  [0, 5, 4]\n",
      "episode 8073, reward 1326.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 140\n",
      "Initial State is  [4, 8, 0]\n",
      "episode 8074, reward 908.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 141\n",
      "Initial State is  [2, 4, 2]\n",
      "episode 8075, reward 825.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 138\n",
      "Initial State is  [1, 15, 5]\n",
      "episode 8076, reward 670.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 141\n",
      "Initial State is  [3, 20, 2]\n",
      "episode 8077, reward 736.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 132\n",
      "Initial State is  [4, 20, 5]\n",
      "episode 8078, reward 741.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 139\n",
      "Initial State is  [4, 19, 2]\n",
      "episode 8079, reward 1034.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 127\n",
      "Initial State is  [0, 20, 5]\n",
      "episode 8080, reward 1108.0, memory_length 2000, epsilon 0.009998671593271896, time 736.0, rides 140\n",
      "Initial State is  [4, 22, 2]\n",
      "episode 8081, reward 1002.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 140\n",
      "Initial State is  [4, 21, 3]\n",
      "episode 8082, reward 1131.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 133\n",
      "Initial State is  [4, 9, 1]\n",
      "episode 8083, reward 1426.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 133\n",
      "Initial State is  [1, 7, 4]\n",
      "episode 8084, reward 889.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 134\n",
      "Initial State is  [3, 23, 4]\n",
      "episode 8085, reward 698.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 142\n",
      "Initial State is  [3, 0, 6]\n",
      "episode 8086, reward 1077.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 125\n",
      "Initial State is  [3, 2, 5]\n",
      "episode 8087, reward 844.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 132\n",
      "Initial State is  [1, 8, 5]\n",
      "episode 8088, reward 951.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 137\n",
      "Initial State is  [0, 13, 5]\n",
      "episode 8089, reward 931.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 131\n",
      "Initial State is  [4, 10, 1]\n",
      "episode 8090, reward 932.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 119\n",
      "Initial State is  [0, 21, 1]\n",
      "episode 8091, reward 874.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 124\n",
      "Initial State is  [2, 16, 0]\n",
      "episode 8092, reward 881.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 131\n",
      "Initial State is  [4, 0, 1]\n",
      "episode 8093, reward 836.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 135\n",
      "Initial State is  [0, 20, 5]\n",
      "episode 8094, reward 818.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 141\n",
      "Initial State is  [0, 12, 0]\n",
      "episode 8095, reward 1140.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 149\n",
      "Initial State is  [3, 9, 0]\n",
      "episode 8096, reward 864.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 132\n",
      "Initial State is  [2, 8, 3]\n",
      "episode 8097, reward 1197.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 133\n",
      "Initial State is  [3, 20, 5]\n",
      "episode 8098, reward 624.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 132\n",
      "Initial State is  [0, 1, 3]\n",
      "episode 8099, reward 827.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 132\n",
      "Initial State is  [0, 2, 3]\n",
      "episode 8100, reward 920.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 128\n",
      "Initial State is  [1, 0, 6]\n",
      "episode 8101, reward 889.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 132\n",
      "Initial State is  [0, 8, 3]\n",
      "episode 8102, reward 1032.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 130\n",
      "Initial State is  [2, 18, 5]\n",
      "episode 8103, reward 683.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 137\n",
      "Initial State is  [2, 6, 6]\n",
      "episode 8104, reward 910.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 140\n",
      "Initial State is  [0, 21, 3]\n",
      "episode 8105, reward 781.0, memory_length 2000, epsilon 0.009998671593271896, time 721.0, rides 132\n",
      "Initial State is  [1, 1, 6]\n",
      "episode 8106, reward 1114.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 138\n",
      "Initial State is  [2, 4, 4]\n",
      "episode 8107, reward 1102.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 139\n",
      "Initial State is  [0, 23, 5]\n",
      "episode 8108, reward 963.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 134\n",
      "Initial State is  [4, 9, 5]\n",
      "episode 8109, reward 725.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 143\n",
      "Initial State is  [4, 2, 0]\n",
      "episode 8110, reward 999.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 141\n",
      "Initial State is  [2, 5, 2]\n",
      "episode 8111, reward 998.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 138\n",
      "Initial State is  [3, 5, 3]\n",
      "episode 8112, reward 975.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 138\n",
      "Initial State is  [2, 1, 4]\n",
      "episode 8113, reward 772.0, memory_length 2000, epsilon 0.009998671593271896, time 738.0, rides 147\n",
      "Initial State is  [4, 6, 5]\n",
      "episode 8114, reward 941.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 154\n",
      "Initial State is  [0, 10, 0]\n",
      "episode 8115, reward 912.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 134\n",
      "Initial State is  [1, 21, 2]\n",
      "episode 8116, reward 794.0, memory_length 2000, epsilon 0.009998671593271896, time 736.0, rides 115\n",
      "Initial State is  [4, 9, 6]\n",
      "episode 8117, reward 966.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 151\n",
      "Initial State is  [2, 8, 1]\n",
      "episode 8118, reward 724.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 142\n",
      "Initial State is  [4, 10, 4]\n",
      "episode 8119, reward 794.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 146\n",
      "Initial State is  [0, 19, 2]\n",
      "episode 8120, reward 825.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 130\n",
      "Initial State is  [0, 12, 4]\n",
      "episode 8121, reward 939.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 134\n",
      "Initial State is  [1, 8, 0]\n",
      "episode 8122, reward 744.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 135\n",
      "Initial State is  [2, 7, 6]\n",
      "episode 8123, reward 881.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 148\n",
      "Initial State is  [1, 19, 2]\n",
      "episode 8124, reward 1343.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 118\n",
      "Initial State is  [1, 13, 6]\n",
      "episode 8125, reward 856.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 126\n",
      "Initial State is  [3, 21, 2]\n",
      "episode 8126, reward 942.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 131\n",
      "Initial State is  [4, 18, 6]\n",
      "episode 8127, reward 1162.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 133\n",
      "Initial State is  [4, 5, 1]\n",
      "episode 8128, reward 852.0, memory_length 2000, epsilon 0.009998671593271896, time 737.0, rides 131\n",
      "Initial State is  [3, 19, 6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 8129, reward 1239.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 138\n",
      "Initial State is  [2, 21, 5]\n",
      "episode 8130, reward 1033.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 136\n",
      "Initial State is  [2, 12, 5]\n",
      "episode 8131, reward 821.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 114\n",
      "Initial State is  [3, 11, 1]\n",
      "episode 8132, reward 802.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 121\n",
      "Initial State is  [1, 16, 4]\n",
      "episode 8133, reward 1000.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 148\n",
      "Initial State is  [4, 3, 1]\n",
      "episode 8134, reward 1076.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 134\n",
      "Initial State is  [0, 0, 0]\n",
      "episode 8135, reward 645.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 139\n",
      "Initial State is  [1, 4, 2]\n",
      "episode 8136, reward 1003.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 128\n",
      "Initial State is  [4, 6, 3]\n",
      "episode 8137, reward 741.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 145\n",
      "Initial State is  [1, 2, 4]\n",
      "episode 8138, reward 781.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 136\n",
      "Initial State is  [2, 12, 6]\n",
      "episode 8139, reward 808.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 131\n",
      "Initial State is  [0, 5, 2]\n",
      "episode 8140, reward 908.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 141\n",
      "Initial State is  [4, 19, 2]\n",
      "episode 8141, reward 973.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 122\n",
      "Initial State is  [3, 0, 0]\n",
      "episode 8142, reward 789.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 120\n",
      "Initial State is  [2, 9, 5]\n",
      "episode 8143, reward 716.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 138\n",
      "Initial State is  [0, 4, 2]\n",
      "episode 8144, reward 961.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 146\n",
      "Initial State is  [3, 23, 6]\n",
      "episode 8145, reward 945.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 132\n",
      "Initial State is  [1, 5, 4]\n",
      "episode 8146, reward 734.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 137\n",
      "Initial State is  [4, 19, 2]\n",
      "episode 8147, reward 1093.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 125\n",
      "Initial State is  [1, 18, 0]\n",
      "episode 8148, reward 792.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 133\n",
      "Initial State is  [4, 23, 6]\n",
      "episode 8149, reward 880.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 129\n",
      "Initial State is  [0, 21, 5]\n",
      "episode 8150, reward 879.0, memory_length 2000, epsilon 0.009998671593271896, time 737.0, rides 148\n",
      "Initial State is  [1, 23, 6]\n",
      "episode 8151, reward 1152.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 141\n",
      "Initial State is  [3, 23, 2]\n",
      "episode 8152, reward 916.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 133\n",
      "Initial State is  [4, 17, 3]\n",
      "episode 8153, reward 985.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 128\n",
      "Initial State is  [2, 1, 5]\n",
      "episode 8154, reward 854.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 143\n",
      "Initial State is  [4, 1, 4]\n",
      "episode 8155, reward 752.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 124\n",
      "Initial State is  [4, 5, 0]\n",
      "episode 8156, reward 664.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 130\n",
      "Initial State is  [2, 21, 5]\n",
      "episode 8157, reward 1095.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 139\n",
      "Initial State is  [0, 21, 1]\n",
      "episode 8158, reward 927.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 133\n",
      "Initial State is  [1, 2, 0]\n",
      "episode 8159, reward 1037.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 140\n",
      "Initial State is  [2, 17, 5]\n",
      "episode 8160, reward 842.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 151\n",
      "Initial State is  [0, 2, 4]\n",
      "episode 8161, reward 967.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 138\n",
      "Initial State is  [0, 18, 6]\n",
      "episode 8162, reward 836.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 131\n",
      "Initial State is  [1, 8, 1]\n",
      "episode 8163, reward 1319.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 139\n",
      "Initial State is  [3, 14, 6]\n",
      "episode 8164, reward 1145.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 146\n",
      "Initial State is  [4, 15, 3]\n",
      "episode 8165, reward 933.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 137\n",
      "Initial State is  [2, 18, 1]\n",
      "episode 8166, reward 706.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 146\n",
      "Initial State is  [4, 17, 6]\n",
      "episode 8167, reward 1001.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 136\n",
      "Initial State is  [0, 1, 5]\n",
      "episode 8168, reward 1049.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 130\n",
      "Initial State is  [2, 16, 6]\n",
      "episode 8169, reward 559.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 134\n",
      "Initial State is  [3, 13, 6]\n",
      "episode 8170, reward 1024.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 149\n",
      "Initial State is  [1, 22, 6]\n",
      "episode 8171, reward 778.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 124\n",
      "Initial State is  [4, 2, 3]\n",
      "episode 8172, reward 982.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 126\n",
      "Initial State is  [1, 8, 4]\n",
      "episode 8173, reward 810.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 135\n",
      "Initial State is  [3, 13, 5]\n",
      "episode 8174, reward 792.0, memory_length 2000, epsilon 0.009998671593271896, time 721.0, rides 144\n",
      "Initial State is  [0, 10, 1]\n",
      "episode 8175, reward 1149.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 134\n",
      "Initial State is  [1, 11, 4]\n",
      "episode 8176, reward 973.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 128\n",
      "Initial State is  [1, 5, 2]\n",
      "episode 8177, reward 788.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 128\n",
      "Initial State is  [2, 9, 1]\n",
      "episode 8178, reward 1067.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 146\n",
      "Initial State is  [0, 2, 5]\n",
      "episode 8179, reward 993.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 127\n",
      "Initial State is  [2, 9, 1]\n",
      "episode 8180, reward 1093.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 140\n",
      "Initial State is  [1, 17, 4]\n",
      "episode 8181, reward 792.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 134\n",
      "Initial State is  [4, 12, 1]\n",
      "episode 8182, reward 882.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 145\n",
      "Initial State is  [2, 16, 4]\n",
      "episode 8183, reward 993.0, memory_length 2000, epsilon 0.009998671593271896, time 721.0, rides 148\n",
      "Initial State is  [4, 6, 5]\n",
      "episode 8184, reward 1051.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 129\n",
      "Initial State is  [3, 1, 1]\n",
      "episode 8185, reward 765.0, memory_length 2000, epsilon 0.009998671593271896, time 720.0, rides 127\n",
      "Initial State is  [4, 1, 1]\n",
      "episode 8186, reward 780.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 138\n",
      "Initial State is  [2, 1, 6]\n",
      "episode 8187, reward 1134.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 132\n",
      "Initial State is  [3, 16, 0]\n",
      "episode 8188, reward 914.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 136\n",
      "Initial State is  [1, 16, 6]\n",
      "episode 8189, reward 883.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 136\n",
      "Initial State is  [2, 15, 6]\n",
      "episode 8190, reward 1165.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 133\n",
      "Initial State is  [3, 5, 6]\n",
      "episode 8191, reward 1032.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 136\n",
      "Initial State is  [4, 10, 6]\n",
      "episode 8192, reward 685.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 126\n",
      "Initial State is  [4, 5, 6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 8193, reward 773.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 134\n",
      "Initial State is  [1, 14, 3]\n",
      "episode 8194, reward 816.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 140\n",
      "Initial State is  [0, 14, 1]\n",
      "episode 8195, reward 718.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 123\n",
      "Initial State is  [3, 4, 1]\n",
      "episode 8196, reward 792.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 128\n",
      "Initial State is  [3, 18, 1]\n",
      "episode 8197, reward 952.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 138\n",
      "Initial State is  [2, 17, 1]\n",
      "episode 8198, reward 1100.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 137\n",
      "Initial State is  [3, 10, 2]\n",
      "episode 8199, reward 853.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 130\n",
      "Initial State is  [1, 17, 2]\n",
      "episode 8200, reward 1164.0, memory_length 2000, epsilon 0.009998671593271896, time 740.0, rides 127\n",
      "Initial State is  [3, 14, 0]\n",
      "episode 8201, reward 741.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 146\n",
      "Initial State is  [4, 22, 5]\n",
      "episode 8202, reward 968.0, memory_length 2000, epsilon 0.009998671593271896, time 736.0, rides 135\n",
      "Initial State is  [3, 7, 3]\n",
      "episode 8203, reward 800.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 127\n",
      "Initial State is  [4, 5, 4]\n",
      "episode 8204, reward 1011.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 127\n",
      "Initial State is  [1, 18, 2]\n",
      "episode 8205, reward 1076.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 138\n",
      "Initial State is  [0, 13, 4]\n",
      "episode 8206, reward 890.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 141\n",
      "Initial State is  [0, 20, 3]\n",
      "episode 8207, reward 1121.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 129\n",
      "Initial State is  [2, 19, 2]\n",
      "episode 8208, reward 814.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 136\n",
      "Initial State is  [2, 13, 6]\n",
      "episode 8209, reward 538.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 133\n",
      "Initial State is  [3, 21, 1]\n",
      "episode 8210, reward 958.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 137\n",
      "Initial State is  [3, 3, 3]\n",
      "episode 8211, reward 1140.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 130\n",
      "Initial State is  [3, 12, 4]\n",
      "episode 8212, reward 857.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 126\n",
      "Initial State is  [2, 22, 6]\n",
      "episode 8213, reward 1189.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 142\n",
      "Initial State is  [3, 4, 6]\n",
      "episode 8214, reward 1192.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 136\n",
      "Initial State is  [0, 2, 2]\n",
      "episode 8215, reward 834.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 136\n",
      "Initial State is  [3, 15, 4]\n",
      "episode 8216, reward 1154.0, memory_length 2000, epsilon 0.009998671593271896, time 736.0, rides 150\n",
      "Initial State is  [0, 11, 1]\n",
      "episode 8217, reward 921.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 135\n",
      "Initial State is  [2, 1, 2]\n",
      "episode 8218, reward 610.0, memory_length 2000, epsilon 0.009998671593271896, time 739.0, rides 131\n",
      "Initial State is  [0, 10, 0]\n",
      "episode 8219, reward 1093.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 131\n",
      "Initial State is  [4, 13, 1]\n",
      "episode 8220, reward 789.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 138\n",
      "Initial State is  [3, 9, 0]\n",
      "episode 8221, reward 923.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 154\n",
      "Initial State is  [4, 16, 4]\n",
      "episode 8222, reward 1132.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 134\n",
      "Initial State is  [1, 23, 3]\n",
      "episode 8223, reward 1161.0, memory_length 2000, epsilon 0.009998671593271896, time 721.0, rides 140\n",
      "Initial State is  [3, 6, 5]\n",
      "episode 8224, reward 1013.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 152\n",
      "Initial State is  [3, 10, 2]\n",
      "episode 8225, reward 1065.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 124\n",
      "Initial State is  [4, 6, 3]\n",
      "episode 8226, reward 843.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 139\n",
      "Initial State is  [1, 18, 1]\n",
      "episode 8227, reward 1148.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 124\n",
      "Initial State is  [4, 15, 4]\n",
      "episode 8228, reward 832.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 136\n",
      "Initial State is  [3, 18, 3]\n",
      "episode 8229, reward 632.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 131\n",
      "Initial State is  [3, 9, 0]\n",
      "episode 8230, reward 784.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 123\n",
      "Initial State is  [0, 21, 3]\n",
      "episode 8231, reward 710.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 135\n",
      "Initial State is  [0, 10, 1]\n",
      "episode 8232, reward 1016.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 126\n",
      "Initial State is  [2, 20, 0]\n",
      "episode 8233, reward 883.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 142\n",
      "Initial State is  [4, 2, 6]\n",
      "episode 8234, reward 1105.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 138\n",
      "Initial State is  [0, 22, 1]\n",
      "episode 8235, reward 885.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 135\n",
      "Initial State is  [1, 21, 2]\n",
      "episode 8236, reward 1152.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 132\n",
      "Initial State is  [1, 17, 2]\n",
      "episode 8237, reward 941.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 142\n",
      "Initial State is  [2, 21, 0]\n",
      "episode 8238, reward 765.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 122\n",
      "Initial State is  [3, 17, 2]\n",
      "episode 8239, reward 999.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 141\n",
      "Initial State is  [4, 14, 2]\n",
      "episode 8240, reward 1202.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 136\n",
      "Initial State is  [0, 8, 0]\n",
      "episode 8241, reward 905.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 137\n",
      "Initial State is  [4, 9, 0]\n",
      "episode 8242, reward 1123.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 143\n",
      "Initial State is  [0, 16, 5]\n",
      "episode 8243, reward 717.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 130\n",
      "Initial State is  [3, 5, 1]\n",
      "episode 8244, reward 856.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 125\n",
      "Initial State is  [1, 21, 6]\n",
      "episode 8245, reward 609.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 145\n",
      "Initial State is  [3, 4, 0]\n",
      "episode 8246, reward 1182.0, memory_length 2000, epsilon 0.009998671593271896, time 737.0, rides 134\n",
      "Initial State is  [2, 21, 4]\n",
      "episode 8247, reward 1079.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 140\n",
      "Initial State is  [4, 4, 0]\n",
      "episode 8248, reward 731.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 136\n",
      "Initial State is  [4, 15, 2]\n",
      "episode 8249, reward 1031.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 134\n",
      "Initial State is  [4, 12, 2]\n",
      "episode 8250, reward 1062.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 138\n",
      "Initial State is  [1, 2, 1]\n",
      "episode 8251, reward 820.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 123\n",
      "Initial State is  [1, 16, 6]\n",
      "episode 8252, reward 834.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 129\n",
      "Initial State is  [1, 5, 1]\n",
      "episode 8253, reward 719.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 141\n",
      "Initial State is  [2, 9, 2]\n",
      "episode 8254, reward 941.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 128\n",
      "Initial State is  [1, 5, 1]\n",
      "episode 8255, reward 1032.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 146\n",
      "Initial State is  [2, 10, 6]\n",
      "episode 8256, reward 1025.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 131\n",
      "Initial State is  [4, 20, 4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 8257, reward 964.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 130\n",
      "Initial State is  [4, 11, 3]\n",
      "episode 8258, reward 1005.0, memory_length 2000, epsilon 0.009998671593271896, time 721.0, rides 153\n",
      "Initial State is  [2, 2, 0]\n",
      "episode 8259, reward 1085.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 144\n",
      "Initial State is  [0, 19, 4]\n",
      "episode 8260, reward 956.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 136\n",
      "Initial State is  [3, 14, 6]\n",
      "episode 8261, reward 753.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 138\n",
      "Initial State is  [2, 22, 3]\n",
      "episode 8262, reward 488.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 123\n",
      "Initial State is  [0, 4, 2]\n",
      "episode 8263, reward 979.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 124\n",
      "Initial State is  [0, 14, 5]\n",
      "episode 8264, reward 1033.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 141\n",
      "Initial State is  [0, 20, 6]\n",
      "episode 8265, reward 567.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 142\n",
      "Initial State is  [4, 20, 2]\n",
      "episode 8266, reward 1051.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 127\n",
      "Initial State is  [2, 1, 1]\n",
      "episode 8267, reward 1161.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 130\n",
      "Initial State is  [1, 2, 4]\n",
      "episode 8268, reward 802.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 139\n",
      "Initial State is  [1, 1, 0]\n",
      "episode 8269, reward 736.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 132\n",
      "Initial State is  [2, 2, 0]\n",
      "episode 8270, reward 734.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 136\n",
      "Initial State is  [4, 8, 4]\n",
      "episode 8271, reward 1142.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 138\n",
      "Initial State is  [3, 8, 6]\n",
      "episode 8272, reward 983.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 138\n",
      "Initial State is  [4, 18, 1]\n",
      "episode 8273, reward 989.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 132\n",
      "Initial State is  [1, 19, 0]\n",
      "episode 8274, reward 914.0, memory_length 2000, epsilon 0.009998671593271896, time 736.0, rides 141\n",
      "Initial State is  [3, 10, 5]\n",
      "episode 8275, reward 920.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 127\n",
      "Initial State is  [2, 11, 6]\n",
      "episode 8276, reward 614.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 131\n",
      "Initial State is  [1, 5, 5]\n",
      "episode 8277, reward 1143.0, memory_length 2000, epsilon 0.009998671593271896, time 739.0, rides 154\n",
      "Initial State is  [2, 8, 0]\n",
      "episode 8278, reward 1070.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 132\n",
      "Initial State is  [1, 11, 3]\n",
      "episode 8279, reward 947.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 131\n",
      "Initial State is  [0, 12, 5]\n",
      "episode 8280, reward 907.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 143\n",
      "Initial State is  [1, 6, 0]\n",
      "episode 8281, reward 982.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 135\n",
      "Initial State is  [4, 21, 1]\n",
      "episode 8282, reward 984.0, memory_length 2000, epsilon 0.009998671593271896, time 736.0, rides 116\n",
      "Initial State is  [3, 7, 4]\n",
      "episode 8283, reward 1140.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 126\n",
      "Initial State is  [3, 13, 5]\n",
      "episode 8284, reward 1082.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 131\n",
      "Initial State is  [3, 15, 4]\n",
      "episode 8285, reward 752.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 128\n",
      "Initial State is  [4, 1, 2]\n",
      "episode 8286, reward 1116.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 135\n",
      "Initial State is  [0, 12, 4]\n",
      "episode 8287, reward 754.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 147\n",
      "Initial State is  [0, 20, 3]\n",
      "episode 8288, reward 975.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 138\n",
      "Initial State is  [4, 7, 3]\n",
      "episode 8289, reward 585.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 135\n",
      "Initial State is  [3, 21, 0]\n",
      "episode 8290, reward 744.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 133\n",
      "Initial State is  [1, 6, 3]\n",
      "episode 8291, reward 1130.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 134\n",
      "Initial State is  [2, 4, 5]\n",
      "episode 8292, reward 941.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 133\n",
      "Initial State is  [3, 0, 2]\n",
      "episode 8293, reward 667.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 121\n",
      "Initial State is  [2, 16, 6]\n",
      "episode 8294, reward 866.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 129\n",
      "Initial State is  [1, 3, 0]\n",
      "episode 8295, reward 616.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 128\n",
      "Initial State is  [1, 12, 1]\n",
      "episode 8296, reward 827.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 127\n",
      "Initial State is  [4, 21, 0]\n",
      "episode 8297, reward 968.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 125\n",
      "Initial State is  [2, 1, 6]\n",
      "episode 8298, reward 763.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 129\n",
      "Initial State is  [2, 6, 4]\n",
      "episode 8299, reward 1079.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 130\n",
      "Initial State is  [3, 6, 3]\n",
      "episode 8300, reward 1011.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 125\n",
      "Initial State is  [2, 21, 1]\n",
      "episode 8301, reward 734.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 127\n",
      "Initial State is  [1, 21, 1]\n",
      "episode 8302, reward 1022.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 122\n",
      "Initial State is  [1, 9, 5]\n",
      "episode 8303, reward 1003.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 122\n",
      "Initial State is  [0, 9, 4]\n",
      "episode 8304, reward 867.0, memory_length 2000, epsilon 0.009998671593271896, time 736.0, rides 142\n",
      "Initial State is  [1, 10, 5]\n",
      "episode 8305, reward 1165.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 135\n",
      "Initial State is  [1, 17, 5]\n",
      "episode 8306, reward 958.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 145\n",
      "Initial State is  [2, 15, 3]\n",
      "episode 8307, reward 836.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 140\n",
      "Initial State is  [2, 11, 3]\n",
      "episode 8308, reward 1196.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 139\n",
      "Initial State is  [1, 9, 4]\n",
      "episode 8309, reward 981.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 136\n",
      "Initial State is  [2, 4, 1]\n",
      "episode 8310, reward 1038.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 142\n",
      "Initial State is  [4, 20, 5]\n",
      "episode 8311, reward 949.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 136\n",
      "Initial State is  [4, 7, 6]\n",
      "episode 8312, reward 1125.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 121\n",
      "Initial State is  [4, 17, 2]\n",
      "episode 8313, reward 899.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 139\n",
      "Initial State is  [3, 4, 3]\n",
      "episode 8314, reward 746.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 131\n",
      "Initial State is  [0, 11, 2]\n",
      "episode 8315, reward 838.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 118\n",
      "Initial State is  [0, 23, 2]\n",
      "episode 8316, reward 876.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 130\n",
      "Initial State is  [2, 2, 3]\n",
      "episode 8317, reward 789.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 132\n",
      "Initial State is  [4, 1, 3]\n",
      "episode 8318, reward 885.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 124\n",
      "Initial State is  [3, 16, 3]\n",
      "episode 8319, reward 1443.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 135\n",
      "Initial State is  [3, 6, 2]\n",
      "episode 8320, reward 1017.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 126\n",
      "Initial State is  [0, 5, 6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 8321, reward 1084.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 134\n",
      "Initial State is  [0, 3, 3]\n",
      "episode 8322, reward 1002.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 125\n",
      "Initial State is  [0, 23, 1]\n",
      "episode 8323, reward 1028.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 142\n",
      "Initial State is  [0, 11, 4]\n",
      "episode 8324, reward 1106.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 123\n",
      "Initial State is  [4, 21, 4]\n",
      "episode 8325, reward 817.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 119\n",
      "Initial State is  [2, 2, 3]\n",
      "episode 8326, reward 1270.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 132\n",
      "Initial State is  [1, 16, 1]\n",
      "episode 8327, reward 876.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 123\n",
      "Initial State is  [2, 3, 4]\n",
      "episode 8328, reward 918.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 131\n",
      "Initial State is  [3, 8, 2]\n",
      "episode 8329, reward 637.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 137\n",
      "Initial State is  [3, 1, 2]\n",
      "episode 8330, reward 683.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 132\n",
      "Initial State is  [2, 18, 1]\n",
      "episode 8331, reward 832.0, memory_length 2000, epsilon 0.009998671593271896, time 738.0, rides 126\n",
      "Initial State is  [1, 11, 6]\n",
      "episode 8332, reward 935.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 130\n",
      "Initial State is  [4, 1, 3]\n",
      "episode 8333, reward 491.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 114\n",
      "Initial State is  [3, 8, 3]\n",
      "episode 8334, reward 945.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 139\n",
      "Initial State is  [0, 12, 2]\n",
      "episode 8335, reward 1016.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 131\n",
      "Initial State is  [0, 5, 3]\n",
      "episode 8336, reward 868.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 141\n",
      "Initial State is  [4, 16, 2]\n",
      "episode 8337, reward 893.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 128\n",
      "Initial State is  [0, 23, 2]\n",
      "episode 8338, reward 935.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 141\n",
      "Initial State is  [0, 23, 2]\n",
      "episode 8339, reward 696.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 136\n",
      "Initial State is  [0, 23, 4]\n",
      "episode 8340, reward 1128.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 137\n",
      "Initial State is  [2, 18, 3]\n",
      "episode 8341, reward 751.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 129\n",
      "Initial State is  [3, 8, 0]\n",
      "episode 8342, reward 637.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 133\n",
      "Initial State is  [1, 9, 2]\n",
      "episode 8343, reward 630.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 133\n",
      "Initial State is  [2, 17, 5]\n",
      "episode 8344, reward 934.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 130\n",
      "Initial State is  [0, 10, 6]\n",
      "episode 8345, reward 848.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 130\n",
      "Initial State is  [1, 13, 2]\n",
      "episode 8346, reward 1272.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 133\n",
      "Initial State is  [2, 14, 6]\n",
      "episode 8347, reward 650.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 138\n",
      "Initial State is  [4, 5, 4]\n",
      "episode 8348, reward 1067.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 119\n",
      "Initial State is  [2, 6, 1]\n",
      "episode 8349, reward 778.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 131\n",
      "Initial State is  [4, 21, 5]\n",
      "episode 8350, reward 1076.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 136\n",
      "Initial State is  [4, 18, 5]\n",
      "episode 8351, reward 1042.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 142\n",
      "Initial State is  [0, 16, 2]\n",
      "episode 8352, reward 1046.0, memory_length 2000, epsilon 0.009998671593271896, time 737.0, rides 137\n",
      "Initial State is  [3, 6, 4]\n",
      "episode 8353, reward 936.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 141\n",
      "Initial State is  [0, 7, 6]\n",
      "episode 8354, reward 813.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 136\n",
      "Initial State is  [0, 20, 1]\n",
      "episode 8355, reward 899.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 130\n",
      "Initial State is  [0, 12, 6]\n",
      "episode 8356, reward 757.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 130\n",
      "Initial State is  [0, 23, 3]\n",
      "episode 8357, reward 830.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 143\n",
      "Initial State is  [2, 15, 4]\n",
      "episode 8358, reward 1073.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 131\n",
      "Initial State is  [0, 22, 5]\n",
      "episode 8359, reward 870.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 123\n",
      "Initial State is  [1, 23, 3]\n",
      "episode 8360, reward 951.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 150\n",
      "Initial State is  [1, 23, 1]\n",
      "episode 8361, reward 1136.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 139\n",
      "Initial State is  [3, 1, 2]\n",
      "episode 8362, reward 941.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 129\n",
      "Initial State is  [4, 14, 3]\n",
      "episode 8363, reward 1218.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 127\n",
      "Initial State is  [0, 15, 6]\n",
      "episode 8364, reward 924.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 135\n",
      "Initial State is  [2, 1, 1]\n",
      "episode 8365, reward 1078.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 132\n",
      "Initial State is  [3, 16, 3]\n",
      "episode 8366, reward 1017.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 136\n",
      "Initial State is  [4, 5, 1]\n",
      "episode 8367, reward 1074.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 135\n",
      "Initial State is  [2, 20, 0]\n",
      "episode 8368, reward 875.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 129\n",
      "Initial State is  [3, 21, 2]\n",
      "episode 8369, reward 1148.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 135\n",
      "Initial State is  [2, 18, 3]\n",
      "episode 8370, reward 959.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 132\n",
      "Initial State is  [2, 18, 2]\n",
      "episode 8371, reward 1088.0, memory_length 2000, epsilon 0.009998671593271896, time 738.0, rides 131\n",
      "Initial State is  [3, 5, 6]\n",
      "episode 8372, reward 1035.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 134\n",
      "Initial State is  [4, 20, 4]\n",
      "episode 8373, reward 936.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 127\n",
      "Initial State is  [0, 2, 6]\n",
      "episode 8374, reward 895.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 138\n",
      "Initial State is  [4, 8, 1]\n",
      "episode 8375, reward 1100.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 130\n",
      "Initial State is  [4, 5, 6]\n",
      "episode 8376, reward 917.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 149\n",
      "Initial State is  [1, 3, 5]\n",
      "episode 8377, reward 842.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 139\n",
      "Initial State is  [0, 14, 1]\n",
      "episode 8378, reward 730.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 116\n",
      "Initial State is  [2, 11, 2]\n",
      "episode 8379, reward 889.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 130\n",
      "Initial State is  [2, 23, 5]\n",
      "episode 8380, reward 827.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 146\n",
      "Initial State is  [4, 18, 3]\n",
      "episode 8381, reward 839.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 140\n",
      "Initial State is  [1, 8, 3]\n",
      "episode 8382, reward 670.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 143\n",
      "Initial State is  [0, 18, 1]\n",
      "episode 8383, reward 819.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 134\n",
      "Initial State is  [4, 15, 0]\n",
      "episode 8384, reward 943.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 134\n",
      "Initial State is  [2, 14, 6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 8385, reward 944.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 138\n",
      "Initial State is  [1, 13, 2]\n",
      "episode 8386, reward 938.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 138\n",
      "Initial State is  [4, 23, 4]\n",
      "episode 8387, reward 1050.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 135\n",
      "Initial State is  [2, 23, 6]\n",
      "episode 8388, reward 907.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 136\n",
      "Initial State is  [4, 0, 4]\n",
      "episode 8389, reward 994.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 137\n",
      "Initial State is  [0, 7, 3]\n",
      "episode 8390, reward 955.0, memory_length 2000, epsilon 0.009998671593271896, time 736.0, rides 148\n",
      "Initial State is  [3, 4, 0]\n",
      "episode 8391, reward 823.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 121\n",
      "Initial State is  [1, 4, 4]\n",
      "episode 8392, reward 967.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 136\n",
      "Initial State is  [1, 11, 4]\n",
      "episode 8393, reward 981.0, memory_length 2000, epsilon 0.009998671593271896, time 737.0, rides 142\n",
      "Initial State is  [3, 1, 4]\n",
      "episode 8394, reward 939.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 131\n",
      "Initial State is  [2, 16, 1]\n",
      "episode 8395, reward 932.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 128\n",
      "Initial State is  [3, 4, 3]\n",
      "episode 8396, reward 936.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 129\n",
      "Initial State is  [4, 1, 5]\n",
      "episode 8397, reward 997.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 131\n",
      "Initial State is  [0, 1, 3]\n",
      "episode 8398, reward 1096.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 146\n",
      "Initial State is  [0, 9, 4]\n",
      "episode 8399, reward 1036.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 139\n",
      "Initial State is  [2, 2, 4]\n",
      "episode 8400, reward 958.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 139\n",
      "Initial State is  [4, 8, 5]\n",
      "episode 8401, reward 666.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 143\n",
      "Initial State is  [1, 6, 0]\n",
      "episode 8402, reward 1012.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 138\n",
      "Initial State is  [1, 19, 0]\n",
      "episode 8403, reward 1075.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 130\n",
      "Initial State is  [2, 18, 4]\n",
      "episode 8404, reward 932.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 125\n",
      "Initial State is  [0, 9, 5]\n",
      "episode 8405, reward 801.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 146\n",
      "Initial State is  [2, 11, 1]\n",
      "episode 8406, reward 974.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 139\n",
      "Initial State is  [1, 4, 3]\n",
      "episode 8407, reward 1231.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 139\n",
      "Initial State is  [0, 2, 2]\n",
      "episode 8408, reward 926.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 134\n",
      "Initial State is  [0, 17, 4]\n",
      "episode 8409, reward 1092.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 132\n",
      "Initial State is  [1, 23, 4]\n",
      "episode 8410, reward 1154.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 138\n",
      "Initial State is  [4, 1, 3]\n",
      "episode 8411, reward 1244.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 131\n",
      "Initial State is  [1, 7, 0]\n",
      "episode 8412, reward 562.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 117\n",
      "Initial State is  [2, 10, 1]\n",
      "episode 8413, reward 1088.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 136\n",
      "Initial State is  [4, 12, 3]\n",
      "episode 8414, reward 1061.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 135\n",
      "Initial State is  [0, 2, 6]\n",
      "episode 8415, reward 1040.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 137\n",
      "Initial State is  [4, 16, 3]\n",
      "episode 8416, reward 881.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 132\n",
      "Initial State is  [2, 4, 4]\n",
      "episode 8417, reward 713.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 128\n",
      "Initial State is  [2, 20, 6]\n",
      "episode 8418, reward 951.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 122\n",
      "Initial State is  [4, 12, 3]\n",
      "episode 8419, reward 1239.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 133\n",
      "Initial State is  [1, 9, 0]\n",
      "episode 8420, reward 722.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 130\n",
      "Initial State is  [2, 6, 6]\n",
      "episode 8421, reward 803.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 138\n",
      "Initial State is  [1, 0, 5]\n",
      "episode 8422, reward 964.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 139\n",
      "Initial State is  [1, 1, 1]\n",
      "episode 8423, reward 1102.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 140\n",
      "Initial State is  [0, 0, 6]\n",
      "episode 8424, reward 869.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 133\n",
      "Initial State is  [2, 11, 0]\n",
      "episode 8425, reward 961.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 136\n",
      "Initial State is  [4, 11, 6]\n",
      "episode 8426, reward 949.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 128\n",
      "Initial State is  [3, 19, 3]\n",
      "episode 8427, reward 750.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 130\n",
      "Initial State is  [2, 9, 6]\n",
      "episode 8428, reward 931.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 127\n",
      "Initial State is  [2, 7, 5]\n",
      "episode 8429, reward 629.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 136\n",
      "Initial State is  [3, 15, 4]\n",
      "episode 8430, reward 909.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 128\n",
      "Initial State is  [4, 23, 5]\n",
      "episode 8431, reward 967.0, memory_length 2000, epsilon 0.009998671593271896, time 740.0, rides 145\n",
      "Initial State is  [2, 0, 1]\n",
      "episode 8432, reward 944.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 141\n",
      "Initial State is  [1, 1, 0]\n",
      "episode 8433, reward 1070.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 127\n",
      "Initial State is  [4, 9, 6]\n",
      "episode 8434, reward 918.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 135\n",
      "Initial State is  [2, 21, 2]\n",
      "episode 8435, reward 974.0, memory_length 2000, epsilon 0.009998671593271896, time 721.0, rides 141\n",
      "Initial State is  [0, 13, 0]\n",
      "episode 8436, reward 894.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 129\n",
      "Initial State is  [2, 3, 6]\n",
      "episode 8437, reward 1034.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 150\n",
      "Initial State is  [1, 8, 5]\n",
      "episode 8438, reward 556.0, memory_length 2000, epsilon 0.009998671593271896, time 736.0, rides 117\n",
      "Initial State is  [4, 11, 6]\n",
      "episode 8439, reward 885.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 137\n",
      "Initial State is  [2, 3, 0]\n",
      "episode 8440, reward 964.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 137\n",
      "Initial State is  [1, 23, 5]\n",
      "episode 8441, reward 924.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 128\n",
      "Initial State is  [2, 13, 3]\n",
      "episode 8442, reward 972.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 145\n",
      "Initial State is  [3, 0, 2]\n",
      "episode 8443, reward 1054.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 140\n",
      "Initial State is  [4, 1, 1]\n",
      "episode 8444, reward 639.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 139\n",
      "Initial State is  [1, 18, 6]\n",
      "episode 8445, reward 813.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 133\n",
      "Initial State is  [0, 4, 1]\n",
      "episode 8446, reward 912.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 125\n",
      "Initial State is  [1, 15, 1]\n",
      "episode 8447, reward 662.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 133\n",
      "Initial State is  [4, 10, 6]\n",
      "episode 8448, reward 1078.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 120\n",
      "Initial State is  [2, 20, 3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 8449, reward 813.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 130\n",
      "Initial State is  [3, 18, 0]\n",
      "episode 8450, reward 884.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 134\n",
      "Initial State is  [4, 17, 3]\n",
      "episode 8451, reward 861.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 142\n",
      "Initial State is  [4, 7, 2]\n",
      "episode 8452, reward 828.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 139\n",
      "Initial State is  [2, 20, 4]\n",
      "episode 8453, reward 831.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 132\n",
      "Initial State is  [4, 21, 4]\n",
      "episode 8454, reward 855.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 132\n",
      "Initial State is  [2, 12, 0]\n",
      "episode 8455, reward 974.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 134\n",
      "Initial State is  [4, 14, 2]\n",
      "episode 8456, reward 1009.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 132\n",
      "Initial State is  [2, 19, 3]\n",
      "episode 8457, reward 582.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 141\n",
      "Initial State is  [2, 0, 3]\n",
      "episode 8458, reward 1051.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 141\n",
      "Initial State is  [1, 21, 1]\n",
      "episode 8459, reward 1190.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 138\n",
      "Initial State is  [4, 0, 3]\n",
      "episode 8460, reward 962.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 145\n",
      "Initial State is  [2, 18, 3]\n",
      "episode 8461, reward 1073.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 130\n",
      "Initial State is  [2, 1, 1]\n",
      "episode 8462, reward 1036.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 142\n",
      "Initial State is  [3, 13, 4]\n",
      "episode 8463, reward 1109.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 146\n",
      "Initial State is  [0, 22, 4]\n",
      "episode 8464, reward 918.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 142\n",
      "Initial State is  [3, 18, 3]\n",
      "episode 8465, reward 890.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 140\n",
      "Initial State is  [2, 20, 5]\n",
      "episode 8466, reward 800.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 130\n",
      "Initial State is  [2, 2, 0]\n",
      "episode 8467, reward 717.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 134\n",
      "Initial State is  [3, 7, 6]\n",
      "episode 8468, reward 844.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 133\n",
      "Initial State is  [3, 20, 4]\n",
      "episode 8469, reward 852.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 132\n",
      "Initial State is  [1, 12, 5]\n",
      "episode 8470, reward 1010.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 144\n",
      "Initial State is  [0, 23, 0]\n",
      "episode 8471, reward 690.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 146\n",
      "Initial State is  [1, 14, 2]\n",
      "episode 8472, reward 1066.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 140\n",
      "Initial State is  [4, 16, 3]\n",
      "episode 8473, reward 1052.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 136\n",
      "Initial State is  [4, 9, 5]\n",
      "episode 8474, reward 918.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 135\n",
      "Initial State is  [0, 8, 0]\n",
      "episode 8475, reward 1088.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 129\n",
      "Initial State is  [2, 10, 0]\n",
      "episode 8476, reward 1033.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 144\n",
      "Initial State is  [1, 6, 2]\n",
      "episode 8477, reward 986.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 135\n",
      "Initial State is  [3, 21, 4]\n",
      "episode 8478, reward 1019.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 138\n",
      "Initial State is  [0, 8, 3]\n",
      "episode 8479, reward 944.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 125\n",
      "Initial State is  [4, 4, 3]\n",
      "episode 8480, reward 773.0, memory_length 2000, epsilon 0.009998671593271896, time 721.0, rides 132\n",
      "Initial State is  [4, 21, 1]\n",
      "episode 8481, reward 1142.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 146\n",
      "Initial State is  [1, 15, 6]\n",
      "episode 8482, reward 679.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 145\n",
      "Initial State is  [1, 15, 5]\n",
      "episode 8483, reward 850.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 131\n",
      "Initial State is  [4, 3, 5]\n",
      "episode 8484, reward 984.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 133\n",
      "Initial State is  [1, 22, 2]\n",
      "episode 8485, reward 652.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 145\n",
      "Initial State is  [4, 4, 3]\n",
      "episode 8486, reward 900.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 138\n",
      "Initial State is  [2, 4, 0]\n",
      "episode 8487, reward 804.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 138\n",
      "Initial State is  [2, 23, 0]\n",
      "episode 8488, reward 1126.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 134\n",
      "Initial State is  [3, 15, 2]\n",
      "episode 8489, reward 938.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 134\n",
      "Initial State is  [1, 16, 1]\n",
      "episode 8490, reward 914.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 133\n",
      "Initial State is  [1, 0, 3]\n",
      "episode 8491, reward 663.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 126\n",
      "Initial State is  [4, 22, 0]\n",
      "episode 8492, reward 809.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 136\n",
      "Initial State is  [1, 7, 3]\n",
      "episode 8493, reward 847.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 141\n",
      "Initial State is  [3, 1, 3]\n",
      "episode 8494, reward 643.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 120\n",
      "Initial State is  [2, 23, 0]\n",
      "episode 8495, reward 734.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 138\n",
      "Initial State is  [0, 7, 3]\n",
      "episode 8496, reward 682.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 126\n",
      "Initial State is  [2, 7, 6]\n",
      "episode 8497, reward 945.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 139\n",
      "Initial State is  [1, 22, 2]\n",
      "episode 8498, reward 972.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 148\n",
      "Initial State is  [2, 13, 0]\n",
      "episode 8499, reward 646.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 133\n",
      "Initial State is  [4, 0, 3]\n",
      "episode 8500, reward 1065.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 137\n",
      "Initial State is  [1, 11, 4]\n",
      "episode 8501, reward 1067.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 130\n",
      "Initial State is  [3, 5, 5]\n",
      "episode 8502, reward 1326.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 133\n",
      "Initial State is  [1, 15, 3]\n",
      "episode 8503, reward 872.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 136\n",
      "Initial State is  [3, 22, 4]\n",
      "episode 8504, reward 947.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 143\n",
      "Initial State is  [4, 7, 6]\n",
      "episode 8505, reward 814.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 134\n",
      "Initial State is  [3, 3, 4]\n",
      "episode 8506, reward 668.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 145\n",
      "Initial State is  [4, 21, 0]\n",
      "episode 8507, reward 659.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 126\n",
      "Initial State is  [4, 16, 4]\n",
      "episode 8508, reward 379.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 135\n",
      "Initial State is  [1, 5, 2]\n",
      "episode 8509, reward 1128.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 130\n",
      "Initial State is  [0, 6, 3]\n",
      "episode 8510, reward 1023.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 125\n",
      "Initial State is  [4, 15, 4]\n",
      "episode 8511, reward 896.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 140\n",
      "Initial State is  [0, 12, 6]\n",
      "episode 8512, reward 1037.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 122\n",
      "Initial State is  [3, 7, 3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 8513, reward 993.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 138\n",
      "Initial State is  [1, 17, 6]\n",
      "episode 8514, reward 964.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 132\n",
      "Initial State is  [0, 19, 3]\n",
      "episode 8515, reward 942.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 143\n",
      "Initial State is  [4, 10, 4]\n",
      "episode 8516, reward 636.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 131\n",
      "Initial State is  [2, 8, 2]\n",
      "episode 8517, reward 943.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 131\n",
      "Initial State is  [4, 15, 2]\n",
      "episode 8518, reward 1184.0, memory_length 2000, epsilon 0.009998671593271896, time 736.0, rides 139\n",
      "Initial State is  [2, 21, 0]\n",
      "episode 8519, reward 965.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 121\n",
      "Initial State is  [3, 14, 3]\n",
      "episode 8520, reward 929.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 133\n",
      "Initial State is  [0, 2, 3]\n",
      "episode 8521, reward 1024.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 128\n",
      "Initial State is  [0, 22, 4]\n",
      "episode 8522, reward 1003.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 120\n",
      "Initial State is  [4, 19, 2]\n",
      "episode 8523, reward 787.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 134\n",
      "Initial State is  [1, 23, 2]\n",
      "episode 8524, reward 1123.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 132\n",
      "Initial State is  [1, 16, 5]\n",
      "episode 8525, reward 987.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 150\n",
      "Initial State is  [3, 3, 1]\n",
      "episode 8526, reward 880.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 141\n",
      "Initial State is  [3, 10, 4]\n",
      "episode 8527, reward 933.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 141\n",
      "Initial State is  [2, 4, 5]\n",
      "episode 8528, reward 1286.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 127\n",
      "Initial State is  [0, 18, 1]\n",
      "episode 8529, reward 1081.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 121\n",
      "Initial State is  [1, 16, 6]\n",
      "episode 8530, reward 1232.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 127\n",
      "Initial State is  [1, 4, 1]\n",
      "episode 8531, reward 969.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 125\n",
      "Initial State is  [0, 7, 6]\n",
      "episode 8532, reward 1202.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 140\n",
      "Initial State is  [3, 13, 5]\n",
      "episode 8533, reward 885.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 155\n",
      "Initial State is  [0, 22, 4]\n",
      "episode 8534, reward 794.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 135\n",
      "Initial State is  [1, 21, 6]\n",
      "episode 8535, reward 1013.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 150\n",
      "Initial State is  [4, 6, 6]\n",
      "episode 8536, reward 915.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 131\n",
      "Initial State is  [4, 6, 3]\n",
      "episode 8537, reward 914.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 125\n",
      "Initial State is  [1, 23, 2]\n",
      "episode 8538, reward 1053.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 142\n",
      "Initial State is  [3, 23, 6]\n",
      "episode 8539, reward 1119.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 136\n",
      "Initial State is  [0, 14, 4]\n",
      "episode 8540, reward 987.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 144\n",
      "Initial State is  [3, 3, 4]\n",
      "episode 8541, reward 986.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 138\n",
      "Initial State is  [2, 12, 6]\n",
      "episode 8542, reward 784.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 135\n",
      "Initial State is  [3, 0, 4]\n",
      "episode 8543, reward 887.0, memory_length 2000, epsilon 0.009998671593271896, time 737.0, rides 129\n",
      "Initial State is  [2, 3, 0]\n",
      "episode 8544, reward 1168.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 140\n",
      "Initial State is  [3, 18, 0]\n",
      "episode 8545, reward 1095.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 131\n",
      "Initial State is  [1, 17, 4]\n",
      "episode 8546, reward 864.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 128\n",
      "Initial State is  [2, 7, 5]\n",
      "episode 8547, reward 731.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 124\n",
      "Initial State is  [3, 11, 5]\n",
      "episode 8548, reward 988.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 133\n",
      "Initial State is  [1, 12, 1]\n",
      "episode 8549, reward 938.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 123\n",
      "Initial State is  [4, 6, 5]\n",
      "episode 8550, reward 1124.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 144\n",
      "Initial State is  [3, 20, 5]\n",
      "episode 8551, reward 979.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 145\n",
      "Initial State is  [0, 15, 4]\n",
      "episode 8552, reward 983.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 135\n",
      "Initial State is  [3, 18, 5]\n",
      "episode 8553, reward 904.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 143\n",
      "Initial State is  [2, 15, 3]\n",
      "episode 8554, reward 730.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 139\n",
      "Initial State is  [1, 10, 6]\n",
      "episode 8555, reward 997.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 129\n",
      "Initial State is  [0, 13, 6]\n",
      "episode 8556, reward 1075.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 129\n",
      "Initial State is  [1, 8, 1]\n",
      "episode 8557, reward 922.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 131\n",
      "Initial State is  [1, 23, 0]\n",
      "episode 8558, reward 730.0, memory_length 2000, epsilon 0.009998671593271896, time 738.0, rides 134\n",
      "Initial State is  [3, 4, 1]\n",
      "episode 8559, reward 834.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 134\n",
      "Initial State is  [0, 11, 0]\n",
      "episode 8560, reward 1048.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 125\n",
      "Initial State is  [0, 4, 4]\n",
      "episode 8561, reward 884.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 133\n",
      "Initial State is  [1, 21, 0]\n",
      "episode 8562, reward 1092.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 128\n",
      "Initial State is  [1, 0, 1]\n",
      "episode 8563, reward 815.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 118\n",
      "Initial State is  [4, 17, 1]\n",
      "episode 8564, reward 1086.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 148\n",
      "Initial State is  [1, 15, 4]\n",
      "episode 8565, reward 1052.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 135\n",
      "Initial State is  [4, 10, 5]\n",
      "episode 8566, reward 1297.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 134\n",
      "Initial State is  [3, 6, 2]\n",
      "episode 8567, reward 1039.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 132\n",
      "Initial State is  [3, 2, 6]\n",
      "episode 8568, reward 395.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 121\n",
      "Initial State is  [2, 22, 2]\n",
      "episode 8569, reward 1128.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 130\n",
      "Initial State is  [2, 17, 2]\n",
      "episode 8570, reward 915.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 121\n",
      "Initial State is  [2, 20, 3]\n",
      "episode 8571, reward 686.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 125\n",
      "Initial State is  [3, 15, 2]\n",
      "episode 8572, reward 873.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 123\n",
      "Initial State is  [2, 13, 1]\n",
      "episode 8573, reward 744.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 114\n",
      "Initial State is  [1, 2, 6]\n",
      "episode 8574, reward 941.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 138\n",
      "Initial State is  [1, 17, 6]\n",
      "episode 8575, reward 1215.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 122\n",
      "Initial State is  [0, 8, 6]\n",
      "episode 8576, reward 878.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 125\n",
      "Initial State is  [3, 1, 4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 8577, reward 806.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 124\n",
      "Initial State is  [1, 20, 6]\n",
      "episode 8578, reward 865.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 116\n",
      "Initial State is  [1, 20, 0]\n",
      "episode 8579, reward 974.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 130\n",
      "Initial State is  [2, 21, 6]\n",
      "episode 8580, reward 1079.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 121\n",
      "Initial State is  [2, 9, 5]\n",
      "episode 8581, reward 424.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 117\n",
      "Initial State is  [0, 19, 4]\n",
      "episode 8582, reward 841.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 134\n",
      "Initial State is  [3, 15, 2]\n",
      "episode 8583, reward 1015.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 127\n",
      "Initial State is  [3, 6, 2]\n",
      "episode 8584, reward 775.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 126\n",
      "Initial State is  [1, 7, 4]\n",
      "episode 8585, reward 904.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 138\n",
      "Initial State is  [2, 9, 2]\n",
      "episode 8586, reward 643.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 121\n",
      "Initial State is  [2, 10, 3]\n",
      "episode 8587, reward 989.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 135\n",
      "Initial State is  [1, 15, 3]\n",
      "episode 8588, reward 963.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 144\n",
      "Initial State is  [2, 21, 3]\n",
      "episode 8589, reward 1234.0, memory_length 2000, epsilon 0.009998671593271896, time 736.0, rides 131\n",
      "Initial State is  [3, 12, 0]\n",
      "episode 8590, reward 1201.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 139\n",
      "Initial State is  [3, 10, 4]\n",
      "episode 8591, reward 564.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 135\n",
      "Initial State is  [2, 11, 3]\n",
      "episode 8592, reward 669.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 129\n",
      "Initial State is  [0, 0, 0]\n",
      "episode 8593, reward 894.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 132\n",
      "Initial State is  [3, 9, 4]\n",
      "episode 8594, reward 792.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 132\n",
      "Initial State is  [4, 3, 4]\n",
      "episode 8595, reward 930.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 134\n",
      "Initial State is  [3, 23, 3]\n",
      "episode 8596, reward 849.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 141\n",
      "Initial State is  [1, 7, 2]\n",
      "episode 8597, reward 968.0, memory_length 2000, epsilon 0.009998671593271896, time 743.0, rides 137\n",
      "Initial State is  [4, 16, 5]\n",
      "episode 8598, reward 969.0, memory_length 2000, epsilon 0.009998671593271896, time 740.0, rides 137\n",
      "Initial State is  [0, 16, 0]\n",
      "episode 8599, reward 1074.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 132\n",
      "Initial State is  [0, 15, 6]\n",
      "episode 8600, reward 854.0, memory_length 2000, epsilon 0.009998671593271896, time 745.0, rides 136\n",
      "Initial State is  [3, 6, 4]\n",
      "episode 8601, reward 1166.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 132\n",
      "Initial State is  [1, 15, 6]\n",
      "episode 8602, reward 495.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 139\n",
      "Initial State is  [1, 7, 5]\n",
      "episode 8603, reward 1110.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 138\n",
      "Initial State is  [3, 3, 0]\n",
      "episode 8604, reward 512.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 122\n",
      "Initial State is  [3, 15, 6]\n",
      "episode 8605, reward 676.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 143\n",
      "Initial State is  [3, 4, 1]\n",
      "episode 8606, reward 876.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 136\n",
      "Initial State is  [3, 8, 0]\n",
      "episode 8607, reward 773.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 121\n",
      "Initial State is  [4, 20, 1]\n",
      "episode 8608, reward 745.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 138\n",
      "Initial State is  [2, 23, 6]\n",
      "episode 8609, reward 810.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 140\n",
      "Initial State is  [1, 7, 4]\n",
      "episode 8610, reward 1085.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 137\n",
      "Initial State is  [3, 19, 2]\n",
      "episode 8611, reward 956.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 129\n",
      "Initial State is  [0, 8, 0]\n",
      "episode 8612, reward 779.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 138\n",
      "Initial State is  [1, 18, 6]\n",
      "episode 8613, reward 920.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 144\n",
      "Initial State is  [2, 14, 3]\n",
      "episode 8614, reward 595.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 135\n",
      "Initial State is  [4, 9, 4]\n",
      "episode 8615, reward 1025.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 137\n",
      "Initial State is  [1, 6, 6]\n",
      "episode 8616, reward 842.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 139\n",
      "Initial State is  [1, 13, 6]\n",
      "episode 8617, reward 839.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 132\n",
      "Initial State is  [2, 21, 5]\n",
      "episode 8618, reward 881.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 116\n",
      "Initial State is  [2, 21, 6]\n",
      "episode 8619, reward 734.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 127\n",
      "Initial State is  [2, 10, 1]\n",
      "episode 8620, reward 1090.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 139\n",
      "Initial State is  [2, 2, 2]\n",
      "episode 8621, reward 1040.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 130\n",
      "Initial State is  [0, 19, 5]\n",
      "episode 8622, reward 1494.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 134\n",
      "Initial State is  [1, 22, 3]\n",
      "episode 8623, reward 811.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 127\n",
      "Initial State is  [1, 2, 6]\n",
      "episode 8624, reward 1181.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 132\n",
      "Initial State is  [4, 7, 5]\n",
      "episode 8625, reward 904.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 133\n",
      "Initial State is  [4, 11, 0]\n",
      "episode 8626, reward 824.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 143\n",
      "Initial State is  [3, 14, 1]\n",
      "episode 8627, reward 672.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 137\n",
      "Initial State is  [1, 21, 3]\n",
      "episode 8628, reward 1351.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 129\n",
      "Initial State is  [1, 8, 5]\n",
      "episode 8629, reward 737.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 126\n",
      "Initial State is  [0, 9, 1]\n",
      "episode 8630, reward 915.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 127\n",
      "Initial State is  [4, 0, 5]\n",
      "episode 8631, reward 697.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 156\n",
      "Initial State is  [4, 21, 1]\n",
      "episode 8632, reward 963.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 127\n",
      "Initial State is  [3, 18, 3]\n",
      "episode 8633, reward 958.0, memory_length 2000, epsilon 0.009998671593271896, time 721.0, rides 130\n",
      "Initial State is  [1, 12, 3]\n",
      "episode 8634, reward 970.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 133\n",
      "Initial State is  [2, 5, 2]\n",
      "episode 8635, reward 959.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 149\n",
      "Initial State is  [4, 12, 4]\n",
      "episode 8636, reward 1026.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 139\n",
      "Initial State is  [1, 6, 6]\n",
      "episode 8637, reward 838.0, memory_length 2000, epsilon 0.009998671593271896, time 721.0, rides 130\n",
      "Initial State is  [4, 20, 2]\n",
      "episode 8638, reward 754.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 139\n",
      "Initial State is  [0, 5, 1]\n",
      "episode 8639, reward 1083.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 150\n",
      "Initial State is  [1, 6, 5]\n",
      "episode 8640, reward 813.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 128\n",
      "Initial State is  [3, 18, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 8641, reward 929.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 133\n",
      "Initial State is  [4, 6, 2]\n",
      "episode 8642, reward 1016.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 124\n",
      "Initial State is  [2, 8, 2]\n",
      "episode 8643, reward 674.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 127\n",
      "Initial State is  [4, 11, 2]\n",
      "episode 8644, reward 806.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 130\n",
      "Initial State is  [4, 8, 6]\n",
      "episode 8645, reward 783.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 139\n",
      "Initial State is  [0, 7, 3]\n",
      "episode 8646, reward 897.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 133\n",
      "Initial State is  [1, 6, 0]\n",
      "episode 8647, reward 871.0, memory_length 2000, epsilon 0.009998671593271896, time 737.0, rides 128\n",
      "Initial State is  [3, 23, 5]\n",
      "episode 8648, reward 947.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 125\n",
      "Initial State is  [2, 3, 1]\n",
      "episode 8649, reward 920.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 125\n",
      "Initial State is  [3, 9, 4]\n",
      "episode 8650, reward 869.0, memory_length 2000, epsilon 0.009998671593271896, time 742.0, rides 147\n",
      "Initial State is  [0, 18, 6]\n",
      "episode 8651, reward 888.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 115\n",
      "Initial State is  [1, 1, 4]\n",
      "episode 8652, reward 1094.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 125\n",
      "Initial State is  [3, 0, 0]\n",
      "episode 8653, reward 968.0, memory_length 2000, epsilon 0.009998671593271896, time 739.0, rides 122\n",
      "Initial State is  [4, 21, 2]\n",
      "episode 8654, reward 934.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 126\n",
      "Initial State is  [2, 4, 0]\n",
      "episode 8655, reward 1014.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 128\n",
      "Initial State is  [3, 14, 6]\n",
      "episode 8656, reward 844.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 122\n",
      "Initial State is  [1, 6, 4]\n",
      "episode 8657, reward 885.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 139\n",
      "Initial State is  [1, 16, 0]\n",
      "episode 8658, reward 897.0, memory_length 2000, epsilon 0.009998671593271896, time 737.0, rides 133\n",
      "Initial State is  [4, 2, 2]\n",
      "episode 8659, reward 988.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 148\n",
      "Initial State is  [4, 0, 3]\n",
      "episode 8660, reward 875.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 135\n",
      "Initial State is  [1, 20, 1]\n",
      "episode 8661, reward 975.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 131\n",
      "Initial State is  [0, 6, 1]\n",
      "episode 8662, reward 1187.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 143\n",
      "Initial State is  [1, 14, 3]\n",
      "episode 8663, reward 1069.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 147\n",
      "Initial State is  [1, 13, 0]\n",
      "episode 8664, reward 766.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 143\n",
      "Initial State is  [0, 5, 3]\n",
      "episode 8665, reward 897.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 126\n",
      "Initial State is  [4, 12, 0]\n",
      "episode 8666, reward 698.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 138\n",
      "Initial State is  [3, 4, 3]\n",
      "episode 8667, reward 900.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 146\n",
      "Initial State is  [2, 8, 5]\n",
      "episode 8668, reward 957.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 119\n",
      "Initial State is  [4, 13, 1]\n",
      "episode 8669, reward 950.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 126\n",
      "Initial State is  [3, 11, 6]\n",
      "episode 8670, reward 1025.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 119\n",
      "Initial State is  [0, 15, 1]\n",
      "episode 8671, reward 929.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 149\n",
      "Initial State is  [4, 5, 2]\n",
      "episode 8672, reward 1366.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 138\n",
      "Initial State is  [4, 11, 3]\n",
      "episode 8673, reward 822.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 127\n",
      "Initial State is  [4, 12, 3]\n",
      "episode 8674, reward 925.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 143\n",
      "Initial State is  [0, 19, 0]\n",
      "episode 8675, reward 1105.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 131\n",
      "Initial State is  [4, 16, 2]\n",
      "episode 8676, reward 1001.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 131\n",
      "Initial State is  [4, 11, 1]\n",
      "episode 8677, reward 704.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 142\n",
      "Initial State is  [1, 21, 6]\n",
      "episode 8678, reward 1260.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 142\n",
      "Initial State is  [1, 22, 1]\n",
      "episode 8679, reward 847.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 146\n",
      "Initial State is  [1, 16, 4]\n",
      "episode 8680, reward 1184.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 129\n",
      "Initial State is  [0, 15, 4]\n",
      "episode 8681, reward 938.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 134\n",
      "Initial State is  [1, 16, 6]\n",
      "episode 8682, reward 962.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 128\n",
      "Initial State is  [2, 16, 5]\n",
      "episode 8683, reward 1168.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 134\n",
      "Initial State is  [2, 20, 1]\n",
      "episode 8684, reward 774.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 125\n",
      "Initial State is  [4, 13, 5]\n",
      "episode 8685, reward 858.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 134\n",
      "Initial State is  [0, 15, 5]\n",
      "episode 8686, reward 772.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 126\n",
      "Initial State is  [3, 23, 5]\n",
      "episode 8687, reward 1022.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 128\n",
      "Initial State is  [3, 11, 0]\n",
      "episode 8688, reward 955.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 135\n",
      "Initial State is  [4, 0, 6]\n",
      "episode 8689, reward 1027.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 131\n",
      "Initial State is  [0, 23, 3]\n",
      "episode 8690, reward 933.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 134\n",
      "Initial State is  [1, 12, 4]\n",
      "episode 8691, reward 1238.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 145\n",
      "Initial State is  [0, 17, 4]\n",
      "episode 8692, reward 1408.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 137\n",
      "Initial State is  [0, 20, 6]\n",
      "episode 8693, reward 707.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 144\n",
      "Initial State is  [2, 14, 2]\n",
      "episode 8694, reward 1012.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 139\n",
      "Initial State is  [1, 3, 6]\n",
      "episode 8695, reward 798.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 131\n",
      "Initial State is  [0, 14, 4]\n",
      "episode 8696, reward 1115.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 146\n",
      "Initial State is  [1, 17, 1]\n",
      "episode 8697, reward 880.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 136\n",
      "Initial State is  [2, 13, 6]\n",
      "episode 8698, reward 1082.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 130\n",
      "Initial State is  [4, 9, 1]\n",
      "episode 8699, reward 932.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 130\n",
      "Initial State is  [4, 17, 4]\n",
      "episode 8700, reward 930.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 124\n",
      "Initial State is  [3, 13, 0]\n",
      "episode 8701, reward 1098.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 145\n",
      "Initial State is  [4, 1, 2]\n",
      "episode 8702, reward 1035.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 134\n",
      "Initial State is  [2, 21, 3]\n",
      "episode 8703, reward 930.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 150\n",
      "Initial State is  [4, 3, 1]\n",
      "episode 8704, reward 991.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 127\n",
      "Initial State is  [4, 16, 1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 8705, reward 789.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 129\n",
      "Initial State is  [1, 0, 5]\n",
      "episode 8706, reward 821.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 139\n",
      "Initial State is  [3, 16, 5]\n",
      "episode 8707, reward 871.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 133\n",
      "Initial State is  [2, 10, 5]\n",
      "episode 8708, reward 868.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 131\n",
      "Initial State is  [0, 21, 0]\n",
      "episode 8709, reward 980.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 131\n",
      "Initial State is  [2, 0, 0]\n",
      "episode 8710, reward 1040.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 130\n",
      "Initial State is  [3, 22, 3]\n",
      "episode 8711, reward 932.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 130\n",
      "Initial State is  [0, 9, 3]\n",
      "episode 8712, reward 947.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 137\n",
      "Initial State is  [0, 10, 4]\n",
      "episode 8713, reward 1339.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 136\n",
      "Initial State is  [0, 17, 2]\n",
      "episode 8714, reward 979.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 139\n",
      "Initial State is  [3, 0, 2]\n",
      "episode 8715, reward 1082.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 145\n",
      "Initial State is  [2, 9, 5]\n",
      "episode 8716, reward 881.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 131\n",
      "Initial State is  [4, 23, 5]\n",
      "episode 8717, reward 688.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 145\n",
      "Initial State is  [3, 22, 2]\n",
      "episode 8718, reward 1218.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 139\n",
      "Initial State is  [2, 8, 5]\n",
      "episode 8719, reward 1195.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 138\n",
      "Initial State is  [3, 2, 5]\n",
      "episode 8720, reward 957.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 142\n",
      "Initial State is  [0, 2, 2]\n",
      "episode 8721, reward 956.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 135\n",
      "Initial State is  [1, 22, 0]\n",
      "episode 8722, reward 1117.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 131\n",
      "Initial State is  [3, 11, 0]\n",
      "episode 8723, reward 1140.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 126\n",
      "Initial State is  [0, 19, 2]\n",
      "episode 8724, reward 1107.0, memory_length 2000, epsilon 0.009998671593271896, time 736.0, rides 129\n",
      "Initial State is  [2, 13, 1]\n",
      "episode 8725, reward 837.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 135\n",
      "Initial State is  [1, 6, 2]\n",
      "episode 8726, reward 1040.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 124\n",
      "Initial State is  [3, 19, 3]\n",
      "episode 8727, reward 807.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 143\n",
      "Initial State is  [4, 1, 4]\n",
      "episode 8728, reward 890.0, memory_length 2000, epsilon 0.009998671593271896, time 738.0, rides 132\n",
      "Initial State is  [3, 17, 6]\n",
      "episode 8729, reward 1099.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 126\n",
      "Initial State is  [4, 11, 2]\n",
      "episode 8730, reward 851.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 133\n",
      "Initial State is  [0, 5, 5]\n",
      "episode 8731, reward 1159.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 141\n",
      "Initial State is  [2, 1, 5]\n",
      "episode 8732, reward 936.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 139\n",
      "Initial State is  [3, 16, 5]\n",
      "episode 8733, reward 974.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 138\n",
      "Initial State is  [3, 16, 3]\n",
      "episode 8734, reward 1193.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 128\n",
      "Initial State is  [1, 7, 1]\n",
      "episode 8735, reward 925.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 131\n",
      "Initial State is  [3, 20, 4]\n",
      "episode 8736, reward 962.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 137\n",
      "Initial State is  [4, 23, 4]\n",
      "episode 8737, reward 676.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 137\n",
      "Initial State is  [3, 10, 0]\n",
      "episode 8738, reward 1126.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 145\n",
      "Initial State is  [3, 4, 6]\n",
      "episode 8739, reward 983.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 138\n",
      "Initial State is  [2, 2, 3]\n",
      "episode 8740, reward 1018.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 128\n",
      "Initial State is  [2, 1, 6]\n",
      "episode 8741, reward 929.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 126\n",
      "Initial State is  [2, 9, 1]\n",
      "episode 8742, reward 1150.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 145\n",
      "Initial State is  [2, 1, 3]\n",
      "episode 8743, reward 1031.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 135\n",
      "Initial State is  [2, 17, 5]\n",
      "episode 8744, reward 1013.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 148\n",
      "Initial State is  [1, 16, 4]\n",
      "episode 8745, reward 1098.0, memory_length 2000, epsilon 0.009998671593271896, time 736.0, rides 132\n",
      "Initial State is  [1, 15, 2]\n",
      "episode 8746, reward 890.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 141\n",
      "Initial State is  [2, 18, 2]\n",
      "episode 8747, reward 1022.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 135\n",
      "Initial State is  [0, 4, 3]\n",
      "episode 8748, reward 1200.0, memory_length 2000, epsilon 0.009998671593271896, time 739.0, rides 139\n",
      "Initial State is  [3, 22, 6]\n",
      "episode 8749, reward 909.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 147\n",
      "Initial State is  [3, 7, 2]\n",
      "episode 8750, reward 828.0, memory_length 2000, epsilon 0.009998671593271896, time 736.0, rides 144\n",
      "Initial State is  [3, 1, 5]\n",
      "episode 8751, reward 1105.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 141\n",
      "Initial State is  [3, 0, 1]\n",
      "episode 8752, reward 938.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 134\n",
      "Initial State is  [3, 18, 0]\n",
      "episode 8753, reward 920.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 130\n",
      "Initial State is  [4, 7, 5]\n",
      "episode 8754, reward 1196.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 136\n",
      "Initial State is  [3, 15, 3]\n",
      "episode 8755, reward 971.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 144\n",
      "Initial State is  [1, 10, 1]\n",
      "episode 8756, reward 1154.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 140\n",
      "Initial State is  [2, 10, 5]\n",
      "episode 8757, reward 868.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 140\n",
      "Initial State is  [4, 9, 2]\n",
      "episode 8758, reward 841.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 138\n",
      "Initial State is  [1, 21, 1]\n",
      "episode 8759, reward 1085.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 146\n",
      "Initial State is  [0, 8, 3]\n",
      "episode 8760, reward 1337.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 136\n",
      "Initial State is  [0, 20, 1]\n",
      "episode 8761, reward 901.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 133\n",
      "Initial State is  [1, 16, 0]\n",
      "episode 8762, reward 756.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 132\n",
      "Initial State is  [2, 21, 5]\n",
      "episode 8763, reward 1155.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 146\n",
      "Initial State is  [3, 1, 4]\n",
      "episode 8764, reward 668.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 129\n",
      "Initial State is  [1, 15, 1]\n",
      "episode 8765, reward 716.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 131\n",
      "Initial State is  [2, 2, 4]\n",
      "episode 8766, reward 1067.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 139\n",
      "Initial State is  [0, 13, 6]\n",
      "episode 8767, reward 832.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 141\n",
      "Initial State is  [4, 22, 4]\n",
      "episode 8768, reward 854.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 134\n",
      "Initial State is  [4, 11, 3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 8769, reward 930.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 141\n",
      "Initial State is  [0, 2, 1]\n",
      "episode 8770, reward 1080.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 136\n",
      "Initial State is  [1, 2, 3]\n",
      "episode 8771, reward 966.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 133\n",
      "Initial State is  [3, 1, 5]\n",
      "episode 8772, reward 788.0, memory_length 2000, epsilon 0.009998671593271896, time 740.0, rides 130\n",
      "Initial State is  [0, 8, 5]\n",
      "episode 8773, reward 863.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 133\n",
      "Initial State is  [2, 17, 6]\n",
      "episode 8774, reward 880.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 135\n",
      "Initial State is  [4, 6, 6]\n",
      "episode 8775, reward 822.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 145\n",
      "Initial State is  [0, 1, 4]\n",
      "episode 8776, reward 921.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 140\n",
      "Initial State is  [4, 15, 5]\n",
      "episode 8777, reward 835.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 133\n",
      "Initial State is  [2, 16, 1]\n",
      "episode 8778, reward 697.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 122\n",
      "Initial State is  [0, 8, 1]\n",
      "episode 8779, reward 1038.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 143\n",
      "Initial State is  [2, 2, 1]\n",
      "episode 8780, reward 863.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 144\n",
      "Initial State is  [0, 17, 2]\n",
      "episode 8781, reward 1122.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 123\n",
      "Initial State is  [3, 19, 0]\n",
      "episode 8782, reward 834.0, memory_length 2000, epsilon 0.009998671593271896, time 737.0, rides 131\n",
      "Initial State is  [0, 2, 5]\n",
      "episode 8783, reward 1028.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 125\n",
      "Initial State is  [4, 20, 2]\n",
      "episode 8784, reward 920.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 138\n",
      "Initial State is  [0, 3, 4]\n",
      "episode 8785, reward 797.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 137\n",
      "Initial State is  [4, 18, 0]\n",
      "episode 8786, reward 764.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 138\n",
      "Initial State is  [2, 18, 2]\n",
      "episode 8787, reward 900.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 129\n",
      "Initial State is  [4, 7, 5]\n",
      "episode 8788, reward 975.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 138\n",
      "Initial State is  [0, 16, 0]\n",
      "episode 8789, reward 972.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 134\n",
      "Initial State is  [4, 5, 2]\n",
      "episode 8790, reward 794.0, memory_length 2000, epsilon 0.009998671593271896, time 737.0, rides 141\n",
      "Initial State is  [2, 17, 1]\n",
      "episode 8791, reward 926.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 137\n",
      "Initial State is  [1, 0, 6]\n",
      "episode 8792, reward 1036.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 146\n",
      "Initial State is  [2, 5, 2]\n",
      "episode 8793, reward 962.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 128\n",
      "Initial State is  [3, 23, 5]\n",
      "episode 8794, reward 1018.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 129\n",
      "Initial State is  [1, 8, 1]\n",
      "episode 8795, reward 1099.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 139\n",
      "Initial State is  [0, 15, 2]\n",
      "episode 8796, reward 716.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 130\n",
      "Initial State is  [1, 0, 1]\n",
      "episode 8797, reward 962.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 129\n",
      "Initial State is  [4, 2, 3]\n",
      "episode 8798, reward 1022.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 135\n",
      "Initial State is  [4, 20, 2]\n",
      "episode 8799, reward 781.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 132\n",
      "Initial State is  [4, 1, 4]\n",
      "episode 8800, reward 782.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 140\n",
      "Initial State is  [1, 20, 2]\n",
      "episode 8801, reward 681.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 141\n",
      "Initial State is  [4, 21, 2]\n",
      "episode 8802, reward 1184.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 150\n",
      "Initial State is  [0, 16, 2]\n",
      "episode 8803, reward 1059.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 138\n",
      "Initial State is  [4, 12, 0]\n",
      "episode 8804, reward 979.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 122\n",
      "Initial State is  [2, 4, 5]\n",
      "episode 8805, reward 964.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 142\n",
      "Initial State is  [2, 10, 1]\n",
      "episode 8806, reward 875.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 133\n",
      "Initial State is  [1, 9, 1]\n",
      "episode 8807, reward 1348.0, memory_length 2000, epsilon 0.009998671593271896, time 737.0, rides 130\n",
      "Initial State is  [1, 8, 3]\n",
      "episode 8808, reward 793.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 131\n",
      "Initial State is  [1, 2, 5]\n",
      "episode 8809, reward 1137.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 139\n",
      "Initial State is  [2, 0, 1]\n",
      "episode 8810, reward 1079.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 124\n",
      "Initial State is  [1, 12, 6]\n",
      "episode 8811, reward 699.0, memory_length 2000, epsilon 0.009998671593271896, time 746.0, rides 128\n",
      "Initial State is  [2, 12, 1]\n",
      "episode 8812, reward 894.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 140\n",
      "Initial State is  [4, 7, 3]\n",
      "episode 8813, reward 934.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 131\n",
      "Initial State is  [4, 17, 0]\n",
      "episode 8814, reward 879.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 143\n",
      "Initial State is  [1, 0, 2]\n",
      "episode 8815, reward 934.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 138\n",
      "Initial State is  [2, 19, 2]\n",
      "episode 8816, reward 1001.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 140\n",
      "Initial State is  [2, 7, 1]\n",
      "episode 8817, reward 1111.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 126\n",
      "Initial State is  [0, 15, 5]\n",
      "episode 8818, reward 855.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 130\n",
      "Initial State is  [0, 4, 2]\n",
      "episode 8819, reward 828.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 135\n",
      "Initial State is  [3, 4, 1]\n",
      "episode 8820, reward 1019.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 124\n",
      "Initial State is  [2, 10, 2]\n",
      "episode 8821, reward 1003.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 131\n",
      "Initial State is  [1, 21, 6]\n",
      "episode 8822, reward 1000.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 132\n",
      "Initial State is  [1, 15, 2]\n",
      "episode 8823, reward 828.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 139\n",
      "Initial State is  [4, 4, 4]\n",
      "episode 8824, reward 795.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 132\n",
      "Initial State is  [0, 7, 6]\n",
      "episode 8825, reward 1168.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 142\n",
      "Initial State is  [2, 21, 6]\n",
      "episode 8826, reward 882.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 134\n",
      "Initial State is  [0, 3, 4]\n",
      "episode 8827, reward 1018.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 132\n",
      "Initial State is  [2, 12, 2]\n",
      "episode 8828, reward 899.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 133\n",
      "Initial State is  [3, 10, 4]\n",
      "episode 8829, reward 896.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 132\n",
      "Initial State is  [3, 15, 3]\n",
      "episode 8830, reward 1163.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 130\n",
      "Initial State is  [2, 20, 0]\n",
      "episode 8831, reward 1076.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 136\n",
      "Initial State is  [4, 1, 6]\n",
      "episode 8832, reward 1034.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 136\n",
      "Initial State is  [3, 2, 2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 8833, reward 970.0, memory_length 2000, epsilon 0.009998671593271896, time 737.0, rides 144\n",
      "Initial State is  [1, 22, 0]\n",
      "episode 8834, reward 893.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 132\n",
      "Initial State is  [0, 8, 5]\n",
      "episode 8835, reward 892.0, memory_length 2000, epsilon 0.009998671593271896, time 742.0, rides 136\n",
      "Initial State is  [3, 22, 0]\n",
      "episode 8836, reward 660.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 128\n",
      "Initial State is  [3, 10, 3]\n",
      "episode 8837, reward 994.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 148\n",
      "Initial State is  [4, 6, 3]\n",
      "episode 8838, reward 950.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 157\n",
      "Initial State is  [4, 19, 2]\n",
      "episode 8839, reward 801.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 149\n",
      "Initial State is  [3, 4, 1]\n",
      "episode 8840, reward 996.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 138\n",
      "Initial State is  [2, 4, 1]\n",
      "episode 8841, reward 877.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 141\n",
      "Initial State is  [3, 14, 6]\n",
      "episode 8842, reward 1085.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 136\n",
      "Initial State is  [1, 10, 3]\n",
      "episode 8843, reward 1135.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 142\n",
      "Initial State is  [1, 20, 1]\n",
      "episode 8844, reward 828.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 137\n",
      "Initial State is  [4, 23, 3]\n",
      "episode 8845, reward 899.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 143\n",
      "Initial State is  [3, 21, 0]\n",
      "episode 8846, reward 1250.0, memory_length 2000, epsilon 0.009998671593271896, time 737.0, rides 127\n",
      "Initial State is  [1, 8, 2]\n",
      "episode 8847, reward 1044.0, memory_length 2000, epsilon 0.009998671593271896, time 736.0, rides 140\n",
      "Initial State is  [4, 18, 4]\n",
      "episode 8848, reward 968.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 151\n",
      "Initial State is  [1, 12, 4]\n",
      "episode 8849, reward 1225.0, memory_length 2000, epsilon 0.009998671593271896, time 739.0, rides 136\n",
      "Initial State is  [3, 12, 5]\n",
      "episode 8850, reward 947.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 143\n",
      "Initial State is  [3, 5, 6]\n",
      "episode 8851, reward 784.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 144\n",
      "Initial State is  [3, 8, 5]\n",
      "episode 8852, reward 899.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 142\n",
      "Initial State is  [3, 19, 3]\n",
      "episode 8853, reward 1031.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 128\n",
      "Initial State is  [0, 8, 5]\n",
      "episode 8854, reward 1050.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 138\n",
      "Initial State is  [2, 9, 6]\n",
      "episode 8855, reward 1041.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 150\n",
      "Initial State is  [4, 19, 0]\n",
      "episode 8856, reward 1110.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 130\n",
      "Initial State is  [4, 14, 3]\n",
      "episode 8857, reward 616.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 142\n",
      "Initial State is  [3, 16, 4]\n",
      "episode 8858, reward 966.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 140\n",
      "Initial State is  [2, 7, 1]\n",
      "episode 8859, reward 880.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 129\n",
      "Initial State is  [3, 21, 1]\n",
      "episode 8860, reward 1162.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 152\n",
      "Initial State is  [2, 4, 1]\n",
      "episode 8861, reward 1219.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 144\n",
      "Initial State is  [1, 2, 5]\n",
      "episode 8862, reward 992.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 134\n",
      "Initial State is  [0, 1, 0]\n",
      "episode 8863, reward 971.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 132\n",
      "Initial State is  [4, 11, 4]\n",
      "episode 8864, reward 907.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 133\n",
      "Initial State is  [4, 9, 3]\n",
      "episode 8865, reward 981.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 134\n",
      "Initial State is  [2, 20, 3]\n",
      "episode 8866, reward 889.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 138\n",
      "Initial State is  [2, 21, 2]\n",
      "episode 8867, reward 969.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 136\n",
      "Initial State is  [4, 11, 2]\n",
      "episode 8868, reward 1230.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 135\n",
      "Initial State is  [3, 8, 5]\n",
      "episode 8869, reward 858.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 124\n",
      "Initial State is  [0, 20, 2]\n",
      "episode 8870, reward 1160.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 123\n",
      "Initial State is  [4, 5, 6]\n",
      "episode 8871, reward 1005.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 134\n",
      "Initial State is  [3, 16, 4]\n",
      "episode 8872, reward 781.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 145\n",
      "Initial State is  [2, 8, 3]\n",
      "episode 8873, reward 862.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 145\n",
      "Initial State is  [1, 23, 4]\n",
      "episode 8874, reward 973.0, memory_length 2000, epsilon 0.009998671593271896, time 721.0, rides 145\n",
      "Initial State is  [3, 15, 5]\n",
      "episode 8875, reward 841.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 138\n",
      "Initial State is  [4, 11, 2]\n",
      "episode 8876, reward 1116.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 145\n",
      "Initial State is  [3, 18, 5]\n",
      "episode 8877, reward 823.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 131\n",
      "Initial State is  [2, 3, 6]\n",
      "episode 8878, reward 804.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 137\n",
      "Initial State is  [3, 11, 0]\n",
      "episode 8879, reward 826.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 134\n",
      "Initial State is  [1, 5, 5]\n",
      "episode 8880, reward 856.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 146\n",
      "Initial State is  [3, 2, 3]\n",
      "episode 8881, reward 796.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 144\n",
      "Initial State is  [1, 8, 4]\n",
      "episode 8882, reward 733.0, memory_length 2000, epsilon 0.009998671593271896, time 739.0, rides 141\n",
      "Initial State is  [4, 10, 2]\n",
      "episode 8883, reward 1052.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 135\n",
      "Initial State is  [4, 2, 2]\n",
      "episode 8884, reward 972.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 136\n",
      "Initial State is  [0, 3, 0]\n",
      "episode 8885, reward 1124.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 148\n",
      "Initial State is  [3, 15, 1]\n",
      "episode 8886, reward 1216.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 143\n",
      "Initial State is  [2, 21, 5]\n",
      "episode 8887, reward 847.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 138\n",
      "Initial State is  [4, 3, 6]\n",
      "episode 8888, reward 1200.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 122\n",
      "Initial State is  [3, 3, 4]\n",
      "episode 8889, reward 932.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 123\n",
      "Initial State is  [2, 15, 6]\n",
      "episode 8890, reward 873.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 137\n",
      "Initial State is  [4, 21, 4]\n",
      "episode 8891, reward 964.0, memory_length 2000, epsilon 0.009998671593271896, time 736.0, rides 145\n",
      "Initial State is  [4, 7, 2]\n",
      "episode 8892, reward 1079.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 130\n",
      "Initial State is  [0, 0, 1]\n",
      "episode 8893, reward 1110.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 143\n",
      "Initial State is  [3, 9, 5]\n",
      "episode 8894, reward 742.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 122\n",
      "Initial State is  [2, 5, 6]\n",
      "episode 8895, reward 877.0, memory_length 2000, epsilon 0.009998671593271896, time 739.0, rides 147\n",
      "Initial State is  [2, 22, 5]\n",
      "episode 8896, reward 1205.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 131\n",
      "Initial State is  [2, 16, 3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 8897, reward 936.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 133\n",
      "Initial State is  [0, 3, 1]\n",
      "episode 8898, reward 923.0, memory_length 2000, epsilon 0.009998671593271896, time 737.0, rides 137\n",
      "Initial State is  [4, 5, 1]\n",
      "episode 8899, reward 1186.0, memory_length 2000, epsilon 0.009998671593271896, time 721.0, rides 128\n",
      "Initial State is  [2, 0, 6]\n",
      "episode 8900, reward 843.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 128\n",
      "Initial State is  [4, 0, 6]\n",
      "episode 8901, reward 828.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 128\n",
      "Initial State is  [3, 23, 5]\n",
      "episode 8902, reward 734.0, memory_length 2000, epsilon 0.009998671593271896, time 739.0, rides 144\n",
      "Initial State is  [0, 5, 1]\n",
      "episode 8903, reward 1048.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 131\n",
      "Initial State is  [4, 18, 4]\n",
      "episode 8904, reward 510.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 137\n",
      "Initial State is  [2, 10, 3]\n",
      "episode 8905, reward 1217.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 134\n",
      "Initial State is  [1, 17, 2]\n",
      "episode 8906, reward 747.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 132\n",
      "Initial State is  [1, 10, 2]\n",
      "episode 8907, reward 746.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 145\n",
      "Initial State is  [4, 20, 6]\n",
      "episode 8908, reward 1277.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 145\n",
      "Initial State is  [2, 14, 3]\n",
      "episode 8909, reward 952.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 129\n",
      "Initial State is  [2, 2, 6]\n",
      "episode 8910, reward 724.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 133\n",
      "Initial State is  [4, 20, 3]\n",
      "episode 8911, reward 814.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 130\n",
      "Initial State is  [2, 3, 4]\n",
      "episode 8912, reward 774.0, memory_length 2000, epsilon 0.009998671593271896, time 736.0, rides 116\n",
      "Initial State is  [4, 2, 5]\n",
      "episode 8913, reward 1226.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 133\n",
      "Initial State is  [1, 23, 6]\n",
      "episode 8914, reward 1100.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 140\n",
      "Initial State is  [4, 5, 0]\n",
      "episode 8915, reward 779.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 140\n",
      "Initial State is  [3, 7, 6]\n",
      "episode 8916, reward 914.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 135\n",
      "Initial State is  [1, 9, 4]\n",
      "episode 8917, reward 1279.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 151\n",
      "Initial State is  [2, 9, 2]\n",
      "episode 8918, reward 1002.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 136\n",
      "Initial State is  [2, 8, 2]\n",
      "episode 8919, reward 692.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 133\n",
      "Initial State is  [4, 13, 1]\n",
      "episode 8920, reward 1138.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 140\n",
      "Initial State is  [1, 13, 1]\n",
      "episode 8921, reward 806.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 134\n",
      "Initial State is  [1, 19, 3]\n",
      "episode 8922, reward 974.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 150\n",
      "Initial State is  [0, 9, 2]\n",
      "episode 8923, reward 808.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 136\n",
      "Initial State is  [3, 8, 4]\n",
      "episode 8924, reward 893.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 134\n",
      "Initial State is  [2, 7, 5]\n",
      "episode 8925, reward 1117.0, memory_length 2000, epsilon 0.009998671593271896, time 737.0, rides 132\n",
      "Initial State is  [0, 9, 0]\n",
      "episode 8926, reward 802.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 130\n",
      "Initial State is  [1, 12, 3]\n",
      "episode 8927, reward 935.0, memory_length 2000, epsilon 0.009998671593271896, time 721.0, rides 132\n",
      "Initial State is  [3, 8, 5]\n",
      "episode 8928, reward 976.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 131\n",
      "Initial State is  [2, 14, 5]\n",
      "episode 8929, reward 1016.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 140\n",
      "Initial State is  [4, 17, 1]\n",
      "episode 8930, reward 714.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 127\n",
      "Initial State is  [0, 3, 1]\n",
      "episode 8931, reward 1240.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 129\n",
      "Initial State is  [4, 10, 0]\n",
      "episode 8932, reward 912.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 131\n",
      "Initial State is  [2, 7, 0]\n",
      "episode 8933, reward 1020.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 150\n",
      "Initial State is  [3, 16, 5]\n",
      "episode 8934, reward 1045.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 131\n",
      "Initial State is  [1, 16, 2]\n",
      "episode 8935, reward 906.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 139\n",
      "Initial State is  [1, 6, 5]\n",
      "episode 8936, reward 788.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 118\n",
      "Initial State is  [3, 8, 5]\n",
      "episode 8937, reward 1039.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 144\n",
      "Initial State is  [4, 1, 5]\n",
      "episode 8938, reward 871.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 130\n",
      "Initial State is  [1, 22, 1]\n",
      "episode 8939, reward 1074.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 132\n",
      "Initial State is  [0, 2, 4]\n",
      "episode 8940, reward 1030.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 126\n",
      "Initial State is  [2, 8, 4]\n",
      "episode 8941, reward 1083.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 139\n",
      "Initial State is  [3, 8, 3]\n",
      "episode 8942, reward 1044.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 136\n",
      "Initial State is  [4, 15, 5]\n",
      "episode 8943, reward 902.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 121\n",
      "Initial State is  [4, 17, 5]\n",
      "episode 8944, reward 1022.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 130\n",
      "Initial State is  [2, 12, 6]\n",
      "episode 8945, reward 894.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 127\n",
      "Initial State is  [1, 1, 6]\n",
      "episode 8946, reward 858.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 132\n",
      "Initial State is  [2, 10, 4]\n",
      "episode 8947, reward 876.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 125\n",
      "Initial State is  [1, 2, 2]\n",
      "episode 8948, reward 1126.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 138\n",
      "Initial State is  [0, 17, 4]\n",
      "episode 8949, reward 871.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 129\n",
      "Initial State is  [3, 17, 0]\n",
      "episode 8950, reward 764.0, memory_length 2000, epsilon 0.009998671593271896, time 721.0, rides 138\n",
      "Initial State is  [2, 17, 6]\n",
      "episode 8951, reward 1098.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 133\n",
      "Initial State is  [2, 12, 2]\n",
      "episode 8952, reward 921.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 128\n",
      "Initial State is  [4, 11, 2]\n",
      "episode 8953, reward 1084.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 139\n",
      "Initial State is  [4, 1, 1]\n",
      "episode 8954, reward 1007.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 134\n",
      "Initial State is  [4, 0, 2]\n",
      "episode 8955, reward 1138.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 138\n",
      "Initial State is  [1, 7, 0]\n",
      "episode 8956, reward 947.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 146\n",
      "Initial State is  [2, 14, 5]\n",
      "episode 8957, reward 1104.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 125\n",
      "Initial State is  [4, 3, 1]\n",
      "episode 8958, reward 795.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 142\n",
      "Initial State is  [4, 12, 5]\n",
      "episode 8959, reward 878.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 142\n",
      "Initial State is  [0, 7, 2]\n",
      "episode 8960, reward 858.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 126\n",
      "Initial State is  [2, 2, 2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 8961, reward 778.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 122\n",
      "Initial State is  [0, 7, 6]\n",
      "episode 8962, reward 789.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 129\n",
      "Initial State is  [4, 9, 0]\n",
      "episode 8963, reward 852.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 145\n",
      "Initial State is  [0, 4, 5]\n",
      "episode 8964, reward 1158.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 139\n",
      "Initial State is  [4, 23, 2]\n",
      "episode 8965, reward 1040.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 138\n",
      "Initial State is  [0, 19, 5]\n",
      "episode 8966, reward 1068.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 137\n",
      "Initial State is  [1, 23, 2]\n",
      "episode 8967, reward 1126.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 125\n",
      "Initial State is  [1, 22, 4]\n",
      "episode 8968, reward 923.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 132\n",
      "Initial State is  [0, 17, 6]\n",
      "episode 8969, reward 1103.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 134\n",
      "Initial State is  [3, 12, 0]\n",
      "episode 8970, reward 1258.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 137\n",
      "Initial State is  [0, 11, 0]\n",
      "episode 8971, reward 756.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 149\n",
      "Initial State is  [4, 6, 4]\n",
      "episode 8972, reward 868.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 129\n",
      "Initial State is  [4, 7, 5]\n",
      "episode 8973, reward 666.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 129\n",
      "Initial State is  [2, 20, 5]\n",
      "episode 8974, reward 886.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 132\n",
      "Initial State is  [0, 21, 5]\n",
      "episode 8975, reward 1130.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 140\n",
      "Initial State is  [2, 0, 2]\n",
      "episode 8976, reward 968.0, memory_length 2000, epsilon 0.009998671593271896, time 741.0, rides 119\n",
      "Initial State is  [0, 12, 2]\n",
      "episode 8977, reward 980.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 135\n",
      "Initial State is  [2, 9, 0]\n",
      "episode 8978, reward 972.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 119\n",
      "Initial State is  [1, 16, 4]\n",
      "episode 8979, reward 863.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 138\n",
      "Initial State is  [2, 1, 1]\n",
      "episode 8980, reward 1166.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 143\n",
      "Initial State is  [4, 6, 0]\n",
      "episode 8981, reward 945.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 136\n",
      "Initial State is  [4, 17, 5]\n",
      "episode 8982, reward 975.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 138\n",
      "Initial State is  [3, 2, 1]\n",
      "episode 8983, reward 830.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 128\n",
      "Initial State is  [4, 10, 2]\n",
      "episode 8984, reward 924.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 133\n",
      "Initial State is  [2, 15, 4]\n",
      "episode 8985, reward 1007.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 135\n",
      "Initial State is  [0, 18, 4]\n",
      "episode 8986, reward 875.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 134\n",
      "Initial State is  [2, 2, 0]\n",
      "episode 8987, reward 958.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 137\n",
      "Initial State is  [1, 9, 3]\n",
      "episode 8988, reward 860.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 138\n",
      "Initial State is  [3, 9, 4]\n",
      "episode 8989, reward 1218.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 143\n",
      "Initial State is  [1, 18, 6]\n",
      "episode 8990, reward 688.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 143\n",
      "Initial State is  [0, 22, 3]\n",
      "episode 8991, reward 725.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 136\n",
      "Initial State is  [1, 20, 2]\n",
      "episode 8992, reward 728.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 128\n",
      "Initial State is  [4, 1, 3]\n",
      "episode 8993, reward 941.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 142\n",
      "Initial State is  [1, 22, 3]\n",
      "episode 8994, reward 629.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 129\n",
      "Initial State is  [1, 23, 4]\n",
      "episode 8995, reward 853.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 129\n",
      "Initial State is  [1, 11, 6]\n",
      "episode 8996, reward 895.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 132\n",
      "Initial State is  [2, 20, 0]\n",
      "episode 8997, reward 1168.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 126\n",
      "Initial State is  [2, 14, 3]\n",
      "episode 8998, reward 1114.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 134\n",
      "Initial State is  [4, 11, 2]\n",
      "episode 8999, reward 1025.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 127\n",
      "Initial State is  [0, 17, 4]\n",
      "episode 9000, reward 1155.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 140\n",
      "Initial State is  [2, 2, 6]\n",
      "episode 9001, reward 954.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 152\n",
      "Initial State is  [0, 20, 5]\n",
      "episode 9002, reward 872.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 135\n",
      "Initial State is  [2, 6, 4]\n",
      "episode 9003, reward 991.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 147\n",
      "Initial State is  [4, 13, 0]\n",
      "episode 9004, reward 887.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 137\n",
      "Initial State is  [2, 22, 4]\n",
      "episode 9005, reward 954.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 130\n",
      "Initial State is  [0, 4, 1]\n",
      "episode 9006, reward 972.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 138\n",
      "Initial State is  [3, 13, 2]\n",
      "episode 9007, reward 1010.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 138\n",
      "Initial State is  [2, 10, 4]\n",
      "episode 9008, reward 851.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 134\n",
      "Initial State is  [3, 17, 5]\n",
      "episode 9009, reward 1094.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 137\n",
      "Initial State is  [1, 12, 1]\n",
      "episode 9010, reward 784.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 119\n",
      "Initial State is  [0, 10, 0]\n",
      "episode 9011, reward 1130.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 144\n",
      "Initial State is  [3, 15, 4]\n",
      "episode 9012, reward 999.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 141\n",
      "Initial State is  [1, 8, 2]\n",
      "episode 9013, reward 866.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 134\n",
      "Initial State is  [4, 14, 6]\n",
      "episode 9014, reward 921.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 142\n",
      "Initial State is  [1, 19, 3]\n",
      "episode 9015, reward 861.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 134\n",
      "Initial State is  [4, 14, 3]\n",
      "episode 9016, reward 911.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 145\n",
      "Initial State is  [0, 9, 2]\n",
      "episode 9017, reward 847.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 133\n",
      "Initial State is  [0, 7, 0]\n",
      "episode 9018, reward 899.0, memory_length 2000, epsilon 0.009998671593271896, time 736.0, rides 131\n",
      "Initial State is  [4, 15, 5]\n",
      "episode 9019, reward 554.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 132\n",
      "Initial State is  [1, 18, 4]\n",
      "episode 9020, reward 966.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 143\n",
      "Initial State is  [1, 1, 1]\n",
      "episode 9021, reward 893.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 149\n",
      "Initial State is  [0, 20, 5]\n",
      "episode 9022, reward 1141.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 142\n",
      "Initial State is  [1, 0, 3]\n",
      "episode 9023, reward 947.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 126\n",
      "Initial State is  [3, 10, 1]\n",
      "episode 9024, reward 975.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 136\n",
      "Initial State is  [2, 8, 1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 9025, reward 771.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 119\n",
      "Initial State is  [1, 16, 2]\n",
      "episode 9026, reward 886.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 129\n",
      "Initial State is  [1, 14, 2]\n",
      "episode 9027, reward 1215.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 139\n",
      "Initial State is  [4, 10, 6]\n",
      "episode 9028, reward 1210.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 132\n",
      "Initial State is  [3, 5, 4]\n",
      "episode 9029, reward 1096.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 125\n",
      "Initial State is  [0, 8, 3]\n",
      "episode 9030, reward 685.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 130\n",
      "Initial State is  [4, 5, 5]\n",
      "episode 9031, reward 798.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 143\n",
      "Initial State is  [1, 17, 2]\n",
      "episode 9032, reward 861.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 138\n",
      "Initial State is  [1, 19, 2]\n",
      "episode 9033, reward 1076.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 141\n",
      "Initial State is  [2, 7, 3]\n",
      "episode 9034, reward 942.0, memory_length 2000, epsilon 0.009998671593271896, time 736.0, rides 148\n",
      "Initial State is  [1, 2, 3]\n",
      "episode 9035, reward 997.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 132\n",
      "Initial State is  [3, 2, 1]\n",
      "episode 9036, reward 776.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 136\n",
      "Initial State is  [2, 1, 6]\n",
      "episode 9037, reward 1085.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 143\n",
      "Initial State is  [3, 17, 6]\n",
      "episode 9038, reward 879.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 145\n",
      "Initial State is  [1, 0, 2]\n",
      "episode 9039, reward 1081.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 149\n",
      "Initial State is  [1, 15, 1]\n",
      "episode 9040, reward 1231.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 139\n",
      "Initial State is  [2, 19, 5]\n",
      "episode 9041, reward 1004.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 134\n",
      "Initial State is  [0, 22, 5]\n",
      "episode 9042, reward 1087.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 146\n",
      "Initial State is  [0, 10, 0]\n",
      "episode 9043, reward 1184.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 139\n",
      "Initial State is  [4, 13, 4]\n",
      "episode 9044, reward 886.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 136\n",
      "Initial State is  [0, 16, 5]\n",
      "episode 9045, reward 1293.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 144\n",
      "Initial State is  [4, 7, 1]\n",
      "episode 9046, reward 950.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 139\n",
      "Initial State is  [0, 22, 0]\n",
      "episode 9047, reward 1009.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 133\n",
      "Initial State is  [2, 20, 3]\n",
      "episode 9048, reward 1111.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 138\n",
      "Initial State is  [1, 9, 1]\n",
      "episode 9049, reward 932.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 132\n",
      "Initial State is  [3, 13, 4]\n",
      "episode 9050, reward 944.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 143\n",
      "Initial State is  [4, 5, 0]\n",
      "episode 9051, reward 905.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 141\n",
      "Initial State is  [1, 13, 2]\n",
      "episode 9052, reward 1063.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 126\n",
      "Initial State is  [3, 22, 5]\n",
      "episode 9053, reward 1114.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 145\n",
      "Initial State is  [4, 3, 2]\n",
      "episode 9054, reward 990.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 133\n",
      "Initial State is  [4, 10, 1]\n",
      "episode 9055, reward 1092.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 132\n",
      "Initial State is  [0, 22, 6]\n",
      "episode 9056, reward 674.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 144\n",
      "Initial State is  [0, 8, 0]\n",
      "episode 9057, reward 1157.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 143\n",
      "Initial State is  [3, 20, 5]\n",
      "episode 9058, reward 908.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 132\n",
      "Initial State is  [1, 2, 5]\n",
      "episode 9059, reward 981.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 141\n",
      "Initial State is  [3, 20, 1]\n",
      "episode 9060, reward 1017.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 128\n",
      "Initial State is  [2, 13, 0]\n",
      "episode 9061, reward 983.0, memory_length 2000, epsilon 0.009998671593271896, time 721.0, rides 125\n",
      "Initial State is  [1, 5, 3]\n",
      "episode 9062, reward 1270.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 130\n",
      "Initial State is  [0, 3, 1]\n",
      "episode 9063, reward 442.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 134\n",
      "Initial State is  [0, 13, 2]\n",
      "episode 9064, reward 950.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 131\n",
      "Initial State is  [2, 5, 3]\n",
      "episode 9065, reward 866.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 135\n",
      "Initial State is  [2, 13, 1]\n",
      "episode 9066, reward 825.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 134\n",
      "Initial State is  [0, 4, 6]\n",
      "episode 9067, reward 513.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 139\n",
      "Initial State is  [2, 21, 6]\n",
      "episode 9068, reward 985.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 120\n",
      "Initial State is  [3, 11, 0]\n",
      "episode 9069, reward 874.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 129\n",
      "Initial State is  [0, 23, 5]\n",
      "episode 9070, reward 1057.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 153\n",
      "Initial State is  [1, 12, 1]\n",
      "episode 9071, reward 988.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 118\n",
      "Initial State is  [4, 22, 0]\n",
      "episode 9072, reward 957.0, memory_length 2000, epsilon 0.009998671593271896, time 736.0, rides 133\n",
      "Initial State is  [1, 15, 0]\n",
      "episode 9073, reward 1057.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 129\n",
      "Initial State is  [4, 8, 4]\n",
      "episode 9074, reward 669.0, memory_length 2000, epsilon 0.009998671593271896, time 736.0, rides 136\n",
      "Initial State is  [0, 2, 4]\n",
      "episode 9075, reward 923.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 131\n",
      "Initial State is  [3, 1, 0]\n",
      "episode 9076, reward 955.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 137\n",
      "Initial State is  [2, 23, 2]\n",
      "episode 9077, reward 968.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 132\n",
      "Initial State is  [4, 10, 4]\n",
      "episode 9078, reward 970.0, memory_length 2000, epsilon 0.009998671593271896, time 721.0, rides 141\n",
      "Initial State is  [1, 12, 0]\n",
      "episode 9079, reward 1069.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 147\n",
      "Initial State is  [4, 1, 0]\n",
      "episode 9080, reward 1022.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 138\n",
      "Initial State is  [4, 5, 4]\n",
      "episode 9081, reward 832.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 133\n",
      "Initial State is  [4, 16, 4]\n",
      "episode 9082, reward 1110.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 132\n",
      "Initial State is  [2, 22, 2]\n",
      "episode 9083, reward 1106.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 130\n",
      "Initial State is  [2, 14, 3]\n",
      "episode 9084, reward 899.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 143\n",
      "Initial State is  [1, 21, 0]\n",
      "episode 9085, reward 1021.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 140\n",
      "Initial State is  [2, 22, 6]\n",
      "episode 9086, reward 1039.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 139\n",
      "Initial State is  [2, 18, 2]\n",
      "episode 9087, reward 1077.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 133\n",
      "Initial State is  [3, 10, 0]\n",
      "episode 9088, reward 1017.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 137\n",
      "Initial State is  [0, 14, 6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 9089, reward 755.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 134\n",
      "Initial State is  [0, 1, 6]\n",
      "episode 9090, reward 1077.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 138\n",
      "Initial State is  [1, 2, 2]\n",
      "episode 9091, reward 997.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 131\n",
      "Initial State is  [4, 8, 1]\n",
      "episode 9092, reward 1032.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 138\n",
      "Initial State is  [1, 0, 6]\n",
      "episode 9093, reward 819.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 129\n",
      "Initial State is  [4, 7, 4]\n",
      "episode 9094, reward 790.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 144\n",
      "Initial State is  [3, 9, 4]\n",
      "episode 9095, reward 974.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 140\n",
      "Initial State is  [3, 0, 6]\n",
      "episode 9096, reward 1291.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 130\n",
      "Initial State is  [0, 11, 3]\n",
      "episode 9097, reward 1059.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 131\n",
      "Initial State is  [1, 22, 1]\n",
      "episode 9098, reward 902.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 146\n",
      "Initial State is  [1, 19, 2]\n",
      "episode 9099, reward 842.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 146\n",
      "Initial State is  [2, 7, 3]\n",
      "episode 9100, reward 970.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 123\n",
      "Initial State is  [3, 14, 4]\n",
      "episode 9101, reward 1169.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 130\n",
      "Initial State is  [2, 23, 0]\n",
      "episode 9102, reward 1015.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 137\n",
      "Initial State is  [4, 17, 0]\n",
      "episode 9103, reward 990.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 139\n",
      "Initial State is  [4, 0, 2]\n",
      "episode 9104, reward 956.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 132\n",
      "Initial State is  [2, 5, 6]\n",
      "episode 9105, reward 1159.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 143\n",
      "Initial State is  [1, 21, 6]\n",
      "episode 9106, reward 1059.0, memory_length 2000, epsilon 0.009998671593271896, time 721.0, rides 142\n",
      "Initial State is  [3, 6, 1]\n",
      "episode 9107, reward 1211.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 136\n",
      "Initial State is  [4, 9, 1]\n",
      "episode 9108, reward 746.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 145\n",
      "Initial State is  [1, 21, 4]\n",
      "episode 9109, reward 781.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 138\n",
      "Initial State is  [0, 9, 1]\n",
      "episode 9110, reward 1068.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 132\n",
      "Initial State is  [0, 12, 0]\n",
      "episode 9111, reward 1012.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 131\n",
      "Initial State is  [3, 7, 2]\n",
      "episode 9112, reward 822.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 151\n",
      "Initial State is  [0, 20, 3]\n",
      "episode 9113, reward 1150.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 144\n",
      "Initial State is  [1, 6, 5]\n",
      "episode 9114, reward 901.0, memory_length 2000, epsilon 0.009998671593271896, time 736.0, rides 144\n",
      "Initial State is  [0, 23, 1]\n",
      "episode 9115, reward 584.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 121\n",
      "Initial State is  [2, 2, 1]\n",
      "episode 9116, reward 781.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 127\n",
      "Initial State is  [2, 11, 2]\n",
      "episode 9117, reward 728.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 122\n",
      "Initial State is  [3, 18, 0]\n",
      "episode 9118, reward 946.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 129\n",
      "Initial State is  [2, 11, 5]\n",
      "episode 9119, reward 857.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 148\n",
      "Initial State is  [4, 20, 0]\n",
      "episode 9120, reward 944.0, memory_length 2000, epsilon 0.009998671593271896, time 737.0, rides 132\n",
      "Initial State is  [4, 7, 3]\n",
      "episode 9121, reward 939.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 142\n",
      "Initial State is  [1, 21, 4]\n",
      "episode 9122, reward 1009.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 136\n",
      "Initial State is  [4, 19, 6]\n",
      "episode 9123, reward 863.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 132\n",
      "Initial State is  [1, 14, 5]\n",
      "episode 9124, reward 751.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 136\n",
      "Initial State is  [1, 19, 4]\n",
      "episode 9125, reward 1132.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 141\n",
      "Initial State is  [1, 14, 4]\n",
      "episode 9126, reward 915.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 138\n",
      "Initial State is  [3, 4, 0]\n",
      "episode 9127, reward 952.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 130\n",
      "Initial State is  [1, 0, 5]\n",
      "episode 9128, reward 856.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 142\n",
      "Initial State is  [2, 14, 5]\n",
      "episode 9129, reward 809.0, memory_length 2000, epsilon 0.009998671593271896, time 736.0, rides 127\n",
      "Initial State is  [2, 11, 6]\n",
      "episode 9130, reward 993.0, memory_length 2000, epsilon 0.009998671593271896, time 737.0, rides 133\n",
      "Initial State is  [4, 6, 5]\n",
      "episode 9131, reward 887.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 132\n",
      "Initial State is  [1, 5, 5]\n",
      "episode 9132, reward 988.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 133\n",
      "Initial State is  [4, 23, 6]\n",
      "episode 9133, reward 1207.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 131\n",
      "Initial State is  [2, 14, 3]\n",
      "episode 9134, reward 851.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 138\n",
      "Initial State is  [1, 15, 2]\n",
      "episode 9135, reward 887.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 132\n",
      "Initial State is  [4, 0, 4]\n",
      "episode 9136, reward 938.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 144\n",
      "Initial State is  [1, 8, 2]\n",
      "episode 9137, reward 761.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 137\n",
      "Initial State is  [3, 20, 0]\n",
      "episode 9138, reward 825.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 125\n",
      "Initial State is  [2, 15, 3]\n",
      "episode 9139, reward 904.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 139\n",
      "Initial State is  [1, 22, 1]\n",
      "episode 9140, reward 820.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 133\n",
      "Initial State is  [3, 3, 5]\n",
      "episode 9141, reward 685.0, memory_length 2000, epsilon 0.009998671593271896, time 737.0, rides 150\n",
      "Initial State is  [0, 18, 3]\n",
      "episode 9142, reward 956.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 130\n",
      "Initial State is  [3, 20, 6]\n",
      "episode 9143, reward 884.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 149\n",
      "Initial State is  [1, 15, 2]\n",
      "episode 9144, reward 958.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 141\n",
      "Initial State is  [2, 23, 6]\n",
      "episode 9145, reward 1253.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 143\n",
      "Initial State is  [0, 18, 3]\n",
      "episode 9146, reward 1191.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 141\n",
      "Initial State is  [0, 13, 2]\n",
      "episode 9147, reward 913.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 145\n",
      "Initial State is  [1, 7, 1]\n",
      "episode 9148, reward 823.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 149\n",
      "Initial State is  [0, 22, 5]\n",
      "episode 9149, reward 904.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 155\n",
      "Initial State is  [4, 4, 2]\n",
      "episode 9150, reward 1013.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 130\n",
      "Initial State is  [4, 14, 0]\n",
      "episode 9151, reward 1085.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 147\n",
      "Initial State is  [1, 11, 6]\n",
      "episode 9152, reward 743.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 121\n",
      "Initial State is  [3, 16, 2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 9153, reward 667.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 125\n",
      "Initial State is  [4, 8, 4]\n",
      "episode 9154, reward 956.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 125\n",
      "Initial State is  [2, 1, 2]\n",
      "episode 9155, reward 1138.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 131\n",
      "Initial State is  [4, 15, 2]\n",
      "episode 9156, reward 1165.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 145\n",
      "Initial State is  [2, 13, 3]\n",
      "episode 9157, reward 1029.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 134\n",
      "Initial State is  [0, 15, 2]\n",
      "episode 9158, reward 949.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 136\n",
      "Initial State is  [1, 22, 6]\n",
      "episode 9159, reward 552.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 148\n",
      "Initial State is  [3, 15, 3]\n",
      "episode 9160, reward 860.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 151\n",
      "Initial State is  [3, 6, 0]\n",
      "episode 9161, reward 906.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 134\n",
      "Initial State is  [3, 19, 1]\n",
      "episode 9162, reward 874.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 134\n",
      "Initial State is  [1, 23, 3]\n",
      "episode 9163, reward 923.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 151\n",
      "Initial State is  [0, 10, 3]\n",
      "episode 9164, reward 894.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 134\n",
      "Initial State is  [3, 17, 6]\n",
      "episode 9165, reward 1057.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 140\n",
      "Initial State is  [0, 0, 0]\n",
      "episode 9166, reward 832.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 137\n",
      "Initial State is  [0, 12, 1]\n",
      "episode 9167, reward 1047.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 131\n",
      "Initial State is  [2, 2, 6]\n",
      "episode 9168, reward 1172.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 153\n",
      "Initial State is  [1, 9, 2]\n",
      "episode 9169, reward 819.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 146\n",
      "Initial State is  [4, 2, 4]\n",
      "episode 9170, reward 940.0, memory_length 2000, epsilon 0.009998671593271896, time 738.0, rides 142\n",
      "Initial State is  [2, 12, 3]\n",
      "episode 9171, reward 787.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 141\n",
      "Initial State is  [4, 16, 5]\n",
      "episode 9172, reward 1012.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 140\n",
      "Initial State is  [4, 11, 6]\n",
      "episode 9173, reward 838.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 141\n",
      "Initial State is  [4, 9, 4]\n",
      "episode 9174, reward 1110.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 140\n",
      "Initial State is  [4, 4, 3]\n",
      "episode 9175, reward 747.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 134\n",
      "Initial State is  [3, 8, 5]\n",
      "episode 9176, reward 1015.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 129\n",
      "Initial State is  [3, 14, 5]\n",
      "episode 9177, reward 674.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 146\n",
      "Initial State is  [1, 3, 3]\n",
      "episode 9178, reward 1036.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 138\n",
      "Initial State is  [4, 13, 2]\n",
      "episode 9179, reward 956.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 148\n",
      "Initial State is  [3, 10, 0]\n",
      "episode 9180, reward 1065.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 148\n",
      "Initial State is  [0, 9, 3]\n",
      "episode 9181, reward 824.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 133\n",
      "Initial State is  [4, 17, 4]\n",
      "episode 9182, reward 1195.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 129\n",
      "Initial State is  [1, 5, 2]\n",
      "episode 9183, reward 841.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 143\n",
      "Initial State is  [4, 6, 4]\n",
      "episode 9184, reward 1087.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 155\n",
      "Initial State is  [2, 11, 0]\n",
      "episode 9185, reward 876.0, memory_length 2000, epsilon 0.009998671593271896, time 737.0, rides 127\n",
      "Initial State is  [1, 3, 1]\n",
      "episode 9186, reward 840.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 134\n",
      "Initial State is  [2, 18, 0]\n",
      "episode 9187, reward 836.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 138\n",
      "Initial State is  [4, 5, 1]\n",
      "episode 9188, reward 936.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 132\n",
      "Initial State is  [2, 19, 2]\n",
      "episode 9189, reward 1031.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 140\n",
      "Initial State is  [4, 5, 6]\n",
      "episode 9190, reward 924.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 129\n",
      "Initial State is  [4, 16, 5]\n",
      "episode 9191, reward 888.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 146\n",
      "Initial State is  [2, 8, 6]\n",
      "episode 9192, reward 952.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 133\n",
      "Initial State is  [0, 19, 0]\n",
      "episode 9193, reward 483.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 130\n",
      "Initial State is  [4, 5, 2]\n",
      "episode 9194, reward 1069.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 139\n",
      "Initial State is  [4, 17, 0]\n",
      "episode 9195, reward 766.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 125\n",
      "Initial State is  [0, 10, 1]\n",
      "episode 9196, reward 1020.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 139\n",
      "Initial State is  [2, 7, 0]\n",
      "episode 9197, reward 801.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 145\n",
      "Initial State is  [0, 13, 6]\n",
      "episode 9198, reward 1018.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 138\n",
      "Initial State is  [0, 11, 5]\n",
      "episode 9199, reward 1033.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 141\n",
      "Initial State is  [1, 12, 6]\n",
      "episode 9200, reward 1299.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 140\n",
      "Initial State is  [1, 0, 1]\n",
      "episode 9201, reward 891.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 134\n",
      "Initial State is  [2, 1, 0]\n",
      "episode 9202, reward 651.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 133\n",
      "Initial State is  [1, 0, 1]\n",
      "episode 9203, reward 1062.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 131\n",
      "Initial State is  [4, 1, 4]\n",
      "episode 9204, reward 1316.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 137\n",
      "Initial State is  [3, 7, 1]\n",
      "episode 9205, reward 958.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 151\n",
      "Initial State is  [2, 10, 5]\n",
      "episode 9206, reward 779.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 126\n",
      "Initial State is  [2, 7, 3]\n",
      "episode 9207, reward 896.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 134\n",
      "Initial State is  [1, 5, 6]\n",
      "episode 9208, reward 886.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 132\n",
      "Initial State is  [1, 2, 6]\n",
      "episode 9209, reward 979.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 141\n",
      "Initial State is  [1, 12, 4]\n",
      "episode 9210, reward 993.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 137\n",
      "Initial State is  [1, 2, 6]\n",
      "episode 9211, reward 1089.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 138\n",
      "Initial State is  [4, 23, 4]\n",
      "episode 9212, reward 652.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 143\n",
      "Initial State is  [1, 5, 4]\n",
      "episode 9213, reward 604.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 133\n",
      "Initial State is  [3, 4, 1]\n",
      "episode 9214, reward 880.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 132\n",
      "Initial State is  [2, 18, 5]\n",
      "episode 9215, reward 1014.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 130\n",
      "Initial State is  [3, 3, 3]\n",
      "episode 9216, reward 962.0, memory_length 2000, epsilon 0.009998671593271896, time 736.0, rides 132\n",
      "Initial State is  [0, 10, 3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 9217, reward 1027.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 134\n",
      "Initial State is  [3, 11, 5]\n",
      "episode 9218, reward 1000.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 143\n",
      "Initial State is  [1, 23, 6]\n",
      "episode 9219, reward 1133.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 141\n",
      "Initial State is  [4, 3, 5]\n",
      "episode 9220, reward 935.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 134\n",
      "Initial State is  [1, 17, 4]\n",
      "episode 9221, reward 626.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 132\n",
      "Initial State is  [3, 14, 5]\n",
      "episode 9222, reward 853.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 148\n",
      "Initial State is  [0, 14, 3]\n",
      "episode 9223, reward 979.0, memory_length 2000, epsilon 0.009998671593271896, time 721.0, rides 144\n",
      "Initial State is  [0, 17, 2]\n",
      "episode 9224, reward 977.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 144\n",
      "Initial State is  [2, 19, 0]\n",
      "episode 9225, reward 978.0, memory_length 2000, epsilon 0.009998671593271896, time 736.0, rides 141\n",
      "Initial State is  [0, 9, 5]\n",
      "episode 9226, reward 911.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 146\n",
      "Initial State is  [4, 15, 1]\n",
      "episode 9227, reward 1122.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 148\n",
      "Initial State is  [0, 16, 3]\n",
      "episode 9228, reward 941.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 138\n",
      "Initial State is  [2, 4, 5]\n",
      "episode 9229, reward 905.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 136\n",
      "Initial State is  [4, 23, 6]\n",
      "episode 9230, reward 911.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 147\n",
      "Initial State is  [0, 1, 5]\n",
      "episode 9231, reward 963.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 149\n",
      "Initial State is  [4, 16, 0]\n",
      "episode 9232, reward 1062.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 140\n",
      "Initial State is  [0, 3, 0]\n",
      "episode 9233, reward 1138.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 133\n",
      "Initial State is  [1, 1, 3]\n",
      "episode 9234, reward 841.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 144\n",
      "Initial State is  [1, 7, 4]\n",
      "episode 9235, reward 1004.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 137\n",
      "Initial State is  [4, 17, 1]\n",
      "episode 9236, reward 815.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 136\n",
      "Initial State is  [0, 6, 0]\n",
      "episode 9237, reward 1199.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 144\n",
      "Initial State is  [2, 3, 0]\n",
      "episode 9238, reward 803.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 139\n",
      "Initial State is  [1, 18, 2]\n",
      "episode 9239, reward 1262.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 138\n",
      "Initial State is  [0, 12, 3]\n",
      "episode 9240, reward 1201.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 134\n",
      "Initial State is  [3, 11, 2]\n",
      "episode 9241, reward 949.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 130\n",
      "Initial State is  [2, 22, 2]\n",
      "episode 9242, reward 1335.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 126\n",
      "Initial State is  [4, 6, 3]\n",
      "episode 9243, reward 741.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 135\n",
      "Initial State is  [0, 7, 3]\n",
      "episode 9244, reward 1040.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 141\n",
      "Initial State is  [2, 1, 0]\n",
      "episode 9245, reward 823.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 131\n",
      "Initial State is  [2, 22, 0]\n",
      "episode 9246, reward 1094.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 140\n",
      "Initial State is  [4, 0, 3]\n",
      "episode 9247, reward 799.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 136\n",
      "Initial State is  [0, 17, 1]\n",
      "episode 9248, reward 1207.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 135\n",
      "Initial State is  [2, 13, 1]\n",
      "episode 9249, reward 898.0, memory_length 2000, epsilon 0.009998671593271896, time 720.0, rides 133\n",
      "Initial State is  [2, 4, 4]\n",
      "episode 9250, reward 872.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 132\n",
      "Initial State is  [2, 10, 6]\n",
      "episode 9251, reward 940.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 133\n",
      "Initial State is  [3, 4, 2]\n",
      "episode 9252, reward 838.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 135\n",
      "Initial State is  [2, 19, 1]\n",
      "episode 9253, reward 900.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 132\n",
      "Initial State is  [1, 9, 3]\n",
      "episode 9254, reward 837.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 132\n",
      "Initial State is  [2, 15, 1]\n",
      "episode 9255, reward 896.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 139\n",
      "Initial State is  [2, 8, 3]\n",
      "episode 9256, reward 834.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 141\n",
      "Initial State is  [4, 17, 2]\n",
      "episode 9257, reward 761.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 131\n",
      "Initial State is  [3, 19, 1]\n",
      "episode 9258, reward 807.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 143\n",
      "Initial State is  [3, 6, 0]\n",
      "episode 9259, reward 902.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 128\n",
      "Initial State is  [0, 4, 1]\n",
      "episode 9260, reward 1222.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 149\n",
      "Initial State is  [3, 7, 5]\n",
      "episode 9261, reward 903.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 133\n",
      "Initial State is  [0, 7, 5]\n",
      "episode 9262, reward 1112.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 133\n",
      "Initial State is  [1, 2, 4]\n",
      "episode 9263, reward 702.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 138\n",
      "Initial State is  [3, 9, 2]\n",
      "episode 9264, reward 1250.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 144\n",
      "Initial State is  [4, 14, 4]\n",
      "episode 9265, reward 840.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 126\n",
      "Initial State is  [1, 17, 5]\n",
      "episode 9266, reward 755.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 135\n",
      "Initial State is  [2, 13, 5]\n",
      "episode 9267, reward 1035.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 142\n",
      "Initial State is  [4, 23, 2]\n",
      "episode 9268, reward 746.0, memory_length 2000, epsilon 0.009998671593271896, time 720.0, rides 127\n",
      "Initial State is  [1, 5, 3]\n",
      "episode 9269, reward 1092.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 139\n",
      "Initial State is  [4, 6, 0]\n",
      "episode 9270, reward 1064.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 129\n",
      "Initial State is  [1, 14, 5]\n",
      "episode 9271, reward 731.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 130\n",
      "Initial State is  [2, 17, 0]\n",
      "episode 9272, reward 822.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 130\n",
      "Initial State is  [0, 2, 2]\n",
      "episode 9273, reward 899.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 124\n",
      "Initial State is  [3, 19, 3]\n",
      "episode 9274, reward 1040.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 124\n",
      "Initial State is  [1, 20, 4]\n",
      "episode 9275, reward 845.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 132\n",
      "Initial State is  [0, 23, 1]\n",
      "episode 9276, reward 760.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 139\n",
      "Initial State is  [1, 5, 5]\n",
      "episode 9277, reward 898.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 138\n",
      "Initial State is  [2, 4, 2]\n",
      "episode 9278, reward 1139.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 144\n",
      "Initial State is  [0, 14, 4]\n",
      "episode 9279, reward 1005.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 141\n",
      "Initial State is  [3, 11, 2]\n",
      "episode 9280, reward 763.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 134\n",
      "Initial State is  [3, 17, 3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 9281, reward 877.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 127\n",
      "Initial State is  [0, 16, 0]\n",
      "episode 9282, reward 548.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 126\n",
      "Initial State is  [1, 23, 2]\n",
      "episode 9283, reward 781.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 132\n",
      "Initial State is  [3, 2, 1]\n",
      "episode 9284, reward 560.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 123\n",
      "Initial State is  [3, 10, 5]\n",
      "episode 9285, reward 1067.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 128\n",
      "Initial State is  [4, 18, 0]\n",
      "episode 9286, reward 1052.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 142\n",
      "Initial State is  [4, 15, 3]\n",
      "episode 9287, reward 911.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 130\n",
      "Initial State is  [4, 9, 2]\n",
      "episode 9288, reward 929.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 130\n",
      "Initial State is  [0, 2, 0]\n",
      "episode 9289, reward 1001.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 113\n",
      "Initial State is  [0, 16, 3]\n",
      "episode 9290, reward 892.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 129\n",
      "Initial State is  [0, 7, 2]\n",
      "episode 9291, reward 775.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 123\n",
      "Initial State is  [4, 6, 2]\n",
      "episode 9292, reward 1028.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 140\n",
      "Initial State is  [0, 16, 0]\n",
      "episode 9293, reward 649.0, memory_length 2000, epsilon 0.009998671593271896, time 739.0, rides 132\n",
      "Initial State is  [0, 5, 6]\n",
      "episode 9294, reward 1050.0, memory_length 2000, epsilon 0.009998671593271896, time 736.0, rides 140\n",
      "Initial State is  [4, 15, 5]\n",
      "episode 9295, reward 929.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 130\n",
      "Initial State is  [1, 9, 1]\n",
      "episode 9296, reward 580.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 123\n",
      "Initial State is  [0, 4, 6]\n",
      "episode 9297, reward 1124.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 138\n",
      "Initial State is  [1, 12, 0]\n",
      "episode 9298, reward 1025.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 135\n",
      "Initial State is  [4, 10, 4]\n",
      "episode 9299, reward 1047.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 143\n",
      "Initial State is  [3, 7, 6]\n",
      "episode 9300, reward 854.0, memory_length 2000, epsilon 0.009998671593271896, time 720.0, rides 127\n",
      "Initial State is  [4, 15, 4]\n",
      "episode 9301, reward 905.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 130\n",
      "Initial State is  [2, 18, 0]\n",
      "episode 9302, reward 982.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 128\n",
      "Initial State is  [3, 23, 0]\n",
      "episode 9303, reward 1085.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 130\n",
      "Initial State is  [0, 6, 6]\n",
      "episode 9304, reward 1060.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 142\n",
      "Initial State is  [2, 23, 3]\n",
      "episode 9305, reward 922.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 146\n",
      "Initial State is  [4, 10, 6]\n",
      "episode 9306, reward 1172.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 133\n",
      "Initial State is  [0, 7, 6]\n",
      "episode 9307, reward 919.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 125\n",
      "Initial State is  [4, 17, 6]\n",
      "episode 9308, reward 943.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 141\n",
      "Initial State is  [2, 6, 1]\n",
      "episode 9309, reward 1111.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 150\n",
      "Initial State is  [1, 2, 4]\n",
      "episode 9310, reward 971.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 122\n",
      "Initial State is  [0, 18, 5]\n",
      "episode 9311, reward 918.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 130\n",
      "Initial State is  [3, 17, 5]\n",
      "episode 9312, reward 1078.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 132\n",
      "Initial State is  [1, 6, 5]\n",
      "episode 9313, reward 565.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 131\n",
      "Initial State is  [1, 20, 6]\n",
      "episode 9314, reward 1022.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 131\n",
      "Initial State is  [4, 11, 2]\n",
      "episode 9315, reward 539.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 127\n",
      "Initial State is  [1, 12, 2]\n",
      "episode 9316, reward 1039.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 131\n",
      "Initial State is  [0, 14, 6]\n",
      "episode 9317, reward 1171.0, memory_length 2000, epsilon 0.009998671593271896, time 720.0, rides 138\n",
      "Initial State is  [2, 17, 2]\n",
      "episode 9318, reward 725.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 140\n",
      "Initial State is  [2, 16, 3]\n",
      "episode 9319, reward 876.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 125\n",
      "Initial State is  [0, 15, 2]\n",
      "episode 9320, reward 924.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 136\n",
      "Initial State is  [3, 10, 3]\n",
      "episode 9321, reward 831.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 122\n",
      "Initial State is  [3, 11, 5]\n",
      "episode 9322, reward 1134.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 134\n",
      "Initial State is  [2, 13, 1]\n",
      "episode 9323, reward 961.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 135\n",
      "Initial State is  [2, 17, 2]\n",
      "episode 9324, reward 847.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 140\n",
      "Initial State is  [1, 8, 0]\n",
      "episode 9325, reward 936.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 133\n",
      "Initial State is  [4, 19, 6]\n",
      "episode 9326, reward 946.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 125\n",
      "Initial State is  [2, 14, 3]\n",
      "episode 9327, reward 788.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 142\n",
      "Initial State is  [3, 17, 4]\n",
      "episode 9328, reward 1292.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 120\n",
      "Initial State is  [1, 15, 6]\n",
      "episode 9329, reward 758.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 131\n",
      "Initial State is  [4, 8, 0]\n",
      "episode 9330, reward 942.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 131\n",
      "Initial State is  [0, 21, 3]\n",
      "episode 9331, reward 1028.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 135\n",
      "Initial State is  [4, 2, 1]\n",
      "episode 9332, reward 1109.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 141\n",
      "Initial State is  [0, 4, 0]\n",
      "episode 9333, reward 1177.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 129\n",
      "Initial State is  [4, 9, 5]\n",
      "episode 9334, reward 702.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 125\n",
      "Initial State is  [4, 16, 4]\n",
      "episode 9335, reward 947.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 133\n",
      "Initial State is  [4, 13, 1]\n",
      "episode 9336, reward 633.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 133\n",
      "Initial State is  [4, 9, 5]\n",
      "episode 9337, reward 1137.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 128\n",
      "Initial State is  [0, 5, 0]\n",
      "episode 9338, reward 791.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 133\n",
      "Initial State is  [1, 19, 6]\n",
      "episode 9339, reward 733.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 123\n",
      "Initial State is  [0, 3, 6]\n",
      "episode 9340, reward 1137.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 128\n",
      "Initial State is  [3, 8, 6]\n",
      "episode 9341, reward 885.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 131\n",
      "Initial State is  [0, 20, 0]\n",
      "episode 9342, reward 798.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 146\n",
      "Initial State is  [0, 19, 2]\n",
      "episode 9343, reward 1038.0, memory_length 2000, epsilon 0.009998671593271896, time 737.0, rides 133\n",
      "Initial State is  [1, 21, 4]\n",
      "episode 9344, reward 724.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 135\n",
      "Initial State is  [4, 15, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 9345, reward 789.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 133\n",
      "Initial State is  [4, 18, 6]\n",
      "episode 9346, reward 1154.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 142\n",
      "Initial State is  [1, 0, 2]\n",
      "episode 9347, reward 810.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 139\n",
      "Initial State is  [0, 6, 0]\n",
      "episode 9348, reward 988.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 134\n",
      "Initial State is  [1, 6, 1]\n",
      "episode 9349, reward 907.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 123\n",
      "Initial State is  [2, 22, 3]\n",
      "episode 9350, reward 916.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 133\n",
      "Initial State is  [1, 8, 4]\n",
      "episode 9351, reward 944.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 134\n",
      "Initial State is  [2, 12, 3]\n",
      "episode 9352, reward 949.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 129\n",
      "Initial State is  [1, 15, 6]\n",
      "episode 9353, reward 1069.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 147\n",
      "Initial State is  [1, 5, 0]\n",
      "episode 9354, reward 890.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 125\n",
      "Initial State is  [3, 8, 2]\n",
      "episode 9355, reward 733.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 129\n",
      "Initial State is  [1, 13, 5]\n",
      "episode 9356, reward 869.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 134\n",
      "Initial State is  [1, 16, 0]\n",
      "episode 9357, reward 1238.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 145\n",
      "Initial State is  [4, 17, 6]\n",
      "episode 9358, reward 1022.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 146\n",
      "Initial State is  [3, 22, 0]\n",
      "episode 9359, reward 1172.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 146\n",
      "Initial State is  [4, 1, 4]\n",
      "episode 9360, reward 763.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 145\n",
      "Initial State is  [1, 23, 1]\n",
      "episode 9361, reward 1045.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 139\n",
      "Initial State is  [1, 0, 6]\n",
      "episode 9362, reward 812.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 130\n",
      "Initial State is  [4, 15, 4]\n",
      "episode 9363, reward 836.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 145\n",
      "Initial State is  [0, 23, 2]\n",
      "episode 9364, reward 875.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 139\n",
      "Initial State is  [0, 4, 6]\n",
      "episode 9365, reward 1074.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 137\n",
      "Initial State is  [0, 22, 0]\n",
      "episode 9366, reward 728.0, memory_length 2000, epsilon 0.009998671593271896, time 737.0, rides 136\n",
      "Initial State is  [2, 14, 3]\n",
      "episode 9367, reward 719.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 129\n",
      "Initial State is  [3, 0, 4]\n",
      "episode 9368, reward 1197.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 131\n",
      "Initial State is  [0, 17, 0]\n",
      "episode 9369, reward 1155.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 142\n",
      "Initial State is  [0, 5, 6]\n",
      "episode 9370, reward 1058.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 132\n",
      "Initial State is  [4, 5, 2]\n",
      "episode 9371, reward 991.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 132\n",
      "Initial State is  [2, 14, 4]\n",
      "episode 9372, reward 733.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 133\n",
      "Initial State is  [4, 14, 5]\n",
      "episode 9373, reward 985.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 132\n",
      "Initial State is  [2, 15, 5]\n",
      "episode 9374, reward 678.0, memory_length 2000, epsilon 0.009998671593271896, time 740.0, rides 131\n",
      "Initial State is  [3, 10, 2]\n",
      "episode 9375, reward 1048.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 144\n",
      "Initial State is  [2, 0, 2]\n",
      "episode 9376, reward 1173.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 141\n",
      "Initial State is  [0, 7, 3]\n",
      "episode 9377, reward 1015.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 146\n",
      "Initial State is  [0, 11, 1]\n",
      "episode 9378, reward 1134.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 139\n",
      "Initial State is  [1, 0, 4]\n",
      "episode 9379, reward 711.0, memory_length 2000, epsilon 0.009998671593271896, time 721.0, rides 140\n",
      "Initial State is  [4, 9, 3]\n",
      "episode 9380, reward 957.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 130\n",
      "Initial State is  [3, 6, 3]\n",
      "episode 9381, reward 1008.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 133\n",
      "Initial State is  [1, 18, 2]\n",
      "episode 9382, reward 894.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 123\n",
      "Initial State is  [2, 0, 0]\n",
      "episode 9383, reward 1042.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 143\n",
      "Initial State is  [0, 8, 6]\n",
      "episode 9384, reward 756.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 132\n",
      "Initial State is  [4, 2, 4]\n",
      "episode 9385, reward 585.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 134\n",
      "Initial State is  [3, 2, 6]\n",
      "episode 9386, reward 903.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 133\n",
      "Initial State is  [1, 20, 2]\n",
      "episode 9387, reward 654.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 133\n",
      "Initial State is  [0, 9, 5]\n",
      "episode 9388, reward 890.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 146\n",
      "Initial State is  [0, 22, 4]\n",
      "episode 9389, reward 1053.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 133\n",
      "Initial State is  [1, 4, 4]\n",
      "episode 9390, reward 927.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 150\n",
      "Initial State is  [2, 9, 6]\n",
      "episode 9391, reward 1255.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 137\n",
      "Initial State is  [0, 0, 4]\n",
      "episode 9392, reward 1001.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 129\n",
      "Initial State is  [1, 11, 0]\n",
      "episode 9393, reward 1480.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 146\n",
      "Initial State is  [1, 20, 2]\n",
      "episode 9394, reward 883.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 135\n",
      "Initial State is  [4, 15, 4]\n",
      "episode 9395, reward 795.0, memory_length 2000, epsilon 0.009998671593271896, time 737.0, rides 136\n",
      "Initial State is  [2, 6, 3]\n",
      "episode 9396, reward 958.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 126\n",
      "Initial State is  [3, 9, 0]\n",
      "episode 9397, reward 848.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 134\n",
      "Initial State is  [3, 13, 0]\n",
      "episode 9398, reward 1122.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 125\n",
      "Initial State is  [4, 5, 3]\n",
      "episode 9399, reward 933.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 137\n",
      "Initial State is  [2, 3, 4]\n",
      "episode 9400, reward 787.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 145\n",
      "Initial State is  [0, 23, 4]\n",
      "episode 9401, reward 1367.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 127\n",
      "Initial State is  [1, 3, 4]\n",
      "episode 9402, reward 935.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 149\n",
      "Initial State is  [0, 13, 1]\n",
      "episode 9403, reward 1107.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 142\n",
      "Initial State is  [2, 12, 5]\n",
      "episode 9404, reward 884.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 135\n",
      "Initial State is  [1, 12, 3]\n",
      "episode 9405, reward 836.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 133\n",
      "Initial State is  [3, 4, 1]\n",
      "episode 9406, reward 863.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 134\n",
      "Initial State is  [1, 20, 3]\n",
      "episode 9407, reward 838.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 134\n",
      "Initial State is  [4, 10, 5]\n",
      "episode 9408, reward 1025.0, memory_length 2000, epsilon 0.009998671593271896, time 721.0, rides 133\n",
      "Initial State is  [2, 21, 6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 9409, reward 1310.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 145\n",
      "Initial State is  [3, 10, 2]\n",
      "episode 9410, reward 1167.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 138\n",
      "Initial State is  [0, 20, 4]\n",
      "episode 9411, reward 705.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 135\n",
      "Initial State is  [0, 19, 1]\n",
      "episode 9412, reward 802.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 133\n",
      "Initial State is  [1, 15, 1]\n",
      "episode 9413, reward 1096.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 128\n",
      "Initial State is  [1, 4, 6]\n",
      "episode 9414, reward 932.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 130\n",
      "Initial State is  [3, 8, 6]\n",
      "episode 9415, reward 750.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 138\n",
      "Initial State is  [0, 13, 6]\n",
      "episode 9416, reward 751.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 126\n",
      "Initial State is  [0, 19, 1]\n",
      "episode 9417, reward 716.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 128\n",
      "Initial State is  [2, 22, 2]\n",
      "episode 9418, reward 901.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 134\n",
      "Initial State is  [3, 18, 3]\n",
      "episode 9419, reward 689.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 122\n",
      "Initial State is  [3, 4, 0]\n",
      "episode 9420, reward 1005.0, memory_length 2000, epsilon 0.009998671593271896, time 721.0, rides 122\n",
      "Initial State is  [3, 23, 5]\n",
      "episode 9421, reward 1081.0, memory_length 2000, epsilon 0.009998671593271896, time 736.0, rides 129\n",
      "Initial State is  [1, 17, 2]\n",
      "episode 9422, reward 1120.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 143\n",
      "Initial State is  [4, 15, 2]\n",
      "episode 9423, reward 921.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 147\n",
      "Initial State is  [3, 19, 0]\n",
      "episode 9424, reward 883.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 140\n",
      "Initial State is  [3, 3, 0]\n",
      "episode 9425, reward 991.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 137\n",
      "Initial State is  [0, 8, 1]\n",
      "episode 9426, reward 848.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 142\n",
      "Initial State is  [2, 5, 4]\n",
      "episode 9427, reward 802.0, memory_length 2000, epsilon 0.009998671593271896, time 737.0, rides 121\n",
      "Initial State is  [2, 0, 3]\n",
      "episode 9428, reward 1127.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 127\n",
      "Initial State is  [2, 7, 0]\n",
      "episode 9429, reward 963.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 138\n",
      "Initial State is  [0, 22, 5]\n",
      "episode 9430, reward 643.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 130\n",
      "Initial State is  [3, 18, 5]\n",
      "episode 9431, reward 927.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 122\n",
      "Initial State is  [0, 15, 4]\n",
      "episode 9432, reward 771.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 133\n",
      "Initial State is  [0, 2, 3]\n",
      "episode 9433, reward 936.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 140\n",
      "Initial State is  [1, 7, 5]\n",
      "episode 9434, reward 1001.0, memory_length 2000, epsilon 0.009998671593271896, time 740.0, rides 124\n",
      "Initial State is  [3, 10, 0]\n",
      "episode 9435, reward 604.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 129\n",
      "Initial State is  [1, 3, 0]\n",
      "episode 9436, reward 915.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 130\n",
      "Initial State is  [2, 9, 1]\n",
      "episode 9437, reward 881.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 137\n",
      "Initial State is  [4, 3, 0]\n",
      "episode 9438, reward 755.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 123\n",
      "Initial State is  [2, 22, 0]\n",
      "episode 9439, reward 915.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 131\n",
      "Initial State is  [1, 15, 1]\n",
      "episode 9440, reward 873.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 141\n",
      "Initial State is  [4, 23, 5]\n",
      "episode 9441, reward 849.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 133\n",
      "Initial State is  [2, 20, 4]\n",
      "episode 9442, reward 904.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 124\n",
      "Initial State is  [1, 14, 2]\n",
      "episode 9443, reward 883.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 141\n",
      "Initial State is  [1, 0, 5]\n",
      "episode 9444, reward 827.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 133\n",
      "Initial State is  [3, 20, 2]\n",
      "episode 9445, reward 776.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 130\n",
      "Initial State is  [2, 16, 3]\n",
      "episode 9446, reward 884.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 145\n",
      "Initial State is  [1, 7, 6]\n",
      "episode 9447, reward 793.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 150\n",
      "Initial State is  [4, 5, 5]\n",
      "episode 9448, reward 866.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 130\n",
      "Initial State is  [0, 14, 1]\n",
      "episode 9449, reward 979.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 150\n",
      "Initial State is  [2, 6, 5]\n",
      "episode 9450, reward 1249.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 136\n",
      "Initial State is  [3, 9, 3]\n",
      "episode 9451, reward 1012.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 138\n",
      "Initial State is  [2, 7, 5]\n",
      "episode 9452, reward 929.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 143\n",
      "Initial State is  [0, 21, 0]\n",
      "episode 9453, reward 1042.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 119\n",
      "Initial State is  [1, 1, 0]\n",
      "episode 9454, reward 938.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 142\n",
      "Initial State is  [2, 1, 6]\n",
      "episode 9455, reward 888.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 145\n",
      "Initial State is  [0, 16, 3]\n",
      "episode 9456, reward 1119.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 131\n",
      "Initial State is  [2, 15, 4]\n",
      "episode 9457, reward 1076.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 160\n",
      "Initial State is  [4, 3, 0]\n",
      "episode 9458, reward 1079.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 141\n",
      "Initial State is  [2, 0, 6]\n",
      "episode 9459, reward 1130.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 137\n",
      "Initial State is  [2, 7, 2]\n",
      "episode 9460, reward 1063.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 136\n",
      "Initial State is  [2, 3, 0]\n",
      "episode 9461, reward 798.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 147\n",
      "Initial State is  [2, 22, 4]\n",
      "episode 9462, reward 1184.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 141\n",
      "Initial State is  [0, 18, 6]\n",
      "episode 9463, reward 1069.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 148\n",
      "Initial State is  [2, 2, 5]\n",
      "episode 9464, reward 980.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 137\n",
      "Initial State is  [0, 7, 1]\n",
      "episode 9465, reward 885.0, memory_length 2000, epsilon 0.009998671593271896, time 737.0, rides 127\n",
      "Initial State is  [0, 18, 3]\n",
      "episode 9466, reward 1078.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 130\n",
      "Initial State is  [2, 14, 3]\n",
      "episode 9467, reward 1026.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 126\n",
      "Initial State is  [3, 2, 2]\n",
      "episode 9468, reward 741.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 126\n",
      "Initial State is  [0, 23, 3]\n",
      "episode 9469, reward 1037.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 131\n",
      "Initial State is  [4, 19, 5]\n",
      "episode 9470, reward 1131.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 131\n",
      "Initial State is  [0, 4, 2]\n",
      "episode 9471, reward 753.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 119\n",
      "Initial State is  [4, 11, 0]\n",
      "episode 9472, reward 1068.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 126\n",
      "Initial State is  [0, 16, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 9473, reward 1132.0, memory_length 2000, epsilon 0.009998671593271896, time 721.0, rides 142\n",
      "Initial State is  [3, 17, 0]\n",
      "episode 9474, reward 767.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 132\n",
      "Initial State is  [0, 7, 3]\n",
      "episode 9475, reward 1041.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 131\n",
      "Initial State is  [2, 0, 6]\n",
      "episode 9476, reward 840.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 140\n",
      "Initial State is  [3, 20, 3]\n",
      "episode 9477, reward 1049.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 125\n",
      "Initial State is  [1, 22, 2]\n",
      "episode 9478, reward 1111.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 137\n",
      "Initial State is  [0, 20, 3]\n",
      "episode 9479, reward 1059.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 129\n",
      "Initial State is  [0, 12, 0]\n",
      "episode 9480, reward 879.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 142\n",
      "Initial State is  [4, 23, 5]\n",
      "episode 9481, reward 967.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 131\n",
      "Initial State is  [1, 4, 5]\n",
      "episode 9482, reward 914.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 157\n",
      "Initial State is  [0, 18, 2]\n",
      "episode 9483, reward 1043.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 124\n",
      "Initial State is  [3, 5, 1]\n",
      "episode 9484, reward 996.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 118\n",
      "Initial State is  [1, 17, 4]\n",
      "episode 9485, reward 949.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 135\n",
      "Initial State is  [0, 15, 5]\n",
      "episode 9486, reward 1155.0, memory_length 2000, epsilon 0.009998671593271896, time 737.0, rides 131\n",
      "Initial State is  [0, 5, 6]\n",
      "episode 9487, reward 874.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 133\n",
      "Initial State is  [1, 17, 0]\n",
      "episode 9488, reward 1128.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 135\n",
      "Initial State is  [3, 21, 6]\n",
      "episode 9489, reward 1112.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 130\n",
      "Initial State is  [0, 8, 2]\n",
      "episode 9490, reward 803.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 140\n",
      "Initial State is  [0, 16, 4]\n",
      "episode 9491, reward 1202.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 138\n",
      "Initial State is  [1, 4, 4]\n",
      "episode 9492, reward 680.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 148\n",
      "Initial State is  [3, 20, 6]\n",
      "episode 9493, reward 842.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 128\n",
      "Initial State is  [1, 10, 1]\n",
      "episode 9494, reward 985.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 124\n",
      "Initial State is  [2, 4, 6]\n",
      "episode 9495, reward 917.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 136\n",
      "Initial State is  [4, 22, 3]\n",
      "episode 9496, reward 981.0, memory_length 2000, epsilon 0.009998671593271896, time 737.0, rides 130\n",
      "Initial State is  [2, 7, 3]\n",
      "episode 9497, reward 1154.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 126\n",
      "Initial State is  [3, 3, 0]\n",
      "episode 9498, reward 1010.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 143\n",
      "Initial State is  [2, 18, 6]\n",
      "episode 9499, reward 650.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 134\n",
      "Initial State is  [4, 8, 2]\n",
      "episode 9500, reward 832.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 133\n",
      "Initial State is  [2, 2, 5]\n",
      "episode 9501, reward 1045.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 149\n",
      "Initial State is  [4, 10, 2]\n",
      "episode 9502, reward 1135.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 131\n",
      "Initial State is  [2, 12, 0]\n",
      "episode 9503, reward 781.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 129\n",
      "Initial State is  [0, 8, 5]\n",
      "episode 9504, reward 875.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 135\n",
      "Initial State is  [1, 9, 3]\n",
      "episode 9505, reward 986.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 123\n",
      "Initial State is  [4, 17, 5]\n",
      "episode 9506, reward 772.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 132\n",
      "Initial State is  [3, 16, 1]\n",
      "episode 9507, reward 780.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 131\n",
      "Initial State is  [4, 18, 5]\n",
      "episode 9508, reward 1043.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 135\n",
      "Initial State is  [3, 5, 1]\n",
      "episode 9509, reward 710.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 141\n",
      "Initial State is  [0, 12, 5]\n",
      "episode 9510, reward 833.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 143\n",
      "Initial State is  [0, 6, 2]\n",
      "episode 9511, reward 1066.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 138\n",
      "Initial State is  [4, 9, 6]\n",
      "episode 9512, reward 779.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 129\n",
      "Initial State is  [1, 9, 0]\n",
      "episode 9513, reward 1139.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 127\n",
      "Initial State is  [3, 10, 5]\n",
      "episode 9514, reward 723.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 127\n",
      "Initial State is  [4, 22, 3]\n",
      "episode 9515, reward 831.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 149\n",
      "Initial State is  [4, 18, 0]\n",
      "episode 9516, reward 992.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 141\n",
      "Initial State is  [1, 7, 4]\n",
      "episode 9517, reward 777.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 134\n",
      "Initial State is  [3, 5, 6]\n",
      "episode 9518, reward 761.0, memory_length 2000, epsilon 0.009998671593271896, time 746.0, rides 139\n",
      "Initial State is  [3, 10, 1]\n",
      "episode 9519, reward 842.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 132\n",
      "Initial State is  [0, 21, 5]\n",
      "episode 9520, reward 941.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 124\n",
      "Initial State is  [2, 16, 2]\n",
      "episode 9521, reward 874.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 136\n",
      "Initial State is  [2, 22, 1]\n",
      "episode 9522, reward 836.0, memory_length 2000, epsilon 0.009998671593271896, time 739.0, rides 121\n",
      "Initial State is  [2, 16, 4]\n",
      "episode 9523, reward 898.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 138\n",
      "Initial State is  [3, 2, 0]\n",
      "episode 9524, reward 791.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 132\n",
      "Initial State is  [0, 0, 2]\n",
      "episode 9525, reward 894.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 126\n",
      "Initial State is  [2, 15, 1]\n",
      "episode 9526, reward 768.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 128\n",
      "Initial State is  [4, 1, 6]\n",
      "episode 9527, reward 727.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 132\n",
      "Initial State is  [3, 0, 2]\n",
      "episode 9528, reward 912.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 127\n",
      "Initial State is  [3, 3, 1]\n",
      "episode 9529, reward 837.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 131\n",
      "Initial State is  [2, 9, 4]\n",
      "episode 9530, reward 856.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 132\n",
      "Initial State is  [3, 11, 4]\n",
      "episode 9531, reward 1089.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 135\n",
      "Initial State is  [0, 9, 1]\n",
      "episode 9532, reward 962.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 137\n",
      "Initial State is  [1, 15, 2]\n",
      "episode 9533, reward 1036.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 124\n",
      "Initial State is  [0, 8, 1]\n",
      "episode 9534, reward 939.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 133\n",
      "Initial State is  [2, 19, 4]\n",
      "episode 9535, reward 723.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 130\n",
      "Initial State is  [2, 21, 3]\n",
      "episode 9536, reward 1037.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 152\n",
      "Initial State is  [3, 6, 6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 9537, reward 992.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 122\n",
      "Initial State is  [0, 13, 4]\n",
      "episode 9538, reward 925.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 141\n",
      "Initial State is  [4, 19, 6]\n",
      "episode 9539, reward 1013.0, memory_length 2000, epsilon 0.009998671593271896, time 740.0, rides 134\n",
      "Initial State is  [4, 10, 6]\n",
      "episode 9540, reward 971.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 143\n",
      "Initial State is  [3, 4, 3]\n",
      "episode 9541, reward 711.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 125\n",
      "Initial State is  [4, 7, 3]\n",
      "episode 9542, reward 1203.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 138\n",
      "Initial State is  [1, 17, 2]\n",
      "episode 9543, reward 760.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 138\n",
      "Initial State is  [3, 21, 4]\n",
      "episode 9544, reward 1044.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 135\n",
      "Initial State is  [0, 11, 0]\n",
      "episode 9545, reward 1036.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 139\n",
      "Initial State is  [2, 7, 4]\n",
      "episode 9546, reward 961.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 132\n",
      "Initial State is  [4, 5, 2]\n",
      "episode 9547, reward 900.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 134\n",
      "Initial State is  [1, 12, 1]\n",
      "episode 9548, reward 891.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 139\n",
      "Initial State is  [4, 0, 6]\n",
      "episode 9549, reward 1010.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 134\n",
      "Initial State is  [0, 14, 6]\n",
      "episode 9550, reward 1276.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 141\n",
      "Initial State is  [2, 1, 3]\n",
      "episode 9551, reward 1083.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 132\n",
      "Initial State is  [1, 9, 5]\n",
      "episode 9552, reward 714.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 136\n",
      "Initial State is  [4, 6, 3]\n",
      "episode 9553, reward 801.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 145\n",
      "Initial State is  [4, 10, 1]\n",
      "episode 9554, reward 1044.0, memory_length 2000, epsilon 0.009998671593271896, time 721.0, rides 141\n",
      "Initial State is  [3, 18, 5]\n",
      "episode 9555, reward 809.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 140\n",
      "Initial State is  [0, 13, 5]\n",
      "episode 9556, reward 969.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 141\n",
      "Initial State is  [0, 0, 3]\n",
      "episode 9557, reward 1078.0, memory_length 2000, epsilon 0.009998671593271896, time 738.0, rides 132\n",
      "Initial State is  [0, 13, 6]\n",
      "episode 9558, reward 668.0, memory_length 2000, epsilon 0.009998671593271896, time 736.0, rides 138\n",
      "Initial State is  [1, 21, 1]\n",
      "episode 9559, reward 1038.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 129\n",
      "Initial State is  [2, 8, 5]\n",
      "episode 9560, reward 995.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 140\n",
      "Initial State is  [2, 13, 5]\n",
      "episode 9561, reward 699.0, memory_length 2000, epsilon 0.009998671593271896, time 736.0, rides 137\n",
      "Initial State is  [3, 4, 0]\n",
      "episode 9562, reward 1166.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 143\n",
      "Initial State is  [0, 21, 3]\n",
      "episode 9563, reward 992.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 136\n",
      "Initial State is  [2, 14, 5]\n",
      "episode 9564, reward 643.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 130\n",
      "Initial State is  [2, 11, 6]\n",
      "episode 9565, reward 743.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 137\n",
      "Initial State is  [2, 20, 4]\n",
      "episode 9566, reward 806.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 129\n",
      "Initial State is  [3, 21, 0]\n",
      "episode 9567, reward 918.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 139\n",
      "Initial State is  [2, 8, 4]\n",
      "episode 9568, reward 1075.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 131\n",
      "Initial State is  [0, 3, 5]\n",
      "episode 9569, reward 1398.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 136\n",
      "Initial State is  [0, 21, 1]\n",
      "episode 9570, reward 873.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 144\n",
      "Initial State is  [2, 21, 5]\n",
      "episode 9571, reward 833.0, memory_length 2000, epsilon 0.009998671593271896, time 736.0, rides 132\n",
      "Initial State is  [3, 7, 3]\n",
      "episode 9572, reward 1073.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 127\n",
      "Initial State is  [2, 8, 2]\n",
      "episode 9573, reward 950.0, memory_length 2000, epsilon 0.009998671593271896, time 721.0, rides 127\n",
      "Initial State is  [0, 17, 0]\n",
      "episode 9574, reward 1123.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 123\n",
      "Initial State is  [3, 22, 3]\n",
      "episode 9575, reward 812.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 138\n",
      "Initial State is  [4, 22, 6]\n",
      "episode 9576, reward 1020.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 139\n",
      "Initial State is  [2, 17, 4]\n",
      "episode 9577, reward 926.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 131\n",
      "Initial State is  [3, 4, 3]\n",
      "episode 9578, reward 999.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 145\n",
      "Initial State is  [1, 4, 1]\n",
      "episode 9579, reward 1123.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 136\n",
      "Initial State is  [4, 7, 3]\n",
      "episode 9580, reward 1084.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 128\n",
      "Initial State is  [2, 10, 4]\n",
      "episode 9581, reward 1182.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 144\n",
      "Initial State is  [0, 22, 4]\n",
      "episode 9582, reward 963.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 142\n",
      "Initial State is  [1, 14, 2]\n",
      "episode 9583, reward 917.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 133\n",
      "Initial State is  [0, 1, 4]\n",
      "episode 9584, reward 793.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 132\n",
      "Initial State is  [4, 4, 6]\n",
      "episode 9585, reward 1106.0, memory_length 2000, epsilon 0.009998671593271896, time 721.0, rides 128\n",
      "Initial State is  [2, 12, 1]\n",
      "episode 9586, reward 837.0, memory_length 2000, epsilon 0.009998671593271896, time 736.0, rides 143\n",
      "Initial State is  [4, 7, 5]\n",
      "episode 9587, reward 750.0, memory_length 2000, epsilon 0.009998671593271896, time 737.0, rides 129\n",
      "Initial State is  [1, 20, 4]\n",
      "episode 9588, reward 958.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 144\n",
      "Initial State is  [4, 9, 1]\n",
      "episode 9589, reward 897.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 152\n",
      "Initial State is  [4, 6, 4]\n",
      "episode 9590, reward 946.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 136\n",
      "Initial State is  [3, 5, 0]\n",
      "episode 9591, reward 1046.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 140\n",
      "Initial State is  [4, 15, 2]\n",
      "episode 9592, reward 873.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 136\n",
      "Initial State is  [2, 14, 5]\n",
      "episode 9593, reward 1153.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 151\n",
      "Initial State is  [2, 12, 0]\n",
      "episode 9594, reward 833.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 137\n",
      "Initial State is  [1, 15, 5]\n",
      "episode 9595, reward 1161.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 136\n",
      "Initial State is  [1, 5, 0]\n",
      "episode 9596, reward 1282.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 139\n",
      "Initial State is  [2, 23, 2]\n",
      "episode 9597, reward 1076.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 132\n",
      "Initial State is  [4, 5, 0]\n",
      "episode 9598, reward 1091.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 140\n",
      "Initial State is  [1, 19, 4]\n",
      "episode 9599, reward 1036.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 137\n",
      "Initial State is  [3, 3, 1]\n",
      "episode 9600, reward 1111.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 130\n",
      "Initial State is  [4, 2, 1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 9601, reward 844.0, memory_length 2000, epsilon 0.009998671593271896, time 720.0, rides 142\n",
      "Initial State is  [0, 23, 3]\n",
      "episode 9602, reward 844.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 137\n",
      "Initial State is  [3, 4, 6]\n",
      "episode 9603, reward 708.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 128\n",
      "Initial State is  [4, 20, 4]\n",
      "episode 9604, reward 724.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 136\n",
      "Initial State is  [3, 7, 6]\n",
      "episode 9605, reward 885.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 145\n",
      "Initial State is  [2, 1, 5]\n",
      "episode 9606, reward 857.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 133\n",
      "Initial State is  [4, 14, 5]\n",
      "episode 9607, reward 1014.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 135\n",
      "Initial State is  [3, 16, 0]\n",
      "episode 9608, reward 797.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 135\n",
      "Initial State is  [0, 0, 5]\n",
      "episode 9609, reward 836.0, memory_length 2000, epsilon 0.009998671593271896, time 741.0, rides 151\n",
      "Initial State is  [2, 6, 1]\n",
      "episode 9610, reward 981.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 146\n",
      "Initial State is  [0, 14, 4]\n",
      "episode 9611, reward 1198.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 139\n",
      "Initial State is  [0, 22, 0]\n",
      "episode 9612, reward 1270.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 135\n",
      "Initial State is  [4, 14, 4]\n",
      "episode 9613, reward 1112.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 145\n",
      "Initial State is  [4, 10, 0]\n",
      "episode 9614, reward 1162.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 134\n",
      "Initial State is  [0, 5, 4]\n",
      "episode 9615, reward 1001.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 135\n",
      "Initial State is  [1, 17, 0]\n",
      "episode 9616, reward 1033.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 142\n",
      "Initial State is  [3, 13, 3]\n",
      "episode 9617, reward 934.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 138\n",
      "Initial State is  [2, 7, 5]\n",
      "episode 9618, reward 1113.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 128\n",
      "Initial State is  [1, 19, 6]\n",
      "episode 9619, reward 898.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 123\n",
      "Initial State is  [2, 18, 3]\n",
      "episode 9620, reward 1037.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 149\n",
      "Initial State is  [2, 12, 6]\n",
      "episode 9621, reward 1152.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 136\n",
      "Initial State is  [3, 10, 1]\n",
      "episode 9622, reward 1187.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 137\n",
      "Initial State is  [4, 11, 0]\n",
      "episode 9623, reward 745.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 141\n",
      "Initial State is  [0, 12, 3]\n",
      "episode 9624, reward 1003.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 136\n",
      "Initial State is  [4, 19, 1]\n",
      "episode 9625, reward 924.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 150\n",
      "Initial State is  [3, 12, 6]\n",
      "episode 9626, reward 922.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 145\n",
      "Initial State is  [4, 10, 6]\n",
      "episode 9627, reward 698.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 130\n",
      "Initial State is  [4, 18, 5]\n",
      "episode 9628, reward 810.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 135\n",
      "Initial State is  [0, 8, 5]\n",
      "episode 9629, reward 904.0, memory_length 2000, epsilon 0.009998671593271896, time 737.0, rides 138\n",
      "Initial State is  [3, 2, 4]\n",
      "episode 9630, reward 819.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 135\n",
      "Initial State is  [4, 1, 3]\n",
      "episode 9631, reward 942.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 130\n",
      "Initial State is  [2, 17, 2]\n",
      "episode 9632, reward 937.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 138\n",
      "Initial State is  [1, 20, 4]\n",
      "episode 9633, reward 936.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 137\n",
      "Initial State is  [0, 11, 4]\n",
      "episode 9634, reward 696.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 135\n",
      "Initial State is  [3, 10, 3]\n",
      "episode 9635, reward 1096.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 142\n",
      "Initial State is  [3, 4, 1]\n",
      "episode 9636, reward 681.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 138\n",
      "Initial State is  [3, 2, 3]\n",
      "episode 9637, reward 561.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 133\n",
      "Initial State is  [1, 6, 3]\n",
      "episode 9638, reward 1002.0, memory_length 2000, epsilon 0.009998671593271896, time 737.0, rides 141\n",
      "Initial State is  [1, 18, 5]\n",
      "episode 9639, reward 980.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 125\n",
      "Initial State is  [2, 18, 2]\n",
      "episode 9640, reward 864.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 140\n",
      "Initial State is  [2, 12, 1]\n",
      "episode 9641, reward 761.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 136\n",
      "Initial State is  [3, 9, 2]\n",
      "episode 9642, reward 983.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 134\n",
      "Initial State is  [4, 5, 1]\n",
      "episode 9643, reward 844.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 135\n",
      "Initial State is  [0, 20, 4]\n",
      "episode 9644, reward 990.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 135\n",
      "Initial State is  [1, 18, 6]\n",
      "episode 9645, reward 919.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 134\n",
      "Initial State is  [0, 14, 4]\n",
      "episode 9646, reward 734.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 129\n",
      "Initial State is  [1, 2, 1]\n",
      "episode 9647, reward 1054.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 127\n",
      "Initial State is  [2, 0, 3]\n",
      "episode 9648, reward 838.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 137\n",
      "Initial State is  [4, 3, 1]\n",
      "episode 9649, reward 978.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 146\n",
      "Initial State is  [2, 6, 0]\n",
      "episode 9650, reward 794.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 135\n",
      "Initial State is  [4, 9, 2]\n",
      "episode 9651, reward 1102.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 135\n",
      "Initial State is  [3, 0, 0]\n",
      "episode 9652, reward 794.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 127\n",
      "Initial State is  [4, 17, 3]\n",
      "episode 9653, reward 925.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 143\n",
      "Initial State is  [4, 18, 1]\n",
      "episode 9654, reward 929.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 137\n",
      "Initial State is  [4, 1, 1]\n",
      "episode 9655, reward 893.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 148\n",
      "Initial State is  [0, 8, 1]\n",
      "episode 9656, reward 972.0, memory_length 2000, epsilon 0.009998671593271896, time 737.0, rides 135\n",
      "Initial State is  [3, 2, 1]\n",
      "episode 9657, reward 953.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 142\n",
      "Initial State is  [2, 23, 5]\n",
      "episode 9658, reward 857.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 146\n",
      "Initial State is  [2, 4, 3]\n",
      "episode 9659, reward 910.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 124\n",
      "Initial State is  [2, 14, 4]\n",
      "episode 9660, reward 824.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 132\n",
      "Initial State is  [4, 23, 5]\n",
      "episode 9661, reward 912.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 136\n",
      "Initial State is  [1, 10, 1]\n",
      "episode 9662, reward 1081.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 134\n",
      "Initial State is  [1, 18, 6]\n",
      "episode 9663, reward 709.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 123\n",
      "Initial State is  [4, 14, 1]\n",
      "episode 9664, reward 926.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 135\n",
      "Initial State is  [1, 3, 5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 9665, reward 892.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 142\n",
      "Initial State is  [3, 20, 4]\n",
      "episode 9666, reward 1206.0, memory_length 2000, epsilon 0.009998671593271896, time 739.0, rides 136\n",
      "Initial State is  [4, 8, 0]\n",
      "episode 9667, reward 859.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 128\n",
      "Initial State is  [1, 2, 5]\n",
      "episode 9668, reward 782.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 129\n",
      "Initial State is  [2, 5, 4]\n",
      "episode 9669, reward 779.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 142\n",
      "Initial State is  [1, 0, 4]\n",
      "episode 9670, reward 754.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 152\n",
      "Initial State is  [2, 16, 0]\n",
      "episode 9671, reward 1305.0, memory_length 2000, epsilon 0.009998671593271896, time 721.0, rides 136\n",
      "Initial State is  [3, 9, 1]\n",
      "episode 9672, reward 990.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 145\n",
      "Initial State is  [2, 20, 1]\n",
      "episode 9673, reward 1025.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 133\n",
      "Initial State is  [2, 10, 2]\n",
      "episode 9674, reward 861.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 138\n",
      "Initial State is  [4, 2, 3]\n",
      "episode 9675, reward 930.0, memory_length 2000, epsilon 0.009998671593271896, time 736.0, rides 123\n",
      "Initial State is  [2, 5, 2]\n",
      "episode 9676, reward 1071.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 131\n",
      "Initial State is  [2, 15, 4]\n",
      "episode 9677, reward 960.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 127\n",
      "Initial State is  [0, 6, 6]\n",
      "episode 9678, reward 792.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 129\n",
      "Initial State is  [4, 0, 3]\n",
      "episode 9679, reward 1010.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 142\n",
      "Initial State is  [1, 10, 0]\n",
      "episode 9680, reward 989.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 134\n",
      "Initial State is  [0, 4, 2]\n",
      "episode 9681, reward 892.0, memory_length 2000, epsilon 0.009998671593271896, time 737.0, rides 136\n",
      "Initial State is  [0, 15, 4]\n",
      "episode 9682, reward 794.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 132\n",
      "Initial State is  [1, 17, 5]\n",
      "episode 9683, reward 919.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 118\n",
      "Initial State is  [2, 10, 6]\n",
      "episode 9684, reward 745.0, memory_length 2000, epsilon 0.009998671593271896, time 736.0, rides 127\n",
      "Initial State is  [2, 1, 0]\n",
      "episode 9685, reward 732.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 132\n",
      "Initial State is  [4, 7, 3]\n",
      "episode 9686, reward 1158.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 141\n",
      "Initial State is  [0, 7, 2]\n",
      "episode 9687, reward 738.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 124\n",
      "Initial State is  [4, 2, 6]\n",
      "episode 9688, reward 767.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 138\n",
      "Initial State is  [1, 0, 0]\n",
      "episode 9689, reward 847.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 122\n",
      "Initial State is  [1, 21, 0]\n",
      "episode 9690, reward 1060.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 139\n",
      "Initial State is  [1, 2, 6]\n",
      "episode 9691, reward 968.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 121\n",
      "Initial State is  [0, 15, 6]\n",
      "episode 9692, reward 1059.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 130\n",
      "Initial State is  [0, 14, 2]\n",
      "episode 9693, reward 1088.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 145\n",
      "Initial State is  [0, 10, 6]\n",
      "episode 9694, reward 1028.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 132\n",
      "Initial State is  [1, 22, 0]\n",
      "episode 9695, reward 1223.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 131\n",
      "Initial State is  [2, 17, 3]\n",
      "episode 9696, reward 1143.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 124\n",
      "Initial State is  [2, 1, 3]\n",
      "episode 9697, reward 1196.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 143\n",
      "Initial State is  [0, 15, 0]\n",
      "episode 9698, reward 705.0, memory_length 2000, epsilon 0.009998671593271896, time 736.0, rides 134\n",
      "Initial State is  [1, 13, 3]\n",
      "episode 9699, reward 1021.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 133\n",
      "Initial State is  [0, 0, 3]\n",
      "episode 9700, reward 675.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 140\n",
      "Initial State is  [2, 10, 1]\n",
      "episode 9701, reward 851.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 122\n",
      "Initial State is  [2, 12, 6]\n",
      "episode 9702, reward 905.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 132\n",
      "Initial State is  [0, 6, 2]\n",
      "episode 9703, reward 1040.0, memory_length 2000, epsilon 0.009998671593271896, time 739.0, rides 125\n",
      "Initial State is  [3, 17, 2]\n",
      "episode 9704, reward 781.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 128\n",
      "Initial State is  [3, 8, 1]\n",
      "episode 9705, reward 942.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 131\n",
      "Initial State is  [3, 5, 3]\n",
      "episode 9706, reward 1266.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 142\n",
      "Initial State is  [2, 14, 1]\n",
      "episode 9707, reward 956.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 128\n",
      "Initial State is  [3, 4, 4]\n",
      "episode 9708, reward 809.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 143\n",
      "Initial State is  [0, 11, 1]\n",
      "episode 9709, reward 883.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 130\n",
      "Initial State is  [3, 6, 1]\n",
      "episode 9710, reward 954.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 124\n",
      "Initial State is  [0, 8, 2]\n",
      "episode 9711, reward 896.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 135\n",
      "Initial State is  [2, 0, 2]\n",
      "episode 9712, reward 748.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 139\n",
      "Initial State is  [0, 20, 0]\n",
      "episode 9713, reward 950.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 128\n",
      "Initial State is  [0, 23, 6]\n",
      "episode 9714, reward 918.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 131\n",
      "Initial State is  [2, 14, 3]\n",
      "episode 9715, reward 879.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 125\n",
      "Initial State is  [0, 20, 5]\n",
      "episode 9716, reward 674.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 127\n",
      "Initial State is  [0, 21, 0]\n",
      "episode 9717, reward 981.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 117\n",
      "Initial State is  [2, 2, 6]\n",
      "episode 9718, reward 878.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 124\n",
      "Initial State is  [4, 16, 3]\n",
      "episode 9719, reward 981.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 129\n",
      "Initial State is  [2, 0, 0]\n",
      "episode 9720, reward 1140.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 131\n",
      "Initial State is  [0, 17, 4]\n",
      "episode 9721, reward 826.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 127\n",
      "Initial State is  [4, 18, 6]\n",
      "episode 9722, reward 935.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 133\n",
      "Initial State is  [0, 0, 0]\n",
      "episode 9723, reward 616.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 130\n",
      "Initial State is  [2, 8, 5]\n",
      "episode 9724, reward 780.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 127\n",
      "Initial State is  [1, 7, 2]\n",
      "episode 9725, reward 724.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 132\n",
      "Initial State is  [2, 7, 0]\n",
      "episode 9726, reward 916.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 134\n",
      "Initial State is  [2, 0, 1]\n",
      "episode 9727, reward 874.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 130\n",
      "Initial State is  [3, 4, 2]\n",
      "episode 9728, reward 1152.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 145\n",
      "Initial State is  [3, 0, 3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 9729, reward 1073.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 135\n",
      "Initial State is  [4, 17, 4]\n",
      "episode 9730, reward 1173.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 136\n",
      "Initial State is  [3, 16, 4]\n",
      "episode 9731, reward 734.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 136\n",
      "Initial State is  [2, 20, 2]\n",
      "episode 9732, reward 954.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 120\n",
      "Initial State is  [4, 7, 2]\n",
      "episode 9733, reward 902.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 126\n",
      "Initial State is  [1, 19, 5]\n",
      "episode 9734, reward 988.0, memory_length 2000, epsilon 0.009998671593271896, time 720.0, rides 127\n",
      "Initial State is  [0, 0, 5]\n",
      "episode 9735, reward 791.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 134\n",
      "Initial State is  [1, 7, 5]\n",
      "episode 9736, reward 1038.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 128\n",
      "Initial State is  [2, 16, 5]\n",
      "episode 9737, reward 1022.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 139\n",
      "Initial State is  [0, 23, 6]\n",
      "episode 9738, reward 889.0, memory_length 2000, epsilon 0.009998671593271896, time 736.0, rides 124\n",
      "Initial State is  [4, 14, 0]\n",
      "episode 9739, reward 887.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 152\n",
      "Initial State is  [2, 2, 5]\n",
      "episode 9740, reward 642.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 140\n",
      "Initial State is  [0, 19, 4]\n",
      "episode 9741, reward 1059.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 139\n",
      "Initial State is  [3, 19, 3]\n",
      "episode 9742, reward 1029.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 130\n",
      "Initial State is  [2, 23, 4]\n",
      "episode 9743, reward 1030.0, memory_length 2000, epsilon 0.009998671593271896, time 737.0, rides 138\n",
      "Initial State is  [3, 14, 1]\n",
      "episode 9744, reward 962.0, memory_length 2000, epsilon 0.009998671593271896, time 738.0, rides 131\n",
      "Initial State is  [4, 20, 0]\n",
      "episode 9745, reward 994.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 138\n",
      "Initial State is  [3, 3, 0]\n",
      "episode 9746, reward 866.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 138\n",
      "Initial State is  [4, 23, 5]\n",
      "episode 9747, reward 943.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 135\n",
      "Initial State is  [0, 13, 0]\n",
      "episode 9748, reward 1024.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 141\n",
      "Initial State is  [3, 19, 1]\n",
      "episode 9749, reward 676.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 131\n",
      "Initial State is  [0, 14, 3]\n",
      "episode 9750, reward 936.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 133\n",
      "Initial State is  [2, 6, 4]\n",
      "episode 9751, reward 886.0, memory_length 2000, epsilon 0.009998671593271896, time 738.0, rides 137\n",
      "Initial State is  [2, 4, 4]\n",
      "episode 9752, reward 1008.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 133\n",
      "Initial State is  [2, 9, 2]\n",
      "episode 9753, reward 901.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 145\n",
      "Initial State is  [1, 19, 0]\n",
      "episode 9754, reward 918.0, memory_length 2000, epsilon 0.009998671593271896, time 736.0, rides 142\n",
      "Initial State is  [4, 12, 5]\n",
      "episode 9755, reward 783.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 124\n",
      "Initial State is  [0, 20, 4]\n",
      "episode 9756, reward 818.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 120\n",
      "Initial State is  [1, 21, 6]\n",
      "episode 9757, reward 934.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 133\n",
      "Initial State is  [1, 12, 5]\n",
      "episode 9758, reward 695.0, memory_length 2000, epsilon 0.009998671593271896, time 720.0, rides 128\n",
      "Initial State is  [2, 10, 3]\n",
      "episode 9759, reward 804.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 132\n",
      "Initial State is  [0, 16, 1]\n",
      "episode 9760, reward 725.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 128\n",
      "Initial State is  [4, 17, 0]\n",
      "episode 9761, reward 912.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 136\n",
      "Initial State is  [0, 9, 4]\n",
      "episode 9762, reward 934.0, memory_length 2000, epsilon 0.009998671593271896, time 745.0, rides 145\n",
      "Initial State is  [4, 6, 0]\n",
      "episode 9763, reward 682.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 137\n",
      "Initial State is  [3, 4, 6]\n",
      "episode 9764, reward 970.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 137\n",
      "Initial State is  [4, 13, 1]\n",
      "episode 9765, reward 842.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 139\n",
      "Initial State is  [3, 20, 0]\n",
      "episode 9766, reward 757.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 137\n",
      "Initial State is  [4, 15, 4]\n",
      "episode 9767, reward 949.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 131\n",
      "Initial State is  [0, 4, 5]\n",
      "episode 9768, reward 764.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 130\n",
      "Initial State is  [0, 11, 0]\n",
      "episode 9769, reward 1100.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 128\n",
      "Initial State is  [4, 19, 1]\n",
      "episode 9770, reward 879.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 131\n",
      "Initial State is  [1, 4, 2]\n",
      "episode 9771, reward 643.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 138\n",
      "Initial State is  [0, 0, 3]\n",
      "episode 9772, reward 1020.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 146\n",
      "Initial State is  [3, 18, 6]\n",
      "episode 9773, reward 1203.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 126\n",
      "Initial State is  [1, 14, 2]\n",
      "episode 9774, reward 968.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 125\n",
      "Initial State is  [2, 10, 3]\n",
      "episode 9775, reward 847.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 126\n",
      "Initial State is  [3, 5, 0]\n",
      "episode 9776, reward 852.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 140\n",
      "Initial State is  [1, 20, 5]\n",
      "episode 9777, reward 921.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 126\n",
      "Initial State is  [3, 1, 6]\n",
      "episode 9778, reward 817.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 124\n",
      "Initial State is  [4, 23, 6]\n",
      "episode 9779, reward 821.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 130\n",
      "Initial State is  [2, 8, 2]\n",
      "episode 9780, reward 845.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 134\n",
      "Initial State is  [3, 17, 3]\n",
      "episode 9781, reward 1100.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 135\n",
      "Initial State is  [4, 20, 4]\n",
      "episode 9782, reward 956.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 125\n",
      "Initial State is  [1, 14, 2]\n",
      "episode 9783, reward 1046.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 128\n",
      "Initial State is  [3, 1, 0]\n",
      "episode 9784, reward 905.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 133\n",
      "Initial State is  [2, 5, 5]\n",
      "episode 9785, reward 990.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 140\n",
      "Initial State is  [0, 17, 6]\n",
      "episode 9786, reward 749.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 129\n",
      "Initial State is  [0, 20, 4]\n",
      "episode 9787, reward 1267.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 128\n",
      "Initial State is  [2, 10, 1]\n",
      "episode 9788, reward 1036.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 127\n",
      "Initial State is  [0, 7, 2]\n",
      "episode 9789, reward 962.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 123\n",
      "Initial State is  [4, 0, 6]\n",
      "episode 9790, reward 727.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 131\n",
      "Initial State is  [2, 15, 5]\n",
      "episode 9791, reward 858.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 142\n",
      "Initial State is  [4, 16, 0]\n",
      "episode 9792, reward 733.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 125\n",
      "Initial State is  [0, 17, 5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 9793, reward 791.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 125\n",
      "Initial State is  [0, 3, 3]\n",
      "episode 9794, reward 832.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 141\n",
      "Initial State is  [2, 1, 5]\n",
      "episode 9795, reward 1023.0, memory_length 2000, epsilon 0.009998671593271896, time 737.0, rides 137\n",
      "Initial State is  [1, 9, 6]\n",
      "episode 9796, reward 622.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 125\n",
      "Initial State is  [2, 22, 3]\n",
      "episode 9797, reward 1221.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 132\n",
      "Initial State is  [1, 20, 1]\n",
      "episode 9798, reward 775.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 143\n",
      "Initial State is  [0, 12, 2]\n",
      "episode 9799, reward 766.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 127\n",
      "Initial State is  [1, 14, 5]\n",
      "episode 9800, reward 807.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 133\n",
      "Initial State is  [3, 6, 1]\n",
      "episode 9801, reward 889.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 131\n",
      "Initial State is  [4, 14, 1]\n",
      "episode 9802, reward 597.0, memory_length 2000, epsilon 0.009998671593271896, time 737.0, rides 131\n",
      "Initial State is  [1, 3, 2]\n",
      "episode 9803, reward 681.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 137\n",
      "Initial State is  [0, 12, 3]\n",
      "episode 9804, reward 1229.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 144\n",
      "Initial State is  [2, 15, 6]\n",
      "episode 9805, reward 824.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 133\n",
      "Initial State is  [3, 23, 4]\n",
      "episode 9806, reward 947.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 130\n",
      "Initial State is  [1, 3, 6]\n",
      "episode 9807, reward 1014.0, memory_length 2000, epsilon 0.009998671593271896, time 737.0, rides 122\n",
      "Initial State is  [1, 13, 0]\n",
      "episode 9808, reward 690.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 132\n",
      "Initial State is  [4, 17, 3]\n",
      "episode 9809, reward 1095.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 134\n",
      "Initial State is  [0, 15, 1]\n",
      "episode 9810, reward 1305.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 130\n",
      "Initial State is  [1, 10, 1]\n",
      "episode 9811, reward 981.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 131\n",
      "Initial State is  [2, 18, 1]\n",
      "episode 9812, reward 767.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 128\n",
      "Initial State is  [2, 9, 0]\n",
      "episode 9813, reward 655.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 132\n",
      "Initial State is  [1, 6, 2]\n",
      "episode 9814, reward 996.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 118\n",
      "Initial State is  [2, 1, 1]\n",
      "episode 9815, reward 1000.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 132\n",
      "Initial State is  [3, 16, 3]\n",
      "episode 9816, reward 849.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 120\n",
      "Initial State is  [1, 11, 0]\n",
      "episode 9817, reward 903.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 138\n",
      "Initial State is  [4, 3, 5]\n",
      "episode 9818, reward 751.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 141\n",
      "Initial State is  [3, 3, 2]\n",
      "episode 9819, reward 835.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 135\n",
      "Initial State is  [3, 1, 2]\n",
      "episode 9820, reward 1084.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 138\n",
      "Initial State is  [1, 2, 6]\n",
      "episode 9821, reward 764.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 129\n",
      "Initial State is  [2, 0, 0]\n",
      "episode 9822, reward 1265.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 141\n",
      "Initial State is  [0, 2, 3]\n",
      "episode 9823, reward 1264.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 144\n",
      "Initial State is  [1, 7, 0]\n",
      "episode 9824, reward 1003.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 127\n",
      "Initial State is  [2, 17, 1]\n",
      "episode 9825, reward 1070.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 142\n",
      "Initial State is  [3, 16, 1]\n",
      "episode 9826, reward 693.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 153\n",
      "Initial State is  [2, 22, 0]\n",
      "episode 9827, reward 977.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 129\n",
      "Initial State is  [1, 3, 5]\n",
      "episode 9828, reward 955.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 132\n",
      "Initial State is  [3, 4, 5]\n",
      "episode 9829, reward 1011.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 146\n",
      "Initial State is  [4, 2, 2]\n",
      "episode 9830, reward 997.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 127\n",
      "Initial State is  [1, 5, 1]\n",
      "episode 9831, reward 990.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 133\n",
      "Initial State is  [0, 5, 3]\n",
      "episode 9832, reward 1208.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 133\n",
      "Initial State is  [4, 7, 5]\n",
      "episode 9833, reward 1184.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 132\n",
      "Initial State is  [2, 13, 6]\n",
      "episode 9834, reward 976.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 124\n",
      "Initial State is  [0, 23, 4]\n",
      "episode 9835, reward 1231.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 142\n",
      "Initial State is  [2, 0, 1]\n",
      "episode 9836, reward 952.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 145\n",
      "Initial State is  [0, 3, 3]\n",
      "episode 9837, reward 1086.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 136\n",
      "Initial State is  [4, 12, 3]\n",
      "episode 9838, reward 1261.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 142\n",
      "Initial State is  [0, 18, 5]\n",
      "episode 9839, reward 804.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 139\n",
      "Initial State is  [1, 9, 1]\n",
      "episode 9840, reward 624.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 143\n",
      "Initial State is  [0, 16, 4]\n",
      "episode 9841, reward 1361.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 154\n",
      "Initial State is  [0, 15, 4]\n",
      "episode 9842, reward 905.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 124\n",
      "Initial State is  [4, 10, 5]\n",
      "episode 9843, reward 767.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 127\n",
      "Initial State is  [2, 22, 6]\n",
      "episode 9844, reward 785.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 132\n",
      "Initial State is  [2, 0, 6]\n",
      "episode 9845, reward 949.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 140\n",
      "Initial State is  [4, 19, 4]\n",
      "episode 9846, reward 959.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 143\n",
      "Initial State is  [3, 11, 6]\n",
      "episode 9847, reward 1082.0, memory_length 2000, epsilon 0.009998671593271896, time 721.0, rides 139\n",
      "Initial State is  [0, 0, 1]\n",
      "episode 9848, reward 982.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 139\n",
      "Initial State is  [4, 10, 6]\n",
      "episode 9849, reward 1279.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 139\n",
      "Initial State is  [0, 6, 6]\n",
      "episode 9850, reward 1085.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 134\n",
      "Initial State is  [0, 0, 6]\n",
      "episode 9851, reward 857.0, memory_length 2000, epsilon 0.009998671593271896, time 721.0, rides 139\n",
      "Initial State is  [2, 20, 3]\n",
      "episode 9852, reward 636.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 132\n",
      "Initial State is  [2, 9, 2]\n",
      "episode 9853, reward 846.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 130\n",
      "Initial State is  [3, 16, 4]\n",
      "episode 9854, reward 739.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 126\n",
      "Initial State is  [0, 14, 2]\n",
      "episode 9855, reward 1058.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 142\n",
      "Initial State is  [4, 4, 5]\n",
      "episode 9856, reward 782.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 122\n",
      "Initial State is  [2, 2, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 9857, reward 1066.0, memory_length 2000, epsilon 0.009998671593271896, time 741.0, rides 142\n",
      "Initial State is  [0, 5, 4]\n",
      "episode 9858, reward 932.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 138\n",
      "Initial State is  [4, 8, 2]\n",
      "episode 9859, reward 805.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 128\n",
      "Initial State is  [0, 22, 1]\n",
      "episode 9860, reward 933.0, memory_length 2000, epsilon 0.009998671593271896, time 737.0, rides 137\n",
      "Initial State is  [1, 7, 4]\n",
      "episode 9861, reward 644.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 117\n",
      "Initial State is  [4, 2, 3]\n",
      "episode 9862, reward 843.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 134\n",
      "Initial State is  [1, 16, 5]\n",
      "episode 9863, reward 741.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 129\n",
      "Initial State is  [3, 22, 6]\n",
      "episode 9864, reward 1151.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 134\n",
      "Initial State is  [3, 15, 1]\n",
      "episode 9865, reward 684.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 139\n",
      "Initial State is  [2, 20, 2]\n",
      "episode 9866, reward 678.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 150\n",
      "Initial State is  [3, 21, 5]\n",
      "episode 9867, reward 684.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 135\n",
      "Initial State is  [0, 2, 2]\n",
      "episode 9868, reward 1472.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 138\n",
      "Initial State is  [0, 10, 6]\n",
      "episode 9869, reward 1073.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 131\n",
      "Initial State is  [1, 11, 4]\n",
      "episode 9870, reward 1006.0, memory_length 2000, epsilon 0.009998671593271896, time 741.0, rides 129\n",
      "Initial State is  [2, 14, 4]\n",
      "episode 9871, reward 1054.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 130\n",
      "Initial State is  [3, 15, 0]\n",
      "episode 9872, reward 933.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 139\n",
      "Initial State is  [3, 6, 4]\n",
      "episode 9873, reward 843.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 136\n",
      "Initial State is  [0, 10, 5]\n",
      "episode 9874, reward 1009.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 128\n",
      "Initial State is  [4, 10, 5]\n",
      "episode 9875, reward 1171.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 139\n",
      "Initial State is  [0, 5, 5]\n",
      "episode 9876, reward 1011.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 143\n",
      "Initial State is  [2, 10, 6]\n",
      "episode 9877, reward 1176.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 144\n",
      "Initial State is  [4, 3, 4]\n",
      "episode 9878, reward 877.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 149\n",
      "Initial State is  [2, 4, 3]\n",
      "episode 9879, reward 958.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 140\n",
      "Initial State is  [3, 11, 2]\n",
      "episode 9880, reward 917.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 132\n",
      "Initial State is  [2, 4, 3]\n",
      "episode 9881, reward 839.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 138\n",
      "Initial State is  [0, 13, 2]\n",
      "episode 9882, reward 885.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 135\n",
      "Initial State is  [2, 15, 2]\n",
      "episode 9883, reward 1055.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 146\n",
      "Initial State is  [0, 7, 4]\n",
      "episode 9884, reward 914.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 155\n",
      "Initial State is  [2, 4, 4]\n",
      "episode 9885, reward 1074.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 141\n",
      "Initial State is  [3, 11, 1]\n",
      "episode 9886, reward 1159.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 133\n",
      "Initial State is  [4, 16, 6]\n",
      "episode 9887, reward 941.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 135\n",
      "Initial State is  [2, 5, 5]\n",
      "episode 9888, reward 1050.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 144\n",
      "Initial State is  [3, 19, 3]\n",
      "episode 9889, reward 979.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 142\n",
      "Initial State is  [4, 22, 0]\n",
      "episode 9890, reward 1106.0, memory_length 2000, epsilon 0.009998671593271896, time 721.0, rides 147\n",
      "Initial State is  [0, 12, 2]\n",
      "episode 9891, reward 814.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 137\n",
      "Initial State is  [4, 2, 6]\n",
      "episode 9892, reward 1013.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 142\n",
      "Initial State is  [2, 23, 5]\n",
      "episode 9893, reward 806.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 128\n",
      "Initial State is  [1, 12, 1]\n",
      "episode 9894, reward 1073.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 149\n",
      "Initial State is  [1, 3, 1]\n",
      "episode 9895, reward 838.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 138\n",
      "Initial State is  [3, 0, 5]\n",
      "episode 9896, reward 1204.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 127\n",
      "Initial State is  [2, 11, 0]\n",
      "episode 9897, reward 779.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 135\n",
      "Initial State is  [2, 0, 5]\n",
      "episode 9898, reward 1140.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 150\n",
      "Initial State is  [1, 11, 4]\n",
      "episode 9899, reward 942.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 149\n",
      "Initial State is  [4, 3, 4]\n",
      "episode 9900, reward 1035.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 144\n",
      "Initial State is  [4, 3, 5]\n",
      "episode 9901, reward 968.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 144\n",
      "Initial State is  [3, 14, 1]\n",
      "episode 9902, reward 592.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 130\n",
      "Initial State is  [1, 23, 0]\n",
      "episode 9903, reward 1142.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 146\n",
      "Initial State is  [1, 18, 2]\n",
      "episode 9904, reward 1181.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 136\n",
      "Initial State is  [4, 0, 1]\n",
      "episode 9905, reward 942.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 144\n",
      "Initial State is  [4, 20, 0]\n",
      "episode 9906, reward 861.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 145\n",
      "Initial State is  [4, 22, 5]\n",
      "episode 9907, reward 837.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 138\n",
      "Initial State is  [3, 22, 5]\n",
      "episode 9908, reward 944.0, memory_length 2000, epsilon 0.009998671593271896, time 736.0, rides 152\n",
      "Initial State is  [3, 4, 0]\n",
      "episode 9909, reward 976.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 137\n",
      "Initial State is  [0, 7, 3]\n",
      "episode 9910, reward 986.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 135\n",
      "Initial State is  [4, 0, 1]\n",
      "episode 9911, reward 984.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 149\n",
      "Initial State is  [3, 13, 0]\n",
      "episode 9912, reward 1241.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 138\n",
      "Initial State is  [1, 12, 2]\n",
      "episode 9913, reward 963.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 150\n",
      "Initial State is  [0, 1, 2]\n",
      "episode 9914, reward 883.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 135\n",
      "Initial State is  [1, 2, 6]\n",
      "episode 9915, reward 858.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 131\n",
      "Initial State is  [2, 15, 5]\n",
      "episode 9916, reward 907.0, memory_length 2000, epsilon 0.009998671593271896, time 737.0, rides 133\n",
      "Initial State is  [4, 17, 5]\n",
      "episode 9917, reward 954.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 142\n",
      "Initial State is  [4, 17, 5]\n",
      "episode 9918, reward 896.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 132\n",
      "Initial State is  [0, 5, 5]\n",
      "episode 9919, reward 986.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 138\n",
      "Initial State is  [2, 10, 5]\n",
      "episode 9920, reward 827.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 141\n",
      "Initial State is  [0, 8, 3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 9921, reward 911.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 131\n",
      "Initial State is  [2, 2, 1]\n",
      "episode 9922, reward 662.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 148\n",
      "Initial State is  [1, 17, 1]\n",
      "episode 9923, reward 1058.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 155\n",
      "Initial State is  [0, 2, 3]\n",
      "episode 9924, reward 1122.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 133\n",
      "Initial State is  [2, 1, 6]\n",
      "episode 9925, reward 1123.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 131\n",
      "Initial State is  [4, 11, 1]\n",
      "episode 9926, reward 815.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 140\n",
      "Initial State is  [1, 12, 5]\n",
      "episode 9927, reward 1205.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 135\n",
      "Initial State is  [0, 0, 5]\n",
      "episode 9928, reward 928.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 129\n",
      "Initial State is  [0, 1, 0]\n",
      "episode 9929, reward 1162.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 131\n",
      "Initial State is  [1, 8, 4]\n",
      "episode 9930, reward 656.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 127\n",
      "Initial State is  [4, 7, 3]\n",
      "episode 9931, reward 767.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 138\n",
      "Initial State is  [1, 22, 3]\n",
      "episode 9932, reward 840.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 128\n",
      "Initial State is  [3, 4, 3]\n",
      "episode 9933, reward 1034.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 143\n",
      "Initial State is  [0, 20, 0]\n",
      "episode 9934, reward 928.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 144\n",
      "Initial State is  [0, 22, 4]\n",
      "episode 9935, reward 925.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 136\n",
      "Initial State is  [2, 14, 2]\n",
      "episode 9936, reward 877.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 134\n",
      "Initial State is  [2, 11, 4]\n",
      "episode 9937, reward 901.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 130\n",
      "Initial State is  [2, 23, 5]\n",
      "episode 9938, reward 1187.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 124\n",
      "Initial State is  [0, 14, 3]\n",
      "episode 9939, reward 947.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 139\n",
      "Initial State is  [4, 2, 0]\n",
      "episode 9940, reward 1096.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 132\n",
      "Initial State is  [1, 5, 0]\n",
      "episode 9941, reward 867.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 131\n",
      "Initial State is  [0, 5, 1]\n",
      "episode 9942, reward 1027.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 137\n",
      "Initial State is  [4, 3, 2]\n",
      "episode 9943, reward 1035.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 134\n",
      "Initial State is  [4, 17, 3]\n",
      "episode 9944, reward 954.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 130\n",
      "Initial State is  [4, 19, 6]\n",
      "episode 9945, reward 1182.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 128\n",
      "Initial State is  [0, 2, 1]\n",
      "episode 9946, reward 968.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 136\n",
      "Initial State is  [2, 5, 0]\n",
      "episode 9947, reward 725.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 138\n",
      "Initial State is  [3, 19, 3]\n",
      "episode 9948, reward 909.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 138\n",
      "Initial State is  [1, 11, 3]\n",
      "episode 9949, reward 1020.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 144\n",
      "Initial State is  [2, 9, 6]\n",
      "episode 9950, reward 996.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 138\n",
      "Initial State is  [2, 12, 6]\n",
      "episode 9951, reward 901.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 135\n",
      "Initial State is  [4, 15, 1]\n",
      "episode 9952, reward 1068.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 136\n",
      "Initial State is  [3, 17, 5]\n",
      "episode 9953, reward 764.0, memory_length 2000, epsilon 0.009998671593271896, time 722.0, rides 147\n",
      "Initial State is  [0, 2, 0]\n",
      "episode 9954, reward 875.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 137\n",
      "Initial State is  [2, 3, 2]\n",
      "episode 9955, reward 737.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 143\n",
      "Initial State is  [1, 21, 6]\n",
      "episode 9956, reward 1053.0, memory_length 2000, epsilon 0.009998671593271896, time 721.0, rides 145\n",
      "Initial State is  [4, 23, 3]\n",
      "episode 9957, reward 551.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 135\n",
      "Initial State is  [0, 1, 1]\n",
      "episode 9958, reward 804.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 140\n",
      "Initial State is  [1, 22, 3]\n",
      "episode 9959, reward 819.0, memory_length 2000, epsilon 0.009998671593271896, time 734.0, rides 139\n",
      "Initial State is  [4, 0, 1]\n",
      "episode 9960, reward 969.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 134\n",
      "Initial State is  [1, 13, 0]\n",
      "episode 9961, reward 1077.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 133\n",
      "Initial State is  [3, 17, 3]\n",
      "episode 9962, reward 623.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 156\n",
      "Initial State is  [2, 13, 5]\n",
      "episode 9963, reward 888.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 131\n",
      "Initial State is  [0, 13, 0]\n",
      "episode 9964, reward 1115.0, memory_length 2000, epsilon 0.009998671593271896, time 733.0, rides 142\n",
      "Initial State is  [3, 7, 5]\n",
      "episode 9965, reward 894.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 123\n",
      "Initial State is  [4, 21, 2]\n",
      "episode 9966, reward 1127.0, memory_length 2000, epsilon 0.009998671593271896, time 724.0, rides 144\n",
      "Initial State is  [3, 18, 0]\n",
      "episode 9967, reward 815.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 145\n",
      "Initial State is  [0, 2, 4]\n",
      "episode 9968, reward 1276.0, memory_length 2000, epsilon 0.009998671593271896, time 738.0, rides 131\n",
      "Initial State is  [1, 23, 1]\n",
      "episode 9969, reward 990.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 137\n",
      "Initial State is  [4, 0, 3]\n",
      "episode 9970, reward 883.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 142\n",
      "Initial State is  [2, 12, 4]\n",
      "episode 9971, reward 1017.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 131\n",
      "Initial State is  [4, 20, 1]\n",
      "episode 9972, reward 998.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 139\n",
      "Initial State is  [4, 16, 0]\n",
      "episode 9973, reward 854.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 136\n",
      "Initial State is  [4, 18, 1]\n",
      "episode 9974, reward 745.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 146\n",
      "Initial State is  [3, 6, 2]\n",
      "episode 9975, reward 799.0, memory_length 2000, epsilon 0.009998671593271896, time 728.0, rides 142\n",
      "Initial State is  [1, 19, 3]\n",
      "episode 9976, reward 1078.0, memory_length 2000, epsilon 0.009998671593271896, time 723.0, rides 138\n",
      "Initial State is  [0, 19, 0]\n",
      "episode 9977, reward 723.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 134\n",
      "Initial State is  [1, 23, 2]\n",
      "episode 9978, reward 1028.0, memory_length 2000, epsilon 0.009998671593271896, time 736.0, rides 145\n",
      "Initial State is  [1, 17, 4]\n",
      "episode 9979, reward 942.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 154\n",
      "Initial State is  [2, 7, 6]\n",
      "episode 9980, reward 1037.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 128\n",
      "Initial State is  [2, 16, 4]\n",
      "episode 9981, reward 835.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 144\n",
      "Initial State is  [0, 14, 5]\n",
      "episode 9982, reward 709.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 145\n",
      "Initial State is  [0, 19, 4]\n",
      "episode 9983, reward 874.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 151\n",
      "Initial State is  [3, 6, 0]\n",
      "episode 9984, reward 843.0, memory_length 2000, epsilon 0.009998671593271896, time 731.0, rides 140\n",
      "Initial State is  [3, 13, 3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 9985, reward 712.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 131\n",
      "Initial State is  [4, 14, 6]\n",
      "episode 9986, reward 883.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 137\n",
      "Initial State is  [3, 18, 3]\n",
      "episode 9987, reward 1022.0, memory_length 2000, epsilon 0.009998671593271896, time 726.0, rides 141\n",
      "Initial State is  [0, 11, 1]\n",
      "episode 9988, reward 859.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 138\n",
      "Initial State is  [1, 14, 2]\n",
      "episode 9989, reward 802.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 136\n",
      "Initial State is  [0, 15, 6]\n",
      "episode 9990, reward 1016.0, memory_length 2000, epsilon 0.009998671593271896, time 735.0, rides 134\n",
      "Initial State is  [0, 18, 1]\n",
      "episode 9991, reward 1008.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 138\n",
      "Initial State is  [0, 20, 1]\n",
      "episode 9992, reward 802.0, memory_length 2000, epsilon 0.009998671593271896, time 730.0, rides 141\n",
      "Initial State is  [2, 4, 2]\n",
      "episode 9993, reward 1169.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 138\n",
      "Initial State is  [4, 1, 3]\n",
      "episode 9994, reward 1178.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 139\n",
      "Initial State is  [1, 22, 0]\n",
      "episode 9995, reward 1136.0, memory_length 2000, epsilon 0.009998671593271896, time 732.0, rides 155\n",
      "Initial State is  [4, 15, 0]\n",
      "episode 9996, reward 1095.0, memory_length 2000, epsilon 0.009998671593271896, time 729.0, rides 135\n",
      "Initial State is  [1, 4, 4]\n",
      "episode 9997, reward 509.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 137\n",
      "Initial State is  [1, 23, 6]\n",
      "episode 9998, reward 934.0, memory_length 2000, epsilon 0.009998671593271896, time 725.0, rides 138\n",
      "Initial State is  [2, 3, 4]\n",
      "episode 9999, reward 1083.0, memory_length 2000, epsilon 0.009998671593271896, time 727.0, rides 139\n"
     ]
    }
   ],
   "source": [
    "agent = DQNAgent(36,21)\n",
    "rewards_per_episode, episodes = [], []\n",
    "\n",
    "for episode in range(Episodes):\n",
    "\n",
    "    # Write code here\n",
    "    # Call the environment\n",
    "    env = CabDriver()\n",
    "    # Call all the initialised variables of the environment\n",
    "    state_space = env.state_space\n",
    "    action_space = env.action_space\n",
    "    state = env.state_init\n",
    "    print(\"Initial State is \",state)\n",
    "    time = 0\n",
    "\n",
    "    #Call the DQN agent\n",
    "    terminal_state = False\n",
    "    score = 0\n",
    "    action = agent.get_action(env.state_encod_arch1(state),env)\n",
    "    score += env.reward_func(state,action_space[action],Time_matrix)\n",
    "    next_state,ride_time = env.next_state_func(state,action_space[action],Time_matrix)\n",
    "    time += ride_time\n",
    "    if time >= 24*30:\n",
    "        agent.append_sample(env.state_encod_arch1(state),action,score,env.state_encod_arch1(next_state),True)\n",
    "    else:\n",
    "        agent.append_sample(env.state_encod_arch1(state),action,score,env.state_encod_arch1(next_state),False)\n",
    "    loop = 0\n",
    "    \n",
    "    \n",
    "    while not terminal_state:\n",
    "        \n",
    "        # Write your code here\n",
    "        if time >= 24*30:\n",
    "            terminal_state = True\n",
    "            pass\n",
    "        state = next_state\n",
    "        # 1. Pick epsilon-greedy action from possible actions for the current state\n",
    "        action = agent.get_action(env.state_encod_arch1(state),env)\n",
    "        # 2. Evaluate your reward and next state\n",
    "        reward_current_ride = env.reward_func(state,action_space[action],Time_matrix)\n",
    "        score+= reward_current_ride\n",
    "        next_state,ride_time = env.next_state_func(next_state,action_space[action],Time_matrix)\n",
    "        time += ride_time\n",
    "        # 3. Append the experience to the memory\n",
    "        if time >= 24*30:\n",
    "            agent.append_sample(env.state_encod_arch1(state),action,reward_current_ride,env.state_encod_arch1(next_state),True)\n",
    "        else:\n",
    "            agent.append_sample(env.state_encod_arch1(state),action,reward_current_ride,env.state_encod_arch1(next_state),False)\n",
    "        # 4. Train the model by calling function agent.train_model\n",
    "        agent.train_model(env)\n",
    "        #print('Time elapsed {} and current loop {}'.format(time,loop))\n",
    "        loop+= 1\n",
    "        # 5. Keep a track of rewards, Q-values, loss\n",
    "    \n",
    "    rewards_per_episode.append(score)   \n",
    "    episodes.append(episode)\n",
    "        \n",
    "    if agent.epsilon > agent.epsilon_min:\n",
    "        agent.epsilon *= agent.epsilon_decay\n",
    "        \n",
    "    # every episode:\n",
    "    print(\"episode {0}, reward {1}, memory_length {2}, epsilon {3}, time {4}, rides {5}\".format(episode,\n",
    "                                                                         score,\n",
    "                                                                         len(agent.memory),\n",
    "                                                                         agent.epsilon,time,loop))\n",
    "\n",
    "    agent.save_tracking_states()\n",
    "    \n",
    "    # every few episodes:\n",
    "    if episode % 1000 == 0:\n",
    "        # save model weights\n",
    "        agent.save(name=\"model.h5\")\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(dict,\n",
       "            {(1,\n",
       "              6,\n",
       "              4): {(1, 4): [0.15365586,\n",
       "               0.0,\n",
       "               0.0,\n",
       "               0.0,\n",
       "               0.07532124,\n",
       "               19.393719,\n",
       "               17.304663,\n",
       "               22.296398,\n",
       "               43.982437,\n",
       "               31.149467,\n",
       "               31.641005,\n",
       "               34.557747,\n",
       "               31.060205,\n",
       "               52.076214,\n",
       "               56.22247,\n",
       "               43.680355,\n",
       "               38.301422,\n",
       "               37.843483,\n",
       "               48.91844,\n",
       "               44.32034,\n",
       "               45.741043,\n",
       "               48.243732,\n",
       "               37.1399,\n",
       "               40.964447,\n",
       "               42.260742,\n",
       "               36.650585,\n",
       "               45.03706,\n",
       "               41.434376,\n",
       "               56.482735,\n",
       "               57.64739,\n",
       "               39.780262,\n",
       "               36.81128,\n",
       "               40.804214,\n",
       "               58.755043,\n",
       "               40.373764,\n",
       "               41.671444,\n",
       "               49.117943,\n",
       "               44.513023,\n",
       "               48.367973,\n",
       "               60.885635,\n",
       "               33.57144,\n",
       "               36.432117,\n",
       "               35.370327,\n",
       "               40.195374,\n",
       "               46.14943,\n",
       "               38.80022,\n",
       "               44.6629,\n",
       "               41.133995,\n",
       "               49.53269,\n",
       "               34.003178,\n",
       "               55.714737,\n",
       "               36.39384,\n",
       "               47.95104,\n",
       "               43.6026,\n",
       "               55.51636,\n",
       "               43.957066,\n",
       "               54.549522,\n",
       "               57.456818,\n",
       "               50.194477,\n",
       "               62.789448,\n",
       "               49.735676,\n",
       "               49.361225,\n",
       "               40.29492,\n",
       "               58.440804,\n",
       "               54.478336,\n",
       "               46.83749,\n",
       "               46.915638,\n",
       "               47.663383,\n",
       "               46.68626,\n",
       "               51.768467,\n",
       "               68.45702,\n",
       "               48.58883,\n",
       "               57.136284,\n",
       "               64.96928,\n",
       "               75.74543,\n",
       "               61.752254,\n",
       "               50.24346,\n",
       "               50.306274,\n",
       "               60.590714,\n",
       "               50.017624,\n",
       "               58.339134,\n",
       "               61.667683,\n",
       "               48.982525,\n",
       "               53.65314,\n",
       "               55.145145,\n",
       "               49.824066,\n",
       "               45.442703,\n",
       "               43.968204,\n",
       "               43.41633,\n",
       "               57.19954,\n",
       "               46.90046,\n",
       "               54.162693,\n",
       "               51.714497,\n",
       "               48.963142,\n",
       "               49.03111,\n",
       "               44.960056,\n",
       "               42.69817,\n",
       "               33.09674,\n",
       "               29.915514,\n",
       "               33.205368,\n",
       "               32.801075,\n",
       "               38.431614,\n",
       "               32.01998,\n",
       "               32.79796,\n",
       "               30.616959,\n",
       "               37.333847,\n",
       "               35.37154,\n",
       "               31.14162,\n",
       "               35.561413,\n",
       "               31.185112,\n",
       "               31.776684,\n",
       "               26.66977,\n",
       "               41.744545,\n",
       "               37.89676,\n",
       "               47.544697,\n",
       "               43.911003,\n",
       "               45.087257,\n",
       "               49.03882,\n",
       "               49.095726,\n",
       "               45.814198,\n",
       "               41.95195,\n",
       "               62.906143,\n",
       "               60.174118,\n",
       "               49.23409,\n",
       "               49.900677,\n",
       "               52.260487,\n",
       "               54.854256,\n",
       "               51.354713,\n",
       "               49.10086,\n",
       "               52.53561,\n",
       "               39.663593,\n",
       "               44.79396,\n",
       "               49.098946,\n",
       "               45.851524,\n",
       "               43.339184,\n",
       "               50.18131,\n",
       "               47.642582,\n",
       "               53.839367,\n",
       "               53.076324,\n",
       "               56.274338,\n",
       "               52.637356,\n",
       "               46.458736,\n",
       "               56.947243,\n",
       "               57.86992,\n",
       "               57.904896,\n",
       "               68.78838,\n",
       "               51.21523,\n",
       "               50.017136,\n",
       "               54.4466,\n",
       "               60.30474,\n",
       "               55.795574,\n",
       "               61.550297,\n",
       "               64.19285,\n",
       "               59.377285,\n",
       "               56.799946,\n",
       "               71.57974,\n",
       "               63.04533,\n",
       "               62.946915,\n",
       "               73.82929,\n",
       "               56.36737,\n",
       "               55.027935,\n",
       "               58.492252,\n",
       "               65.19339,\n",
       "               62.329494,\n",
       "               57.473232,\n",
       "               66.683075,\n",
       "               58.675003,\n",
       "               65.703804,\n",
       "               66.0374,\n",
       "               62.65674,\n",
       "               68.3591,\n",
       "               73.17864,\n",
       "               60.218098,\n",
       "               60.965534,\n",
       "               58.881306,\n",
       "               57.74046,\n",
       "               66.22514,\n",
       "               51.44489,\n",
       "               56.574516,\n",
       "               53.479294,\n",
       "               55.462257,\n",
       "               63.955025,\n",
       "               69.30272,\n",
       "               61.63394,\n",
       "               69.923645,\n",
       "               68.48498,\n",
       "               76.64365,\n",
       "               64.4905,\n",
       "               65.44772,\n",
       "               69.89936,\n",
       "               66.72859,\n",
       "               64.7724,\n",
       "               60.443333,\n",
       "               62.4958,\n",
       "               70.58892,\n",
       "               62.22245,\n",
       "               62.255913,\n",
       "               64.543106,\n",
       "               65.90612,\n",
       "               72.10537,\n",
       "               64.291504,\n",
       "               60.58537,\n",
       "               60.545315,\n",
       "               58.264843,\n",
       "               58.997135,\n",
       "               65.20229,\n",
       "               65.880486,\n",
       "               60.151413,\n",
       "               53.032574,\n",
       "               55.212177,\n",
       "               61.65079,\n",
       "               58.151974,\n",
       "               62.0857,\n",
       "               53.462715,\n",
       "               51.994644,\n",
       "               56.267956,\n",
       "               63.720764,\n",
       "               69.23897,\n",
       "               79.91661,\n",
       "               79.90423,\n",
       "               74.17279,\n",
       "               71.29743,\n",
       "               64.068115,\n",
       "               70.13361,\n",
       "               56.862034,\n",
       "               55.76522,\n",
       "               56.788612,\n",
       "               66.8142,\n",
       "               76.0837,\n",
       "               80.60341,\n",
       "               72.430466,\n",
       "               76.23694,\n",
       "               73.593475,\n",
       "               68.13351,\n",
       "               65.34952,\n",
       "               74.75325,\n",
       "               67.624344,\n",
       "               80.4252,\n",
       "               68.98452,\n",
       "               63.043358,\n",
       "               63.754513,\n",
       "               64.18453,\n",
       "               59.99203,\n",
       "               62.630627,\n",
       "               57.50146,\n",
       "               61.51288,\n",
       "               62.250072,\n",
       "               59.772778,\n",
       "               58.843746,\n",
       "               52.84976,\n",
       "               57.14895,\n",
       "               60.066216,\n",
       "               58.50631,\n",
       "               64.57856,\n",
       "               59.29596,\n",
       "               54.151817,\n",
       "               56.714363,\n",
       "               57.46752,\n",
       "               60.441673,\n",
       "               65.57282,\n",
       "               60.229897,\n",
       "               67.26982,\n",
       "               63.90349,\n",
       "               70.28145,\n",
       "               66.87978,\n",
       "               55.86346,\n",
       "               60.349297,\n",
       "               55.464382,\n",
       "               56.540585,\n",
       "               62.86296,\n",
       "               61.278984,\n",
       "               66.51722,\n",
       "               64.82917,\n",
       "               70.6496,\n",
       "               59.7332,\n",
       "               56.249928,\n",
       "               52.679375,\n",
       "               53.525772,\n",
       "               62.546154,\n",
       "               52.89442,\n",
       "               53.127792,\n",
       "               43.68753,\n",
       "               47.569546,\n",
       "               53.524418,\n",
       "               49.807507,\n",
       "               48.89562,\n",
       "               50.006924,\n",
       "               48.439487,\n",
       "               54.683445,\n",
       "               55.199585,\n",
       "               49.92661,\n",
       "               56.92702,\n",
       "               61.100674,\n",
       "               53.98816,\n",
       "               55.252914,\n",
       "               60.807816,\n",
       "               65.609055,\n",
       "               59.73665,\n",
       "               59.35331,\n",
       "               66.89315,\n",
       "               62.62729,\n",
       "               65.19351,\n",
       "               64.27036,\n",
       "               56.51191,\n",
       "               65.5665,\n",
       "               63.330963,\n",
       "               63.716312,\n",
       "               67.49348,\n",
       "               66.13835,\n",
       "               67.351166,\n",
       "               54.815002,\n",
       "               58.315647,\n",
       "               77.32867,\n",
       "               70.02307,\n",
       "               64.80949,\n",
       "               59.88014,\n",
       "               76.61831,\n",
       "               83.09145,\n",
       "               73.696945,\n",
       "               70.867134,\n",
       "               71.76619,\n",
       "               69.06949,\n",
       "               59.68873,\n",
       "               62.755245,\n",
       "               61.15074,\n",
       "               71.62896,\n",
       "               69.44919,\n",
       "               71.38336,\n",
       "               63.74551,\n",
       "               56.84729,\n",
       "               66.377625,\n",
       "               61.525963,\n",
       "               60.298042,\n",
       "               59.206676,\n",
       "               59.495186,\n",
       "               58.530422,\n",
       "               53.43649,\n",
       "               57.874584,\n",
       "               57.254173,\n",
       "               57.12726,\n",
       "               63.590523,\n",
       "               58.92847,\n",
       "               55.757195,\n",
       "               57.982925,\n",
       "               57.913746,\n",
       "               57.786438,\n",
       "               49.628525,\n",
       "               54.42347,\n",
       "               51.259502,\n",
       "               57.13977,\n",
       "               61.6848,\n",
       "               68.471725,\n",
       "               60.5633,\n",
       "               62.46972,\n",
       "               61.75503,\n",
       "               69.48779,\n",
       "               70.4788,\n",
       "               64.18244,\n",
       "               72.434425,\n",
       "               76.6539,\n",
       "               66.434425,\n",
       "               68.6045,\n",
       "               64.25174,\n",
       "               61.81066,\n",
       "               68.679184,\n",
       "               66.7527,\n",
       "               62.630104,\n",
       "               66.68353,\n",
       "               66.97388,\n",
       "               68.93822,\n",
       "               76.98633,\n",
       "               73.46701,\n",
       "               70.90775,\n",
       "               56.332615,\n",
       "               56.644142,\n",
       "               60.798553,\n",
       "               58.610855,\n",
       "               49.2341,\n",
       "               46.80077,\n",
       "               51.403267,\n",
       "               57.47857,\n",
       "               51.347595,\n",
       "               49.6577,\n",
       "               48.52609,\n",
       "               42.44618,\n",
       "               50.584423,\n",
       "               51.048306,\n",
       "               47.372448,\n",
       "               49.889675,\n",
       "               45.788788,\n",
       "               50.678696,\n",
       "               50.337933,\n",
       "               44.865425,\n",
       "               47.94022,\n",
       "               46.894714,\n",
       "               45.156593,\n",
       "               43.94081,\n",
       "               43.30269,\n",
       "               44.90898,\n",
       "               46.5546,\n",
       "               47.89386,\n",
       "               44.032757,\n",
       "               47.77798,\n",
       "               45.36344,\n",
       "               49.90625,\n",
       "               46.95913,\n",
       "               52.760727,\n",
       "               55.447964,\n",
       "               48.767265,\n",
       "               47.52155,\n",
       "               47.793022,\n",
       "               43.99874,\n",
       "               47.197895,\n",
       "               54.374226,\n",
       "               54.58046,\n",
       "               60.11204,\n",
       "               55.87484,\n",
       "               60.185345,\n",
       "               59.353184,\n",
       "               59.61178,\n",
       "               52.999588,\n",
       "               51.62199,\n",
       "               51.316685,\n",
       "               59.433678,\n",
       "               55.327415,\n",
       "               49.881546,\n",
       "               58.716606,\n",
       "               60.816387,\n",
       "               52.977932,\n",
       "               53.758087,\n",
       "               53.3465,\n",
       "               50.156498,\n",
       "               56.412254,\n",
       "               56.485374,\n",
       "               51.17209,\n",
       "               52.64995,\n",
       "               55.67183,\n",
       "               54.981926,\n",
       "               49.829903,\n",
       "               49.15819,\n",
       "               46.044468,\n",
       "               47.45061,\n",
       "               42.349358,\n",
       "               40.862003,\n",
       "               49.839527,\n",
       "               46.859642,\n",
       "               44.812363,\n",
       "               41.804848,\n",
       "               35.91711,\n",
       "               43.920464,\n",
       "               39.717865,\n",
       "               49.584526,\n",
       "               51.017025,\n",
       "               48.844086,\n",
       "               54.82688,\n",
       "               66.06309,\n",
       "               53.09346,\n",
       "               51.09041,\n",
       "               54.974667,\n",
       "               62.65253,\n",
       "               59.225426,\n",
       "               57.178326,\n",
       "               66.79127,\n",
       "               72.45115,\n",
       "               62.06061,\n",
       "               57.559624,\n",
       "               60.975666,\n",
       "               58.387196,\n",
       "               55.53086,\n",
       "               57.841988,\n",
       "               59.373165,\n",
       "               58.71672,\n",
       "               55.869884,\n",
       "               53.342773,\n",
       "               56.602127,\n",
       "               51.751602,\n",
       "               54.995247,\n",
       "               55.776237,\n",
       "               50.57088,\n",
       "               51.568554,\n",
       "               50.70272,\n",
       "               51.343647,\n",
       "               54.201935,\n",
       "               53.31365,\n",
       "               50.549786,\n",
       "               57.456898,\n",
       "               55.76443,\n",
       "               54.53929,\n",
       "               57.47919,\n",
       "               46.524906,\n",
       "               53.655247,\n",
       "               47.25002,\n",
       "               49.269516,\n",
       "               52.784492,\n",
       "               54.596535,\n",
       "               57.126095,\n",
       "               55.336365,\n",
       "               63.602898,\n",
       "               58.407734,\n",
       "               67.94862,\n",
       "               58.39896,\n",
       "               54.894794,\n",
       "               53.499767,\n",
       "               64.584076,\n",
       "               57.248814,\n",
       "               51.939526,\n",
       "               86.51338,\n",
       "               157.14386,\n",
       "               186.98883,\n",
       "               194.40901,\n",
       "               176.93939,\n",
       "               188.49234,\n",
       "               183.58626,\n",
       "               163.44829,\n",
       "               174.06798,\n",
       "               173.40387,\n",
       "               161.0908,\n",
       "               180.78123,\n",
       "               172.1684,\n",
       "               173.36977,\n",
       "               156.10367,\n",
       "               166.1397,\n",
       "               165.88586,\n",
       "               161.45395,\n",
       "               152.57423,\n",
       "               162.81723,\n",
       "               166.21552,\n",
       "               164.29352,\n",
       "               158.72765,\n",
       "               156.02676,\n",
       "               166.48393,\n",
       "               142.71007,\n",
       "               149.18289,\n",
       "               158.8874,\n",
       "               167.76799,\n",
       "               162.9521,\n",
       "               163.0283,\n",
       "               162.34494,\n",
       "               172.29387,\n",
       "               166.78786,\n",
       "               171.98778,\n",
       "               177.7281,\n",
       "               143.6389,\n",
       "               159.10912,\n",
       "               160.45473,\n",
       "               162.38046,\n",
       "               168.72395,\n",
       "               159.88283,\n",
       "               160.87431,\n",
       "               167.25967,\n",
       "               166.24629,\n",
       "               167.35994,\n",
       "               165.04567,\n",
       "               158.61641,\n",
       "               157.22308,\n",
       "               159.4848,\n",
       "               166.41241,\n",
       "               186.481,\n",
       "               173.64488,\n",
       "               182.1748,\n",
       "               169.92351,\n",
       "               172.04858,\n",
       "               161.67583,\n",
       "               157.59166,\n",
       "               157.8613,\n",
       "               167.01453,\n",
       "               148.63202,\n",
       "               138.76466,\n",
       "               146.32808,\n",
       "               150.1037,\n",
       "               151.76135,\n",
       "               148.13132,\n",
       "               149.19775,\n",
       "               149.36186,\n",
       "               151.94957,\n",
       "               156.17346,\n",
       "               152.20354,\n",
       "               161.31195,\n",
       "               165.9175,\n",
       "               161.19803,\n",
       "               164.56519,\n",
       "               164.95116,\n",
       "               155.92033,\n",
       "               171.78862,\n",
       "               174.0508,\n",
       "               161.39272,\n",
       "               163.03781,\n",
       "               158.41528,\n",
       "               158.45967,\n",
       "               162.07881,\n",
       "               172.29163,\n",
       "               164.83484,\n",
       "               165.97017,\n",
       "               163.40572,\n",
       "               168.07362,\n",
       "               163.75368,\n",
       "               165.50124,\n",
       "               168.16223,\n",
       "               154.1224,\n",
       "               164.45782,\n",
       "               167.90015,\n",
       "               157.90984,\n",
       "               173.11455,\n",
       "               191.18593,\n",
       "               187.54718,\n",
       "               177.5037,\n",
       "               175.76117,\n",
       "               174.16959,\n",
       "               166.65463,\n",
       "               179.5806,\n",
       "               199.99167,\n",
       "               173.19067,\n",
       "               199.11133,\n",
       "               181.72758,\n",
       "               187.76967,\n",
       "               194.5319,\n",
       "               183.55919,\n",
       "               178.59624,\n",
       "               175.15666,\n",
       "               162.71881,\n",
       "               164.96692,\n",
       "               171.99086,\n",
       "               173.14253,\n",
       "               167.13277,\n",
       "               181.48268,\n",
       "               162.2103,\n",
       "               172.60004,\n",
       "               174.9215,\n",
       "               162.7598,\n",
       "               163.22182,\n",
       "               170.57904,\n",
       "               166.3824,\n",
       "               175.03592,\n",
       "               162.82332,\n",
       "               160.45988,\n",
       "               158.55542,\n",
       "               151.31786,\n",
       "               139.92615,\n",
       "               144.88042,\n",
       "               148.06636,\n",
       "               154.7612,\n",
       "               154.8999,\n",
       "               148.41275,\n",
       "               145.04613,\n",
       "               158.2333,\n",
       "               162.7961,\n",
       "               153.29659,\n",
       "               154.9448,\n",
       "               154.84406,\n",
       "               158.15279,\n",
       "               161.86185,\n",
       "               161.89162,\n",
       "               157.40996,\n",
       "               153.43692,\n",
       "               155.78394,\n",
       "               160.51952,\n",
       "               149.85493,\n",
       "               159.11313,\n",
       "               153.22264,\n",
       "               155.10094,\n",
       "               152.9165,\n",
       "               161.24725,\n",
       "               145.81308,\n",
       "               161.22191,\n",
       "               154.79367,\n",
       "               161.68213,\n",
       "               158.181,\n",
       "               155.1983,\n",
       "               151.0209,\n",
       "               150.61125,\n",
       "               156.95505,\n",
       "               154.9458,\n",
       "               151.7958,\n",
       "               146.12148,\n",
       "               151.5323,\n",
       "               156.69955,\n",
       "               151.38371,\n",
       "               150.6955,\n",
       "               156.07982,\n",
       "               165.72693,\n",
       "               163.3527,\n",
       "               180.7436,\n",
       "               186.49472,\n",
       "               173.9221,\n",
       "               173.03125,\n",
       "               172.05757,\n",
       "               176.04208,\n",
       "               167.01448,\n",
       "               165.64719,\n",
       "               167.03688,\n",
       "               155.33377,\n",
       "               151.7541,\n",
       "               150.82578,\n",
       "               155.00948,\n",
       "               160.55852,\n",
       "               154.52332,\n",
       "               156.25954,\n",
       "               158.60457,\n",
       "               155.43727,\n",
       "               146.89949,\n",
       "               153.74818,\n",
       "               148.04073,\n",
       "               129.67215,\n",
       "               141.98402,\n",
       "               146.05849,\n",
       "               145.81708,\n",
       "               129.85576,\n",
       "               143.7074,\n",
       "               139.51733,\n",
       "               149.31522,\n",
       "               163.20168,\n",
       "               158.5609,\n",
       "               162.54132,\n",
       "               159.75198,\n",
       "               160.08832,\n",
       "               167.11678,\n",
       "               174.24522,\n",
       "               178.66502,\n",
       "               168.21309,\n",
       "               165.61237,\n",
       "               165.3501,\n",
       "               155.5641,\n",
       "               154.99147,\n",
       "               161.4171,\n",
       "               160.9948,\n",
       "               160.06702,\n",
       "               161.48653,\n",
       "               165.36302,\n",
       "               162.41403,\n",
       "               178.27182,\n",
       "               169.57166,\n",
       "               162.03683,\n",
       "               167.0876,\n",
       "               160.85802,\n",
       "               149.33958,\n",
       "               155.67296,\n",
       "               149.0909,\n",
       "               150.61469,\n",
       "               153.92755,\n",
       "               134.50204,\n",
       "               146.47792,\n",
       "               144.79819,\n",
       "               140.20107,\n",
       "               135.79274,\n",
       "               137.41364,\n",
       "               147.62561,\n",
       "               144.86533,\n",
       "               143.09003,\n",
       "               153.27545,\n",
       "               157.99002,\n",
       "               148.99768,\n",
       "               149.08646,\n",
       "               151.59036,\n",
       "               160.03484,\n",
       "               144.68263,\n",
       "               150.85454,\n",
       "               172.6627,\n",
       "               158.14134,\n",
       "               164.70131,\n",
       "               159.59952,\n",
       "               160.2385,\n",
       "               147.1424,\n",
       "               146.22284,\n",
       "               145.44643,\n",
       "               150.23642,\n",
       "               138.35936,\n",
       "               140.85808,\n",
       "               131.2614,\n",
       "               139.96799,\n",
       "               134.07759,\n",
       "               146.39807,\n",
       "               139.38634,\n",
       "               156.47992,\n",
       "               143.87758,\n",
       "               143.2222,\n",
       "               132.0179,\n",
       "               143.20529,\n",
       "               150.96179,\n",
       "               137.20732,\n",
       "               141.83324,\n",
       "               150.0209,\n",
       "               145.09328,\n",
       "               138.48466,\n",
       "               135.42107,\n",
       "               125.26039,\n",
       "               140.08542,\n",
       "               138.09381,\n",
       "               132.28851,\n",
       "               137.31906,\n",
       "               146.27852,\n",
       "               143.54451,\n",
       "               143.01553,\n",
       "               144.76965,\n",
       "               145.97588,\n",
       "               136.93936,\n",
       "               149.2917,\n",
       "               140.11086,\n",
       "               139.97708,\n",
       "               141.38994,\n",
       "               150.02048,\n",
       "               139.39915,\n",
       "               133.98112,\n",
       "               135.69987,\n",
       "               148.32077,\n",
       "               146.04152,\n",
       "               142.94208,\n",
       "               143.63191,\n",
       "               148.57332,\n",
       "               139.95586,\n",
       "               152.29088,\n",
       "               143.2874,\n",
       "               141.25064,\n",
       "               142.65848,\n",
       "               154.58755,\n",
       "               155.01042,\n",
       "               141.0085,\n",
       "               156.5465,\n",
       "               146.07254,\n",
       "               152.1286,\n",
       "               153.58191,\n",
       "               149.24686,\n",
       "               150.64119,\n",
       "               143.93443,\n",
       "               147.6051,\n",
       "               155.44351,\n",
       "               153.25185,\n",
       "               153.59143,\n",
       "               159.8861,\n",
       "               135.06076,\n",
       "               128.47243,\n",
       "               106.365166,\n",
       "               105.8378,\n",
       "               114.12995,\n",
       "               107.98758,\n",
       "               119.05247,\n",
       "               108.06804,\n",
       "               100.36075,\n",
       "               97.51433,\n",
       "               106.240295,\n",
       "               110.71401,\n",
       "               99.7973,\n",
       "               88.385544,\n",
       "               115.45817,\n",
       "               106.028145,\n",
       "               118.84606,\n",
       "               112.21086,\n",
       "               131.05992,\n",
       "               132.72208,\n",
       "               133.06348,\n",
       "               129.53323,\n",
       "               137.8544,\n",
       "               131.81834,\n",
       "               135.04433,\n",
       "               138.47914,\n",
       "               138.00227,\n",
       "               149.44785,\n",
       "               144.67677,\n",
       "               144.8128,\n",
       "               151.70326,\n",
       "               142.70708,\n",
       "               147.91122,\n",
       "               138.21912,\n",
       "               149.65698,\n",
       "               150.8634,\n",
       "               164.52687,\n",
       "               167.00067,\n",
       "               158.99525,\n",
       "               151.68405,\n",
       "               167.11621,\n",
       "               149.80719,\n",
       "               153.07867,\n",
       "               159.32039,\n",
       "               162.09479,\n",
       "               147.29703,\n",
       "               132.68875,\n",
       "               143.1387,\n",
       "               127.22383,\n",
       "               134.22356,\n",
       "               128.11931,\n",
       "               111.14893,\n",
       "               115.077225,\n",
       "               110.21841,\n",
       "               107.06787,\n",
       "               107.35823,\n",
       "               101.55276,\n",
       "               102.33983,\n",
       "               94.31982,\n",
       "               105.40366,\n",
       "               84.565994,\n",
       "               88.079384,\n",
       "               92.21964,\n",
       "               95.71414,\n",
       "               88.61808,\n",
       "               82.23674,\n",
       "               78.03679,\n",
       "               68.90518,\n",
       "               70.52281,\n",
       "               74.89735,\n",
       "               98.6995,\n",
       "               77.10404,\n",
       "               90.37855,\n",
       "               81.452545,\n",
       "               92.219696,\n",
       "               82.14261,\n",
       "               88.42493,\n",
       "               98.77345,\n",
       "               99.142715,\n",
       "               109.72123,\n",
       "               126.876495,\n",
       "               140.79762,\n",
       "               142.31157,\n",
       "               147.16006,\n",
       "               150.13753,\n",
       "               147.55475,\n",
       "               145.07414,\n",
       "               135.46716,\n",
       "               164.20746,\n",
       "               155.41406,\n",
       "               171.67923,\n",
       "               154.03148,\n",
       "               177.01363,\n",
       "               171.93193,\n",
       "               165.25618,\n",
       "               165.2752,\n",
       "               161.0955,\n",
       "               165.09694,\n",
       "               147.92819,\n",
       "               149.70045,\n",
       "               158.95473,\n",
       "               166.87535,\n",
       "               156.80118,\n",
       "               162.54584,\n",
       "               148.36998,\n",
       "               158.76163,\n",
       "               149.06055,\n",
       "               150.94621,\n",
       "               144.6809,\n",
       "               152.72044,\n",
       "               162.74893,\n",
       "               147.68564,\n",
       "               149.47098,\n",
       "               146.01225,\n",
       "               149.75343,\n",
       "               163.94084,\n",
       "               164.91356,\n",
       "               148.96568,\n",
       "               146.50816,\n",
       "               151.4692,\n",
       "               150.16936,\n",
       "               155.9034,\n",
       "               151.42407,\n",
       "               144.23207,\n",
       "               143.6526,\n",
       "               145.88206,\n",
       "               153.1579,\n",
       "               154.63469,\n",
       "               156.13936,\n",
       "               153.84982,\n",
       "               147.51965,\n",
       "               147.47229,\n",
       "               141.9796,\n",
       "               151.11452,\n",
       "               156.7902,\n",
       "               148.38428,\n",
       "               152.03503,\n",
       "               153.31241,\n",
       "               146.02777,\n",
       "               151.73407,\n",
       "               150.4228,\n",
       "               148.10558,\n",
       "               138.71935,\n",
       "               139.69835,\n",
       "               148.35329,\n",
       "               140.20462,\n",
       "               141.35056,\n",
       "               130.69217,\n",
       "               140.06465,\n",
       "               134.30908,\n",
       "               143.77454,\n",
       "               153.12685,\n",
       "               149.5063,\n",
       "               148.53702,\n",
       "               149.73137,\n",
       "               147.32468,\n",
       "               153.81424,\n",
       "               134.30168,\n",
       "               146.8477,\n",
       "               143.3775,\n",
       "               137.67747,\n",
       "               141.92854,\n",
       "               134.10455,\n",
       "               128.82867,\n",
       "               130.55894,\n",
       "               133.77605,\n",
       "               139.57327,\n",
       "               138.2749,\n",
       "               136.99638,\n",
       "               146.22615,\n",
       "               146.19673,\n",
       "               144.04573,\n",
       "               ...]},\n",
       "             (1,\n",
       "              11,\n",
       "              5): {(1, 4): [0.8278823,\n",
       "               0.0,\n",
       "               0.0,\n",
       "               0.0,\n",
       "               0.0,\n",
       "               0.86176974,\n",
       "               0.0,\n",
       "               0.0,\n",
       "               0.0,\n",
       "               0.0,\n",
       "               6.135123,\n",
       "               7.0958357,\n",
       "               0.25160977,\n",
       "               14.056435,\n",
       "               19.095007,\n",
       "               18.180346,\n",
       "               12.726164,\n",
       "               12.373506,\n",
       "               30.486311,\n",
       "               33.542618,\n",
       "               38.736565,\n",
       "               43.28467,\n",
       "               32.80128,\n",
       "               45.18132,\n",
       "               43.337643,\n",
       "               45.18195,\n",
       "               50.09452,\n",
       "               50.478558,\n",
       "               62.802334,\n",
       "               68.853935,\n",
       "               54.096817,\n",
       "               49.355145,\n",
       "               48.847416,\n",
       "               55.0745,\n",
       "               41.110897,\n",
       "               41.85909,\n",
       "               63.268776,\n",
       "               53.009575,\n",
       "               59.033455,\n",
       "               64.98553,\n",
       "               37.915394,\n",
       "               42.530766,\n",
       "               42.448257,\n",
       "               46.44703,\n",
       "               55.173927,\n",
       "               45.503002,\n",
       "               46.319412,\n",
       "               52.081753,\n",
       "               53.6024,\n",
       "               47.930664,\n",
       "               66.19654,\n",
       "               49.391792,\n",
       "               66.15928,\n",
       "               55.533073,\n",
       "               51.67621,\n",
       "               45.433727,\n",
       "               53.171448,\n",
       "               63.128757,\n",
       "               49.296585,\n",
       "               64.45618,\n",
       "               51.731014,\n",
       "               54.66887,\n",
       "               51.505013,\n",
       "               57.5078,\n",
       "               61.333202,\n",
       "               54.624104,\n",
       "               45.45182,\n",
       "               44.90064,\n",
       "               35.729317,\n",
       "               40.79024,\n",
       "               51.694633,\n",
       "               35.962395,\n",
       "               43.567192,\n",
       "               51.287106,\n",
       "               71.615364,\n",
       "               52.69575,\n",
       "               39.20671,\n",
       "               39.574043,\n",
       "               43.865547,\n",
       "               37.374466,\n",
       "               43.988686,\n",
       "               50.74931,\n",
       "               43.366806,\n",
       "               45.532032,\n",
       "               44.1388,\n",
       "               43.04232,\n",
       "               34.78931,\n",
       "               38.435,\n",
       "               37.284447,\n",
       "               57.738747,\n",
       "               50.61198,\n",
       "               50.28611,\n",
       "               46.447456,\n",
       "               47.79507,\n",
       "               40.533516,\n",
       "               41.50853,\n",
       "               45.984188,\n",
       "               34.1447,\n",
       "               34.077885,\n",
       "               46.412872,\n",
       "               44.096184,\n",
       "               57.851635,\n",
       "               52.253254,\n",
       "               48.814026,\n",
       "               50.045074,\n",
       "               57.848885,\n",
       "               58.32334,\n",
       "               60.051716,\n",
       "               62.365257,\n",
       "               61.004936,\n",
       "               60.798416,\n",
       "               52.114193,\n",
       "               59.41257,\n",
       "               54.963043,\n",
       "               64.38812,\n",
       "               52.50595,\n",
       "               50.013863,\n",
       "               57.448936,\n",
       "               57.513428,\n",
       "               56.916183,\n",
       "               56.566845,\n",
       "               69.027336,\n",
       "               67.84911,\n",
       "               64.69229,\n",
       "               69.91524,\n",
       "               66.820915,\n",
       "               75.731,\n",
       "               71.214035,\n",
       "               64.829865,\n",
       "               71.61926,\n",
       "               67.87198,\n",
       "               78.77719,\n",
       "               80.56703,\n",
       "               78.49398,\n",
       "               79.83085,\n",
       "               105.94692,\n",
       "               93.60837,\n",
       "               102.62354,\n",
       "               106.67245,\n",
       "               107.10117,\n",
       "               104.43911,\n",
       "               94.08088,\n",
       "               106.62573,\n",
       "               111.891205,\n",
       "               113.52702,\n",
       "               119.09608,\n",
       "               86.66068,\n",
       "               104.77963,\n",
       "               110.978004,\n",
       "               108.024734,\n",
       "               103.69123,\n",
       "               101.12561,\n",
       "               103.00411,\n",
       "               103.63201,\n",
       "               88.53343,\n",
       "               97.49715,\n",
       "               96.852295,\n",
       "               105.1036,\n",
       "               99.01206,\n",
       "               93.686485,\n",
       "               82.523796,\n",
       "               80.624535,\n",
       "               93.63616,\n",
       "               77.757835,\n",
       "               72.94579,\n",
       "               82.95033,\n",
       "               76.03123,\n",
       "               78.92659,\n",
       "               78.682304,\n",
       "               74.29972,\n",
       "               80.502266,\n",
       "               85.37631,\n",
       "               73.39416,\n",
       "               74.499504,\n",
       "               73.60845,\n",
       "               73.943245,\n",
       "               83.41302,\n",
       "               72.86944,\n",
       "               76.75987,\n",
       "               78.88472,\n",
       "               81.75581,\n",
       "               88.98397,\n",
       "               86.51046,\n",
       "               81.00557,\n",
       "               81.52146,\n",
       "               79.09129,\n",
       "               75.801575,\n",
       "               72.04502,\n",
       "               73.95355,\n",
       "               70.78599,\n",
       "               73.26815,\n",
       "               66.60146,\n",
       "               59.50376,\n",
       "               59.18285,\n",
       "               56.99809,\n",
       "               54.415646,\n",
       "               51.1223,\n",
       "               62.753708,\n",
       "               59.261517,\n",
       "               63.228893,\n",
       "               58.32005,\n",
       "               53.010197,\n",
       "               54.40478,\n",
       "               58.6604,\n",
       "               55.10699,\n",
       "               64.40558,\n",
       "               63.118923,\n",
       "               59.757324,\n",
       "               52.16928,\n",
       "               56.247562,\n",
       "               66.4175,\n",
       "               69.91564,\n",
       "               81.32924,\n",
       "               72.2367,\n",
       "               74.58635,\n",
       "               84.62408,\n",
       "               86.48983,\n",
       "               86.495384,\n",
       "               100.817055,\n",
       "               97.99727,\n",
       "               81.76591,\n",
       "               83.932106,\n",
       "               66.19767,\n",
       "               70.68515,\n",
       "               62.053,\n",
       "               60.902245,\n",
       "               62.856415,\n",
       "               63.457138,\n",
       "               70.36386,\n",
       "               80.69304,\n",
       "               74.34048,\n",
       "               79.00857,\n",
       "               70.09098,\n",
       "               71.227684,\n",
       "               64.68566,\n",
       "               73.27632,\n",
       "               72.24547,\n",
       "               72.19943,\n",
       "               76.08541,\n",
       "               71.26465,\n",
       "               71.68673,\n",
       "               68.123314,\n",
       "               65.13844,\n",
       "               64.99327,\n",
       "               63.882656,\n",
       "               58.695965,\n",
       "               60.403633,\n",
       "               59.05251,\n",
       "               52.1039,\n",
       "               51.891315,\n",
       "               58.271828,\n",
       "               52.26207,\n",
       "               56.148914,\n",
       "               59.17964,\n",
       "               60.27941,\n",
       "               55.993423,\n",
       "               55.205425,\n",
       "               56.7001,\n",
       "               58.549416,\n",
       "               57.529736,\n",
       "               58.45228,\n",
       "               66.33896,\n",
       "               72.20174,\n",
       "               70.17031,\n",
       "               72.65646,\n",
       "               65.25682,\n",
       "               63.68327,\n",
       "               64.565,\n",
       "               65.0215,\n",
       "               66.59141,\n",
       "               65.55328,\n",
       "               68.76636,\n",
       "               69.53369,\n",
       "               68.07056,\n",
       "               63.28308,\n",
       "               67.79218,\n",
       "               66.202774,\n",
       "               71.2544,\n",
       "               72.82159,\n",
       "               68.31282,\n",
       "               68.1618,\n",
       "               56.529835,\n",
       "               68.430885,\n",
       "               75.87219,\n",
       "               69.731735,\n",
       "               67.76095,\n",
       "               78.82919,\n",
       "               70.34064,\n",
       "               84.13357,\n",
       "               80.4148,\n",
       "               78.60633,\n",
       "               80.70345,\n",
       "               87.050156,\n",
       "               75.1766,\n",
       "               94.63372,\n",
       "               88.90964,\n",
       "               90.35146,\n",
       "               76.32568,\n",
       "               83.984695,\n",
       "               90.30638,\n",
       "               85.590675,\n",
       "               98.42295,\n",
       "               88.73181,\n",
       "               65.700035,\n",
       "               81.111084,\n",
       "               71.602005,\n",
       "               65.44092,\n",
       "               69.71379,\n",
       "               61.20986,\n",
       "               60.787666,\n",
       "               53.718086,\n",
       "               54.844437,\n",
       "               73.60813,\n",
       "               70.34525,\n",
       "               65.68062,\n",
       "               58.76991,\n",
       "               69.45384,\n",
       "               69.37751,\n",
       "               67.96468,\n",
       "               63.080276,\n",
       "               73.205086,\n",
       "               70.077774,\n",
       "               62.070305,\n",
       "               59.8086,\n",
       "               57.466923,\n",
       "               79.58885,\n",
       "               87.469444,\n",
       "               83.97567,\n",
       "               74.68171,\n",
       "               74.06352,\n",
       "               74.560394,\n",
       "               72.634155,\n",
       "               71.786766,\n",
       "               67.57796,\n",
       "               76.11298,\n",
       "               79.73788,\n",
       "               68.04041,\n",
       "               74.235374,\n",
       "               70.08395,\n",
       "               71.102974,\n",
       "               76.1871,\n",
       "               70.999466,\n",
       "               64.71192,\n",
       "               72.04547,\n",
       "               63.670288,\n",
       "               60.50437,\n",
       "               58.09919,\n",
       "               70.67351,\n",
       "               63.105064,\n",
       "               72.16466,\n",
       "               72.48835,\n",
       "               78.24289,\n",
       "               63.93702,\n",
       "               68.95244,\n",
       "               66.86571,\n",
       "               69.23667,\n",
       "               72.71717,\n",
       "               65.60861,\n",
       "               78.491005,\n",
       "               91.102264,\n",
       "               89.040245,\n",
       "               84.699844,\n",
       "               81.06235,\n",
       "               81.96775,\n",
       "               91.72334,\n",
       "               79.3466,\n",
       "               66.983925,\n",
       "               77.57228,\n",
       "               81.11387,\n",
       "               77.11843,\n",
       "               69.195724,\n",
       "               63.01509,\n",
       "               59.655613,\n",
       "               51.515938,\n",
       "               52.075527,\n",
       "               48.081944,\n",
       "               45.997215,\n",
       "               41.6883,\n",
       "               41.26179,\n",
       "               47.67311,\n",
       "               51.41255,\n",
       "               47.307762,\n",
       "               41.51188,\n",
       "               46.259792,\n",
       "               39.381306,\n",
       "               49.348778,\n",
       "               48.913494,\n",
       "               46.340954,\n",
       "               60.671127,\n",
       "               56.686516,\n",
       "               56.299282,\n",
       "               61.83454,\n",
       "               56.80972,\n",
       "               65.64861,\n",
       "               60.11973,\n",
       "               57.159622,\n",
       "               59.192867,\n",
       "               57.915638,\n",
       "               65.163246,\n",
       "               67.07285,\n",
       "               66.072266,\n",
       "               61.065075,\n",
       "               64.58403,\n",
       "               61.12777,\n",
       "               61.724728,\n",
       "               54.741867,\n",
       "               58.283348,\n",
       "               66.31049,\n",
       "               65.13449,\n",
       "               74.0716,\n",
       "               68.579605,\n",
       "               74.30779,\n",
       "               67.79234,\n",
       "               76.486984,\n",
       "               77.93979,\n",
       "               86.295616,\n",
       "               80.37135,\n",
       "               90.790855,\n",
       "               92.52788,\n",
       "               92.26078,\n",
       "               73.21748,\n",
       "               78.07516,\n",
       "               83.60845,\n",
       "               97.888565,\n",
       "               91.06419,\n",
       "               83.57196,\n",
       "               95.40518,\n",
       "               94.549446,\n",
       "               79.25053,\n",
       "               88.26625,\n",
       "               88.433846,\n",
       "               79.350815,\n",
       "               78.03879,\n",
       "               80.28744,\n",
       "               74.28118,\n",
       "               68.335266,\n",
       "               68.2592,\n",
       "               70.628746,\n",
       "               61.791023,\n",
       "               64.898544,\n",
       "               63.5169,\n",
       "               63.980244,\n",
       "               62.738064,\n",
       "               61.988083,\n",
       "               71.10267,\n",
       "               71.082825,\n",
       "               75.111374,\n",
       "               76.03081,\n",
       "               75.1228,\n",
       "               83.58614,\n",
       "               76.487404,\n",
       "               86.174644,\n",
       "               76.362724,\n",
       "               67.81968,\n",
       "               66.32226,\n",
       "               69.61153,\n",
       "               70.39442,\n",
       "               68.9031,\n",
       "               65.990944,\n",
       "               71.86189,\n",
       "               68.15647,\n",
       "               80.77126,\n",
       "               87.72831,\n",
       "               84.75358,\n",
       "               73.51678,\n",
       "               73.699196,\n",
       "               68.839325,\n",
       "               69.37088,\n",
       "               70.99972,\n",
       "               67.97273,\n",
       "               63.276783,\n",
       "               67.78315,\n",
       "               60.793392,\n",
       "               57.71595,\n",
       "               67.25888,\n",
       "               62.563953,\n",
       "               62.44587,\n",
       "               63.939445,\n",
       "               56.552086,\n",
       "               56.60064,\n",
       "               55.284092,\n",
       "               62.272346,\n",
       "               60.72832,\n",
       "               59.03881,\n",
       "               58.83252,\n",
       "               67.86701,\n",
       "               66.10804,\n",
       "               65.750084,\n",
       "               64.74661,\n",
       "               53.971046,\n",
       "               69.41291,\n",
       "               63.81291,\n",
       "               69.49537,\n",
       "               68.684006,\n",
       "               70.40168,\n",
       "               72.3061,\n",
       "               68.46901,\n",
       "               81.03625,\n",
       "               68.55872,\n",
       "               77.46234,\n",
       "               65.5904,\n",
       "               60.62254,\n",
       "               62.2558,\n",
       "               76.45915,\n",
       "               80.81907,\n",
       "               73.154175,\n",
       "               93.90834,\n",
       "               166.4737,\n",
       "               192.3903,\n",
       "               198.87045,\n",
       "               191.8464,\n",
       "               201.67674,\n",
       "               194.97543,\n",
       "               177.94667,\n",
       "               190.7502,\n",
       "               170.96622,\n",
       "               166.97739,\n",
       "               185.932,\n",
       "               194.44638,\n",
       "               193.67831,\n",
       "               203.78896,\n",
       "               223.07272,\n",
       "               217.97925,\n",
       "               215.26642,\n",
       "               223.42795,\n",
       "               226.955,\n",
       "               226.48079,\n",
       "               232.44998,\n",
       "               210.42334,\n",
       "               200.86137,\n",
       "               219.74576,\n",
       "               202.91084,\n",
       "               209.34845,\n",
       "               216.1493,\n",
       "               211.77393,\n",
       "               209.84558,\n",
       "               213.28337,\n",
       "               218.93945,\n",
       "               209.4871,\n",
       "               208.47253,\n",
       "               221.07468,\n",
       "               210.79437,\n",
       "               191.40186,\n",
       "               212.74086,\n",
       "               190.5917,\n",
       "               185.31308,\n",
       "               174.69351,\n",
       "               161.53944,\n",
       "               167.87611,\n",
       "               171.58212,\n",
       "               167.54395,\n",
       "               155.55783,\n",
       "               162.35953,\n",
       "               169.56674,\n",
       "               166.2586,\n",
       "               162.37123,\n",
       "               174.04758,\n",
       "               189.01102,\n",
       "               184.59193,\n",
       "               186.4925,\n",
       "               175.3668,\n",
       "               181.60655,\n",
       "               186.06593,\n",
       "               184.16606,\n",
       "               185.481,\n",
       "               192.33133,\n",
       "               183.21649,\n",
       "               172.55025,\n",
       "               162.33891,\n",
       "               159.2594,\n",
       "               159.91898,\n",
       "               161.29176,\n",
       "               166.02563,\n",
       "               159.27306,\n",
       "               167.72067,\n",
       "               167.58624,\n",
       "               161.60576,\n",
       "               158.87392,\n",
       "               172.72012,\n",
       "               163.44675,\n",
       "               167.76416,\n",
       "               173.97108,\n",
       "               163.01389,\n",
       "               177.85551,\n",
       "               182.72374,\n",
       "               170.22177,\n",
       "               169.73122,\n",
       "               179.0996,\n",
       "               173.9008,\n",
       "               176.01137,\n",
       "               179.47823,\n",
       "               178.59789,\n",
       "               176.7391,\n",
       "               175.98317,\n",
       "               175.98753,\n",
       "               176.15962,\n",
       "               165.87292,\n",
       "               176.28372,\n",
       "               164.41153,\n",
       "               160.32718,\n",
       "               177.45018,\n",
       "               181.11667,\n",
       "               179.13113,\n",
       "               194.37848,\n",
       "               197.84398,\n",
       "               191.74023,\n",
       "               178.00658,\n",
       "               199.976,\n",
       "               197.1523,\n",
       "               200.0779,\n",
       "               224.30788,\n",
       "               193.66528,\n",
       "               211.05989,\n",
       "               195.27255,\n",
       "               198.74564,\n",
       "               207.3912,\n",
       "               199.2829,\n",
       "               189.46158,\n",
       "               195.64926,\n",
       "               188.57726,\n",
       "               192.04846,\n",
       "               192.04785,\n",
       "               201.02748,\n",
       "               192.06194,\n",
       "               199.9023,\n",
       "               184.42569,\n",
       "               200.46306,\n",
       "               201.89919,\n",
       "               189.32878,\n",
       "               189.98833,\n",
       "               199.5873,\n",
       "               199.19072,\n",
       "               203.47548,\n",
       "               177.74329,\n",
       "               170.0589,\n",
       "               168.6253,\n",
       "               154.84007,\n",
       "               137.9554,\n",
       "               158.1108,\n",
       "               160.04803,\n",
       "               164.03214,\n",
       "               174.01505,\n",
       "               180.74762,\n",
       "               180.77135,\n",
       "               189.36493,\n",
       "               199.84073,\n",
       "               180.01923,\n",
       "               176.44566,\n",
       "               173.87512,\n",
       "               174.66116,\n",
       "               183.86673,\n",
       "               178.56387,\n",
       "               175.28017,\n",
       "               171.75111,\n",
       "               174.00562,\n",
       "               181.34274,\n",
       "               170.6923,\n",
       "               182.79295,\n",
       "               175.94891,\n",
       "               182.21388,\n",
       "               179.69635,\n",
       "               190.95747,\n",
       "               166.00641,\n",
       "               172.62997,\n",
       "               155.29698,\n",
       "               155.81685,\n",
       "               138.57773,\n",
       "               124.121765,\n",
       "               140.28871,\n",
       "               142.37268,\n",
       "               144.78705,\n",
       "               146.8625,\n",
       "               147.29694,\n",
       "               144.56863,\n",
       "               159.01732,\n",
       "               155.05814,\n",
       "               131.32486,\n",
       "               111.06012,\n",
       "               134.80411,\n",
       "               166.48767,\n",
       "               153.65744,\n",
       "               155.86034,\n",
       "               164.89076,\n",
       "               164.49582,\n",
       "               181.58377,\n",
       "               177.75107,\n",
       "               176.14017,\n",
       "               172.15285,\n",
       "               169.93182,\n",
       "               178.26761,\n",
       "               166.61896,\n",
       "               168.55504,\n",
       "               166.70609,\n",
       "               185.94983,\n",
       "               196.04141,\n",
       "               171.18172,\n",
       "               172.88992,\n",
       "               172.99916,\n",
       "               178.59966,\n",
       "               166.45529,\n",
       "               169.60631,\n",
       "               169.66212,\n",
       "               149.28468,\n",
       "               158.6881,\n",
       "               163.68875,\n",
       "               164.38551,\n",
       "               144.59856,\n",
       "               151.80643,\n",
       "               150.23512,\n",
       "               161.40817,\n",
       "               180.07506,\n",
       "               179.85439,\n",
       "               183.43831,\n",
       "               175.28696,\n",
       "               180.97325,\n",
       "               183.09474,\n",
       "               194.81319,\n",
       "               200.37158,\n",
       "               188.04214,\n",
       "               185.4405,\n",
       "               183.62912,\n",
       "               167.36829,\n",
       "               168.17622,\n",
       "               173.65002,\n",
       "               176.93394,\n",
       "               176.53001,\n",
       "               173.72313,\n",
       "               176.94173,\n",
       "               180.34383,\n",
       "               190.25127,\n",
       "               178.99323,\n",
       "               166.61267,\n",
       "               181.60803,\n",
       "               175.14359,\n",
       "               168.1919,\n",
       "               174.6829,\n",
       "               161.50404,\n",
       "               163.6475,\n",
       "               167.16539,\n",
       "               137.72409,\n",
       "               151.76361,\n",
       "               149.53012,\n",
       "               145.68227,\n",
       "               137.50807,\n",
       "               140.3448,\n",
       "               169.45982,\n",
       "               167.52676,\n",
       "               155.94028,\n",
       "               168.7809,\n",
       "               179.03792,\n",
       "               157.74402,\n",
       "               162.49713,\n",
       "               173.12827,\n",
       "               180.29971,\n",
       "               164.11426,\n",
       "               173.34578,\n",
       "               182.71263,\n",
       "               166.21545,\n",
       "               181.90752,\n",
       "               167.87823,\n",
       "               178.52173,\n",
       "               162.45279,\n",
       "               155.12567,\n",
       "               163.55624,\n",
       "               171.78961,\n",
       "               164.46571,\n",
       "               166.90823,\n",
       "               159.27097,\n",
       "               163.2907,\n",
       "               150.8824,\n",
       "               158.0708,\n",
       "               150.7267,\n",
       "               168.15324,\n",
       "               150.99545,\n",
       "               144.38425,\n",
       "               130.77745,\n",
       "               132.28435,\n",
       "               150.02919,\n",
       "               131.07208,\n",
       "               129.50658,\n",
       "               139.10114,\n",
       "               136.05597,\n",
       "               130.24521,\n",
       "               135.24788,\n",
       "               124.35462,\n",
       "               124.652885,\n",
       "               125.391174,\n",
       "               103.80975,\n",
       "               114.82366,\n",
       "               131.61504,\n",
       "               129.08656,\n",
       "               125.826454,\n",
       "               138.16675,\n",
       "               138.32625,\n",
       "               128.54877,\n",
       "               143.4073,\n",
       "               145.74042,\n",
       "               151.19812,\n",
       "               153.46863,\n",
       "               169.02951,\n",
       "               153.52037,\n",
       "               147.37975,\n",
       "               147.65385,\n",
       "               163.84097,\n",
       "               154.7311,\n",
       "               150.4497,\n",
       "               157.86996,\n",
       "               157.76424,\n",
       "               150.00424,\n",
       "               163.5753,\n",
       "               149.80165,\n",
       "               147.52303,\n",
       "               146.7852,\n",
       "               157.50473,\n",
       "               156.3295,\n",
       "               145.85844,\n",
       "               159.06728,\n",
       "               142.24231,\n",
       "               154.65787,\n",
       "               157.49554,\n",
       "               156.53644,\n",
       "               163.31567,\n",
       "               153.61179,\n",
       "               159.91212,\n",
       "               175.312,\n",
       "               166.04451,\n",
       "               164.2965,\n",
       "               170.52989,\n",
       "               152.02315,\n",
       "               146.61028,\n",
       "               137.322,\n",
       "               149.37148,\n",
       "               158.38538,\n",
       "               137.67792,\n",
       "               146.0988,\n",
       "               137.58148,\n",
       "               130.20055,\n",
       "               128.02866,\n",
       "               126.998604,\n",
       "               134.09158,\n",
       "               125.20298,\n",
       "               122.5556,\n",
       "               130.09567,\n",
       "               125.10758,\n",
       "               134.01479,\n",
       "               132.11691,\n",
       "               141.60231,\n",
       "               143.32182,\n",
       "               142.22626,\n",
       "               138.53006,\n",
       "               149.95805,\n",
       "               150.95265,\n",
       "               152.3652,\n",
       "               158.18294,\n",
       "               153.50969,\n",
       "               163.34317,\n",
       "               165.03561,\n",
       "               169.78362,\n",
       "               182.57672,\n",
       "               173.1022,\n",
       "               176.68787,\n",
       "               170.57335,\n",
       "               168.93037,\n",
       "               169.53328,\n",
       "               175.60263,\n",
       "               178.81262,\n",
       "               169.32994,\n",
       "               167.96878,\n",
       "               178.82942,\n",
       "               160.47375,\n",
       "               161.93045,\n",
       "               165.4845,\n",
       "               166.56738,\n",
       "               153.7158,\n",
       "               133.56812,\n",
       "               146.90631,\n",
       "               137.10583,\n",
       "               134.79237,\n",
       "               124.6212,\n",
       "               114.217766,\n",
       "               116.47807,\n",
       "               100.16898,\n",
       "               108.82113,\n",
       "               100.14486,\n",
       "               86.518456,\n",
       "               91.023895,\n",
       "               89.75807,\n",
       "               97.03871,\n",
       "               83.213486,\n",
       "               87.55026,\n",
       "               97.040474,\n",
       "               107.02905,\n",
       "               115.74369,\n",
       "               103.50301,\n",
       "               104.58812,\n",
       "               104.03054,\n",
       "               112.82037,\n",
       "               110.137314,\n",
       "               123.86177,\n",
       "               109.2532,\n",
       "               114.790436,\n",
       "               101.97494,\n",
       "               117.501945,\n",
       "               104.588,\n",
       "               101.47081,\n",
       "               106.21468,\n",
       "               101.33551,\n",
       "               107.185356,\n",
       "               112.63185,\n",
       "               116.52612,\n",
       "               120.27895,\n",
       "               124.241776,\n",
       "               139.4405,\n",
       "               136.53273,\n",
       "               132.02917,\n",
       "               125.914505,\n",
       "               136.27013,\n",
       "               130.01042,\n",
       "               140.00525,\n",
       "               129.33432,\n",
       "               134.30164,\n",
       "               141.71461,\n",
       "               139.94295,\n",
       "               135.5327,\n",
       "               144.08557,\n",
       "               142.38332,\n",
       "               126.21795,\n",
       "               129.19806,\n",
       "               134.85818,\n",
       "               144.47935,\n",
       "               133.13087,\n",
       "               134.98325,\n",
       "               129.19185,\n",
       "               133.87103,\n",
       "               123.718185,\n",
       "               119.74151,\n",
       "               117.188446,\n",
       "               123.12992,\n",
       "               124.68522,\n",
       "               116.40145,\n",
       "               107.37678,\n",
       "               97.7883,\n",
       "               105.8533,\n",
       "               110.72177,\n",
       "               119.24589,\n",
       "               118.675316,\n",
       "               111.09769,\n",
       "               109.40631,\n",
       "               110.6859,\n",
       "               115.143875,\n",
       "               115.02763,\n",
       "               113.57777,\n",
       "               132.12973,\n",
       "               117.35506,\n",
       "               122.685745,\n",
       "               133.60347,\n",
       "               145.2644,\n",
       "               148.56354,\n",
       "               147.7206,\n",
       "               150.13264,\n",
       "               149.80313,\n",
       "               151.71542,\n",
       "               151.55702,\n",
       "               145.28162,\n",
       "               149.99359,\n",
       "               150.60617,\n",
       "               148.0935,\n",
       "               147.9465,\n",
       "               144.92435,\n",
       "               139.97717,\n",
       "               135.28056,\n",
       "               135.96738,\n",
       "               138.42276,\n",
       "               132.11551,\n",
       "               136.20557,\n",
       "               129.87393,\n",
       "               132.51509,\n",
       "               135.36403,\n",
       "               140.85104,\n",
       "               142.70055,\n",
       "               133.19176,\n",
       "               135.77515,\n",
       "               131.63303,\n",
       "               131.21301,\n",
       "               131.36975,\n",
       "               129.88748,\n",
       "               140.01312,\n",
       "               136.87457,\n",
       "               140.34282,\n",
       "               142.94403,\n",
       "               134.95013,\n",
       "               132.98978,\n",
       "               130.57393,\n",
       "               133.42812,\n",
       "               141.12178,\n",
       "               142.01399,\n",
       "               146.01733,\n",
       "               150.55568,\n",
       "               148.10779,\n",
       "               149.57158,\n",
       "               ...]}})"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "States_track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tracking Convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZAAAAEWCAYAAABIVsEJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fca9eb8b2b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(episodes,rewards_per_episode)\n",
    "plt.title('Reward per episode')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q value convergence for state action pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAj8AAAEKCAYAAAD98zS0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngeLJj5mZmQ2U/wJW/0EiZgnjFQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fca9f61e978>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(0, figsize=(16, 7))\n",
    "plt.subplot(241)\n",
    "plt.title('state=(1-1-1) action=(0, 1)')\n",
    "plt.plot(np.arange(len(States_track[(1,6,4)][(1, 4)])),\n",
    "         np.asarray(States_track[(1,6,4)][(1, 4)]))\n",
    "plt.subplot(242)\n",
    "plt.title('state=(1-1-1) action=(0, 1)')\n",
    "plt.plot(np.arange(len(States_track[(1,11,5)][(1, 4)])),\n",
    "         np.asarray(States_track[(1,11,5)][(1, 4)]))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Epsilon-decay sample function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Try building a similar epsilon-decay function for your model.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = np.arange(0,10000)\n",
    "epsilon = []\n",
    "for i in range(0,10000):\n",
    "    epsilon.append(0 + (1 - 0) * np.exp(-0.0009*i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(time, epsilon)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "epsilon2=[]\n",
    "for i in range(0,10000):\n",
    "    if i==0:\n",
    "        epsilon2.append(1)\n",
    "    else:\n",
    "        epsilon2.append(0.997 * epsilon2[i-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon2=[]\n",
    "for i in range(0,10000):\n",
    "    if i==0:\n",
    "        epsilon2.append(1)\n",
    "    else:\n",
    "        epsilon2.append(epsilon2[i-1] * 0.999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAFK4i4gE0P8Hjtf29Ffx4IsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fcac4647eb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(time, epsilon2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
